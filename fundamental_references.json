{"Agrawal, Manish, Piyush Kumar Shukla, Rajit Nair, Anand Nayyar, and\nMehedi Masud. 2022. \u201cStock Prediction Based on Technical Indicators\nUsing Deep Learning Model.\u201d Computers, Materials & Continua 70 (1).\n\nBox, George EP, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung.\n2015. Time Series Analysis: Forecasting and Control. John Wiley & Sons.\n\nCho, Kyunghyun, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. \u201cLearning\nPhrase Representations Using RNN Encoder-Decoder for Statistical Machine\nTranslation.\u201d arXiv Preprint arXiv:1406.1078.\n\nDu, Dazhao, Bing Su, and Zhewei Wei. 2023. \u201cPreformer: Predictive\nTransformer with Multi-Scale Segment-Wise Correlations for Long-Term\nTime Series Forecasting.\u201d In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 1\u20135.\nIEEE.\n\nEkambaram, Vijay, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and\nJayant Kalagnanam. 2023. \u201cTsmixer: Lightweight Mlp-Mixer Model for\nMultivariate Time Series Forecasting.\u201d In Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining, 459\u201369.\n\nHan, Lu, Han-Jia Ye, and De-Chuan Zhan. 2024. \u201cThe Capacity and\nRobustness Trade-Off: Revisiting the Channel Independent Strategy for\nMultivariate Time Series Forecasting.\u201d IEEE Transactions on Knowledge\nand Data Engineering.\n\nHendrycks, Dan, and Kevin Gimpel. 2016. \u201cGaussian Error Linear Units\n(Gelus).\u201d arXiv Preprint arXiv:1606.08415.\n\nHochreiter, S. 1997. \u201cLong Short-Term Memory.\u201d Neural Computation\nMIT-Press.\n\nHornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. \u201cMultilayer\nFeedforward Networks Are Universal Approximators.\u201d Neural Networks 2\n(5): 359\u201366.\n\nLi, Shiyang, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang\nWang, and Xifeng Yan. 2019. \u201cEnhancing the Locality and Breaking the\nMemory Bottleneck of Transformer on Time Series Forecasting.\u201d Advances\nin Neural Information Processing Systems 32.\n\nLiu, Shizhan, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu,\nand Schahram Dustdar. 2022. \u201cPyraformer: Low-Complexity Pyramidal\nAttention for Long-Range Time Series Modeling and Forecasting.\u201d In #\nPLACEHOLDER_PARENT_METADATA_VALUE#.\n\nLiu, Yong, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and\nMingsheng Long. 2023. \u201cItransformer: Inverted Transformers Are Effective\nfor Time Series Forecasting.\u201d arXiv Preprint arXiv:2310.06625.\n\nLiu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen\nLin, and Baining Guo. 2021. \u201cSwin Transformer: Hierarchical Vision\nTransformer Using Shifted Windows.\u201d In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 10012\u201322.\n\nNie, Tong, Guoyang Qin, Wei Ma, Yuewen Mei, and Jian Sun. 2024.\n\u201cImputeFormer: Low Rankness-Induced Transformers for Generalizable\nSpatiotemporal Imputation.\u201d In Proceedings of the 30th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, 2260\u201371.\n\nNie, Yuqi, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.\n2022. \u201cA Time Series Is Worth 64 Words: Long-Term Forecasting with\nTransformers.\u201d arXiv Preprint arXiv:2211.14730.\n\nVaswani, A. 2017. \u201cAttention Is All You Need.\u201d Advances in Neural\nInformation Processing Systems.\n\nVeli\u010dkovi\u0107, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero,\nPietro Lio, and Yoshua Bengio. 2017. \u201cGraph Attention Networks.\u201d arXiv\nPreprint arXiv:1710.10903.\n\nWu, Haixu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021.\n\u201cAutoformer: Decomposition Transformers with Auto-Correlation for\nLong-Term Series Forecasting.\u201d Advances in Neural Information Processing\nSystems 34: 22419\u201330.\n\nXiang, Sheng, Dawei Cheng, Chencheng Shang, Ying Zhang, and Yuqi Liang.\n2022. \u201cTemporal and Heterogeneous Graph Neural Network for Financial\nTime Series Prediction.\u201d In Proceedings of the 31st ACM International\nConference on Information & Knowledge Management, 3584\u201393.\n\nYu, Bing, Haoteng Yin, and Zhanxing Zhu. 2017. \u201cSpatio-Temporal Graph\nConvolutional Networks: A Deep Learning Framework for Traffic\nForecasting.\u201d arXiv Preprint arXiv:1709.04875.\n\nZeng, Ailing, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. \u201cAre\nTransformers Effective for Time Series Forecasting?\u201d In Proceedings of\nthe AAAI Conference on Artificial Intelligence, 37:11121\u201328.\n\nZhang, Yunhao, and Junchi Yan. 2023. \u201cCrossformer: Transformer Utilizing\nCross-Dimension Dependency for Multivariate Time Series Forecasting.\u201d In\nThe Eleventh International Conference on Learning Representations.\n\nZhou, Haoyi, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui\nXiong, and Wancai Zhang. 2021. \u201cInformer: Beyond Efficient Transformer\nfor Long Sequence Time-Series Forecasting.\u201d In Proceedings of the AAAI\nConference on Artificial Intelligence, 35:11106\u201315.\n\nZhou, Tian, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.\n2022. \u201cFedformer: Frequency Enhanced Decomposed Transformer for\nLong-Term Series Forecasting.\u201d In International Conference on Machine\nLearning, 27268\u201386. PMLR.": null}