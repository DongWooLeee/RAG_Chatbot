{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My First Own AI assistant using OLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 주요 라이브러리 및 모듈\n",
    "\n",
    "    - langchain 관련 모듈을 다수 임포트하여 사용 (프롬프트 템플릿, 검색기, 임베딩, LLM 등)\n",
    "    - sys 및 os, pathlib을 사용하여 파일 경로 및 환경 설정을 처리\n",
    "    - Config 클래스를 통해 다양한 설정을 관리, 보다 쉬운 프로젝트 관리를 도움\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import sys\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain_community.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#import PATH\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    SEED = 42\n",
    "    ALLOWED_FILE_EXTENSIONS = set([\".pdf\",\".md\",\".txt\",\".docx\"]) # 지원하는 파일 확장자\n",
    "    \n",
    "    class Model:\n",
    "        NAME = \"deepseek-r1:14b\" # 기본 LLM으로 deepseek-r1:14b 사용\n",
    "        TEMPERATURE = 0.6 # 응답 창의성 및 다양성 조절 파라미터\n",
    "        \n",
    "    class Preprocessing:\n",
    "        CHUNK_SIZE = 2048 # 문서를 2048 길이로 청킹 (CHUNK_SIZE = 2048)\n",
    "        CHUNK_OVERLAP = 128 # Chunk들이 겹치도록 --> Overlap\n",
    "        EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\" # Embedding 모델은 BAAI/bge-small-en-v1.5 사용\n",
    "        RERANKER = \"ms-marco-MiniLM-L-12-v2\" # Reranker 모델은 ms-marco-MiniLM-L-12-v2 사용\n",
    "        LLM = \"llama3.2\" # LLM 모델은 llama3.2 사용. 여기서의 LLM은 전처리 단계에서의 LLM이고 사용할 Chatbot에서의 LLM은 다른 걸 사용할 수 있다.\n",
    "        CONTEXTUALIZE_CHUNKS = True # \n",
    "        N_SEMANTIC_CHUNKS = 5 #  의미적 검색 시 반환할 최대 청크 수\n",
    "        N_BM25_RESULTS = 5 # BM25 기반 검색 시 반환할 최대 결과 개수\n",
    "        \n",
    "    class Chatbot:\n",
    "        N_CONTEXT_RESULTS = 3 # 챗봇이 제공하는 문맥 수 (N_CONTEXT_RESULTS = 3)\n",
    "        \n",
    "    class Path:\n",
    "        try:\n",
    "            APP_HOME = Path(os.getenv(\"APP_HOME\", Path(__file__).parent.parent))\n",
    "        except NameError:\n",
    "            APP_HOME = Path(os.getenv(\"APP_HOME\", Path.cwd().parent))  # 현재 디렉토리 기준으로 설정\n",
    "        DATA_DIR = APP_HOME / \"data/pdf\" # 데이터 디렉토리를 APP_HOME/data/pdf로 설정 (DATA_DIR)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def config_logging():\n",
    "    config = {\n",
    "        \n",
    "        \"handlers\": [\n",
    "            {\n",
    "            \"sink\": sys.stdout,\n",
    "            \"colorize\": True,\n",
    "            \"format\": \"<green{time:YYYY-MM-DD HH:mm:ss}> <level>{level: <8}</level> <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        \n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Utils 정의.\n",
    "\n",
    "    - TEXT_FILE_EXTENSION, MD_FILE_EXTENSION, PDF_FILE_EXTENSION 변수를 선언하여 지원하는 파일 유형을 정의.\n",
    "    - File 데이터 클래스를 정의하여 name과 content 필드를 저장.\n",
    "    - extract_pdf_content() 함수를 사용하여 PDF 문서에서 텍스트를 추출.\n",
    "    - load_uploaded_file() 함수는 Streamlit의 UploadedFile을 받아 파일 확장자에 따라 적절히 처리.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from pypdfium2 import PdfDocument\n",
    "from streamlit.runtime.uploaded_file_manager import UploadedFile\n",
    "\n",
    "\n",
    "TEXT_FILE_EXTENSION = \".txt\"\n",
    "MD_FILE_EXTENSION = \".md\"\n",
    "PDF_FILE_EXTENSION = \".pdf\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class File:\n",
    "    name: str\n",
    "    extension: str\n",
    "    content: str\n",
    "    \n",
    "\n",
    "def extract_pdf_content(data: bytes) -> str:\n",
    "    \"\"\"PDF 문서에서 텍스트를 추출하는 함수\"\"\"\n",
    "    pdf = PdfDocument(data)\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text_page = page.get_textpage()\n",
    "        text += f\"{text_page.get_text()}\\n\"  # 페이지별 전체 텍스트 추출\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_uploaded_file(uploaded_file: UploadedFile) -> File:\n",
    "    \"\"\"업로드된 파일을 처리하여 File 객체로 변환하는 함수\"\"\"\n",
    "    file_extension = Path(uploaded_file.name).suffix.lower()  # 확장자 소문자로 변환\n",
    "    if file_extension not in Config.ALLOWED_FILE_EXTENSIONS:\n",
    "        raise ValueError(f\"Invalid file extension. Allowed extensions are {Config.ALLOWED_FILE_EXTENSIONS}\")\n",
    "    \n",
    "    content = \"\"\n",
    "    if file_extension == PDF_FILE_EXTENSION:\n",
    "        content = extract_pdf_content(uploaded_file.getvalue())\n",
    "    else:\n",
    "        try:\n",
    "            content = uploaded_file.getvalue().decode(\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            content = uploaded_file.getvalue().decode(\"latin1\")  # 예외 처리 추가\n",
    "    \n",
    "    return File(name=uploaded_file.name, extension=file_extension, content=content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RAG 기반 문서 검색 및 컨텍스트 생성 시스템 구현\n",
    "    - 주요 기능 요약\n",
    "        - ✅ 문서 청킹 및 컨텍스트 생성\n",
    "        - ✅ LLM 기반 요약 및 컨텍스트 추가\n",
    "        - ✅ Semantic + BM25 검색 결합 (Ensemble Retriever)\n",
    "        - ✅ 리랭킹을 통한 최종 검색 결과 압축\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다.\"),\n",
    "        (\"human\", \"반가워요!\"),\n",
    "        (\"ai\", \"안녕하세요! 무엇을 도와드릴까요?\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 테디 입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='반가워요!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='안녕하세요! 무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='당신의 이름은 무엇입니까?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 챗 message 를 생성합니다.\n",
    "messages = chat_template.format_messages(\n",
    "    name=\"테디\", user_input=\"당신의 이름은 무엇입니까?\"\n",
    ")\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 테디 입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='반가워요!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='안녕하세요! 무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='당신의 이름은 무엇입니까?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT_PROMPT: 청크별 컨텍스트 생성 프롬프트\n",
    "\n",
    "CONTEXT_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \n",
    "    \"\"\"\n",
    "    You're an expert in document analysis. Your task is to provide brief, relevant context for a chunk of text.\n",
    "    \n",
    "    Here is the document:\n",
    "    \n",
    "    <document>\n",
    "    {document}\n",
    "    </document>\n",
    "    \n",
    "    Here is the chunk we want to situate within the whole document:\n",
    "    \n",
    "    <chunk>\n",
    "    {chunk}\n",
    "    </chunk>\n",
    "    \n",
    "    Provide a concise context (2-3 sentences) for this chunk, considering the following guidelines:\n",
    "    1. Identify the main topic or concept discussed in the chunk.\n",
    "    2. Mention any relevant information or comparisons from the broader document context.\n",
    "    3. If applicable, note how this information relates to the overall theme or purpose of the document.\n",
    "    4. Include any key figures, dates, or percentages that provide important context.\n",
    "    5. Do not use phrases like \"This chunk discusses...\" or \"The chunk is about...\". Instead, directly state that information.\n",
    "    \n",
    "    Please give a short succint context to situate this chunk within the overall document for the purpose of summarization.\n",
    "    \n",
    "    Context:\n",
    "    \"\"\".strip()\n",
    "    \n",
    ")\n",
    "\n",
    "# RecursiveCharacterTextSplitter: 재귀적 문자 기반 텍스트 분할기. 문서를 일정한 크기(chunk_size)로 나누되, 청크 간 chunk_overlap 만큼 중첩을 유지.\n",
    "# RecursiveCharacterTextSplitter는 문장 구조를 유지하면서 효율적으로 청킹.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=Config.Preprocessing.CHUNK_SIZE,\n",
    "    chunk_overlap=Config.Preprocessing.CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "# ChatOllama 객체를 생성하여 Open-Source LLM(llama3.2)을 활용. ChatOllama는 Chatbot을 위한 Ollama 모델을 사용하는 객체이며 temperature=0.6이 적절한지 테스트 필요 (정확성과 창의성 균형 조정).\n",
    "def create_llm() -> ChatOllama:\n",
    "    return ChatOllama(model = Config.Preprocessing.LLM, temperature=Config.Model.TEMPERATURE, seed=Config.SEED, keep_alive = -1)\n",
    "\n",
    "\n",
    "# FastEmbedEmbeddings를 사용하여 문서의 임베딩 벡터 생성. Embedding Mo   \n",
    "def create_embeddings() -> FastEmbedEmbeddings:\n",
    "    return FastEmbedEmbeddings(model=Config.Preprocessing.EMBEDDING_MODEL, seed=Config.SEED)\n",
    "\n",
    "# FlashrankRerank를 사용하여 검색된 문서를 랭킹 정렬.\n",
    "def create_reranker() -> FlashrankRerank:\n",
    "    return FlashrankRerank(model=Config.Preprocessing.RERANKER, seed=Config.SEED,\n",
    "                           top_k=Config.Chatbot.N_CONTEXT_RESULTS)\n",
    "    \n",
    "# (4) 문서 청크 생성 및 컨텍스트 추가 - CONTEXT_PROMPT를 이용해 LLM이 청크의 문맥을 파악하여 컨텍스트를 생성.\n",
    "def _generate_context(llm:ChatOllama, document:str, chunk:str)->str:\n",
    "    messages = CONTEXT_PROMPT.format_messages(document=document, chunk=chunk)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# _generate_context()를 활용해 각 청크에 문맥 정보를 추가.\n",
    "# Config.Preprocessing.CONTEXTUALIZE_CHUNKS 설정에 따라 컨텍스트 생략 가능.\n",
    "def _create_chunks(document:Document)->List[Document]: # 여기서의 Document는 langchain_core.documents.Document 객체임\n",
    "    chunks = text_splitter.split_documents([document])\n",
    "    if not Config.Preprocessing.CONTEXTUALIZE_CHUNKS:\n",
    "        return chunks\n",
    "    llm = create_llm()\n",
    "    contextual_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        context = _generate_context(llm, document.page_content, chunk.page_content)\n",
    "        chunk_with_context = f\"{context}\\n\\n{chunk.page_content}\"\n",
    "        contextual_chunks.append(Document(page_content=chunk_with_context,\n",
    "                                          metadata=chunk.metadata))\n",
    "    return contextual_chunks\n",
    "\n",
    "# (5) 파일 인제스트 및 검색기 생성\n",
    "\n",
    "def ingest_files(files:List[File]) -> BaseRetriever:\n",
    "    documents = [Document(file.content, metadata={'source':file.name}) for file in files] # 파일을 받아 문서(Document) 리스트로 변환.\n",
    "    chunks = []\n",
    "    for document in documents:\n",
    "        chunks.extend(_create_chunks(document))\n",
    "    \n",
    "    # BM25Retriever (키워드 기반)와 InMemoryVectorStore (임베딩 기반)로 검색기 생성.\n",
    "    semantic_retriever = InMemoryVectorStore.from_documents(\n",
    "        chunks, create_embeddings().as_retriever(search_kwargs={\"k\": Config.Preprocessing.N_SEMANTIC_RESULTS}))\n",
    "    \n",
    "    bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "    bm25_retriever.k = Config.Preprocessing.N_BM25_RESULTS\n",
    "    # 두 개의 검색기를 가중치를 조정해 EnsembleRetriever로 결합.\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever([semantic_retriever, bm25_retriever],\n",
    "                                           weights = [0.6, 0.4])\n",
    "    \n",
    "    # ContextualCompressionRetriever를 활용해 최종 결과 정제.\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor = create_reranker(), base_retriever = ensemble_retriever\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LangChain 기반 RAG 챗봇의 구조\n",
    "    - 주요 기능\n",
    "        - ✅ 문서 검색 + LLM 답변 생성 (RAG 구조)\n",
    "        - ✅ LangGraph 기반 대화 상태 관리\n",
    "        - ✅ 답변을 스트리밍하여 순차적으로 반환\n",
    "        - ✅ BM25 + Semantic 검색 결합\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from typing import Iterable,TypedDict\n",
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "# 시스템 프롬프트 - 사용자의 문서에서 정보를 검색하여 대화할 수 있도록 유도.\n",
    "\n",
    "SYSTEM_PROMPT=\"\"\"\n",
    "    You're having a conversation with an user about excerpts of their files.\n",
    "    Try to be helpful and answer their questions.\n",
    "    \n",
    "    \n",
    "    If you do not know the answer, say that you do not know and try to clarify the question.\n",
    "    \"\"\".strip()\n",
    "\n",
    "# 메시지 템플릿 - context에 검색된 문서를 포함하고, question을 입력받아 답변 생성.\n",
    "PROMPT = \"\"\"\n",
    "Here's the infornation you have about the excerpts of the files:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "One file can have multiple excerpts.\n",
    "\n",
    "Please, respond to the query below\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "FILE_TEMPLATE = \"\"\"\n",
    "<file>\n",
    "    <name>{name}</name>\n",
    "    <content>{content}</content>\n",
    "</file>\n",
    "\"\"\".strip()\n",
    "\n",
    "# 전체 대화 흐름 템플릿 - 기존 대화 기록 (chat_history) 포함하여 일관된 대화를 유지.\n",
    "PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "(\n",
    "    \"system\",\n",
    "    SYSTEM_PROMPT,\n",
    "),\n",
    "\n",
    "MessagesPlaceholder(variable_name = \"chat_history\"),\n",
    "(\"human\", PROMPT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 역할 및 데이터 클래스 정의 \n",
    "class Role(Enum):\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    \n",
    "@dataclass\n",
    "class Message:\n",
    "    # Message 객체는 '대화의 한 단위'를 저장.\n",
    "    role:Role\n",
    "    context:str\n",
    "    \n",
    "@dataclass\n",
    "class ChunkEvent:\n",
    "    # 이벤트 데이터 클래스 (스트리밍 응답) - 스트리밍된 답변의 일부 청크(부분)를 저장.\n",
    "    content: str\n",
    "    \n",
    "@dataclass\n",
    "class SourcesEvent:\n",
    "    # 검색된 문서 목록을 저장.\n",
    "    sources:List[Document]\n",
    "    \n",
    "@dataclass\n",
    "class FinalAnswerEvent:\n",
    "    #최종적으로 생성된 LLM 답변을 저장.\n",
    "    content:str\n",
    "\n",
    "class State(TypedDict):\n",
    "    # 현재 챗봇의 상태를 저장하는 TypedDict.\n",
    "    # 대화 이력(chat_history), 검색된 문서(context)를 포함.\n",
    "    question:str\n",
    "    chat_history:List[BaseMessage]\n",
    "    context:List[Document]\n",
    "    answer:str\n",
    "    \n",
    "\n",
    "def _remove_thinking_from_message(message:str)->str:\n",
    "    close_tag = \"</think>\"\n",
    "    tag_length = len(close_tag)\n",
    "    \n",
    "    \n",
    "    return message[message.find(close_tag) + tag_length :].strip()\n",
    "\n",
    "def create_history(welcome_message:Message)->List[Message]:\n",
    "    return [welcome_message]\n",
    "\n",
    "\n",
    "# **문서 검색기(retriever)를 생성하여 파일 검색 가능하도록 설정.\n",
    "# LLM(ChatOllama)을 초기화하여 답변 생성.\n",
    "# LangGraph 기반 workflow를 설정하여 대화 흐름을 관리.\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, files:List[File]):\n",
    "        self.files = files\n",
    "        self.retriever = ingest_files(files)\n",
    "        self.llm = ChatOllama(\n",
    "            model = Config.Model.Name,\n",
    "            temperataure = Config.Model.Temperature,\n",
    "            seed = Config.SEED,\n",
    "            keep_alive = -1,\n",
    "            verbose = False\n",
    "        )\n",
    "        self.workflow = self._create_workflow()\n",
    "        \n",
    "        \n",
    "    def _format_docs(self, docs:List[Document])->str:\n",
    "        #  문서 포맷팅\n",
    "        return \"\\n\\n\".join(\n",
    "            FILE_TEMPLATE.format(name=doc.metadata[\"source\"], content=doc.page_content) # format 메소드를 활용해서 변수에 값을 넣어줌\n",
    "            for doc in docs\n",
    "        )\n",
    "\n",
    "\n",
    "    def _retrieve(self, state:State): # 문서 검색\n",
    "        context = self.retriever.invoke(state[\"question\"])\n",
    "        return {\"context\":context}\n",
    "    \n",
    "    def _generate(self, state:State): # LLM을 사용해 답변 생성\n",
    "        messages = PROMPT_TEMPLATE.invoke(\n",
    "            {\n",
    "                \"question\": state[\"question\"],\n",
    "                \"context\": self._format_docs(state[\"context\"]),\n",
    "                \"chat_history\": state[\"chat_history\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        answer = self.llm.invoke(messages)\n",
    "        return {\"answer\":answer}\n",
    "    \n",
    "    \n",
    "    def _create_workflow(self) -> CompiledStateGraph: # \n",
    "        graph_builder = StateGraph(State).add_sequence([self._retrieve, self._generate])\n",
    "        graph_builder.add_edge(START, \"_retrieve\")\n",
    "        return graph_builder.compile()\n",
    "\n",
    "    \n",
    "\n",
    "    def _ask_model(self, prompt: str, chat_history: List[Message]) -> Iterable[Union[SourcesEvent, ChunkEvent, FinalAnswerEvent]]: # 스트리밍 방식 답변 생성\n",
    "        history = [\n",
    "            AIMessage(m.content) if m.role == Role.ASSISTANT else HumanMessage(m.content)\n",
    "            for m in chat_history\n",
    "        ]\n",
    "        payload = {\"question\":prompt, \"chat_history\":history}\n",
    "        \n",
    "        config = {\n",
    "            \"configurable\": {\"thread_id\": 42},\n",
    "        }\n",
    "        for event_type, event_data in self.workflow.stream(\n",
    "            payload,\n",
    "            config = config,\n",
    "            stream_mode=  [\"updates\",\"messages\"],\n",
    "        ):\n",
    "            \n",
    "            if event_type == \"messages\":\n",
    "                chunk, _ = event_data\n",
    "                yield ChunkEvent(chunk.content)\n",
    "                \n",
    "            if event_type == \"updates\":\n",
    "                if \"_retrieve\" in event_data:\n",
    "                    documents = event_data[\"_retrieve\"][\"context\"]\n",
    "                    yield SourcesEvent(sources=documents)\n",
    "                if \"_generate\" in event_data:\n",
    "                    answer = event_data[\"_generate\"][\"answer\"]\n",
    "                    yield FinalAnswerEvent(content=answer.content)\n",
    "                \n",
    "    \n",
    "    \n",
    "    def ask(self, prompt:str, chat_history:List[Message])->Iterable[Union[SourcesEvent, ChunkEvent, FinalAnswerEvent]]:  \n",
    "        for event in self._ask_model(prompt, chat_history):\n",
    "            yield event\n",
    "            if isinstance(event, FinalAnswerEvent):\n",
    "                response = _remove_thinking_from_message(\"\".join(event.content))\n",
    "                chat_history.append(Message(role=Role.USER,content = prompt))\n",
    "                chat_history.append(Message(role=Role.ASSISTANT, context=response))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
