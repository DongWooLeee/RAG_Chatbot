{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import bibtexparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "def parse_bibtex(bibtex_file):\n",
    "    with open(bibtex_file, \"r\") as file:\n",
    "        bib_database = bibtexparser.load(file)\n",
    "\n",
    "    papers = []\n",
    "    for entry in bib_database.entries:\n",
    "        if \"title\" in entry:\n",
    "            papers.append(entry[\"title\"])\n",
    "\n",
    "    with open(\"/workspace/dongwoo/chatbot_project/data/titles.json\", \"w\") as f:\n",
    "        json.dump(papers, f, indent=4)\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "bib_file_path = \"/workspace/dongwoo/chatbot_project/references.bib\"\n",
    "\n",
    "paper_titles = parse_bibtex(bib_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting',\n",
       " 'Are transformers effective for time series forecasting?',\n",
       " 'Time series analysis: forecasting and control',\n",
       " 'Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting',\n",
       " 'Stock prediction based on technical indicators using deep learning model.',\n",
       " 'Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting',\n",
       " 'Informer: Beyond efficient transformer for long sequence time-series forecasting',\n",
       " 'Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting',\n",
       " 'Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting',\n",
       " 'Swin transformer: Hierarchical vision transformer using shifted windows',\n",
       " 'Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting',\n",
       " 'Preformer: predictive transformer with multi-scale segment-wise correlations for long-term time series forecasting',\n",
       " 'A time series is worth 64 words: Long-term forecasting with transformers',\n",
       " 'itransformer: Inverted transformers are effective for time series forecasting',\n",
       " 'The capacity and robustness trade-off: Revisiting the channel independent strategy for multivariate time series forecasting',\n",
       " 'Temporal and heterogeneous graph neural network for financial time series prediction',\n",
       " 'Multilayer feedforward networks are universal approximators',\n",
       " 'Long Short-term Memory',\n",
       " 'Attention is all you need',\n",
       " 'Gaussian error linear units (gelus)',\n",
       " 'Graph attention networks',\n",
       " 'ImputeFormer: Low rankness-induced transformers for generalizable spatiotemporal imputation',\n",
       " 'Learning phrase representations using RNN encoder-decoder for statistical machine translation',\n",
       " 'Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting',\n",
       " 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting',\n",
       " 'Correlation networks: Interdisciplinary approaches beyond thresholding',\n",
       " 'Connecting the dots: Multivariate time series forecasting with graph neural networks',\n",
       " 'Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting',\n",
       " 'Bimodal characteristic returns and predictability enhancement via machine learning',\n",
       " 'A Machine Learning Approach: Enhancing the Predictive Performance of Pharmaceutical Stock Price Movement during COVID',\n",
       " 'Forecasting stock prices from the limit order book using convolutional neural networks',\n",
       " 'Stock price prediction using attention-based multi-input LSTM',\n",
       " 'Enhancing stock movement prediction with adversarial training',\n",
       " 'A stock closing price prediction model based on CNN-BiSLSTM',\n",
       " 'Modeling the stock relation with graph network for overnight stock movement prediction',\n",
       " 'Stock selection via spatiotemporal hypergraph attention network: A learning to rank approach',\n",
       " 'Efficient integration of multi-order dynamics and internal dynamics in stock movement prediction',\n",
       " 'Mlp-mixer: An all-mlp architecture for vision',\n",
       " 'A decoder-only foundation model for time-series forecasting',\n",
       " 'A hybrid model integrating deep learning with investor sentiment analysis for stock price prediction',\n",
       " 'Decision support system for stock trading using multiple indicators decision tree',\n",
       " 'Enhanced stock price forecasting through a regularized ensemble framework with graph convolutional networks',\n",
       " 'Reversible instance normalization for accurate time-series forecasting against distribution shift',\n",
       " 'The behavior of stock-market prices',\n",
       " 'Stock return predictability and variance risk premia: Statistical inference and international evidence',\n",
       " 'Efficiently inefficient: how smart money invests and market prices are determined',\n",
       " 'Temporal relational ranking for stock prediction',\n",
       " 'A LSTM-based method for stock returns prediction: A case study of China stock market',\n",
       " 'Risk guarantee prediction in networked-loans',\n",
       " '(Re-) Imag (in) ing price trends',\n",
       " 'Stock price prediction using the ARIMA model']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "PDF_DIR = \"data/pdf\"\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(\"data/titles.json\", \"r\") as f:\n",
    "    titles = json.load(f)\n",
    "    \n",
    "    \n",
    "def download_paper_from_arxiv(title):\n",
    "    \"\"\"Arxiv에서 논문을 검색하여 PDF를 다운로드\"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=title,\n",
    "        max_results=1,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    \n",
    "    for result in search.results():\n",
    "        pdf_url = result.pdf_url\n",
    "        pdf_path = os.path.join(PDF_DIR, f\"{result.entry_id.split('/')[-1]}.pdf\")\n",
    "\n",
    "        # PDF 다운로드\n",
    "        response = requests.get(pdf_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(pdf_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return pdf_path\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def download_all_papers():\n",
    "    \"\"\"titles.json의 제목을 기반으로 논문을 다운로드\"\"\"\n",
    "    \n",
    "    # ✅ titles를 함수 내부에서 로드하도록 변경\n",
    "    with open(\"data/titles.json\", \"r\") as f:\n",
    "        titles = json.load(f)\n",
    "\n",
    "    downloaded_papers = []\n",
    "    missing_titles = []\n",
    "\n",
    "    for title in tqdm(titles, desc=\"Downloading Papers\"):\n",
    "        pdf_path = download_paper_from_arxiv(title)\n",
    "        if pdf_path:\n",
    "            downloaded_papers.append({\"title\": title, \"pdf_path\": pdf_path})\n",
    "        else:\n",
    "            missing_titles.append(title)\n",
    "\n",
    "    # 다운로드된 논문 목록 저장\n",
    "    with open(\"data/downloaded_papers.json\", \"w\") as f:\n",
    "        json.dump(downloaded_papers, f, indent=4)\n",
    "\n",
    "    # Arxiv에서 찾을 수 없는 논문 저장\n",
    "    with open(\"data/missing_titles.json\", \"w\") as f:\n",
    "        json.dump(missing_titles, f, indent=4)\n",
    "\n",
    "    print(\"📄 모든 논문 다운로드 완료.\")\n",
    "    if missing_titles:\n",
    "        print(f\"⚠️ {len(missing_titles)}개의 논문을 찾을 수 없습니다. 'missing_titles.json'에서 확인하세요.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Papers:   0%|          | 0/51 [00:00<?, ?it/s]/tmp/ipykernel_19404/391042050.py:23: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n",
      "Downloading Papers: 100%|██████████| 51/51 [03:18<00:00,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 모든 논문 다운로드 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_all_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
