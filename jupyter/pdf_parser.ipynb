{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import sys\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "from langchain_community.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from config import Config, config_logging\n",
    "\n",
    "\n",
    "import pypdfium2\n",
    "import os\n",
    "\n",
    "def extract_pdf_content(pdf_path: str) -> str:\n",
    "    \"\"\"pypdfium2를 사용하여 PDF에서 텍스트 추출\"\"\"\n",
    "    pdf = pypdfium2.PdfDocument(pdf_path)\n",
    "    text = \"\"\n",
    "    for i in range(len(pdf)):  # 페이지 수만큼 반복\n",
    "        page = pdf[i]  # 페이지 객체 가져오기\n",
    "        text_page = page.get_textpage()  # 텍스트 객체 생성\n",
    "        text += text_page.get_text_bounded() + \"\\n\"  # 전체 텍스트 추출\n",
    "    return text.strip()\n",
    "\n",
    "def extract_pdfs_from_folder(folder_path: str):\n",
    "    \"\"\" 폴더 내 모든 PDF 파일을 텍스트로 변환 \"\"\"\n",
    "    documents = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, file)\n",
    "            text = extract_pdf_content(pdf_path)\n",
    "            documents.append({\"name\": file, \"content\": text})\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '1912.13110v1.pdf',\n",
       "  'content': 'arXiv:1912.13110v1 [q-fin.MF] 30 Dec 2019\\r\\nOpen Markets\\r\\nDonghan Kim ∗\\r\\nJanuary 1, 2020\\r\\nAbstract\\r\\nAn open market is a subset of entire equity market composed of a certain fixed number of\\r\\ntop-capitalization stocks. Though the number of stocks in the open market is fixed, the con\\x02stituents of the market change over time as each company’s rank by its market capitalization\\r\\nfluctuates. When one is allowed to invest also in money market, the open market resembles\\r\\nthe entire ‘closed’ equity market in the sense that the equivalence of market viability (lack of\\r\\narbitrage) and the existence of num´eraire portfolio (portfolio which cannot be outperformed)\\r\\nholds. When the access to the money market is prohibited, some topics such as Capital Asset\\r\\nPricing Model (CAPM), construction of functionally-generated portfolios, and the concept\\r\\nof universal portfolio are presented in open market setting.\\r\\n1 Introduction\\r\\nEquity markets are conventionally assumed to be closed, in the sense that they are almost\\r\\nuniversally assumed to consist of a given, fixed number of stocks at all times. However, this\\r\\nassumption fails to represent most real markets, where new stocks enter and some others exit\\r\\ndue to privatization, bankruptcy, or simply bad luck.\\r\\nThe number of companies in the U.S. stock market has undergone wide fluctuations. In\\r\\n1975, there were around 4,800 U.S. domiciled firms listed on the NYSE, Amex, and Nasdaq.\\r\\nThis number reached a peak of 7,500 listed firms in 1997, and then decreased by more than half\\r\\nto 3,600 firms 20 years later in 2017.\\r\\nTo mitigate the assumption of a fixed, immutable collection of companies, and to model\\r\\nstock markets more realistically, we study here markets that are open. These are constructed by\\r\\nrestricting our ‘investing space’ from the entire market to the subset composed of a certain fixed\\r\\nnumber n of top-capitalization stocks. More precisely, within the entire stock market, we keep\\r\\ntrack of the price dynamics of all stocks, rank them by order of market capitalization, consider\\r\\nan open market consisting of the top n stocks, and only invest in stocks that belong to this open\\r\\nmarket. High-capitalization indexes, such as the S&P 500 index, where one invests only in the\\r\\nn = 500 highest-capitalization companies, and any given stock is replaced by another one when\\r\\nits capitalization falls, are of this type.\\r\\nIn this paper, we present some results from closed markets which remain valid also in open\\r\\nmarkets. The main result of this type involves the concept of “market viability”, which is\\r\\n∗Department of Mathematics, Columbia University, New York, NY 10027 (E-mail: dk2571@columbia.edu).\\r\\n1\\nunderstood as “lack of a certain egregious form of arbitrage”, the condition which prohibits\\r\\nfinancing non-trivial liabilities starting from arbitrarily small initial capital. The result shows\\r\\nthat in an open stock market, with an access to the money market, viability is equivalent to\\r\\nany one of the following conditions: (i) a portfolio with the local martingale num´eraire property\\r\\nexists; (ii) a local martingale deflator exists; (iii) the market has locally finite maximal growth;\\r\\n(iv) a growth-optimal portfolio exists; and (v) a portfolio with the log-optimality property exists.\\r\\nWe provide precise definitions for these terms, and show that this equivalence can be formulated\\r\\nin terms of the drifts and covariations of the underlying stock prices, modeled by continuous\\r\\nsemimartingales.\\r\\nWhen the access to the money market is forbidden and one can only allowed to invest in\\r\\nfixed number n of top-capitalization stocks, the class of eligible portfolios diminishes significantly\\r\\nas portfolios must satisfy ‘self-financing’ condition. Under this extra condition, we provide a\\r\\nconnection of the above viability theory to the Capital Asset Pricing Model (CAPM), develop\\r\\na way for constructing functionally-generated portfolios, and discuss universal portfolio in an\\r\\nopen market.\\r\\nPreview : Section 2 defines open markets, investment strategies, and portfolios, as well as other\\r\\nnotions which will be needed throughout this paper. Section 3 develops arbitrage theory in open\\r\\nmarkets, along with the concepts of market viability and num´eraire. We provide definitions and\\r\\nproperties for these concepts, then state and prove the main result. Section 4 explores stock\\r\\nportfolios, CAPM theory, functional generation of portfolios, and universal portfolio in open\\r\\nmarkets. Section 5 provides some concluding remarks.\\r\\n2 Portfolios in Open Markets\\r\\nLet us suppose that the “whole equity market universe” is composed of N stocks and that we are\\r\\nonly interested in investing in the top n largest capitalization stocks, for some fixed 1 ≤ n < N.\\r\\nFor example, when our investing universe is the entire U.S. stock market, by setting n = 500\\r\\nwe are investing in those large companies which form the S&P 500 index. In order to invest in\\r\\nthese top n stocks, we must keep track of the rank of each stock’s capitalization at all times,\\r\\nand put together a portfolio composed of the n stocks with the largest capitalizations.\\r\\nThroughout this paper, we fix two positive integers n and N satisfying 1 ≤ n < N as above.\\r\\nWe suppose that trading is continuous in time, with no transaction costs or taxes, and that\\r\\nshares are infinitely divisible. We also assume without loss of generality that each stock has a\\r\\nsingle share outstanding, and the price of a stock is equal to its capitalization; thus, we use the\\r\\nterms ‘price of stock’ and ‘capitalization of stock’ interchangeably. We also assume that stock\\r\\nprices are discounted by the money market, and adopt the convention that the money market\\r\\npays and charges zero interest.\\r\\n2.1 Stock prices and their ranks\\r\\nWe first present the following definition of price process in the market described above.\\r\\nDefinition 2.1 (Price process). For an N-dimensional vector S ≡ (S1, · · · , Sn, · · · , SN ) of\\r\\ncontinuous semimartingales on a filtered probability space (Ω, F, F(·), P), we call S a vector of\\r\\nprice processes if each component is strictly positive, i.e., the inequalities Si(t) > 0 hold for all\\r\\n2\\ni ∈ {1, · · · , N} at any time t ≥ 0. The component processes of S represent the stock prices, or\\r\\nthe capitalizations, of N companies.\\r\\nWe need now to clarify some notation regarding ranks. Given the vector S of price processes,\\r\\nwe define the k-th ranked process S(k)(·) of S1, · · · , SN by\\r\\nS(k)(·) := max\\r\\n1≤i1<···<ik≤N\\r\\nmin{Si1(·), · · · , Sik(·)}. (2.1)\\r\\nTo be more specific, for any t ≥ 0, we have\\r\\nmax\\r\\ni=1,··· ,N\\r\\nSi(t) = S(1)(t) ≥ S(2)(t) ≥ · · · ≥ S(N−1)(t) ≥ S(N)(t) = min\\r\\ni=1,··· ,N\\r\\nSi(t); (2.2)\\r\\nthat is, we rank the components of the vector process S = (S1, · · · , SN ) in descending order,\\r\\nwith the lexicographic rule for breaking ties that always assigns a higher rank (i.e., a smaller\\r\\n(k)) to the smallest index i.\\r\\nDefinition 2.2 (Price process by rank). For the vector S of price processes in Definition 2.1,\\r\\nwe call the N-dimensional vector process\\r\\nS(t) ≡\\r\\n\\r\\nS(1)(t), · · · , S(N)(t)\\r\\n\\x01\\r\\n, t ≥ 0 (2.3)\\r\\nwhere each component is defined via (2.1), the vector of price processes by rank. In particular,\\r\\nthe k-th component Sk(t) = S(k)(t) of the vector S(t) represents the price of k-th ranked stock\\r\\namong N companies at time t.\\r\\nEach component of the vector process S(·) is also a continuous semimartingale, from the\\r\\nresults in Banner and Ghomrasni (2008). Along with the notation (2.1), we define a process\\r\\n{1, · · · , N} × [0,∞) ∋ (i, t) 7→ ui(t) ∈ {1, · · · , N}, such that each ui(·) is predictable and\\r\\nsatisfies\\r\\nSi(t) = S\\r\\n\\r\\nui(t)\\r\\n\\x01(t), ∀ t ≥ 0, (2.4)\\r\\nfor every i = 1, · · · , N. In other words, ui(t) is the rank of the i-th stock Si(t) at time t, for\\r\\nany given index i = 1, · · · , N. Note that for every fixed t ≥ 0, the function u·(t) : {1, · · · , N} →\\r\\n{1, · · · , N} is a bijection, because we break ties using the lexicographic rule when defining (2.1).\\r\\n2.2 Cumulative return processes\\r\\nIn this subsection, we present the notion of cumulative returns of the market. We first define\\r\\nthe stochastic logarithm L(Y ) of a positive continuous semimartingale Y with Y (0) = 1 by\\r\\nL(Y ) := Z ·\\r\\n0\\r\\ndY (t)\\r\\nY (t)\\r\\n(2.5)\\r\\nand consider the vector R ≡ (R1, · · · , RN ), whose every component is the stochastic logarithm\\r\\nof the corresponding normalized component of S in Definition 2.1:\\r\\nRi:= L\\r\\n\\x12\\r\\nSi\\r\\nSi(0)\\x13\\r\\n, i = 1, · · · , N. (2.6)\\r\\n3\\nEach component process Ri\\r\\nis again a semimartingale, and represents the cumulative returns of\\r\\nthe i-th stock, since its dynamic is represented as\\r\\ndRi(t) = dSi(t)\\r\\nSi(t)\\r\\n, t ≥ 0, and Ri(0) = 0 for i = 1, · · · , N. (2.7)\\r\\nWe posit the semimartingale decomposition\\r\\nRi = Ai + Mi, i = 1, · · · , N, (2.8)\\r\\nfor each component of the vector R = (R1, · · · , RN ). Here, the component Ai of the vector\\r\\nprocess A ≡ (A1, · · · , AN ) with Ai(0) = 0 is adapted, continuous and of finite variation on\\r\\ncompact time intervals; whereas each component Mi of the vector process M ≡ (M1, · · · , MN )\\r\\nis a continuous local martingale with Mi(0) = 0, for i = 1, · · · , N. We think of the finite\\r\\nvariation processes Ai as the ‘drift components’, and of the local martinagles Mi as the ‘noise\\r\\ncomponents’, of R.\\r\\nWe define next the continuous, nondecreasing scalar process\\r\\nO := X\\r\\nN\\r\\ni=1\\r\\n\\x12 Z ·\\r\\n0\\r\\n|dAi(t)| + d[Mi, Mi](t)\\r\\n\\x13\\r\\n, (2.9)\\r\\nwhere R T\\r\\n0\\r\\n|dAi(t)| denotes the total variation of Ai on the interval [0, T] for T ≥ 0 and [Mi, Mj ]\\r\\nrepresents the covariation process of the continuous semimartingales Mi and Mj for 1 ≤ i, j ≤ N.\\r\\nHere, we note that [Ri, Rj ] = [Mi, Mj ] holds from (2.8). This scalar process O plays the role\\r\\nof an “operational clock” for the vector R. All processes Ai and [Mi, Mj ] for 1 ≤ i, j ≤ N are\\r\\nabsolutely continuous with respect to this clock, and thus, by the Radon-Nikod´ym Theorem,\\r\\nthere exist two predictable processes\\r\\nα ≡ (αi)1≤i≤N and c ≡ (ci,j )1≤i,j≤N , (2.10)\\r\\nvector-valued and matrix-valued, respectively, such that\\r\\nA =\\r\\nZ ·\\r\\n0\\r\\nα(t)dO(t), and C ≡ [M, M] = Z ·\\r\\n0\\r\\nc(t)dO(t). (2.11)\\r\\nHere and in what follows, we write C ≡ (Ci,j )1≤i,j≤N for the nonnegative-definite, matrix-valued\\r\\nprocess of covariations\\r\\nCi,j := [Mi, Mj ] = [Ri, Rj ], for 1 ≤ i, j ≤ N. (2.12)\\r\\nThe component αiin (2.10) represents the local rate of return of the i-th stock in the market;\\r\\nwhereas the entry ci,j stands for the local covariation rate of the i-th and j-th stocks. We call\\r\\nthe collection of local rates α, c in (2.10) the local characteristics of the market, and these rates\\r\\nare measured with respect to the operational clock O in (2.9).\\r\\nFor a continuous vector-valued semimartingale Y = (Y1, · · · , YN ), we denote by I(Y ) the\\r\\nclass of predictable vector processes π = (π1, · · · , πN ) which are integrable with respect to the\\r\\nvector Y . In particular, for the collection I(R) of the vector R in (2.6) and (2.8), we have a very\\r\\n4\\nconvenient characterization: A predictable vector process π = (π1, · · · , πN ) belongs to I(R), if\\r\\nand only if\\r\\nZ T\\r\\n0\\r\\n\\x10\\r\\n|π\\r\\n′\\r\\n(t)α(t)| + π\\r\\n′\\r\\n(t)c(t)π(t)\\r\\n\\x11\\r\\ndO(t) < ∞, for any T ≥ 0. (2.13)\\r\\nWe denote then by\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRi(t) ≡\\r\\nZ ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dR(t) = Z ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dA(t) + Z ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dM(t),\\r\\nthe stochastic integral of π ∈ I(R), with respect to the vector semimartingale R.\\r\\n2.3 Investment strategies and portfolios\\r\\nAlong with the N-dimensional vector S of Definition 2.1, representing the stock prices of the\\r\\nmarket, we introduce the following notions.\\r\\nDefinition 2.3 (Investment strategy, wealth process, and num´eraire). We call an N-dimensional\\r\\nvector of predictable process ϑ ≡ (ϑ1, · · · , ϑN ) investment strategy, if it is integrable with respect\\r\\nto the price vector S, i.e., ϑ ∈ I(S). For any nonnegative real number x, we call\\r\\nX(·; x, ϑ) := x +\\r\\nZ ·\\r\\n0\\r\\nϑ\\r\\n′\\r\\n(t)dS(t) ≡ x +\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nϑi(t)dSi(t) (2.14)\\r\\nthe wealth process generated by ϑ with initial capital x. We call the wealth process num´eraire,\\r\\nif X(·; 1, ϑ) > 0 holds for the normalized initial capital x = 1. The collection of all num´eraires\\r\\nis denoted by X .\\r\\nThe i-th component ϑi(t) represents the units of investment (or number of shares) held in\\r\\nthe i-th stock at time t, and plays the role of integrand with integrator dSi(t) in the stochastic\\r\\nintegral of (2.14). The requirement X(0) = x = 1 in defining num´eraires is a simple normaliza\\x02tion, because X(·; cx, cϑ) = cX(·; x, ϑ) holds for any positive real number c. Since we consider\\r\\ninvestment only in the top n stocks, we need a similar definition of investment strategy for this\\r\\nparticular case.\\r\\nDefinition 2.4 (Investment strategy among the top n stocks). We call an investment strategy\\r\\nϑ ∈ I(S) an investment strategy among the top n stocks, if the “sensoring” equalities\\r\\nϑi(t)1{ui(t)>n} = 0, for i = 1, · · · , N, t ≥ 0, (2.15)\\r\\nhold with the notation (2.4).\\r\\nThe wealth process and the num´eraire associated with this investment strategy ϑ among the\\r\\ntop n stocks, are defined in the same manner as in Definition 2.3. We denote the collection of N\\x02dimensional predictable processes ϑ satisfying the condition (2.15) by T (n), and the collection\\r\\nof investment strategies among the top n stocks by I(S) ∩ T (n).\\r\\nThe collection of all num´eraires generated by investment strategies ϑ ∈ I(S) ∩ T (n) among\\r\\nthe top n stocks is denoted by X n.\\r\\n5\\nThe condition (2.15) prohibits the strategy ϑ from investing in the i-th stock at time t ≥ 0,\\r\\nif this stock fails to rank at that time among the top n stocks in terms of capitalization. We\\r\\npresent another definition, that of a portfolio rule, which plays the role of integrand with respect\\r\\nto the integrator dRi(t) of (2.6).\\r\\nDefinition 2.5 (Portfolio). We call an N-dimensional predictable, vector-valued process π ≡\\r\\n(π1, · · · , πN ) ∈ I(R) a portfolio, if it is integrable with respect to the cumulative return vector\\r\\nR of (2.6). We call a portfolio π ∈ I(R) a portfolio among the top n stocks, if the equalities\\r\\nπi(t)1{ui(t)>n} = 0, for i = 1, · · · , N, t ≥ 0, (2.16)\\r\\nhold with the notation (2.4). We denote the collection of portfolios among the top n stocks by\\r\\nI(R) ∩ T (n).\\r\\nSince the function u·(t) : {1, · · · , N} → {1, · · · , N} is bijective for every t ≥ 0, the collection\\r\\n\\x08\\r\\n1{ui(t)=k}\\r\\n\\t\\r\\nk=1,···,N constitutes a partition of unity for any given i = 1, · · · , N, t ≥ 0, and the\\r\\nconditions (2.15), (2.16) can also be formulated respectively as\\r\\nϑi(t) = Xn\\r\\nk=1\\r\\nϑi(t)1{ui (t)=k} = ϑi(t)1{ui(t)≤n}, for i = 1, · · · , N, t ≥ 0, (2.17)\\r\\nπi(t) = Xn\\r\\nk=1\\r\\nπi(t)1{ui (t)=k} = πi(t)1{ui(t)≤n}, for i = 1, · · · , N, t ≥ 0. (2.18)\\r\\nWe present next the connection between investment strategies ϑ and portfolios π. For any\\r\\nscalar continuous semimartingale Z with Z(0) = 0, we denote the stochastic exponential of Z\\r\\nby\\r\\nE(Z) := exp \\x10Z −\\r\\n1\\r\\n2\\r\\n[Z, Z]\\r\\n\\x11\\r\\n. (2.19)\\r\\nIt can be shown that this is also the unique process satisfying the linear stochastic integral\\r\\nequation\\r\\nE(Z) = 1 + Z ·\\r\\n0\\r\\nE(Z)(t)dZ(t). (2.20)\\r\\nIt is straightforward to check that the stochastic logarithm operator L(·) in (2.5), is the inverse\\r\\nof the stochastic exponential operator E(·) in (2.19).\\r\\nWe introduce now the cumulative returns process of a portfolio π as in Definition 2.5, via\\r\\nthe vector stochastic integral\\r\\nRπ := Z ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dR(t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRi(t), (2.21)\\r\\nand consider its stochastic exponential\\r\\nXπ := E(Rπ) = E\\r\\n\\x10 Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRi(t)\\r\\n\\x11\\r\\n. (2.22)\\r\\n6\\nIn particular, we note that Xπ is positive. Then, from (2.22), (2.21), and (2.7), we obtain the\\r\\ndynamics\\r\\ndXπ(t)\\r\\nXπ(t)\\r\\n= dRπ(t) = X\\r\\nN\\r\\ni=1\\r\\nπi(t)dRi(t) = X\\r\\nN\\r\\ni=1\\r\\nπi(t)\\r\\ndSi(t)\\r\\nSi(t)\\r\\n, Xπ(0) = 1. (2.23)\\r\\nBy setting\\r\\nϑi:=\\r\\nXππi\\r\\nSi\\r\\nfor i = 1, · · · , N, (2.24)\\r\\nwe arrive at the equation (2.14) with X(·; 1, ϑ) replaced by Xπ(·). Thus, from the portfolio\\r\\nπ ∈ I(R), we can obtain the corresponding investment strategy ϑ and its num´eraire X(·; 1, ϑ), via\\r\\nthe recipe (2.24). Here, we denote the num´eraire generated by the portfolio π by X(·; 1, ϑ) := Xπ,\\r\\nas in (2.22).\\r\\nConversely, for a given investment strategy ϑ generating a positive wealth process, i.e., the\\r\\nnum´eraire X(·; 1, ϑ), we define a predictable, vector-valued process π ≡ (π1, · · · , πN ) as\\r\\nπi:=\\r\\nSiϑi\\r\\nX(·; 1, ϑ)\\r\\nfor i = 1, · · · , N. (2.25)\\r\\nIt can be easily checked that π is indeed a portfolio, i.e., R-integrable and (2.14) can be written\\r\\nas\\r\\nX(·; 1, ϑ) = 1 + Z ·\\r\\n0\\r\\nX(t; 1, ϑ)\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRi(t),\\r\\nwith the help of (2.7). This last equation gives the dynamics in (2.23) with Xπ(·) ≡ X(·; 1, ϑ).\\r\\nThus, whether we start from an investment strategy ϑ (generating a num´eraire) or from a\\r\\nportfolio π, the counterpart can always be obtained via (2.25) or (2.24), respectively, and we\\r\\nwill denote the corresponding num´eraire X(·; , 1, ϑ) in (2.14) by Xπ as in (2.22).\\r\\nIn the relationship (2.25), the product Si(t)ϑi(t) represents the amount of wealth invested in\\r\\ni-th stock at time t, thus πi(t) can be interpreted as the proportion of current wealth invested\\r\\nin i-th stock at time t. The remaining proportion of wealth\\r\\nπ0 := 1 −\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi (2.26)\\r\\nis then considered to be placed in the money market.\\r\\nWe present now a few more concepts regarding portfolios. For any two portfolios π, ρ in I(R),\\r\\nwe consider the covariation process between the cumulative returns Rπ, Rρ in (2.21), namely\\r\\nCπρ := [Rπ, Rρ] = Z ·\\r\\n0\\r\\ncπρ(t)dO(t), with cπρ := π\\r\\n′\\r\\ncρ =\\r\\nX\\r\\nN\\r\\ni=1\\r\\nX\\r\\nN\\r\\nj=1\\r\\nπici,jρj . (2.27)\\r\\nHere, we recall the definitions of the matrix-valued processes c and C in (2.10), (2.11) and note\\r\\nthe notational consistency with (2.27). In particular, when the portfolio is given as the unit\\r\\nvector e\\r\\ni of RN for some i = 1, · · · , N, we use the subscript ‘i’ instead of ‘ei\\r\\n’ to write Ciρ ≡ Ce\\r\\niρ\\r\\nand ciρ ≡ ce\\r\\niρ\\r\\nin order to ease notation. This convention is consistent with the actual equalities\\r\\nCi,j = Ce\\r\\niej and ci,j = ceiej for 1 ≤ i, j ≤ N.\\r\\n7\\nBy recalling the wealth process Xπ generated by the portfolio π as in (2.22), (2.23), we can\\r\\nexpress the logarithm of Xπ as\\r\\nlog Xπ = Rπ −\\r\\n1\\r\\n2\\r\\nCππ =\\r\\nZ ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dA(t) −\\r\\n1\\r\\n2\\r\\nCππ +\\r\\nZ ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dM(t). (2.28)\\r\\nWe call the finite-variation part of log Xπ the cumulative growth of the portfolio π, and denote\\r\\nit by\\r\\nΓπ := Aπ −\\r\\n1\\r\\n2\\r\\nCππ, where Aπ := Z ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dA(t). (2.29)\\r\\nIn a similar manner, the local martingale part of the decomposition in (2.28) is denoted by\\r\\nMπ := Z ·\\r\\n0\\r\\nπ\\r\\n′\\r\\n(t)dM(t). (2.30)\\r\\nIn particular, the cumulative return Rπ in (2.22) is the stochastic logarithm L(Xπ) of Xπ, and\\r\\nhas ‘drift’ component Aπ as in (2.29), from (2.21) and (2.8); whereas the natural logarithm\\r\\nlog Xπ in (2.28) of Xπ has ‘drift’ term Γπ.\\r\\nWe further define the predictable processes\\r\\nαπ := π\\r\\n′α, γπ := π′α −\\r\\n1\\r\\n2\\r\\nπ\\r\\n′\\r\\ncπ = απ −\\r\\n1\\r\\n2\\r\\ncππ, (2.31)\\r\\nand call απ the rate of return, and γπ the growth rate, of the portfolio π. The ‘drift parts’ Aπ\\r\\nand Γπ, of L(Xπ) and log Xπ, respectively, are then represented as the integrals of these rates\\r\\nwith respect to the ‘operational clock’ in (2.9):\\r\\nAπ =\\r\\nZ ·\\r\\n0\\r\\nαπ(t)dO(t), Γπ =\\r\\nZ ·\\r\\n0\\r\\nγπ(t)dO(t). (2.32)\\r\\n2.4 Portfolios among the top n stocks\\r\\nIn this subsection we provide definitions, similar to those introduced in the previous subsections,\\r\\nfor portfolios that invest only among the top n stocks.\\r\\nFor ϑ ∈ I(S) ∩ T (n) and π ∈ I(R) ∩ T (n), representing a strategy that invests only among\\r\\nthe top n stocks and a portfolio among the top n stocks, respectively, the equations (2.21)-(2.26)\\r\\ncan be used in the same manner. In particular, the bidirectional connections (2.24) and (2.25)\\r\\nbetween ϑ ∈ I(S) ∩ T (n) and π ∈ I(R) ∩ T (n) still hold, because of the similarity in the\\r\\nconditions (2.15) and (2.16).\\r\\nWe define next a new N-dimensional vector Re ≡ (Re1, · · · , ReN ) by\\r\\nRei(t) := Z t\\r\\n0\\r\\n1{ui(s)≤n}dRi(s), for i = 1, · · · , N, t ≥ 0. (2.33)\\r\\nEach component Rei(t) represents the cumulative return of the i-th stock, accumulated over\\r\\n[0, t] but only at times when the stock ranks among the top n by capitalization. Then, for\\r\\nπ ∈ I(R) ∩ T (n), (2.21) can be also cast as\\r\\nRπ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRi(t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)1{ui (t)≤n}dRi(t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRei(t), (2.34)\\r\\n8\\nwhere the second equality follows from (2.18). We then have the semimartingale decomposition\\r\\nRei = Aei + Mfi, i = 1, · · · , N, (2.35)\\r\\nwhere\\r\\nAei(t) := Z t\\r\\n0\\r\\n1{ui(s)≤n}dAi(s), Mfi(t) := Z t\\r\\n0\\r\\n1{ui(s)≤n}dMi(s), i = 1, · · · , N, (2.36)\\r\\nfrom (2.8). In the decomposition Rπ = Aπ + Mπ, with Aπ as in (2.29) and Mπ as in (2.30), we\\r\\nnote that Aπ and Mπ can be expressed in terms of the components of Ae and Mf, respectively, as\\r\\nAπ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dAi(t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dAei(t), Mπ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dMi(t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dMfi(t)\\r\\n(2.37)\\r\\nby analogy with (2.34). Also in a manner similar to (2.12), we define\\r\\nCei,j := [Mfi, Mfj ] = [Rei, Rej ], for 1 ≤ i, j ≤ N. (2.38)\\r\\nNote the relationship\\r\\ndCei,j (t) = d[Rei, Rej ](t) = 1{ui(t)≤n}1{uj (t)≤n}d[Ri, Rj ](t) = 1{ui(t)≤n}1{uj (t)≤n}dCi,j (t) (2.39)\\r\\nbetween Ce and C. We further define a vector-valued process αe ≡ (αe1, · · · , αeN ) and a matrix\\x02valued process ec ≡ (eci,j )1≤i,j≤N as\\r\\nαei(t) := 1{ui(t)≤n}αi(t), i = 1, · · · , N, (2.40)\\r\\neci,j (t) := 1{ui(t)≤n}1{uj (t)≤n}ci,j (t), 1 ≤ i, j ≤ N; (2.41)\\r\\nthen it is straightforward to obtain the relationships\\r\\nAe =\\r\\nZ ·\\r\\n0\\r\\nαe(t)dO(t), and Ce ≡ [M, f Mf] = Z ·\\r\\n0\\r\\nec(t)dO(t) (2.42)\\r\\nin accordance with (2.11), where the vector-valued and matrix-valued processes Ae ≡ (Ae1, · · · , AeN )\\r\\nand Ce ≡ (Cei,j )1≤i,j≤N , respectively, are as in (2.36), (2.38).\\r\\nThe definition of Cπρ in (2.27) can be also invoked when π, ρ ∈ I(R) ∩ T (n), but we have\\r\\nalso with the help of (2.34) and (2.38) the alternative representation\\r\\nCπρ = [Rπ, Rρ] = \\x14 Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRei(t),\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\nj=1\\r\\nρj (t)dRej (t)\\r\\n\\x15\\r\\n=\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nX\\r\\nN\\r\\nj=1\\r\\nπi(t)ρj (t)dCei,j (t).\\r\\n(2.43)\\r\\nIn particular, consider the portfolio π among the top n stocks, defined as\\r\\nπ(·) := 1{ui(·)≤n}e\\r\\ni\\r\\n(2.44)\\r\\nfor a fixed i = 1, · · · , N. This portfolio π invests all wealth in the i-th stock, when this stock\\r\\nranks among the top n; otherwise, it puts all wealth in the money market. From (2.34), the\\r\\nidentity\\r\\nRπ = Rei (2.45)\\r\\n9\\nholds, and we shall use the subscript ‘ei’ instead of ‘π’ to write Xπ ≡ Xei\\r\\nand\\r\\nC eiρ ≡ Cπρ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\nj=1\\r\\nρj (t)dCei,j (t), as well as ceiρ ≡ cπρ =\\r\\nX\\r\\nN\\r\\nj=1\\r\\nρj (t)eci,j (t), (2.46)\\r\\nin order to ease notation for the specific π in (2.44). This convention is consistent with the\\r\\nequalities C ei,ej = [Rei, Rej ] = Cei,j for 1 ≤ i, j ≤ N.\\r\\nIt is useful to write succinctly the above relationships in this subsection, between symbols\\r\\nwith tilde and corresponding symbols without tilde, in matrix notation. We do this by intro\\x02ducing the predictable matrix-valued process D ≡ (Di,j )1≤i,j≤N with entries\\r\\nDi,j (t) := (\\r\\n1{ui(t)≤n}i = j,\\r\\n0 i 6= j,\\r\\n(2.47)\\r\\nfor each t ≥ 0. Here, we note that D(t) is a diagonal, idempotent matrix, whose (i, i)-th entry\\r\\nis 1 if the i-th stock belongs to the top n stocks at time t ≥ 0, otherwise it is zero. Because at\\r\\nleast N − n diagonal entries of D(t) are zero, D(·) is always singular. Then, any N-dimensional\\r\\npredictable process ν in T (n) as in Definition 2.4, satisfies Dν = ν; in particular,\\r\\nDϑ = ϑ, Dπ = π, (2.48)\\r\\nhold for all ϑ ∈ I(S) ∩ T (n) and π ∈ I(R) ∩ T (n) from the conditions (2.17) and (2.18). Also,\\r\\nthe identities (2.33), (2.36), (2.39), (2.40), and (2.41) can be reformulated as\\r\\ndRe(t) = D(t)dR(t), dAe(t) = D(t)dA(t), dMf(t) = D(t)dM(t), (2.49)\\r\\ndCe(t) = D(t)dC(t)D(t),\\r\\nas well as\\r\\nαe = Dα, ec = DcD. (2.50)\\r\\nMoreover, we have another expression of the type (2.31) for π ∈ I(R)∩T (n): using the property\\r\\n(2.48), we write\\r\\nαπ = π\\r\\n′α = π′Dα = π′α, γ e π = π′α −\\r\\n1\\r\\n2\\r\\nπ\\r\\n′\\r\\ncπ = π\\r\\n′αe −\\r\\n1\\r\\n2\\r\\nπ\\r\\n′DcDπ = π′αe −\\r\\n1\\r\\n2\\r\\nπ\\r\\n′\\r\\necπ. (2.51)\\r\\nWe present now the following results regarding the integrability condition with respect to R\\r\\n(or Re), which will be used in the next section.\\r\\nLemma 2.6 (Null portfolio). For an N-dimensional predictable process η ∈ T (n), suppose that\\r\\nη\\r\\n′αe = 0 and ecη = 0 hold in the (P ⊗ O)-a.e. sense.\\r\\nThen η is a portfolio, i.e., η ∈ I(R) ∩ T (n), and the identity Rη =\\r\\nR ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dRe(t) ≡ 0 holds.\\r\\nIn this case, we call η a null portfolio.\\r\\n10\\nProof. As η ∈ T (n), we have Dη = η, or η\\r\\n′ = η′D. Recalling (2.49), (2.35), and (2.42), we have\\r\\nZ ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dR(t) = Z ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)D(t)dR(t) = Z ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dRe(t)\\r\\n=\\r\\nZ ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dAe(t) + Z ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dMf(t) = Z ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)αe(t)dO(t) + Z ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dMf(t). (2.52)\\r\\nThe first integral on the right-hand side of (2.52) vanishes, thanks to the assumption η\\r\\n′αe = 0.\\r\\nThe second integral R ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dMf(t) is a continuous local martingale, and has quadratic varia\\x02tion R ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)ec(t)η(t)dO(t) from (2.42). This quadratic variation also vanishes on account of the\\r\\nassumption ecη = 0, and the result follows.\\r\\nLemma 2.7 (Integrability condition with respect to R). An N-dimensional predictable vector\\r\\nprocess π ∈ T (n) belongs to I(R), if and only if\\r\\nZ T\\r\\n0\\r\\n\\x10\\r\\n|π\\r\\n′\\r\\n(t)αe(t)| + π\\r\\n′\\r\\n(t)ec(t)π(t)\\r\\n\\x11\\r\\ndO(t) < ∞, for any T ≥ 0. (2.53)\\r\\nProof. From the assumption π ∈ T (n), we have Dπ = π, and π\\r\\n′D = π′\\r\\n. The condition (2.13)\\r\\ncan be rewritten with the help of (2.50) as\\r\\nZ T\\r\\n0\\r\\n\\x10\\r\\n|π\\r\\n′\\r\\n(t)α(t)| + π\\r\\n′\\r\\n(t)c(t)π(t)\\r\\n\\x11\\r\\ndO(t) = Z T\\r\\n0\\r\\n\\x10\\r\\n|π\\r\\n′\\r\\n(t)D(t)α(t)| + π\\r\\n′\\r\\n(t)D(t)c(t)D(t)π(t)\\r\\n\\x11\\r\\ndO(t)\\r\\n=\\r\\nZ T\\r\\n0\\r\\n\\x10\\r\\n|π\\r\\n′\\r\\n(t)αe(t)| + π\\r\\n′\\r\\n(t)ec(t)π(t)\\r\\n\\x11\\r\\ndO(t) < ∞.\\r\\n3 Num´eraires and Market Viability\\r\\nThis section presents the fundamental result in arbitrage theory of equity market, in open market\\r\\ncontext. Before we state and prove the result, we explain several necessary concepts one after\\r\\nanother.\\r\\n3.1 Auxiliary market\\r\\nConsider a portfolio ρ ∈ I(R) which generates the num´eraire Xρ as in Definition 2.5 and (2.22),\\r\\nand fix ρ throughout this subsection. We regard this portfolio ρ as a ‘baseline’, in the sense that\\r\\nwant to compare the relative performance of any other portfolio π ∈ I(R) with respect to ρ, by\\r\\nunderstanding the relative wealth process\\r\\nX\\r\\nρ\\r\\nπ\\r\\n:=\\r\\nXπ\\r\\nXρ\\r\\n. (3.1)\\r\\n11\\nAs the wealth Xπ is denominated relative to Xρ in (3.1), we consider an auxiliary market, in\\r\\nwhich all the components of the price vector S in Definition 2.1 are denominated in units of Xρ:\\r\\nS\\r\\nρ\\r\\ni\\r\\n:=\\r\\nSi\\r\\nXρ\\r\\n, i = 1, · · · , N. (3.2)\\r\\nHere, we also consider the money market S0 ≡ 1, with S\\r\\nρ\\r\\n0\\r\\n:= 1/Xρ, as we assume that the\\r\\nmoney market pays and charges zero interest in the introductory part of Section 2. Since S\\r\\nρ\\r\\n0\\r\\nis\\r\\nno longer trivial, we will consider the (N + 1)-dimensional vector S\\r\\nρ ≡ (S\\r\\nρ\\r\\n0\\r\\n, Sρ\\r\\n1\\r\\n, · · · S\\r\\nρ\\r\\nN\\r\\n) as the\\r\\nprice process vector in this auxiliary market.\\r\\nRecalling the notation (2.21) and (2.27), we define two (N + 1)-dimensional vectors of semi\\x02martingales Rρ ≡ (R\\r\\nρ\\r\\n0\\r\\n, · · · , Rρ\\r\\nN\\r\\n), and Reρ ≡ (Reρ\\r\\n0\\r\\n, · · · , Reρ\\r\\nN\\r\\n) with components\\r\\nR\\r\\nρ\\r\\n0\\r\\n:= Cρρ − Rρ, and R\\r\\nρ\\r\\ni\\r\\n:= R\\r\\nρ\\r\\n0 + (Ri − Ciρ), for i = 1, · · · , N. (3.3)\\r\\nReρ\\r\\n0\\r\\n:= R\\r\\nρ\\r\\n0 = Cρρ − Rρ, and Reρ\\r\\ni\\r\\n:= Reρ\\r\\n0 + (Rei − Ceiρ), for i = 1, · · · , N. (3.4)\\r\\nThe following result, which builds on Proposition 1.29 of Karatzas and Kardaras (2020), shows\\r\\nthat the vectors Rρ, Reρ play the role of cumulative returns in the auxiliary market. We also\\r\\nrecall the ‘money market proportion’ π0 of a portfolio π in (2.26).\\r\\nProposition 3.1. For any two portfolios ρ, π ∈ I(R), the relative wealth process X\\r\\nρ\\r\\nπ of (3.1)\\r\\nadmits the representation\\r\\nXρ\\r\\nπ = E(R\\r\\nρ\\r\\nπ\\r\\n), where R\\r\\nρ\\r\\nπ\\r\\n:= Rπ−ρ − Cπ−ρ,ρ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=0\\r\\nπi(t)dRρ\\r\\ni\\r\\n(t). (3.5)\\r\\nIn particular, for any two portfolios ρ, π ∈ I(R) ∩ T (n) among the top n stocks, the process R\\r\\nρ\\r\\nπ\\r\\nin (3.5) admits the additional representation\\r\\nR\\r\\nρ\\r\\nπ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=0\\r\\nπi(t)dReρ\\r\\ni\\r\\n(t). (3.6)\\r\\nProof. The first part is exactly Proposition 1.29 of Karatzas and Kardaras (2020). Thus, it is\\r\\nenough to show R ·\\r\\n0\\r\\nPN\\r\\ni=0 πi(t)dRρi\\r\\n(t) = R ·\\r\\n0\\r\\nPN\\r\\ni=0 πi(t)dReρi\\r\\n(t) when ρ, π belong to I(R) ∩ T (n).\\r\\nSince Reρ\\r\\n0 = R\\r\\nρ\\r\\n0\\r\\nin (3.3) and (3.4), this reduces to showing\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)d\\r\\n\\r\\nRi − Ciρ\\x01\\r\\n(t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)d\\r\\n\\r\\nRei − C eiρ\\x01\\r\\n(t).\\r\\nThanks to the condition (2.18) and the definition (2.33), this can be easily checked:\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)d\\r\\n\\r\\nRi − Ciρ\\x01\\r\\n(t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)1{ui(t)≤n}d\\r\\n\\r\\nRi − Ciρ\\x01\\r\\n(t)\\r\\n=\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)d\\r\\n\\r\\nRei − C eiρ\\x01(t)\\r\\nwhere, in the last equality, we used the string of identities\\r\\n1{ui(t)≤n}dCiρ(t) = 1{ui(t)≤n}d[Ri, Rρ](t) = d[Rei, Rρ](t) = dC eiρ. (3.7)\\r\\n12\\nIn the special case π ≡ e\\r\\ni\\r\\n, that is, when the portfolio π invests all wealth in the i-th stock at\\r\\nall times, the relative wealth process X\\r\\nρ\\r\\nπ and its stochastic logarithm R\\r\\nρ\\r\\nπ in (3.1), (3.5) become\\r\\nXρ\\r\\nπ =\\r\\nSi\\r\\nXρ\\r\\n= S\\r\\nρ\\r\\ni\\r\\n, Rρ\\r\\nπ = R\\r\\nρ\\r\\ni\\r\\n,\\r\\nand Proposition 3.1 yields\\r\\nS\\r\\nρ\\r\\ni = E(R\\r\\nρ\\r\\ni\\r\\n) (3.8)\\r\\nfor any given i = 1, · · · , N. Therefore, the component R\\r\\nρ\\r\\ni\\r\\nof (3.3) is the stochastic logarithm of\\r\\nthe i-th component of the price vector S\\r\\nρ\\r\\nin the auxiliary market, and the vector Rρ plays the\\r\\nrole of cumulative returns in the auxiliary market.\\r\\nBy analogy with (2.23), we also have\\r\\ndXρ\\r\\nπ(t)\\r\\nX\\r\\nρ\\r\\nπ(t)\\r\\n= dRρ\\r\\nπ\\r\\n(t) = X\\r\\nN\\r\\ni=0\\r\\nπi(t)dRρ\\r\\ni\\r\\n(t) = X\\r\\nN\\r\\ni=0\\r\\nπi(t)\\r\\ndSρ\\r\\ni\\r\\n(t)\\r\\nS\\r\\nρ\\r\\ni\\r\\n(t)\\r\\n, Xρ\\r\\nπ\\r\\n(0) = 1, (3.9)\\r\\nfor ρ, π in I(R), from (3.5), (3.8). It is very important that the summation in (3.9) should\\r\\ninclude the index i = 0, as indeed it does, in contrast to the summation in (2.23).\\r\\n3.2 Supermartingale Num´eraire and local martingale Num´eraire\\r\\nWe introduce now the notions of supermartingale num´eraire and local martingale num´eraire.\\r\\nDefinition 3.2 (Supermartingale num´eraire and local martingale num´eraire). A given portfolio\\r\\nρ ∈ I(R) is called supermatingale num´eraire portfolio (local martingale num´eraire portfolio)\\r\\nin the whole market, if the relative wealth process X\\r\\nρ\\r\\nπ = Xπ/Xρ of (3.1) is a supermartingale\\r\\n(local martingale) for every portfolio π ∈ I(R) in the market. In this case, the wealth process\\r\\nXρ is called a supermartingale num´eraire (local martingale num´eraire, respectively) in the whole\\r\\nmarket.\\r\\nSimilarly, a given portfolio ρ ∈ I(R)∩ T (n) among the top n stocks is called supermatingale\\r\\nnum´eraire portfolio (local martingale num´eraire portfolio) among the top n stocks, if the relative\\r\\nwealth process X\\r\\nρ\\r\\nπ is a supermartingale (local martingale) for every portfolio π ∈ I(R) ∩ T (n)\\r\\namong the top n stocks. In this case, the wealth process Xρ is called supermartingale num´eraire\\r\\n(local martingale num´eraire, respectively) among the top n stocks.\\r\\nBy Fatou’s lemma, every nonnegative local martingale is a supermartingale; thus, every local\\r\\nmartingale num´eraire is in particular a supermatingale num´eraire. We also have the following\\r\\nuniqueness result for supermartingale (local martingale) num´eraires (respectively, among the\\r\\ntop n stocks).\\r\\nLemma 3.3. There is a unique supermartingale (local martingale) num´eraire portfolio in the\\r\\nentire market (respectively, among the top n stocks).\\r\\nProof. Suppose that there are two local martingale (or two supermartingale) num´eraire portfolios\\r\\nρ and ν with the same initial wealth Xρ(0) = Xν(0). Then, the relative wealth process Xρ/Xν\\r\\nand its reciprocal Xν /Xρ are positive supermartingales. From the Doob-Meyer decomposition\\r\\nof semimartingales, it is easy to show that a continuous, positive supermartingale Y is almost\\r\\neverywhere constant, if its reciprocal is also a supermartingale. Thus, Xρ/Xν ≡ 1 almost\\r\\neverywhere, and the two portfolios ρ and ν generate the same wealth process.\\r\\n13\\nIt can be shown that the supermartingale num´eraire is actually the local martingale num´eraire,\\r\\nthus the two num´eraires are equivalent, in the whole market where no constraint is imposed on\\r\\nportfolios. This is Proposition 2.4 of Karatzas and Kardaras (2020), which we repeat here for\\r\\nthe convenience of the reader.\\r\\nProposition 3.4. For a portfolio ρ ∈ I(R), the following statements are equivalent:\\r\\n(1) ρ is a supermartingale num´eraire portfolio in the whole market.\\r\\n(2) ρ is a local martingale num´eraire portfolio in the whole market.\\r\\n(3) The equality Ai = Ciρ holds for all i = 1, · · · , N.\\r\\nThe statement (3) gives a very simple structural condition, derived from the cumulative\\r\\nreturn process of the market, which characterizes this equivalence. It is no surprise that the\\r\\nresult also holds for the portfolios among the top n stocks; but in this case, the cumulative\\r\\nreturn process vector R in (2.6) should be replaced by Re of (2.33) instead.\\r\\nProposition 3.5. For a portfolio ρ ∈ I(R) ∩ T (n), the following statements are equivalent:\\r\\n(1) f ρ is a supermartingale num´eraire portfolio among the top n stocks.\\r\\n(2) f ρ is a local martingale num´eraire portfolio among the top n stocks.\\r\\n(3) f The equality Aei = Ceiρ holds for all i = 1, · · · , N.\\r\\nProof. The proof follows the same general outline as that for Proposition 2.4 in Karatzas and Kardaras\\r\\n(2020). We first assume statement (3), which is equivalent to the requirement that Rei−Ceiρ = Mfi\\r\\nis a local martingale for all i = 1, · · · , N from (2.35). Recalling the notation of (3.4), (2.27)\\r\\nwith the identities (2.18), (2.34) and (3.7), we obtain that the process\\r\\nReρ\\r\\n0 = Cρρ − Rρ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nρi(t)dCiρ(t) −\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nρi(t)dRei(t)\\r\\n=\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nρi(t)1{ui(t)≤n}dCiρ(t) −\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nρi(t)dRei(t)\\r\\n=\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nρi(t)dCeiρ(t) −\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nρi(t)dRei(t) = −\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nρi(t)dMfi(t) (3.10)\\r\\nis then also a local martingale. This in turn implies that all the components Reρ\\r\\ni = Reρ\\r\\n0+(Rei−Ceiρ)\\r\\nfor i = 1, · · · , N in (3.4) are local martingales as well. Moreover, from Proposition 3.1, the\\r\\nprocesses R\\r\\nρ\\r\\nπ and X\\r\\nρ\\r\\nπ are also local martingales for every portfolio π ∈ I(R) ∩ T (n) among the\\r\\ntop n stocks, so the implication (3) ⇒ (2) has been proved.\\r\\nSince statement (2) trivially implies statement (1), it remains to establish the implication\\r\\n(1) ⇒ (3). Assuming statement (1), we first fix any i in {1, · · · , N}, consider a specific portfolio\\r\\nπ among the top n stocks defined as in (2.44), and recall the notation Xπ ≡ Xeias well as\\r\\nRπ ≡ Rei. Then, the processes\\r\\nX\\r\\nρ\\r\\nρ+ei\\r\\n=\\r\\nX\\r\\nρ+ei\\r\\nXρ\\r\\n, Xρ\\r\\nρ−ei\\r\\n=\\r\\nX\\r\\nρ−ei\\r\\nXρ\\r\\n,\\r\\n14\\nare supermartingales. In view of Proposition 3.1 along with (2.45), all processes\\r\\nL(X\\r\\nρ\\r\\nρ+ei\\r\\n) = R\\r\\nρ\\r\\nρ+ei\\r\\n= Rei − Ceiρ, L(X\\r\\nρ\\r\\nρ−ei\\r\\n) = R\\r\\nρ\\r\\nρ−ei\\r\\n= −(Rei − Ceiρ),\\r\\nare local supermartingales, implying that Rei − Ceiρ is a local martingale. Since i ∈ {1, · · · , N}\\r\\ncan be chosen arbitrarily, we arrive at statement (3).\\r\\nRemark 3.6 (Representation of wealth relative to the supermartingale num´eraire). When ρ ∈\\r\\nI(R) ∩ T (n) is a supermartingale num´eraire portfolio among the top n stocks, statement (3)\\r\\nof Proposition 3.5 implies that Rei − Ceiρ = Mfiis a local martingale for all i = 1, · · · , N. Then,\\r\\nProposition 3.1 with the notation (3.4) yields the following representation of the relative wealth\\r\\nprocess X\\r\\nρ\\r\\nπ for any portfolio π ∈ I(R) ∩ T (n) among the top n stocks, namely,\\r\\nX\\r\\nρ\\r\\nπ = E\\r\\n\\x12 Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=0\\r\\nπi(t)dReρ\\r\\ni\\r\\n(t)\\r\\n\\x13\\r\\n= E\\r\\n\\x12\\r\\nReρ\\r\\n0 +\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dMfi(t)\\r\\n\\x13\\r\\n= E\\r\\n\\x12 Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\n\\r\\nπi(t) − ρi(t)\\r\\n\\x01\\r\\ndMfi(t)\\r\\n\\x13\\r\\n= 1 + Z ·\\r\\n0\\r\\nXρ\\r\\nπ\\r\\n(t)\\r\\nX\\r\\nN\\r\\ni=1\\r\\n\\r\\nπi(t) − ρi(t)\\r\\n\\x01\\r\\ndMfi(t),\\r\\nwhere the second-last equality is from (3.10). Thus, the relative wealth process X\\r\\nρ\\r\\nπ is a stochastic\\r\\nintegral with respect to the local martingale vector Mf, defined in (2.36).\\r\\nRemark 3.7 (Equivalent condition of statement (3)). The statement (3) of Proposition 3.5 can\\r\\nbe reformulated using the ‘rate processes’ αe, ec of (2.42), namely,\\r\\nZ ·\\r\\n0\\r\\nαei(t)dO(t) = Aei = Ceiρ =\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\nj=1\\r\\nρj (t)dCei,j (t) = Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\nj=1\\r\\nρj (t)eci,j (t)dO(t),\\r\\nwith the help of (2.46). Thus, we have the following statement (3) of Proposition 3.5, namely f\\r\\n(3) f′αe = ecρ, (P ⊗ O) − a.e. (3.11)\\r\\nin matrix notation. In the same manner, the statement (3) of Proposition 3.4 also has the\\r\\nequivalent formulation:\\r\\n(3)′ α = cρ, (P ⊗ O) − a.e. (3.12)\\r\\n3.3 Structural conditions\\r\\nIn this subsection, we present another equivalent requirement for statement (3) of Proposi\\x02tion 3.5, in the form of what we call ‘structural conditions’. First, we note that ec of (2.50) is a\\r\\nsingular symmetric matrix, thus not invertible, from the fact that D is singular. Before proceed\\x02ing to the next result, we need the following definition of ‘pseudo-inverse’ for the matrix-valued\\r\\nprocess ec of (2.41):\\r\\nec\\r\\n†\\r\\n:= lim m→∞ \\x10ec +\\r\\n1\\r\\nm\\r\\nI\\r\\n\\x01−2\\r\\nec\\r\\n\\x11\\r\\n, (3.13)\\r\\nwhere I is the identity operator on R\\r\\nN . This process ec† will play the role of ‘pseudo-inverse’ for\\r\\nec, because it is easily checked that\\r\\n15\\n(a) ec\\r\\n†\\r\\nis the inverse of ec when restricted on range(ec),\\r\\n(b) ec ec\\r\\n†\\r\\ncoincides with the projection operator of R\\r\\nN onto range(ec),\\r\\n(c) ec\\r\\n†\\r\\nis predictable, since matrix inversion is a continuous operation when restricted to strictly\\r\\npositive-definite matrices.\\r\\nWe are now ready to present the structural conditions.\\r\\nProposition 3.8. The existence of the supermartingale num´eraire portfolio among the top n\\r\\nstocks, is equivalent to the conjunction of the conditions:\\r\\n(i) αe ∈ range(ec), (P ⊗ O) − a.e., (3.14)\\r\\n(ii)\\r\\nZ T\\r\\n0\\r\\nαe\\r\\n′\\r\\n(t)ec\\r\\n†\\r\\n(t)αe(t)dO(t) < ∞, for any T ≥ 0. (3.15)\\r\\nProof. First, we assume that the supermartingale num´eraire portfolio ρ among the top n stocks\\r\\nexists; then, from statement (3’) of (3.11), the identity αe = ecρ holds. The condition (i) follows\\r\\nimmediately, and we obtain ec ec\\r\\n†αe = αe from the property (b) above. This also implies that the\\r\\nset {αe ∈ range(ec)} is predictable. We set the predictable process\\r\\nν := ec\\r\\n†α, e (3.16)\\r\\nwhich is range(ec)-valued in the (P ⊗ O)-a.e. sense, and satisfies αe = ec ν. Then, every super\\x02martingale num´eraire portfolio among the top n stocks should be of the form\\r\\nρ = ν + η = ec\\r\\n†αe + η, (3.17)\\r\\nfor a suitable predictable process η which is in ker(ec), the kernel of ec, (P ⊗ O)-a.e. We have\\r\\necη = 0 and η\\r\\n′αe = 0, thus η is a null portfolio in the sense of Lemma 2.6.\\r\\nOn the other hand, the assumption that the supermartingale num´eraire portfolio among the\\r\\ntop n stocks exists, implies that some N-dimensional process of the form ρ = ec\\r\\n†αe+η of the form\\r\\n(3.17) should be a portfolio, i.e., R-integrable. The integrability condition (2.53) in Lemma 2.7\\r\\nwith the observation\\r\\nρ\\r\\n′ecρ = ρ′ec (ec†αe + η) = ρ′αe = αe′\\r\\nρ = αe\\r\\n′ec†α, e\\r\\ngives the condition (ii).\\r\\nWe next assume the conjunction of conditions (i), (ii) and find the supermartingale num´eraire\\r\\nportfolio among the top n stocks. We define the two predictable processes\\r\\nν := ec\\r\\n†α, e and ρ := Dν = Dec†α, e (3.18)\\r\\nand claim that ρ is the supermartingale num´eraire portfolio among the top n stocks. Thanks\\r\\nto the condition (i), we obtain the identity ecν = ec ec\\r\\n†αe = αe, (P ⊗ O)-a.e. Then, we observe the\\r\\nseries of identities\\r\\nν\\r\\n′\\r\\necν = ν\\r\\n′αe = αe′\\r\\nν = αe\\r\\n′\\r\\nec\\r\\n†α, e (P ⊗ O) − a.e.,\\r\\nas well as\\r\\nρ\\r\\n′\\r\\necρ = ν\\r\\n′DecDν = ν′\\r\\necν = αe\\r\\n′\\r\\nec\\r\\n†α, ρ e′αe = ν′Dαe = ν′αe = αe′\\r\\nec\\r\\n†α, e (P ⊗ O) − a.e. (3.19)\\r\\n16\\nHere, we used the identities Dαe = αe, and DecD = ec which can be obtained from (2.50). Combin\\x02ing equations of (3.19) with the condition (ii) yields the integrability condition (2.53) for ρ ≡ π\\r\\nin Lemma 2.7, i.e., ρ ∈ I(R). Also, from the construction (3.18), we have Dρ = DDν = Dν = ρ,\\r\\nthus ρ ∈ T (n). Therefore, we have shown that ρ is a portfolio among the top n stocks, i.e.,\\r\\nρ ∈ I(R) ∩ T (n).\\r\\nFurthermore, we deduce\\r\\necρ = ecDν = ecν = ecec\\r\\n†αe = α, e (P ⊗ O) − a.e., (3.20)\\r\\nwhere the second equation uses the identity ecD = ec, a consequence of (2.50) and of the fact that\\r\\nD is idempotent. Thus, we have obtained the condition (3.11), which is equivalent to statement\\r\\n(3) of Proposition 3.5, and ρ is indeed the supermartingale num´eraire portfolio among the top\\r\\nn stocks.\\r\\nThe conjunction of the two conditions in Proposition 3.8 can be formulated as one equivalent\\r\\ncondition, as follows. We first recall the ‘growth rate’ γπ of the portfolio π ∈ I(R)∩T (n) among\\r\\nthe top n stocks in (2.51). We denote R\\r\\nN ∩ T (n) the collection of elements in RN such that\\r\\nat most n components are nonzero; then π(t) takes values in R\\r\\nN ∩ T (n) for each t ≥ 0, by the\\r\\nproperty (2.16). Let us define the [0,∞]-valued process\\r\\neg := sup\\r\\np∈RN\\r\\n\\x10\\r\\np\\r\\n′αe −\\r\\n1\\r\\n2\\r\\np\\r\\n′ecp\\x11\\r\\n= sup\\r\\np∈RN ∩T (n)\\r\\n\\x10\\r\\np\\r\\n′αe −\\r\\n1\\r\\n2\\r\\np\\r\\n′ecp\\x11\\r\\n. (3.21)\\r\\nThe last equality follows because of the identities\\r\\np\\r\\n′αe −\\r\\n1\\r\\n2\\r\\np\\r\\n′ecp = p′Dαe −\\r\\n1\\r\\n2\\r\\np\\r\\n′DecDp = pe′αe −\\r\\n1\\r\\n2\\r\\npe\\r\\n′ecp, e\\r\\nvalid for any p ∈ R\\r\\nN , where pe := Dp ∈ RN ∩ T (n), by recalling the properties αe = Dαe and\\r\\nec = DecD which can be deduced from (2.50). This process eg can be interpreted as the maximal\\r\\ngrowth rate achievable for all portfolios among the top n stocks. Note that ge is predictable,\\r\\nbecause the supremum can be restricted over a countable, dense subset of R\\r\\nN . We then easily\\r\\nrewrite the process eg in the form\\r\\nge =\\r\\n1\\r\\n2\\r\\n\\r\\nαe\\r\\n′\\r\\nec\\r\\n†αe\\r\\n\\x01\\r\\n1{αe∈range(ec)} + ∞1{α / e∈range(ec)}, (3.22)\\r\\nand the supremum of (3.21) is attained if and only if eg < ∞, at p ≡ ρ := Dec\\r\\n†αe as in (3.18) and\\r\\n(3.19). Then, the conjunction of conditions (i) + (ii) in Proposition 3.8 becomes simply\\r\\nGe(T) < ∞, for all T ≥ 0, (3.23)\\r\\nwhere Ge is an adapted nondecreasing process\\r\\nGe := Z ·\\r\\n0\\r\\nge(t)dO(t). (3.24)\\r\\nWe call this Ge the aggregate maximal growth from portfolios among the top n stocks; and say\\r\\nthat the market consisting of the top n stocks has locally finite growth, if the process Ge satisfies\\r\\nthe condition (3.23). We formalize this argument into the next proposition.\\r\\n17\\nProposition 3.9. The requirement of (3.23) of locally finite growth among the top n stocks, is\\r\\nequivalent to the conjunction of the two conditions (i) + (ii) of Proposition 3.8, thus sufficient\\r\\nand necessary for a supermartinagle num´eraire portfolio among the top n stocks to exist. In this\\r\\ncase, we have\\r\\nGe = Γρ,\\r\\nwhere ρ is a supermartingale num´eraire portfolio among the top n stocks.\\r\\nWe present the following results which will be used later.\\r\\nLemma 3.10. Suppose the market has locally finite growth among the top n stocks, i.e., that\\r\\n(3.23) holds, and let ρ be the supermartingale num´eraire portfolio among the top n stocks. Re\\x02calling (2.51), (2.32), (2.27), and (2.37), we have\\r\\nGe = Γρ =\\r\\n1\\r\\n2\\r\\nCρρ,\\r\\nas well as the representation\\r\\n1\\r\\nXρ\\r\\n= E(−Mρ). (3.25)\\r\\nProof. As with (3.18) in the proof of Proposition 3.8, the supermartingale num´eraire portfolio ρ\\r\\namong the top n stocks is of the form Dec\\r\\n†αe. With (3.19), the claim Ge = Γρ is easily obtained.\\r\\nFurthermore, again by (3.19) with (2.51), we have\\r\\nγρ = ρ\\r\\n′αe −\\r\\n1\\r\\n2\\r\\nρ\\r\\n′\\r\\necρ =\\r\\n1\\r\\n2\\r\\nρ\\r\\n′\\r\\necρ =\\r\\n1\\r\\n2\\r\\ncρρ = eg (3.26)\\r\\nthus Γρ =\\r\\n1\\r\\n2Cρρ, as well as Aρ = Cρρ. We then write (3.5), (3.6) with π ≡ (0, · · · , 0) ∈\\r\\nI(R) ∩ T (n):\\r\\n1\\r\\nXρ\\r\\n= Xρ\\r\\nπ = E(Reρ\\r\\n0\\r\\n) = E(Cρρ − Rρ) = E(Cρρ − Aρ − Mρ) = E(−Mρ).\\r\\nLemma 3.11. Let ρ be the supermartingale num´eraire portfolio among the top n stocks. For\\r\\nany investment strategy ϑ ∈ I(S) ∩ T (n) among the top n stocks, and for any initial capital\\r\\nx ≥ 0, let us recall the wealth process X ≡ X(·; x, ϑ) generated by ϑ and x in the manner of\\r\\n(2.14). Then there exists a process η = (η1, · · · , ηN ) ∈ I(Mf) ∩ T (n), such that\\r\\nX\\r\\nXρ\\r\\n= x +\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nηi(t)dMfi(t). (3.27)\\r\\nConversely, for any x ≥ 0 and η ∈ I(M) ∩ T (n), there exists a process ϑ ∈ I(S) ∩ T (n) such\\r\\nthat (3.27) holds.\\r\\n18\\nProof. From (3.25) and (2.37), we have d\\r\\n\\r\\n1/Xρ(t)\\r\\n\\x01\\r\\n=\\r\\n\\r\\n1/Xρ(t)\\r\\n\\x01 PN\\r\\ni=1 \\r\\n− ρi(t)\\r\\n\\x01\\r\\ndMfi(t), as well\\r\\nas the dynamics\\r\\ndX(t) = X\\r\\nN\\r\\ni=1\\r\\nϑi(t)dSi(t) = X\\r\\nN\\r\\ni=1\\r\\nϑi(t)Si(t)dRei(t) = X\\r\\nN\\r\\ni=1\\r\\nϑi(t)Si(t)\\r\\n\\r\\ndAei(t) + dMfi(t)\\r\\n\\x01\\r\\n,\\r\\nfrom (2.35). Combining two equations via Itˆo’s formula, we obtain\\r\\nd\\r\\n\\r\\nX(t)/Xρ(t)\\r\\n\\x01\\r\\n=\\r\\nX\\r\\nN\\r\\ni=1\\r\\nϑi(t)Si(t)\\r\\nXρ(t)\\r\\n\\r\\ndAei(t) + dMfi(t)\\r\\n\\x01\\r\\n+\\r\\nX(t)\\r\\nXρ(t)\\r\\nX\\r\\nN\\r\\ni=1\\r\\n\\r\\n− ρi(t)\\r\\n\\x01\\r\\ndMfi(t)\\r\\n+\\r\\nX\\r\\nN\\r\\ni=1\\r\\nX\\r\\nN\\r\\nj=1\\r\\nϑi(t)Si(t)\\r\\nXρ(t)\\r\\n\\r\\n− ρj (t)\\r\\n\\x01\\r\\nd[Mfi, Mfj ](t).\\r\\nHere, the finite variation terms vanish because of the relationship dAei(t) = PN\\r\\nj=1 ρj (t)d[Mfi\\r\\n, Mfj ](t)\\r\\nfor i = 1, · · · , N, which is valid on the strength of condition (3) in Proposition 3.5. Thus, by f\\r\\nsetting\\r\\nηi(t) := ϑi(t)Si(t) − X(t)ρi(t)\\r\\nXρ(t)\\r\\n, i = 1, · · · , N,\\r\\nit is straightforward to check η ∈ T (n), and the result follows. The converse can be easily shown\\r\\nby reversing the above procedure.\\r\\n3.4 Local martingale deflator and market viability\\r\\nDefinition 3.12 (Local martingale deflator). We call an adapted, right-continuous with left\\x02limited process Y , a local martingale deflator among the top n stocks, if it satisfies Y (0) = 1,\\r\\nY > 0, and the process Y X is a local martingale for every X ∈ X n of Definition 2.4. We denote\\r\\nby Y\\r\\nn the collection of all local martingale deflators among the top n stocks.\\r\\nSince X ≡ 1 ∈ X n, every deflator Y ∈ Y\\r\\nn\\r\\nis in particular local martingale.\\r\\nDefinition 3.13 (cumulative withdrawal stream). We denote by K the collection of all non\\x02decreasing, adapted and right-continuous processes K with K(0) = 0. Any element K of K is\\r\\ncalled cumulative withdrawal process, and K(t) represents for the cumulative capital withdrawn\\r\\nup to time t ≥ 0; actual withdrawals in each infinitesimal interval (t, t + dt] are represented as\\r\\ndK(t). We say that K ∈ K is nonzero, if P\\r\\n\\r\\nK(∞) > 0\\r\\n\\x01\\r\\n> 0.\\r\\nFor x ≥ 0, ϑ ∈ I(S) or ϑ ∈ I(S) ∩ T (n), the wealth process X(·; x, ϑ) defined in (2.14) is\\r\\nsaid to finance a given cumulative withdrawal process K ∈ K, if X ≥ K holds. In this case, we\\r\\nsay the process K is financeable from the initial capital x ≥ 0 with the investment strategy ϑ.\\r\\nWe denote by K(x), Kn(x) the subset of K consisting of cumulative capital withdrawal\\r\\nprocesses financeable from initial capital x; namely:\\r\\nK(x) := {K ∈ K | ∃ ϑ ∈ I(S) such that X(·; x, ϑ) ≥ K}, (3.28)\\r\\nK\\r\\nn\\r\\n(x) := {K ∈ K | ∃ ϑ ∈ I(S) ∩ T (n) such that X(·; x, ϑ) ≥ K}, (3.29)\\r\\n19\\nWe introduce also the collection of cumulative withdrawal processes in K which can be\\r\\nfinanced starting from any positive initial capital:\\r\\nK(0+) := \\\\\\r\\nx>0\\r\\nK(x) ⊂ K, K\\r\\nn\\r\\n(0+) := \\\\\\r\\nx>0\\r\\nK\\r\\nn\\r\\n(x) ⊂ K. (3.30)\\r\\nDefinition 3.14 (Superhedging capital). For any cumulative withdrawal process K ∈ K, we\\r\\ncall the quantities\\r\\nx(K) := inf{x ≥ 0 | K ∈ K(x)} = inf{x ≥ 0 | ∃ ϑ ∈ I(S) such that X(·; x, ϑ) ≥ K}, (3.31)\\r\\nx\\r\\nn\\r\\n(K) := inf{x ≥ 0 | K ∈ K\\r\\nn\\r\\n(x)} = inf{x ≥ 0 | ∃ ϑ ∈ I(S) ∩ T (n) such that X(·; x, ϑ) ≥ K}\\r\\n(3.32)\\r\\nthe superhedging capital associated with the withdrawal stream K in the entire market, and in\\r\\nthe market consisting of the top n stocks, respectively. We follow here the standard convention\\r\\nthat the infimum of an empty set is equal to infinity.\\r\\nLemma 3.15. Suppose that Y\\r\\nn\\r\\nis nonempty. For a fixed cumulative withdrawal process K ∈\\r\\nK, we assume that it is financeable from the initial capital x ≥ 0 with investment strategy\\r\\nϑ ∈ I(S) ∩ T (n), i.e.,\\r\\nX ≡ X(·; x, ϑ) = x +\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nϑi(t)dSi(t) ≥ K.\\r\\nThen, the process\\r\\nY (X − K) + Z ·\\r\\n0\\r\\nY (t−)dK(t)\\r\\nis a nonnegative local martingale, thus also a supermartingale, for every local martingale deflator\\r\\nY ∈ Y\\r\\nn among the top n stocks. In particular, Y (X − K) is nonnegative supermartingale, for\\r\\nevery Y ∈ Y\\r\\nn\\r\\n. Furthermore, for the quantity x\\r\\nn\\r\\n(K) of (3.32) we have the inequality\\r\\nx\\r\\nn\\r\\n(K) ≥ sup\\r\\nY ∈Yn\\r\\nE\\r\\nP\\r\\n\\x14 Z ∞\\r\\n0\\r\\nY (t−)dK(t)\\r\\n\\x15\\r\\n. (3.33)\\r\\nProof. For every Y ∈ Y\\r\\nn\\r\\n, integration by parts gives\\r\\nY (X − K) = Y X −\\r\\nZ ·\\r\\n0\\r\\nY (t−)dK(t) −\\r\\nZ ·\\r\\n0\\r\\nK(t−)dY (t),\\r\\nthus\\r\\nY (X − K) + Z ·\\r\\n0\\r\\nY (t−)dK(t) = Y X −\\r\\nZ ·\\r\\n0\\r\\nK(t−)dY (t). (3.34)\\r\\nBoth terms on the right-hand side of (3.34) are local martingales, and the terms on the left hand\\r\\nside of (3.34) are nonnegative; thus the first claim follows. Also, the process R ·\\r\\n0\\r\\nY (t−)dK(t) is\\r\\nnondecreasing, therefore Y (X − K) is nonnegative supermartingale. We denote the left hand\\r\\nside of (3.34) by Q := Y (X − K) + R ·\\r\\n0\\r\\nY (t−)dK(t), then we obtain\\r\\nQ(0) = x ≥ E\\r\\nP\\r\\nh\\r\\nQ(∞)\\r\\ni\\r\\n≥ E\\r\\nP\\r\\nh Z ∞\\r\\n0\\r\\nY (t−)dK(t)\\r\\ni\\r\\n.\\r\\nBy taking the supremum over Y ∈ Y\\r\\nn and then the infimum over the initial capital x ≥ 0, the\\r\\nlast claim follows.\\r\\n20\\nDefinition 3.16 (Viability). We say that the entire market is viable if, whenever x(K) = 0\\r\\nholds for some cumulative withdrawal process K ∈ K, we have K ≡ 0.\\r\\nIn the same manner, we say the market consisting of the top n stocks is viable, if whenever\\r\\nx\\r\\nn\\r\\n(K) = 0 holds for some cumulative withdrawal process K ∈ K, we have K ≡ 0.\\r\\nThe viability of the market consisting of the top n stocks, is actually equivalent to the\\r\\nidentity\\r\\nK\\r\\nn\\r\\n(0+) = {0}; (3.35)\\r\\nwhereas the failure of such viability implies the strict inclusion Kn(0+) ⊃ {0}. When the\\r\\nviability of the market consisting of the top n stocks fails, there exists a nonzero cumulative\\r\\nwithdrawal process K ∈ K which is financeable from any initial capital x > 0, no matter how\\r\\nminuscule; or equivalently, there exists an investment strategy ϑm ∈ I(R)∩T (n) for each m ∈ N,\\r\\nsuch that\\r\\nX(·;\\r\\n1\\r\\nm\\r\\n, ϑm) ≥ K. (3.36)\\r\\nWe further present the following lemma; it can be proven in the same manner as Exercise 2.22\\r\\nof Karatzas and Kardaras (2020).\\r\\nLemma 3.17. The market consisting of the top n stocks fails to be viable if, and only if,\\r\\nthere exist a real number T ≥ 0 and a nonnegative F(T)-measurable random variable h with\\r\\nP[h > 0] > 0 such that for every m ∈ N, there exists an Xm ∈ X n with Xm(T) ≥ mh.\\r\\nThe following result presents another equivalent characterization of viability for the market\\r\\nconsisting of the top n stocks.\\r\\nProposition 3.18 (Boundedness in probability). The market consisting of the top n stocks is\\r\\nviable if, and only if,\\r\\nlim m→∞sup\\r\\nX∈X n\\r\\nP[X(T) > m] = 0, ∀ T ≥ 0. (3.37)\\r\\nProof. We first assume that the market consisting of the top n stocks is not viable. Then,\\r\\nfrom Lemma 3.17, there exist a real number T ≥ 0, a nonnegative F(T)-measurable random\\r\\nvariable h with P[h > 0] > 0, and a sequence (Xm)m∈N of wealth processes Xm ∈ X nsatisfying\\r\\nXm(T) ≥ mh. Pick ǫ > 0 sufficiently small, so that P[h > ǫ] > ǫ holds. We then have\\r\\nlim inf\\r\\nm→∞\\r\\nP[X\\r\\nm(T) > ǫm] ≥ lim inf\\r\\nm→∞\\r\\nP[X\\r\\nm(T) > mh, h > ǫ] ≥ ǫ,\\r\\nthus the condition (3.37) is violated.\\r\\nConversely, we assume that for some T ≥ 0, there exist ǫ > 0 and a sequence (Xm)m∈N ⊂ X n\\r\\nsuch that P[Xm(T) > m2\\r\\nm] > ǫ hold for all m ∈ N. Consider the set\\r\\nH := \\\\∞\\r\\nm=1\\r\\n[∞\\r\\nk=m\\r\\n\\x08\\r\\nX\\r\\nk\\r\\n(T) > k2\\r\\nk\\r\\n\\t\\r\\n∈ F(T),\\r\\nand note that P(H) ≥ ǫ. For every m ∈ N, the inclusion\\r\\nH ⊆\\r\\n[∞\\r\\nk=m+1\\r\\n{Xk(T) > k2\\r\\nk\\r\\n}\\r\\n21\\nholds, so there exists a sufficiently large number Km > m such that the set\\r\\nHm := H ∩\\r\\n\\x10\\r\\nK\\r\\n[m\\r\\nk=m+1\\r\\n{X\\r\\nk\\r\\n(T) > k2\\r\\nk\\r\\n}\\r\\n\\x11\\r\\n∈ F(T)\\r\\nsatisfies P[H \\\\ Hm] ≤\\r\\nP[H]\\r\\n2m+1 . Then, the countable intersection\\r\\nE := \\\\∞\\r\\nm=1\\r\\nHm ∈ F(T)\\r\\nis a subset of H, and we have\\r\\nP[H \\\\ E] = P\\r\\nh [∞\\r\\nm=1\\r\\n(H \\\\ Hm)\\r\\ni\\r\\n≤\\r\\nX∞\\r\\nm=1\\r\\nP[H]\\r\\n2m+1 =\\r\\nP[H]\\r\\n2\\r\\n,\\r\\nthus, P[E] ≥\\r\\nP[H]\\r\\n2\\r\\nand P[E] ≥\\r\\nǫ\\r\\n2 > 0. Let us define a sequence of num´eraires (Ξm)m∈N\\r\\nΞ\\r\\nm := X\\r\\nKm\\r\\nk=m+1\\r\\n2\\r\\n−(k−m)Xk\\r\\n, for each m ∈ N,\\r\\nand it is straightforward that Ξm ∈ X n, as all Xk ∈ X nfor k ∈ N. Furthermore, for every m ∈ N,\\r\\nwe have E ⊆ Hm ⊆ {Ξ\\r\\nm(T) > m}, from which Ξm(T) ≥ m1E follows. Set h := 1E ∈ F(T),\\r\\nthen\\r\\nP[h > 0] = P[E] ≥\\r\\nǫ\\r\\n2\\r\\n> 0.\\r\\nLemma 3.17 yields that the market consisting of the top n stocks is not viable.\\r\\nWe are now ready to state and prove the main result of this section.\\r\\nTheorem 3.19. The following statements are equivalent:\\r\\n(1) The market consisting of the top n stocks is viable.\\r\\n(2) There exists a local martingale deflator among the top n stocks, i.e., Y\\r\\nn 6= ∅.\\r\\n(3) The supermartingale num´eraire among the top n stocks exists.\\r\\n(4) The market consisting of the top n stocks has locally finite growth; namely, the condition\\r\\n(3.23) of the aggregate maximal growth process Ge among the top n stocks of (3.24) holds.\\r\\nProof. The implication (4) ⇒ (3) follows from Proposition 3.9. The implication (3) ⇒ (2)\\r\\nalso follows easily, because the supermartingale num´eraire among the top n stocks is a local\\r\\nmartingale num´eraire among the top n stocks from Proposition 3.5, and the reciprocal of the\\r\\nlocal martingale num´eraire among the top n stocks is a local martingale deflator among the top\\r\\nn stocks.\\r\\nIn order to prove (2) ⇒ (1), let Y ∈ Y\\r\\nn be a local martingale deflator and pick a cumulative\\r\\nwithdrawal process K ∈ K such that x\\r\\nn\\r\\n(K) = 0. From (3.33) of Lemma 3.15, we have\\r\\nE\\r\\nP\\r\\n\\x14 Z ∞\\r\\n0\\r\\nY (t−)dK(t)\\r\\n\\x15\\r\\n= 0.\\r\\n22\\nSince Y is strictly positive and K is nondecreasing with K(0) = 0, it follows that K(∞) = 0\\r\\nholds P-a.e., which is equivalent to K ≡ 0. The market consisting of the top n stocks is then\\r\\nviable.\\r\\nThe remaining part is to show the implication (1) ⇒ (4), which is quite technical. Suppose\\r\\nthat the market fails to have locally finite growth among the top n stocks, i.e., one of the\\r\\nstructural conditions (3.14), (3.15) is violated. Thus, we need to consider two cases:\\r\\n(A) the set {αe 6∈ range(ec)} fails to be (P ⊗ O)-null,\\r\\n(B) the set {αe 6∈ range(ec)} is (P ⊗ O)-null, but P[Ge(T) = ∞] > 0 holds for some T > 0.\\r\\nWe shall show that the market is not viable in each of the cases (A) and (B) below.\\r\\n∗ Case (A). Recalling the notation (3.13) with its properties (a)-(c), we first note that the\\r\\npredictable process\\r\\nϕ :=\\r\\n1\\r\\n||αe − ecec\\r\\n†αe||2\\r\\n\\r\\nαe − ecec\\r\\n†αe\\r\\n\\x01\\r\\n1{αe6∈range(ec)}, (3.38)\\r\\nis well-defined, because αe 6∈ range(ec) holds if and only if ecec\\r\\n†αe 6= αe. Note that Dϕ = ϕ, thus\\r\\nϕ ∈ T (n), thanks to the properties Dαe = αe, Dec = ec from (2.50). Since the process αe − ecec\\r\\n†αe is\\r\\northogonal to range(ec), we have ecϕ = 0. Furthermore, we have ϕ\\r\\n′αe = 1{αe6∈range(ec)}\\r\\n, because\\r\\n(αe − ecec\\r\\n†αe)′αe = ||αe − ecec†αe||2 + (αe − ecec†αe)′\\r\\n(ecec\\r\\n†αe) = ||αe − ecec†αe||2\\r\\n.\\r\\nThus, from Lemma 2.7, ϕ is a portfolio among the top n stocks, i.e., ϕ ∈ I(R)∩T (n). Also, the\\r\\nlocal martingale vanishes: R ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)dMf(t) ≡ 0, because its quadratic variation process vanishes\\r\\nh Z ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)dMf(t)\\r\\ni\\r\\n=\\r\\nZ ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)ec(t)ϕ(t)dO(t) ≡ 0. (3.39)\\r\\nThus,\\r\\nZ ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)dRe(t) = Z ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)dAe(t) = Z ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)αe(t)dO(t) = Z ·\\r\\n0\\r\\n1{αe6∈range(ec)}(t)dO(t) =: K.\\r\\nWe define the vector process ϑ ≡ (ϑ1, · · · , ϑN ) with components given by ϑi = ϕi/Sifor i =\\r\\n1, · · · , N. It is then easy to check that mϑ is an investment strategy among the top n stocks,\\r\\ni.e., mϑ ∈ I(S) ∩ T (n), for any m ∈ N, and\\r\\nX(·; 0, mϑ) = Z ·\\r\\n0\\r\\nmϑ′(t)dS(t) = m\\r\\nZ ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)dR(t) = m\\r\\nZ ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)dRe(t) = mK.\\r\\nIn other words, for any m ∈ N, the wealth process generated by the investment strategy mϑ\\r\\namong the top n stocks has vanishing local martingale part, and is equal to the non-trivial,\\r\\nnondecreasing part mK of finite variation. This process mK can be arbitrarily scaled by the\\r\\nmultiplicative constant m ∈ N, and thus x\\r\\nn\\r\\n(K) = 0, by recalling (3.32). We conclude that the\\r\\nmarket consisting of the top n stocks is not viable.\\r\\n∗ Case (B). We assume that the set {αe 6∈ range(ec)} is (P ⊗ O)-null, but P[Ge(T) = ∞] > 0\\r\\nholds for some T > 0. In this case, the aggregate maximal growth process Ge of (3.24) becomes\\r\\nGe =\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nαe\\r\\n′\\r\\n(t)ec\\r\\n†\\r\\n(t)αe(t)dO(t). (3.40)\\r\\n23\\nWe consider first the portfolio ρ := Dec\\r\\n†αe ∈ T (n) as in (3.18), and also set ρm := ρ1{||ρ||≤m} ∈\\r\\nI(R) ∩ T (n). The log-wealth process of (2.28) can be represented, with the help of (2.37) and\\r\\n(3.19), as\\r\\nlog Xρm =\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\n1{||ρ(t)||≤m}ρ\\r\\n′\\r\\n(t)ec(t)ρ(t)dO(t) + Z ·\\r\\n0\\r\\n1{||ρ(t)||≤m}ρ\\r\\n′\\r\\n(t)dMf(t). (3.41)\\r\\nNote that the first integral on the right-hand side of (3.41), namely\\r\\n2G\\r\\nm := Z ·\\r\\n0\\r\\n1{||ρ(t)||≤m}ρ\\r\\n′\\r\\n(t)ec(t)ρ(t)dO(t),\\r\\nis the quadratic variation of the local martingale R ·\\r\\n0\\r\\n1{||ρ(t)||≤m}ρ\\r\\n′\\r\\n(t)dMf(t), which is the sec\\x02ond integral on the right-hand side of (3.41). The Dambis-Dubins-Schwarz representation (cf.\\r\\nTheorem 3.4.6 and Problem 3.4.7 of Karatzas and Shreve (1991)), with the scaling property\\r\\nof Brownian motion, implies that there exists a Brownian motion Wm, on a possibly enlarged\\r\\nfiltered probability space, such that\\r\\nlog Xρm = G\\r\\nm +\\r\\n√\\r\\n2Wm(G\\r\\nm), (3.42)\\r\\nfor every m ∈ N. The sequence {Gm(T)}m∈N is nondecreasing and converges to\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nρ\\r\\n′\\r\\n(t)ec(t)ρ(t)dO(t) = 1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nαe\\r\\n′\\r\\n(t)ec\\r\\n†\\r\\n(t)αe(t)dO(t) = Ge(T),\\r\\nas in (3.40), again with the help of (3.19). The strong law of large numbers for Brownian motion\\r\\ngives\\r\\nlim m→∞P\\r\\n\\x14Wm\\r\\n\\r\\nGm(T)\\r\\n\\x01\\r\\nGm(T)\\r\\n≤ −\\r\\n1\\r\\n2\\r\\n√\\r\\n2\\r\\n, Ge(T) = ∞\\r\\n\\x15\\r\\n= 0.\\r\\nFrom the representation (3.42), we obtain\\r\\nlim m→∞P\\r\\n\\x14\\r\\nlog Xρm (T)\\r\\nGm(T)\\r\\n≤\\r\\n1\\r\\n2\\r\\n, Ge(T) = ∞\\r\\n\\x15\\r\\n= 0.\\r\\nTherefore, in case (B), the collection of random variables {Xρm (T) | m ∈ N} ⊆ {X(T) | X ∈ X n}\\r\\nfails to be bounded in probability, and Proposition 3.18 concludes that the market consisting of\\r\\nthe top n stocks is not viable.\\r\\n3.5 Growth optimality and relative log-optimality\\r\\nThe results in the previous subsection characterize the supermartingale num´eraire portfolio\\r\\namong the top n stocks, via the ‘structural condition’, in terms of αe and ec. More specifically, in\\r\\nthe argument leading to Proposition 3.8 and in the proof of Lemma 3.10, the maximal growth\\r\\nrate among the top n stocks eg of (3.21) is attained when the portfolio is the supermartingale\\r\\nnum´eraire portfolio among the top n stocks, as in (3.26). In this subsection, we reformulate\\r\\nthis property and show that the supermartingale num´eraire portfolio is ‘optimal’ in some sense\\r\\namong portfolios of top n stocks.\\r\\n24\\nDefinition 3.20 (Relative growth and growth optimality). We define the relative growth of a\\r\\ngiven portfolio π ∈ I(R) with respect to another portfolio ρ ∈ I(R) as\\r\\nΓ\\r\\nρ\\r\\nπ\\r\\n:= Γπ − Γρ, (3.43)\\r\\nnamely, the difference between the finite variation process of the log-relative wealth process\\r\\nlog X\\r\\nρ\\r\\nπ = log(Xπ/Xρ) from (2.29), (3.1).\\r\\nWe call a portfolio ρ ∈ I(R) ∩ T (n) growth-optimal among the top n stocks, if for every\\r\\nportfolio π ∈ I(R) ∩ T (n) the process Γρ\\r\\nπ = Γπ − Γρ is non-increasing.\\r\\nProposition 3.21. A portfolio is growth-optimal among the top n stocks, if and only if it is a\\r\\nsupermartingale num´eraire portfolio among the top n stocks.\\r\\nProof. (i) Let us first assume that ρ ∈ I(R) ∩ T (n) is the supermartingale num´eraire portfolio\\r\\namong the top n stocks. From Proposition 3.8 and (3.11), we know that αe ∈ range(ec) and\\r\\nαe = ecρ hold (P ⊗ O) − a.e. Recalling (2.51), (3.21) and the fact that the supremum of g is\\r\\nattained at the supermartingale num´eraire portfolio among the top n stocks, γρ = ge ≥ γπ holds\\r\\n(P ⊗ O) − a.e. for every π ∈ I(R) ∩ T (n). Thus, ρ is growth-optimal.\\r\\n(ii) Next, we assume that ν ∈ I(R) ∩ T (n) is a growth-optimal portfolio among the top\\r\\nn stocks. We pick a portfolio ϕ ∈ I(R) ∩ T (n) satisfying ecϕ = 0 and ϕ\\r\\n′αe = 1 on the set\\r\\n{αe 6∈ range(ec)} (for example, as in (3.38) in the proof of Theorem 3.19). We then have\\r\\nγν+ϕ = γν + 1 on {αe 6∈ range(ec)} from (2.51), violating the growth-optimality of ν. This\\r\\nimplies that the latter set is (P ⊗ O)-null. In particular, g <e ∞ in the (P ⊗ O) − a.e. sense, from\\r\\n(3.22).\\r\\nOn the other hand, we let ρ := Dec\\r\\n†αe ∈ T (n) and define ρm := ρ1{||ρ||≤m} ∈ I(R)∩T (n) for\\r\\nm ∈ N. The equation (3.26) yields γν ≥ γρm = eg1{||ρ||≤m}, and thus γν ≥ ge holds (P ⊗ O) − a.e.\\r\\nby taking the limit m → ∞. We conclude that ν is also a supermartingale num´eraire portfolio\\r\\namong the top n stocks.\\r\\nThe supermartingale num´eraire portfolio among the top n stocks is ‘optimal’ also in another\\r\\nsense, as follows.\\r\\nDefinition 3.22. A portfolio ρ ∈ I(R) ∩ T (n) is called relatively log-optimal among the top n\\r\\nstocks, if for all portfolios π ∈ I(R) ∩ T (n) and for all stopping times τ of F, we have\\r\\nE\\r\\nP\\r\\n\\x02 log X\\r\\nρ\\r\\nπ\\r\\n(τ )\\r\\n\\x01+\\x03\\r\\n< ∞, and E\\r\\nP\\r\\n\\x02\\r\\nlog X\\r\\nρ\\r\\nπ\\r\\n(τ )\\r\\n\\x03\\r\\n≤ 0. (3.44)\\r\\nProposition 3.23. A portfolio is relatively log-optimal among the top n stocks, if and only if\\r\\nit is a supermartingale num´eraire portfolio among the top n stocks.\\r\\nProof. (i) We first suppose that ρ ∈ I(R) ∩ T (n) is the supermartingale num´eraire portfolio\\r\\namong the top n stocks. Then, we obtain\\r\\nE\\r\\nP\\r\\n\\x02 log X\\r\\nρ\\r\\nπ\\r\\n(τ )\\r\\n\\x01+\\x03\\r\\n=\\r\\nZ ∞\\r\\n0\\r\\nP(X\\r\\nρ\\r\\nπ\\r\\n(τ ) > et)dt ≤\\r\\nZ ∞\\r\\n0\\r\\nP(X\\r\\nρ\\r\\nπ\\r\\n(τ ) > t)dt ≤ E\\r\\nP\\r\\n[X\\r\\nρ\\r\\nπ\\r\\n(τ )] ≤ 1,\\r\\nwhere the last inequality is from the Optional Sampling Theorem. By applying Jensen’s in\\x02equality to this last inequality, the second condition of (3.44) also holds, and we conclude that\\r\\nρ is relatively log-optimal among the top n stocks.\\r\\n25\\n(ii) For the converse implication, we assume that ν ∈ I(R) ∩ T (n) is relatively log-optimal\\r\\namong the top n stocks. As in the proof of Proposition 3.21, we pick a portfolio ϕ ∈ I(R)∩T (n)\\r\\nas in (3.38), satisfying ecϕ = 0 and ϕ\\r\\n′αe = 1 on the set {αe 6∈ range(ec)}. By recalling (2.28),\\r\\n(2.29), (2.32), (2.37), and (2.51), straightforward computations show\\r\\nlog Xν\\r\\nν+ϕ = log Xν+ϕ − log Xν =\\r\\nZ ·\\r\\n0\\r\\n\\r\\nγν+ϕ(t) − γν(t)\\r\\n\\x01\\r\\ndO(t) + Z ·\\r\\n0\\r\\nϕ\\r\\n′\\r\\n(t)dMf(t) (3.45)\\r\\n=\\r\\nZ ·\\r\\n0\\r\\n1{αe6∈range(ec)}\\r\\n(t)dO(t).\\r\\nHere, the last integral on the right-hand side of (3.45) vanishes, because of the equation (3.39)\\r\\nabove. The relative log-optimality of ν implies that the set {αe 6∈ range(ec)} is (P ⊗ O)-null. We\\r\\nthen consider a process ρ := Dec\\r\\n†αe ∈ T (n) of (3.18), as in the proof of Proposition 3.21. Note\\r\\nthat αe = ecρ holds (P⊗O)−a.e. from (3.20), or equivalently, Aei = Ceiρ hold for i = 1, · · · , N, from\\r\\nRemark 3.7. This last requirement implies that Aπ = Cπρ, thus Rπ − Cπρ is local martingale for\\r\\nevery π ∈ I(R) ∩ T (n). We further define\\r\\nν\\r\\nm := ν1{αe=ecν} + ν1{αe6=ecν}1{||ρ||>m} + ρ1{αe6=ecν}1{||ρ||≤m}\\r\\n, for m ∈ N,\\r\\nand it is easy to check that ν\\r\\nm ∈ I(R) ∩ T (n) for all m ∈ N.\\r\\nWe now claim that the ratio Xν/Xνm for every m ∈ N is a local martingale. Proposition 3.1\\r\\nimplies that it is sufficient to show Rνm\\r\\nν = Rπ − Cπνm =: Q is a local martingale, where we set\\r\\nπ := ν − ν\\r\\nm ∈ I(R) ∩ T (n). On the set ζ := {αe 6= ecν, ||ρ|| ≤ m}, we have νm = ρ, thus Q is\\r\\nlocal martingale. On the complement set ζ\\r\\nc\\r\\n, we have π = ν − ν\\r\\nm = 0, thus Q = 0. In other\\r\\nwords, we showed that\\r\\nQ =\\r\\nZ ·\\r\\n0\\r\\n1ζ (t)dQ(t) = Z ·\\r\\n0\\r\\n1ζ (t)d(Rπ − Cπρ)(t)\\r\\nis local martingale, verifying our claim that Xν/Xνm is a local martingale for every m ∈ N. As\\r\\nthe ratio is positive, Xν/Xνm is also a supermartingale.\\r\\nIf we assume that P[Xν(T) 6= Xνm(T)] > 0 were true for some T > 0, we obtain\\r\\nE\\r\\nP\\r\\n\\x14\\r\\nlog Xν\\r\\n(T)\\r\\nXνm(T)\\r\\n\\x15\\r\\n< log E\\r\\nP\\r\\n\\x14\\r\\nXν(T)\\r\\nXνm(T)\\r\\n\\x15\\r\\n≤ 0,\\r\\ncontradicting the relative log-optimality of ν. Thus, we conclude that Xν = Xνm, from the\\r\\ncontinuity of Xν /Xνm, and ν − ν\\r\\nm is a null portfolio in the sense of Lemma 2.6. We then have\\r\\necνm = ecν = ecρ = αe, (P ⊗ O) − a.e, on the set ζ = {αe 6= ecν, ||ρ|| ≤ m} defined above, which\\r\\nimplies that ζ is (P ⊗ O)-null. Since this property is true for every m ∈ N, the identity αe = ecν is\\r\\nvalid (P⊗O)−a.e, thus ν is the supermartingale num´eraire portfolio among the top n stocks.\\r\\nIn part (ii) of the proofs of both Proposition 3.21 and Proposition 3.23, we did not assume the\\r\\nexistence of supermartingale num´eraire portfolio among the top n stocks. Thus, the existence\\r\\nof growth-optimal or relatively log-optimal portfolio among the top n stocks is equivalent to the\\r\\nexistence of the supermartingale num´eraire portfolio among the top n stocks, and we can add\\r\\nthe following two statements to the list of equivalences in Theorem 3.19:\\r\\n(5) A growth-optimal portfolio among the top n stocks exists.\\r\\n(6) A relatively log-optimal portfolio among the top n stocks exists.\\r\\n26\\n3.6 The optional decomposition\\r\\nSuppose that we are given a nonnegative, adapted process with RCLL paths and X(0) = x ≥ 0.\\r\\nIn this subsection, we characterize the condition when X belongs to X n of Definition 2.4, i.e.,\\r\\nwhen X is the wealth process generated by an investment strategy that invests in the top n\\r\\nstocks of the market, and study how can we construct this strategy from X. The following\\r\\nTheorem 3.25 (or Corollary 3.26), which we call the Optional Decomposition Theorem, gives\\r\\nthe answer to this question.\\r\\nWe first present the following result, originally from Theorem 1 of Schweizer (1995). See\\r\\nalso Propositions 2.3 and 3.2 of Larsen and Zitkovi´c (2007). We recall for this purpose the ˇ\\r\\nsemimartingale vector Mf defined in (2.36) and write M⊥\\r\\nloc(Mf) the collection of scalar local\\r\\nmartingales L with RCLL paths, satisfying L(0) = 0 and the orthogonality [L, Mfi] = 0 for all\\r\\ni = 1, · · · , N.\\r\\nLemma 3.24. If the supermartingale num´eraire portfolio ρ among the top n stocks exists, then\\r\\nthe collections Y\\r\\nn of local martingale deflators among the top n stocks, defined in Definition 3.12,\\r\\nadmits the representation:\\r\\nY\\r\\nn =\\r\\nn 1\\r\\nXρ\\r\\nE(L) : L ∈ M⊥\\r\\nloc(Mf) with ∆L > −1\\r\\no\\r\\n. (3.46)\\r\\nIn order to simplify the proof of the Optional Decomposition Theorem, we shall work under\\r\\nthe following assumption. The general case of the Theorem can be proven as in the Subsection\\r\\n3.1.3 of Karatzas and Kardaras (2020).\\r\\nAssumption ⋆ : All local martingales on the filtered probability space (Ω, F, F(·), P) have\\r\\ncontinuous paths.\\r\\nTheorem 3.25 (Optional Decomposition). Suppose that the market consisting of the top n\\r\\nstocks is viable. For a nonnegative, adapted process X with RCLL paths satisfying X(0) = x ≥ 0,\\r\\nthe following statements are equivalent:\\r\\n(1) The process Y X is a supermartingale, for every Y ∈ Y\\r\\nn\\r\\n.\\r\\n(2) There exist an investment strategy ϑ ∈ I(S) ∩ T (n) among the top n stocks, and a cumu\\x02lative withdrawal process K ∈ K, such that\\r\\nX = x +\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nϑi(t)dSi(t) − K. (3.47)\\r\\nProof. We first show the implication (2) =⇒ (1). For any Y ∈ Y\\r\\nn\\r\\n, we write Y = E(L)/Xρ for\\r\\nsome L ∈ M⊥\\r\\nloc(Mf) with ∆L > −1 from Lemma 3.24, where we denote by ρ the supermartingale\\r\\nnum´eraire portfolio among the top n stocks. Then, we have from Lemma 3.11,\\r\\nY X + Y K =\\r\\nx +\\r\\nR ·\\r\\n0\\r\\nPN\\r\\ni=1 ϑi(t)dSi(t)\\r\\nXρ\\r\\nE(L) = \\x10x +\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nηi(t)dMfi(t)\\r\\n\\x11\\r\\nE(L),\\r\\nfor some process η ∈ I(Mf) ∩ T (n). The last expression is a product of two nonnegative,\\r\\northogonal local martingales, thus it is a nonnegative local martingale. The claim that Y X is a\\r\\nsupermartingale follows.\\r\\n27\\nWe now show the implication (1) =⇒ (2) which is more involved, under the above As\\x02sumption ⋆. We assume that (1) holds and recall the collections Y\\r\\nn and M⊥\\r\\nloc(Mf) of (3.46).\\r\\nAll processes in M⊥\\r\\nloc(Mf) have continuous paths under the Assumption ⋆. From Lemma 3.24,\\r\\n(X/Xρ)E(L) is a supermartingale for every L ∈ M⊥\\r\\nloc(Mf), and in particular, X/Xρ is a super\\x02martingale itself. The Doob-Meyer and Kunita-Watanabe decompositions give\\r\\nX\\r\\nXρ\\r\\n= x + Mη + L − B, where Mη := Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nηi(t)dMfi(t).\\r\\nHere, η ≡ (η1, · · · , ηN ) ∈ I(Mf), L ∈ M⊥\\r\\nloc(Mf) and B is an adapted, nondecreasing and right\\x02continuous process with B(0) = 0, i.e., B is a cumulative withdrawal process in K. Recalling the\\r\\ndiagonal matrix D of (2.47) with its property DdMf(t) = dMf(t), we further define ηe := Dη ∈\\r\\nI(Mf) ∩ T (n), and we have\\r\\nMη =\\r\\nZ ·\\r\\n0\\r\\nη\\r\\n′\\r\\n(t)dMf(t) = Z ·\\r\\n0\\r\\n(Dη)\\r\\n′\\r\\n(t)dMf(t) = Mηe.\\r\\nConsequently, we obtain\\r\\nX\\r\\nXρ\\r\\n= x + Mηe + L − B, with ηe ∈ I(Mf) ∩ T (n), L ∈ M⊥\\r\\nloc(Mf). (3.48)\\r\\nWe next show that L ≡ 0 in (3.48). Again from Lemma 3.24, (1/Xρ)E(mL) is a local\\r\\nmartingale, thus (X/Xρ)E(mL) is a supermartingale for every m ∈ N. Since [E(mL), Mfi] = 0\\r\\nfor i = 1, · · · , N, we have [E(mL), Mηe] = 0 and consequently, E(mL)Mηe is a local martingale as\\r\\na product of two orthogonal local martingales. Thus, from (3.48), the process\\r\\nE(mL)(L − B) = E(mL)\\r\\nX\\r\\nXρ\\r\\n− E(mL)(x + Mηe)\\r\\nis a local supermartingale for every m ∈ N. On the other hand, the integration by parts gives\\r\\nE(mL)(L−B) = Z ·\\r\\n0\\r\\n(L−B)(t−)dE(mL)(t)+Z ·\\r\\n0\\r\\nE(mL)(t)dL(t)+Z ·\\r\\n0\\r\\nE(mL)(t)d\\r\\n\\r\\n[mL, L]−B\\r\\n\\x01\\r\\n(t).\\r\\nThen, the last integrator m[L, L] − B should be a local supermartingale for every m ∈ N, which\\r\\nimplies [L, L] ≡ 0, thus L ≡ 0.\\r\\nAs a result, the equation (3.48) becomes\\r\\nX\\r\\nXρ\\r\\n= x + Mηe − B,\\r\\nand we apply the product rule to obtain the decomposition of X = Xρ(X/Xρ):\\r\\nX = x +\\r\\nZ ·\\r\\n0\\r\\nX(t−)ρ\\r\\n′\\r\\n(t)dR(t) + Z ·\\r\\n0\\r\\nXρ(t)d(Mηe − B)(t) + Z ·\\r\\n0\\r\\nXρ(t)dCηρe (t),\\r\\nin conjunction with (2.23) and (2.43). Moreover, the condition (e3) of Proposition 3.5 implies\\r\\nCηρe = Aηe = Rηe − Mηe, and we deduce\\r\\nX = x +\\r\\nZ ·\\r\\n0\\r\\n\\r\\nX(t−)ρ\\r\\n′\\r\\n(t) − Xρ(t)ηe\\r\\n′\\r\\n(t)\\r\\n\\x01\\r\\ndR(t) −\\r\\nZ ·\\r\\n0\\r\\nXρ(t)dB(t).\\r\\n28\\nTherefore, if we define\\r\\nϑi(t) := X(t−)ρ\\r\\n′\\r\\n(t) − Xρ(t)ηe\\r\\n′\\r\\n(t)\\r\\nSi(t)\\r\\n, i = 1, · · · , N, K := Z ·\\r\\n0\\r\\nXρ(t)dB(t),\\r\\nthen it is easy to check that ϑ ≡ (ϑ1, · · · , ϑN ) ∈ I(S) ∩ T (n) and K ∈ K.\\r\\nCorollary 3.26. Suppose that the market consisting of the top n stocks is viable. For a non\\x02negative, adapted process X with RCLL paths satisfying X(0) = x ≥ 0, the following statements\\r\\nare then equivalent:\\r\\n(1) The process Y X is a local martingale, for every Y ∈ Y\\r\\nn\\r\\n.\\r\\n(2) There exists an investment strategy ϑ ∈ I(S) ∩ T (n) among the top n stocks, such that\\r\\nX = x +\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nϑi(t)dSi(t). (3.49)\\r\\nProof. We first assume (1); then Y X is a supermartingale for every Y ∈ Y\\r\\nn\\r\\n. From Theo\\x02rem 3.25, we have a decomposition (3.47) for some ϑ ∈ I(S) ∩ T (n) and K ∈ K. In particular,\\r\\nif we take Y = 1/Xρ, the reciprocal of the local martingale num´eraire, we obtain\\r\\nY K =\\r\\nX(·; x, ϑ)\\r\\nXρ\\r\\n− Y X,\\r\\nwith the notation in (2.14). Since the terms on the right-hand side are local martingales, Y K\\r\\nis a local martingale, and so is\\r\\nY K −\\r\\nZ ·\\r\\n0\\r\\nK(t−)dY (t) = Z ·\\r\\n0\\r\\nY (t)dK(t).\\r\\nHowever, the last integral is nondecreasing and is a supermartingale (as a non-negative local\\r\\nmartingale), and therefore identically equal to zero. Thus, K ≡ 0 as Y is positive, and the\\r\\nstatement (2) follows.\\r\\nIn order to show the reverse implication, we assume (2), then X/Xρ is a local martingale\\r\\nwhere ρ is the local martingale num´eraire portfolio among the top n stocks, as before. From\\r\\nLemma 3.11, X/Xρ can be cast as a stochastic integral with respect to the local martingale\\r\\nvector Mf. Furthermore, from Lemma 3.24, every Y ∈ Y\\r\\nn\\r\\nis of the form Y = (1/Xρ)E(L)\\r\\nfor some local martingale L satisfying [L, Mfi] = 0 for i = 1, · · · , N. Therefore, the product\\r\\nY X = (X/Xρ)E(L) of these two orthogonal local martingales is again a local martingale.\\r\\n29\\n3.7 Entire market versus top n market\\r\\nWe present first the following result, which can be easily proven from the equivalence between\\r\\nthe existence of supermartingale num´eraire portfolio and the market viability.\\r\\nTheorem 3.27. The existence of a supermartingale num´eraire portfolio in the whole market,\\r\\nimplies the existence of supermartingale num´eraire portfolio among the top n stocks.\\r\\nProof. From Theorem 2.34 of Karatzas and Kardaras (2020), the existence of a supermartingale\\r\\nnum´eraire portfolio in the whole market, is equivalent to the viability of the whole market. The\\r\\nviability of the whole market implies the viability of the market consisting of the top n stocks,\\r\\nthanks to the inequality 0 ≤ x(K) ≤ x\\r\\nn\\r\\n(K) in Definition 3.14. We conclude that there exists a\\r\\nsupermartingale num´eraire portfolio among the top n stocks, from Theorem 3.19.\\r\\nTheorem 3.27 shows that the viability of the entire market, composed of N stocks, implies\\r\\nthe viability of the ‘top n market’. Thus, if the entire market is viable, there exist both a super\\x02martingale num´eraire portfolio for the whole market, and a supermartingale num´eraire portfolio\\r\\namong the top n stocks, and the former dominates the latter in the sense of growth-optimality.\\r\\nIn the following proposition, we study this dominance by expressing the asymptotic behavior of\\r\\nlog-relative wealth process between these two portfolios in terms of the ‘local characteristics’ of\\r\\nthe market. We first need the following definitions which are similar to those in (3.21)-(3.24).\\r\\nWe call a [0,∞]-valued, predictable process\\r\\ng := sup\\r\\np∈RN\\r\\n\\x10\\r\\np\\r\\n′α −\\r\\n1\\r\\n2\\r\\np\\r\\n′\\r\\ncp\\x11(3.50)\\r\\nmaximal growth rate achievable in the whole market. This process can be rewritten in the form\\r\\ng =\\r\\n1\\r\\n2\\r\\n\\r\\nα\\r\\n′\\r\\nc\\r\\n†α\\r\\n\\x01\\r\\n1{α∈range(c)} + ∞1{α /∈range(c)}, (3.51)\\r\\nand the supremum of (3.50) is attained if and only if g < ∞, at p ≡ ρ := c\\r\\n†α, i.e., when ρ is\\r\\nthe supermartingale num´eraire portfolio of whole market. Here, c\\r\\n†\\r\\nis the ‘pseudo-inverse’ of c,\\r\\ndefined as in (3.13). Then, the viability of the whole market can be shown to be equivalent to\\r\\nthe condition\\r\\nG(T) := Z T\\r\\n0\\r\\ng(t)dO(t) < ∞, for all T ≥ 0. (3.52)\\r\\nHere, the adapted nondecreasing process G is called as aggregate maximal growth of whole\\r\\nmarket.\\r\\nWhen the whole market is viable, the growth rates eg of (3.22) and g of (3.51) have simpler\\r\\nforms\\r\\nge =\\r\\n1\\r\\n2\\r\\nαe\\r\\n′\\r\\nec\\r\\n†αe = γρe, g =\\r\\n1\\r\\n2\\r\\nα\\r\\n′\\r\\nc\\r\\n†α = γρ, (3.53)\\r\\nrespectively, as from (3.26), with ρe := Dec\\r\\n†αe the supermartingale num´eraire portfolio among\\r\\nthe top n stocks, and ρ := c\\r\\n†α the supermartingale num´eraire portfolio for the whole market.\\r\\nWe denote the difference of aggregate maximal growth between the whole market and the top n\\r\\nmarket by\\r\\nG := G − Ge =\\r\\nZ ·\\r\\n0\\r\\n\\r\\ng(t) − ge(t)\\r\\n\\x01\\r\\ndO(t) = Z ·\\r\\n0\\r\\n\\r\\nγρ(t) − γρe(t)\\r\\n\\x01\\r\\ndO(t) = Γρ − Γρe. (3.54)\\r\\n30\\nSince ρ is also a growth-optimal portfolio (as a supermartingale num´eraire portfolio) in the whole\\r\\nmarket, the relative growth Γρ\\r\\nρe = Γρe − Γρ of Definition 3.20 is non-increasing, from which we\\r\\nconclude that G is nondecreasing and nonnegative.\\r\\nProposition 3.28. Suppose that the whole market is viable and let ρ and ρe be the supermartin\\x02gale num´eraire portfolio for the whole market and the supermartingale num´eraire portfolio among\\r\\nthe top n stocks, respectively. Then, the asymptotic growth rate of the log-relative wealth process\\r\\nlog X\\r\\nρ\\r\\nρe\\r\\nis the same as −G of (3.54), namely:\\r\\nlim\\r\\nT→∞\\r\\n1\\r\\nG(T)\\r\\nlog \\x12\\r\\nXρe(T)\\r\\nXρ(T)\\r\\n\\x13\\r\\n= −1 holds P − a.e. on the set { lim\\r\\nT→∞\\r\\nG(T) = ∞}. (3.55)\\r\\nProof. We recall the notations (2.28)-(2.32) and write for T ≥ 0,\\r\\nlog X\\r\\nρ\\r\\nρe\\r\\n(T) = log Xρe(T)−log Xρ(T) = Z T\\r\\n0\\r\\n\\r\\nγρe(t)−γρ(t)\\r\\n\\x01\\r\\ndO(t)+Z T\\r\\n0\\r\\n\\r\\nρe(t)−ρ(t)\\r\\n\\x01′\\r\\ndM(t). (3.56)\\r\\nThe first integral on the right-hand side is just −G(T) of (3.54) and it can be rewritten as\\r\\n− G(T) = Z T\\r\\n0\\r\\n\\r\\nγρe(t) − γρ(t)\\r\\n\\x01\\r\\ndO(t) = 1\\r\\n2\\r\\nZ T\\r\\n0\\r\\n\\r\\nαe\\r\\n′\\r\\nec\\r\\n†αe − α′\\r\\nc\\r\\n†α\\r\\n\\x01\\r\\n(t)dO(t) (3.57)\\r\\nfrom (3.53). On the other hand, from ρ = c\\r\\n†α and ρe = Dec†αe, we obtain series of equations as\\r\\nin (3.19):\\r\\n(ρe− ρ)\\r\\n′\\r\\nc(ρe− ρ) = ρe\\r\\n′\\r\\ncρe+ ρ\\r\\n′\\r\\ncρ − ρe\\r\\n′\\r\\ncρ − ρ\\r\\n′\\r\\ncρe = αe\\r\\n′ec†αe + α′\\r\\nc\\r\\n†α − 2ρe′\\r\\ncρ,\\r\\nas well as\\r\\nρe\\r\\n′\\r\\ncρ = ρe\\r\\n′\\r\\ncc†α = ρe\\r\\n′α = αe′\\r\\n(ec\\r\\n†\\r\\n)\\r\\n′Dα = αe′\\r\\n(ec\\r\\n†\\r\\n)\\r\\n′α. e\\r\\nCombining these equations, we have\\r\\n(ρe− ρ)\\r\\n′\\r\\nc(ρe− ρ) = α\\r\\n′\\r\\nc\\r\\n†α − αe′ec†α. e\\r\\nThus, the quadratic variation of the last integral on the right hand-side of (3.56) is written as\\r\\n\\x14 Z T\\r\\n0\\r\\n\\r\\nρe(t) − ρ(t)\\r\\n\\x01′\\r\\ndM(t)\\r\\n\\x15\\r\\n=\\r\\nZ T\\r\\n0\\r\\n(ρe− ρ)\\r\\n′\\r\\nc(ρe− ρ)(t)dO(t)\\r\\n=\\r\\nZ T\\r\\n0\\r\\n(α\\r\\n′\\r\\nc\\r\\n†α − αe′ec†αe)(t)dO(t) = 2G(T).\\r\\nThe Dambis-Dubins-Schwarz representation (Theorem 3.4.6, Problem 3.4.7 of Karatzas and Shreve\\r\\n(1991)) with the scaling property of Brownian motion implies that there exists a Brownian mo\\x02tion W, on a possibly enlarged filtered probability space, such that\\r\\nlog X\\r\\nρ\\r\\nρe\\r\\n(T) = −G(T) + √\\r\\n2W\\r\\n\\r\\nG(T)\\r\\n\\x01\\r\\n. (3.58)\\r\\nThe strong law of large numbers for Brownian motion gives the result (3.55).\\r\\nThe expression (3.57) shows that the asymptotic growth rate of the log-relative wealth pro\\x02cess log X\\r\\nρ\\r\\nρe\\r\\nis expressed in terms of ‘local characteristics’ of the market: α, α, c e , and ec.\\r\\n31\\n4 Stock Portfolios in Open Markets\\r\\nThe open market described in the previous section consists of the top n stocks and the money\\r\\nmarket. The existence of this money market gives us a flexibility to construct portfolios among\\r\\ntop n stocks. To be more specific, for any given portfolio π ∈ I(R), multiplying the diagonal\\r\\nmatrix D of (2.47) transforms it into a new portfolio Dπ among top n stocks. The proportion\\r\\nof assets, which is supposed to be invested in ‘bottom’ N − n stocks by π, is now assigned to\\r\\nthe money market by Dπ. In the absence of the money market, building portfolios among top\\r\\nn stocks is more subtle, and this section focuses on these subtleties.\\r\\n4.1 Stock portfolios and the market portfolio\\r\\nAn important subclass of portfolios in Definition 2.5 is the c\\r\\nP\\r\\nollection of portfolios π satisfying\\r\\nN\\r\\ni=1 πi ≡ 1, or π0 ≡ 0 in (2.26). Such a portfolio never invests in the money market; and this\\r\\ncondition can be formulated as π ∈ ∆N−1, where we denote\\r\\n∆N−1:= n(x1, · · · , xN ) ∈ R\\r\\nN\\r\\n\\x0c\\r\\n\\x0c X\\r\\nN\\r\\ni=1\\r\\nxi = 1o.\\r\\nDefinition 4.1 (Stock Portfolio). We call a portfolio π ∈ I(R) stock portfolio, if it takes values\\r\\nin ∆N−1, i.e., satisfies PN\\r\\ni=1 πi ≡ 1. We denote the collection of stock portfolios by I(R)∩∆N−1\\r\\n.\\r\\nWe call a stock portfolio π stock portfolio among the top n stocks, if in addition it belongs\\r\\nto T (n), i.e., satisfies the condition (2.16), or equivalently, (2.18). We denote the collection of\\r\\nstock portfolios among the top n stocks by I(R) ∩ ∆N−1 ∩ T (n).\\r\\nRemark 4.2 (Self-financibility of stock portfolios). For any stock portfolio π, we sum over (2.25)\\r\\nfor all indices i = 1, · · · , N to obtain\\r\\n1 ≡\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(·) =\\r\\nPN\\r\\ni=1 Si(·)ϑi(·)\\r\\nX(·; 1, ϑ)\\r\\n,\\r\\nand from (2.14),\\r\\nX(·; 1, ϑ) = 1 + Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nϑi(t)dSi(t) = X\\r\\nN\\r\\ni=1\\r\\nϑi(·)Si(·).\\r\\nThis last equation shows the ‘self-financing’ property of the stock portfolios; (see Definition 2.1\\r\\nof Karatzas and Ruf (2017)) the sum of product between the trading strategy ϑi and the stock\\r\\nprice Siis equal to the sum of stochastic integrals of each trading strategy with respect to the\\r\\ncorresponding stock price, along with the initial capital 1, at any time t ≥ 0. There are neither\\r\\nwithdrawals nor infusions of capital from the money market; gains are re-invested, losses are\\r\\nabsorbed.\\r\\nBefore we present the most important example of stock portfolios, we introduce the notation\\r\\nΣ := S1 + · · · + SN , (4.1)\\r\\nrepresenting the total capitalization of whole equity market.\\r\\n32\\nExample 4.3 (Market portfolio). Suppose that an investment strategy ϑ is given as ϑ ≡ 1/Σ(0) ≡\\r\\n(1, 1, · · · , 1)/Σ(0) with initial wealth x = 1. Then, its wealth process is just the total capital\\x02ization normalized by its initial value:\\r\\nX(·; 1, ϑ) = Σ(·)\\r\\nΣ(0). (4.2)\\r\\nWhereas, from (2.25), the corresponding portfolio π ≡ µ ≡ (µ1, · · · , µN ) can be expressed as\\r\\nµi(·) = Si(·)\\r\\nΣ(·)\\r\\n=\\r\\nSi(·)\\r\\nS1(·) + · · · + SN (·)\\r\\n, for i = 1, · · · , N. (4.3)\\r\\nWe call this special stock portfolio µ the market portfolio, and its component processes in (4.3)\\r\\nmarket weights; it is considered as the most important stock portfolio, as its wealth process\\r\\ngives the evolution of total market capitalization.\\r\\nIn an analogous manner, we define the top n market portfolio, which we denote by µe ≡\\r\\n(µe1, · · · , µeN ), with components\\r\\nµei(·) := Sei(·)\\r\\nΣ( e ·)\\r\\n=\\r\\nSei(·)\\r\\nSe1(·) + · · · + SeN (·)\\r\\n, for i = 1, · · · , N, (4.4)\\r\\nwhere\\r\\nΣ := e X\\r\\nN\\r\\ni=1\\r\\nSei = S(1)+· · ·+S(n), and Sei(·) := 1{ui(·)≤n}Si(·), for i = 1, · · · , N. (4.5)\\r\\nThe denominator Σ of (4.4) represents the sum of the capitalizations of the to e p n stocks; thus,\\r\\nµei(t) is the proportion of the capitalization of stock i, if this stock belongs to the top n, to\\r\\nthe total capitalization of the top n stocks at time t. In other words, µei can be interpreted\\r\\nas the ‘market weight’ of i-th stock in the restricted market composed of the top n stocks by\\r\\ncapitalization. It is easy to check that µe is a stock portfolio among the top n stocks, i.e.,\\r\\nµe ∈ I(R) ∩ ∆N−1 ∩ T (n).\\r\\n4.2 Capital Asset Pricing Model\\r\\nThe Capital Asset Pricing Model assumes that individual stocks cannot systematically outper\\x02form the market. In our open market setting, this requirement can be cast as saying that each\\r\\nindividual stock, whenever it belongs to the top n stocks, cannot outperform the top n market.\\r\\nIn this subsection, we briefly discuss this model for the top n market. Recalling the top n stock\\r\\nportfolio µe defined in (4.4), we have the next definition.\\r\\nDefinition 4.4 (CAPM). We say that the top n market is in the realm of the Capital Asset\\r\\nPricing Model (CAPM), if\\r\\nRei =\\r\\nZ ·\\r\\n0\\r\\nβi(t)dRµe(t) + Ni, i = 1, · · · , N, (4.6)\\r\\nhold for appropriate processes βi ∈ I(Rµe), i = 1, · · · , N, and for continuous local martingales\\r\\nNi with Ni(0) = 0 which are orthogonal to Rµe for all i = 1, · · · , N:\\r\\n[Ni, Rµe] ≡ 0.\\r\\n33\\nThe following proposition characterizes this property, in terms of the local characteristics of\\r\\nthe top market introduced in Section 2.4.\\r\\nProposition 4.5 (Characterization of CAPM). The top n market is in the realm of the CAPM\\r\\nif, and only if, the following two conditions hold.\\r\\n(A) There exists a scalar “leverage” predictable process b such that\\r\\nX\\r\\nN\\r\\ni=1\\r\\nZ T\\r\\n0\\r\\n|b(t)|1{cµeµe>0}|dC eiµe(t)| < ∞, for T ≥ 0, (4.7)\\r\\nand the equalities hold (P ⊗ O)-a.e.:\\r\\nαei = bceiµe, on {cµeµe > 0} for i = 1, · · · , N. (4.8)\\r\\n(B) On the set {cµeµe = 0}, we have (P ⊗ O)-a.e.:\\r\\nαµe = 0 ⇐⇒ αei = 0, i = 1, · · · , N. (4.9)\\r\\nWhen these conditions are satisfied, the process b of (4.7) and the processes βi ∈ I(Rµe) of (4.6)\\r\\ncan be chosen, respectively, as\\r\\nb =\\r\\nαµe\\r\\ncµeµe\\r\\n1{cµeµe>0}, (4.10)\\r\\nβi =\\r\\nceiµe\\r\\ncµeµe\\r\\n1{cµeµe>0} +\\r\\nαei\\r\\nαµe\\r\\n1{cµeµe=0, αµe6=0}, i = 1, · · · , N. (4.11)\\r\\nProof. Let us assume first that the top n market is in the realm of the CAPM. Recalling the\\r\\nnotation (2.46), we have\\r\\nC eiµe = [Rei, Rµe] = Z ·\\r\\n0\\r\\nβi(t)d[Rµe, Rµe](t) + [Ni, Rµe] = Z ·\\r\\n0\\r\\nβi(t)dCµeµe(t),\\r\\nwhich implies that ceiµe = βicµeµe also hold (P ⊗ O)-a.e., for i = 1, · · · , N. On {cµeµe > 0}, it follows\\r\\nthat βi = ceiµe/cµeµe for i = 1, · · · , N. Moreover, since Rei −\\r\\nR ·\\r\\n0\\r\\nβi(t)dRµe(t) is a local martingale,\\r\\nwe obtain Aei =\\r\\nR ·\\r\\n0\\r\\nβi(t)dAµe(t), and also αei = βiαµe holds (P ⊗ O)-a.e. for i = 1, · · · , N. As a\\r\\nconsequence, the identities of (4.8)\\r\\nαei =\\r\\nαµe\\r\\ncµeµe\\r\\nceiµe = bceiµe, hold for i = 1, · · · , N, (P ⊗ O) − a.e. on {cµeµe > 0},\\r\\nwith b given as in (4.10). Also, the (P ⊗ O)-a.e. identities αei = βiαµe, combined with αµe =\\r\\nµe\\r\\n′α, lead to the condition (B). Finally, b = αei/ceiµe\\r\\non {cµeµe > 0, ceiµe6= 0} implies that\\r\\n|b|1{cµeµe>0}|ceiµe| ≤ |αei| hold for i = 1, · · · , N, and thus the condition (4.7):\\r\\nX\\r\\nN\\r\\ni=1\\r\\nZ T\\r\\n0\\r\\n|b(t)|1{cµeµe>0}|dC eiµe(t)| ≤ X\\r\\nN\\r\\ni=1\\r\\nZ T\\r\\n0\\r\\n|dAei(t)| < ∞, for all T ≥ 0.\\r\\n34\\nConversely, suppose that the conditions (A) and (B) are valid. For i = 1, · · · , N, defining βi\\r\\nvia (4.11), we have\\r\\nZ T\\r\\n0\\r\\n|βi(t)||dAµe(t)| ≤ Z T\\r\\n0\\r\\n|b(t)|1{cµeµe>0}|dC eiµe(t)| +\\r\\nZ T\\r\\n0\\r\\n|dAei(t)| < ∞,\\r\\nas well as\\r\\nZ T\\r\\n0\\r\\n|βi(t)|\\r\\n2\\r\\ndCµeµe(t) = Z T\\r\\n0\\r\\n|ceiµe(t)|\\r\\n2\\r\\ncµeµe(t)\\r\\n1{cµeµe>0}dO(t) ≤\\r\\nZ T\\r\\n0\\r\\ncei,ei(t)dO(t) = Cei,i(T) < ∞.\\r\\nThese inequalities imply that βi ∈ I(Rµe) for i = 1, · · · , N. Furthermore, recalling the semi\\x02martingale decomposition (2.35), we observe that\\r\\nZ ·\\r\\n0\\r\\nβi(t)dRµe(t) = Z ·\\r\\n0\\r\\nβi(t)µe\\r\\n′\\r\\n(t)dAe(t) + Z ·\\r\\n0\\r\\nβi(t)µe\\r\\n′\\r\\n(t)dMf(t)\\r\\n=\\r\\nZ ·\\r\\n0\\r\\nβi(t)1{cµeµe(t)>0}b(t)dCµeµe(t) + Z ·\\r\\n0\\r\\nβi(t)1{cµeµe(t)=0}µe\\r\\n′\\r\\n(t)dAe(t) (4.12)\\r\\n+\\r\\nZ ·\\r\\n0\\r\\nβi(t)µe\\r\\n′\\r\\n(t)dMf(t),\\r\\nfrom (4.10). The first two integrals on the right hand side of (4.12) can be expressed as\\r\\nZ ·\\r\\n0\\r\\nβi(t)1{cµeµe(t)>0}b(t)dCµeµe(t) = Z ·\\r\\n0\\r\\n1{cµeµe(t)>0}b(t)dC eiµe(t) = Z ·\\r\\n0\\r\\n1{cµeµe(t)>0}dAei(t),\\r\\nand\\r\\nZ ·\\r\\n0\\r\\nβi(t)1{cµeµe(t)=0}µe\\r\\n′\\r\\n(t)dAe(t) = Z ·\\r\\n0\\r\\n1{cµeµe(t)=0}dAei(t),\\r\\nfor i = 1, · · · , N, on account of (4.11). Thus, we obtain\\r\\nZ ·\\r\\n0\\r\\nβi(t)dRµe(t) = Aei +\\r\\nZ ·\\r\\n0\\r\\nβi(t)µe\\r\\n′\\r\\n(t)dMf(t) = Rei −\\r\\nZ ·\\r\\n0\\r\\n\\r\\ne\\r\\ni − βi(t)µe(t)\\r\\n\\x01′\\r\\ndMf(t) =: Rei − Ni,\\r\\nwhich is (4.6), where we define Ni =\\r\\nR ·\\r\\n0\\r\\n\\r\\ne\\r\\ni − βi(t)µe(t)\\r\\n\\x01′\\r\\ndMf(t) for i = 1, · · · , N. We observe\\r\\nthat the identities (e\\r\\ni − βiµe)′ecµe = ceiµe − βicµeµe = 0 hold on the set {cµeµe > 0} from the definition\\r\\n(4.11), as well as on the set {cµeµe = 0} since ceiµe = 0 holds there. Finally, we obtain\\r\\n[Ni, Rµe] = Z ·\\r\\n0\\r\\n\\r\\ne\\r\\ni − βi(t)µe(t)\\r\\n\\x01′\\r\\nec(t)µe(t)dO(t) ≡ 0, i = 1, · · · , N,\\r\\nwhich shows that the top n market is in the realm of the CAPM.\\r\\n4.3 Functional generation of portfolios\\r\\nFunctionally generated portfolios were first introduced by Fernholz (1999). Given a function\\r\\nG : ∆N−1\\r\\n+ → (0,∞) of class C2 with the notation\\r\\n∆\\r\\nN−1\\r\\n+ := ∆N−1 ∩ R\\r\\nN\\r\\n+ =\\r\\nn\\r\\n(x1, · · · , xN ) ∈ R\\r\\nN\\r\\n\\x0c\\r\\n\\x0c xi ≥ 0 for i = 1, · · · , N, X\\r\\nN\\r\\ni=1\\r\\nxi = 1o, (4.13)\\r\\n35\\nwe can generate a portfolio π\\r\\nG from G, depending on the vector of market weights µ. The\\r\\nformula (11.2) of Fernholz and Karatzas (2009), colloquially known as the ‘master formula’, gives\\r\\na simply way to compare the relative wealth process of π\\r\\nG with respect to the ‘market’, namely,\\r\\nthe market portfolio µ (see, Chapter III of Fernholz and Karatzas (2009) for an overview).\\r\\nIn what follows, we present a new way to generate portfolios from a function having the\\r\\nmarket portfolio among the top n stocks µe in (4.4) as its input. We also derive a new ‘master\\r\\nformula’ to compare the wealth of the so-generated portfolio, relative to µe, the market portfolio\\r\\namong the top n stocks.\\r\\nFor any stock portfolio π ∈ I(R)∩∆N−1, we have π0 ≡ 0 (no investing in the money market),\\r\\nthus\\r\\ndXµe\\r\\nπ (t)\\r\\nX\\r\\nµe\\r\\nπ (t)\\r\\n=\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)dRµe\\r\\ni\\r\\n(t) = X\\r\\nN\\r\\ni=1\\r\\nπi(t)\\r\\nS\\r\\nµe\\r\\ni\\r\\n(t)\\r\\ndSµe\\r\\ni\\r\\n(t), (4.14)\\r\\nfrom (3.9). Here, we recall from (3.2) that S\\r\\nµe\\r\\nis the vector of stock prices denominated by the\\r\\nwealth process Xµe of the market portfolio among the top n stocks.\\r\\nWe note at this point, that the market portfolio µ in Example 4.3, has a very nice property:\\r\\nthe denominated stock price S\\r\\nµ\\r\\ni\\r\\nhas a simple representation, namely S\\r\\nµ\\r\\ni\\r\\n(·) = Σ(0)µi(·), for\\r\\ni = 1, · · · , N. Thus, if we used µ instead of µe in deriving (4.14), the last integrator would be\\r\\nrewritten as dSµ\\r\\ni\\r\\n(t) = Σ(0)dµi(t). However, unlike µ, the components of µe in (4.4) do not admit\\r\\nsuch a simple representation. For this reason, we will use the denominated stock price S\\r\\nµe\\r\\ni\\r\\n(t)\\r\\nas integrators, and will let the generating function G depend on S\\r\\nµe\\r\\ni\\r\\n(t) instead of µei(t) in what\\r\\nfollows.\\r\\nFor a given function G : (0,∞)\\r\\nN → (0,∞) of class C2\\r\\n, we want to write the relative-log\\r\\nwealth as\\r\\nlog X\\r\\nµe\\r\\nπ\\r\\n(t) = log \\x12\\r\\nG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(0)\\x01\\r\\n\\x13\\r\\n+ J\\r\\nµe\\r\\nπ\\r\\n(t), for any t ≥ 0, (4.15)\\r\\nfor some function J\\r\\nµe\\r\\nπ (·) of finite variation. In order to find J\\r\\nµe\\r\\nπ (·), we apply Itˆo’s rule, to obtain\\r\\ndXµe\\r\\nπ (t)\\r\\nX\\r\\nµe\\r\\nπ (t)\\r\\n= dJµe\\r\\nπ\\r\\n(t) +X\\r\\nN\\r\\ni=1\\r\\nDiG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 dSµe\\r\\ni\\r\\n(t) + 1\\r\\n2\\r\\nX\\r\\nN\\r\\ni=1\\r\\nX\\r\\nN\\r\\nj=1\\r\\nD2\\r\\ni,jG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 d[S\\r\\nµe\\r\\ni\\r\\n, Sµe\\r\\nj\\r\\n](t). (4.16)\\r\\nComparing the two equations (4.14) and (4.16), suppose we can find a portfolio π such that\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)\\r\\nS\\r\\nµe\\r\\ni\\r\\n(t)\\r\\ndSµe\\r\\ni\\r\\n(t) = X\\r\\nN\\r\\ni=1\\r\\nDiG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 dSµe\\r\\ni\\r\\n(t), (4.17)\\r\\nholds, then we have\\r\\ndJµe\\r\\nπ\\r\\n(t) = −\\r\\n1\\r\\n2\\r\\nX\\r\\nN\\r\\ni=1\\r\\nX\\r\\nN\\r\\nj=1\\r\\nD2\\r\\ni,jG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 d[S\\r\\nµe\\r\\ni\\r\\n, Sµe\\r\\nj\\r\\n](t).\\r\\nNow, a candidate portfolio π satisfying (4.17), is given as\\r\\nπi(t) = S\\r\\nµe\\r\\ni\\r\\n(t)\\r\\nDiG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 , i = 1, · · · , N;\\r\\n36\\nbut it need not belong to ∆N−1\\r\\n. Instead, we set\\r\\nπi(t) := S\\r\\nµe\\r\\ni\\r\\n(t)\\r\\nDiG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 + µei(t) − µei(t)\\r\\nX\\r\\nN\\r\\nj=1\\r\\nS\\r\\nµe\\r\\nj\\r\\n(t)\\r\\nDjG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 , i = 1, · · · , N, (4.18)\\r\\nthen it is easy to show that π ∈ ∆N−1. To check the condition (4.17), we note\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(t)\\r\\nS\\r\\nµe\\r\\ni\\r\\n(t)\\r\\ndSµe\\r\\ni\\r\\n(t) = X\\r\\nN\\r\\ni=1\\r\\nDiG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 dSµe\\r\\ni\\r\\n(t) + n1 −\\r\\nX\\r\\nN\\r\\nj=1\\r\\nS\\r\\nµe\\r\\nj\\r\\n(t)\\r\\nDjG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01\\r\\noX\\r\\nN\\r\\ni=1\\r\\nµei(t)\\r\\nS\\r\\nµe\\r\\ni\\r\\n(t)\\r\\ndSµe\\r\\ni\\r\\n(t),\\r\\nand the last term vanishes because\\r\\nX\\r\\nN\\r\\ni=1\\r\\nµei(t)\\r\\nS\\r\\nµe\\r\\ni\\r\\n(t)\\r\\ndSµe\\r\\ni\\r\\n(t) = X\\r\\nN\\r\\ni=1\\r\\nµei(t)dRµe\\r\\ni\\r\\n(t) = dRµe\\r\\nµe\\r\\n(t) = 0.\\r\\nHere, R\\r\\nµe\\r\\nµe = L(X\\r\\nµe\\r\\nµe\\r\\n) = L(1) ≡ 0, from Proposition 3.1.\\r\\nThe construction described above can be formulated as the following definition and propo\\x02sition.\\r\\nDefinition 4.6 (Functionally generated portfolio). Let G : (0,∞)\\r\\nN → (0,∞) be a twice con\\x02tinuously differentiable function. Then, the vector π\\r\\nG ≡ π = (π1, · · · , πN ) defined as in (4.18)\\r\\nis called the stock portfolio generated by the function G via the market portfolio among the top\\r\\nn stocks.\\r\\nProposition 4.7 (Master Formula). For the stock portfolio π\\r\\nG generated by G with the market\\r\\nportfolio among the top n stocks, we have the decomposition\\r\\nlog \\x12\\r\\nXπG\\r\\nXµe\\r\\n\\x13\\r\\n= log \\x12\\r\\nG\\r\\n\\r\\nS\\r\\nµe\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(0)\\x01\\r\\n\\x13\\r\\n−\\r\\n1\\r\\n2\\r\\nX\\r\\nN\\r\\ni=1\\r\\nX\\r\\nN\\r\\nj=1\\r\\nZ ·\\r\\n0\\r\\nD2\\r\\ni,jG\\r\\n\\r\\nS\\r\\nµe\\r\\n(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nSµe(t)\\r\\n\\x01 d[S\\r\\nµe\\r\\ni\\r\\n, Sµe\\r\\nj\\r\\n](t). (4.19)\\r\\nThe above arguments, leading to Definition 4.6 and Proposition 4.7, have two weaknesses.\\r\\nFirst, the functionally-generated stock portfolio π\\r\\nG in (4.18) is not generally a portfolio among\\r\\nthe top n stocks, i.e., it can fail to belong to T (n). Thus, the Master formula (4.19) compares a\\r\\nportfolio π\\r\\nG which is not a portfolio among the top n stocks, with µe, which is a portfolio among\\r\\nthe top n stocks. We will fix this issue by restricting the class of generating functions G in the\\r\\nnext subsection.\\r\\nSecondly, when we construct a portfolio via (4.18) or use the Master formula (4.19), we need\\r\\nto know at each time t ≥ 0 the entire history of the process Xµe, up to time t, because these\\r\\nequations require the values of the vector S\\r\\nµe = S/Xµe. This issue is unfortunately inevitable in\\r\\nthe open market, because of its own nature of µe; as it is composed of the top n stocks, we need\\r\\nto keep track of the ranks of N stocks all the time, whereas computing the wealth Xµ generated\\r\\nby the market portfolio µ only requires current stock prices (and the stock prices at time t = 0),\\r\\nfrom its simple representation (4.2). Though we cannot resolve this second issue, we will give a\\r\\nrepresentation of Xµe in the following subsection.\\r\\n37\\n4.4 Functionally generated portfolios using ranks\\r\\nRecalling the rank notation in Definition 2.2, we define the random permutation process pk(t)\\r\\nof {1, · · · , N} such that for k = 1, · · · , N,\\r\\nSpk(t)(t) = S(k)(t), (4.20)\\r\\npk(t) < pk+1(t) if S(k)(t) = S(k+1)(t).\\r\\npk(t) represents the index name of the stock at rank k at time t, breaking ties with the lexico\\x02graphic rule, so it is the inverse permutation of ui(t), introduced in (2.4): ui(t) = k ⇐⇒ pk(t) = i,\\r\\nfor all t ≥ 0.\\r\\nFor any continuous semimartingale Y , we denote the local time accumulated at the origin\\r\\nby Y (·) up to time t ≥ 0 by L\\r\\nY\\r\\n(t);\\r\\nL\\r\\nY\\r\\n(t) := 1\\r\\n2\\r\\n\\x12\\r\\n|Y (t)| − |Y (0)| − Z t\\r\\n0\\r\\nsignY (s)\\r\\n\\x01\\r\\ndY (s)\\r\\n\\x13\\r\\n, where sign(x) = 2 × 1(0,∞)(x) − 1.\\r\\nThen, L\\r\\nS(k)−S(ℓ) (t) can be interpreted as the ‘collision local time’ accumulated up to time t,\\r\\nwhenever the k-th and ℓ-th ranked processes of S collide. In order to simplify the local time terms\\r\\nthroughout this section, we introduce the following definition which prohibits the accumulation\\r\\nof local times of ‘triple collisions’ between the stock prices.\\r\\nDefinition 4.8. The components of the price vector S = (S1, · · · , SN ) in Definition 2.1 are\\r\\ncalled pathwise mutually nondegenerate, if\\r\\n(i) the set {t : Si(t) = Sj (t)} has Lebesgue measure zero, P-a.e., for all i 6= j; and if\\r\\n(ii) L\\r\\nS(k)−S(ℓ) (t) ≡ 0 holds P-a.e., for |k − ℓ| ≥ 2.\\r\\nProposition 4.9. Suppose that the components of the price vector S are pathwise mutually non\\x02degenerate. Then, with the notation (4.5), the wealth process Xµe of µe admits the representation\\r\\nXµe(·) = Σ( e ·)\\r\\nΣ(0) e\\r\\nexp \\x12−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\n1\\r\\nΣ( e t)\\r\\ndLS(n)−S(n+1) (t)\\r\\n\\x13\\r\\n. (4.21)\\r\\nProof. From Proposition 3.1 and the fact that µe is a stock portfolio, we have\\r\\nXµe(·) = E\\r\\n\\x10 Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nµei(t)dRi(t)\\r\\n\\x11\\r\\n= E\\r\\n\\x10 Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nSei(t)\\r\\nΣ( e t)Si(t)\\r\\ndSi(t)\\r\\n\\x11\\r\\n= E\\r\\n\\x10 Z ·\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nXn\\r\\nk=1\\r\\n1{ui(t)=k}\\r\\nΣ( e t)\\r\\ndSi(t)\\r\\n\\x11\\r\\n.\\r\\nOn the other hand, from Proposition 4.1.11 of Fernholz (2002), we have\\r\\nX\\r\\nN\\r\\ni=1\\r\\n1{ui(t)=k}dSi(t) = dS(k)(t) −\\r\\n1\\r\\n2\\r\\ndLS(k)−S(k+1) (t) + 1\\r\\n2\\r\\ndLS(k−1)−S(k) (t),\\r\\nfor k = 1, · · · , N and t ≥ 0, with the conventions L\\r\\nS(0)−S(1) ≡ 0 and LS(N)−S(N+1) ≡ 0. Thus, we\\r\\nobtain\\r\\nXµe(·) = E\\r\\n\\x10 Z ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\ndS(k)(t)\\r\\nΣ( e t)\\r\\n+\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\ndLS(k−1)−S(k) (t)\\r\\nΣ( e t)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\ndLS(k)−S(k+1) (t)\\r\\nΣ( e t)\\r\\n\\x11\\r\\n= E\\r\\n\\x10 Z ·\\r\\n0\\r\\ndΣ( e t)\\r\\nΣ( e t)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\ndLS(n)−S(n+1) (t)\\r\\nΣ( e t)\\r\\n\\x11\\r\\n=\\r\\nΣ( e ·)\\r\\nΣ(0) e\\r\\nexp \\x12−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\ndLS(n)−S(n+1) (t)\\r\\nΣ( e t)\\r\\n\\x13\\r\\n.\\r\\n38\\nThe exponential term of (4.21) shows the ‘leakage’, the effect caused by stocks which cross\\r\\nover from the top n league to the bottom. Due to this effect, we need to keep track of the\\r\\ncollision local time L\\r\\nS(n)−S(n+1) in order to compute Xµe, as we pointed out at the end of the\\r\\nprevious subsection.\\r\\nWe next present Fernholz’s original method of constructing rank-dependent portfolios from\\r\\ngenerating functions. We write µ(k)to represent the k-th ranked market weight among µ1, · · · , µN\\r\\nfor k = 1, · · · , N, and introduce the vector µ = (µ(1), · · · , µ(N)) with components µ(k) = S(k)/Σ,\\r\\nk = 1, · · · , N, as in (2.2), (4.1). The following result is based on Theorem 4.2.1 of Fernholz\\r\\n(2002).\\r\\nTheorem 4.10 (Functionally generated portfolios using ranked market weights). Suppose that\\r\\nthe price vector S is pathwise mutually nondegenerate. Let pk(·), k = 1, · · · , N be the random\\r\\npermutation process defined by (4.20) and let G be a function defined on a neighborhood U of\\r\\n∆\\r\\nN−1\\r\\n+ . Suppose that there exists a positive C\\r\\n2\\r\\nfunction G such that for (x1, · · · , xN ) ∈ U,\\r\\nG(x1, · · · , xN ) = G(x(1), · · · , x(N)). (4.22)\\r\\nThen G generates the stock portfolio π\\r\\nG such that for k = 1, · · · , N,\\r\\nπ\\r\\nG\\r\\npk(t)\\r\\n(t) = \\x12\\r\\nDkG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01 + 1 −\\r\\nX\\r\\nN\\r\\nℓ=1\\r\\nµ(ℓ)\\r\\nDℓG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\n\\x13\\r\\nµ(k)(t), for t ≥ 0. (4.23)\\r\\nThe log-relative wealth process of π\\r\\nG with respect to the market portfolio µ, can be expressed via\\r\\nthe ‘master formula’ :\\r\\nlog \\x12\\r\\nXπG\\r\\nXµ\\r\\n\\x13\\r\\n= log \\x12\\r\\nG(µ)\\r\\nG\\r\\n\\r\\nµ(0)\\x01\\r\\n\\x13\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nN\\r\\nX−1\\r\\nk=1\\r\\n\\x12π\\r\\nG\\r\\npk(t)\\r\\n(t)\\r\\nµ(k)(t)\\r\\n−\\r\\nπ\\r\\nG\\r\\npk+1(t)\\r\\n(t)\\r\\nµ(k+1)(t)\\r\\n\\x13\\r\\ndLµ(k)−µ(k+1) (t)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nX\\r\\nN\\r\\nk=1\\r\\nX\\r\\nN\\r\\nℓ=1\\r\\nD2\\r\\nk,ℓG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01 d[µ(k)\\r\\n, µ(ℓ)](t).\\r\\nThe portfolio π\\r\\nG generated via the recipe (4.23) is easily checked to be a stock portfolio, i.e.,\\r\\nπ\\r\\nG ∈ I(R) ∩ ∆N−1\\r\\n; however, it is not generally a portfolio among the top n stocks. In order to\\r\\nmake it a portfolio among the top n stocks, we need to impose two conditions on the function\\r\\nG in Theorem 4.10:\\r\\n(A) G is ‘balanced’, i.e., satisfies the identity\\r\\nG(x1, · · · , xN ) = X\\r\\nN\\r\\nj=1\\r\\nxjDjG(x1, · · · , xN ), for any x ∈ U, (4.24)\\r\\n(B) G(x) depends only on the first n components of x.\\r\\nIf the condition (A) is satisfied, then the portfolio π\\r\\nG of (4.23) has a simpler representation as\\r\\nπ\\r\\nG\\r\\npk(t)\\r\\n(t) = DkG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01 µ(k)\\r\\n(t), for t ≥ 0. (4.25)\\r\\nMoreover, if the condition (B) holds as well, then DkG\\r\\n\\r\\nµ\\r\\n\\x01\\r\\n= 0 for k > n, thus π\\r\\nG\\r\\npk(t)\\r\\n(t) = 0 for\\r\\nk > n. This means that the portfolio π\\r\\nG does not invest in the i = pk(t)-th stock at time t, if\\r\\nthe rank k of this i-th stock is bigger than n at time t.\\r\\n39\\nDefinition 4.11 (Admissible generating function in open market). We call a function G in\\r\\nTheorem 4.10 an admissible generating function of market consisting of the top n stocks, if it\\r\\nsatisfies conditions (A) and (B) above.\\r\\nCorollary 4.12. If G in Theorem 4.10 is an admissible generating function of market consisting\\r\\nof top n stocks, then G generates the stock portfolio among the top n stocks π\\r\\nG ∈ I(R)∩T (n)∩\\r\\n∆N−1, given as (4.25) for k = 1, · · · , N. In this case, we have the master formula\\r\\nlog \\x12\\r\\nXπG\\r\\nXµ\\r\\n\\x13\\r\\n= log \\x12\\r\\nG(µ)\\r\\nG\\r\\n\\r\\nµ(0)\\x01\\r\\n\\x13\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\n\\x12\\r\\nDkG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01 −\\r\\nDk+1G\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\n\\x13\\r\\ndLµ(k)−µ(k+1) (t)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\nXn\\r\\nℓ=1\\r\\nD2\\r\\nk,ℓG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01 d[µ(k)\\r\\n, µ(ℓ)](t). (4.26)\\r\\nExample 4.13 (Balanced functions). By solving the partial differential equation of (4.24), a\\r\\nbalanced function G can be shown to be homogeneous of degree 1, i.e, the identity\\r\\nG(ax) = aG(x) (4.27)\\r\\nholds for any x ∈ U and a > 0. From this simple characterization of balanced functions, we\\r\\nillustrate three types of balanced functions here:\\r\\n(i) G(x) = 1\\r\\nc1+···+cN\\r\\nPN\\r\\ni=1 cixi\\r\\n,\\r\\n(ii) G(x) =  QN\\r\\ni=1 xi\\r\\n\\x011/N\\r\\n,\\r\\n(iii) G(x) = PN\\r\\ni=1 x\\r\\np\\r\\ni\\r\\n\\x01 1\\r\\np\\r\\n.\\r\\nThese functions are closely related to ‘three Pythagorean means’; (i) and (ii) are just the\\r\\nweighted-arithmetic and geometric means of the components of x, and (iii) becomes the har\\x02monic mean when p = −1. A plethora of examples of these types can be found in the literature.\\r\\nThe “capitalization-weighted portfolios” of large and small stocks from Example 6.2, Example\\r\\n6.3 of Karatzas and Ruf (2017), or from Example 4.3.2 of Fernholz (2002) are special cases of\\r\\n(i). The “equal-weighted portfolio”, which holds equal weights across all assets, in Section 4.3\\r\\nof Karatzas and Ruf (2017), is generated by (ii). The portfolio generated by (iii) for 0 < p < 1\\r\\nis called “diversity-weighted portfolio”, and is discussed in detail in Example 3.4.4 and Section\\r\\n6.2 of Fernholz (2002). Diversity-weighted portfolios with negative parameter p < 0 in (iii) are\\r\\nthe main subject of Vervuurt and Karatzas (2015).\\r\\nWe can slightly generalize and make these functions satisfy conditions (A) and (B) as well:\\r\\n(i’) G(x) = Pn\\r\\ni=1 cixi\\r\\n,\\r\\n(ii’) G(x) = Qn\\r\\ni=1 x\\r\\nci\\r\\ni\\r\\n, with Pn\\r\\ni=1 ci = 1,\\r\\n(iii’) G(x) = \\x10Pn\\r\\ni=1 x\\r\\np\\r\\ni\\r\\n\\x11 1\\r\\np\\r\\n,\\r\\nfor some constants ci’s and p.\\r\\n40\\nThe following example further devolops Example 4.3.2 of Fernholz (2002), and shows that\\r\\nthe top n market portfolio µe, defined in (4.4), can be generated functionally.\\r\\nExample 4.14 (Top n market portfolio). Consider the function\\r\\nG(x) = G(x(1), · · · , x(n)) = Xn\\r\\nk=1\\r\\nx(k)\\r\\nsatisfying the conditions (A) and (B) above. Corollary 4.12 implies that G generates the portfolio\\r\\nπ\\r\\nG\\r\\npk(·)\\r\\n(·) =\\r\\nµ(k)(·)\\r\\nµ(1)(·) + · · · + µ(n)(·)\\r\\n1{k≤n} =\\r\\nS(k)(·)\\r\\nS(1)(·) + · · · + S(n)(·)\\r\\n1{k≤n}.\\r\\nThis coincides with the top n market portfolio µe, because\\r\\nS(k)(·)1{k≤n}\\r\\nS(1)(·) + · · · + S(n)(·)\\r\\n=\\r\\nSpk(·)(·)1{k≤n}\\r\\nΣ( e ·)\\r\\n= µepk(·)(·),\\r\\nholds for k = 1, · · · , N, from (4.4). The master formula (4.26) is then\\r\\nlog \\x12\\r\\nXµe\\r\\nXµ\\r\\n\\x13\\r\\n= log \\x12\\r\\nµ(1)(·) + · · · + µ(n)(·)\\r\\nµ(1)(0) + · · · + µ(n)(0)\\x13\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\ndLµ(n)−µ(n+1) (t)\\r\\nµ(1)(t) + · · · + µ(n)(t)\\r\\n. (4.28)\\r\\nIn Corollary 4.12, the portfolio π\\r\\nG is indeed a stock portfolio among the top n stocks; but\\r\\nthe master formula (4.26) compares its performance with the market portfolio µ, which is not a\\r\\nportfolio among the top n stocks. In the open market setting, since we only consider portfolios\\r\\namong the top n stocks, it is more appropriate to compare a portfolio’s performance with respect\\r\\nto µe, rather than µ. This can be done by combining (4.26) and (4.28).\\r\\nCorollary 4.15 (Master formula in top n market). For a functionally generated portfolio π\\r\\nG\\r\\nas in Corollary 4.12, the master formula, which compares the log-relative wealth of π\\r\\nG to that\\r\\ngenerated by µe, the top n market, is given as\\r\\nlog \\x12\\r\\nXπG\\r\\nXµe\\r\\n\\x13\\r\\n= log \\x12\\r\\nG(µ)\\r\\nG\\r\\n\\r\\nµ(0)\\x01\\r\\n\\x13\\r\\n− log \\x12\\r\\nµ(1)(·) + · · · + µ(n)(·)\\r\\nµ(1)(0) + · · · + µ(n)(0)\\x13\\r\\n(4.29)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\n\\x12\\r\\nDkG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01 −\\r\\nDk+1G\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\n\\x13\\r\\ndLµ(k)−µ(k+1) (t)\\r\\n+\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\ndLµ(n)−µ(n+1) (t)\\r\\nµ(1)(t) + · · · + µ(n)(t)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\nXn\\r\\nℓ=1\\r\\nD2\\r\\nk,ℓG\\r\\n\\r\\nµ(t)\\r\\n\\x01\\r\\nG\\r\\n\\r\\nµ(t)\\r\\n\\x01 d[µ(k)\\r\\n, µ(ℓ)](t).\\r\\nWe call this formula of (4.29), the ‘master formula for the top n market’ to distinguish it\\r\\nfrom the formula of (4.26), which we call the ‘master formula in the entire market’.\\r\\nExample 4.16 (Diversity-weighted portfolio). Consider a function\\r\\nG(x) = G(x(1), · · · , x(N)) = \\x12Xn\\r\\nk=1\\r\\nx\\r\\np\\r\\n(k)\\r\\n\\x131/p\\r\\n41\\nwith a fixed constant p ∈ (0, 1). Corollary 4.12 implies that G generates the “diversity-weighted\\r\\nportfolio”\\r\\nπ\\r\\nG\\r\\npk(·)\\r\\n(·) =\\r\\nµ\\r\\np\\r\\n(k)\\r\\n(·)\\r\\nµ\\r\\np\\r\\n(1)(·) + · · · + µ\\r\\np\\r\\n(n)\\r\\n(·)\\r\\n1{k≤n}, k = 1, · · · , N.\\r\\nThe master formula in the top n market in (4.29) is then given as\\r\\nlog \\x12\\r\\nXπG\\r\\nXµe\\r\\n\\x13\\r\\n=\\r\\n1\\r\\np\\r\\nlog \\x12 µ\\r\\np\\r\\n(1)(·) + · · · + µ\\r\\np\\r\\n(n)\\r\\n(·)\\r\\nµ\\r\\np\\r\\n(1)(0) + · · · + µ\\r\\np\\r\\n(n)\\r\\n(0)\\x13\\r\\n− log \\x12\\r\\nµ(1)(·) + · · · + µ(n)(·)\\r\\nµ(1)(0) + · · · + µ(n)(0)\\x13\\r\\n−\\r\\n1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nµ\\r\\np−1\\r\\n(n)\\r\\n(t)\\r\\nµ\\r\\np\\r\\n(1)(t) + · · · + µ\\r\\np\\r\\n(n)\\r\\n(t)\\r\\ndLµ(n)−µ(n+1) (t) + 1\\r\\n2\\r\\nZ ·\\r\\n0\\r\\ndLµ(n)−µ(n+1) (t)\\r\\nµ(1)(t) + · · · + µ(n)(t)\\r\\n(4.30)\\r\\n−\\r\\n1 − p\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\nXn\\r\\nℓ=1\\r\\nµ\\r\\np−1\\r\\n(k)\\r\\n(t)µ\\r\\np−1\\r\\n(ℓ)\\r\\n(t)\\r\\n\\r\\nµ\\r\\np\\r\\n(1)(t) + · · · + µ\\r\\np\\r\\n(n)\\r\\n(t)\\r\\n\\x012\\r\\nd[µ(k), µ(ℓ)](t)\\r\\n+\\r\\n1 − p\\r\\n2\\r\\nZ ·\\r\\n0\\r\\nXn\\r\\nk=1\\r\\nµ\\r\\np−2\\r\\n(k)\\r\\n(t)\\r\\nµ\\r\\np\\r\\n(1)(t) + · · · + µ\\r\\np\\r\\n(n)\\r\\n(t)\\r\\nd[µ(k), µ(k)](t).\\r\\nHere, in the first integral of (4.30), we use the fact that the local time process L\\r\\nµ(k)−µ(k+1) (·) is\\r\\nflat off the set {s ≥ 0 : µ(k)(s) = µ(k+1)(s)} for k = 1, · · · , n − 1.\\r\\n4.5 Universal portfolio\\r\\nIn this subsection, we explore Cover’s universal portfolio theory in open markets. This portfolio\\r\\nwas first introduced by Cover (1991) in discrete time, and its extension to continuous time was\\r\\ndeveloped by Jamshidian (1992). More recent work under the setting of Stochastic Portfolio\\r\\nTheory can be found in Cuchiero et al. (2019).\\r\\nRecalling the notation ∆N−1\\r\\n+ from (4.13), we need first the following notation\\r\\n∆\\r\\nN−1,n\\r\\n+ := \\x08\\r\\nx ∈ R\\r\\nN\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\nxk ≥ 0 for k = 1, · · · , N, Xn\\r\\nk=1\\r\\nxk = 1, xn+1 = · · · = xN = 0\\t(4.31)\\r\\nthroughout this subsection. Since we are only allowed to invest in the top n stocks in an open\\r\\nmarket, the notion of Cover’s ‘constant rebalanced portfolio’ needs to be amended, as follows.\\r\\nDefinition 4.17 (constant rebalanced portfolio by rank). If a stock portfolio π ∈ I(R)∩T (n)∩\\r\\n∆N−1 among the top n stocks satisfies\\r\\nπpk(t)(t) = ξk for t ≥ 0, k = 1, · · · , N (4.32)\\r\\nwith some ξ = (ξ1, · · · , ξN ) ∈ ∆\\r\\nN−1,n\\r\\n+ , we call π a constant rebalanced portfolio among the top n\\r\\nstocks by rank. This portfolio re-balances at all times to maintain a constant proportion ξk of\\r\\ncurrent wealth invested in the k-th ranked stock, for k ≤ n. We denote the collection of constant\\r\\nrebalanced portfolios among the top n stocks by CRn.\\r\\nProposition 4.18. Every constant rebalanced portfolio among the top n stocks by rank is func\\x02tionally generated.\\r\\n42\\nProof. For a fixed ξ ∈ ∆\\r\\nN−1,n\\r\\n+ , consider a function\\r\\nG(x) = G(x(1), · · · , x(N)) = Yn\\r\\nk=1\\r\\nx\\r\\nξk\\r\\n(k)\\r\\n. (4.33)\\r\\nIt is easy to check that G is an admissible generating function of market consisting of the top n\\r\\nstocks, and it generates the portfolio via the recipe (4.25):\\r\\nπ\\r\\nG\\r\\npk(t)\\r\\n(t) = ξk, for t ≥ 0, k = 1, · · · , N.\\r\\nSince ξ is chosen arbitrarily from ∆N−1,n\\r\\n+ , the claim follows.\\r\\nThanks to Proposition 4.18, for every ξ ∈ ∆\\r\\nN−1,n\\r\\n+ there exists a corresponding portfolio\\r\\nπ ∈ CRnas in Definition 4.17, and we write Xξ(t) to represent the wealth process of π at time\\r\\nt in the manner of (2.22), namely,\\r\\nXξ(t) ≡ Xπ(t) = E\\r\\n\\x12 Z t\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\nπi(s)dRi(s)\\r\\n\\x13\\r\\n= E\\r\\n\\x12Xn\\r\\nk=1\\r\\nξk\\r\\nZ t\\r\\n0\\r\\nX\\r\\nN\\r\\ni=1\\r\\n1{ui(s)=k}dRi(s)\\r\\n\\x13\\r\\nfor t ≥ 0.\\r\\n(4.34)\\r\\nFor T > 0 fixed, we define\\r\\nX∗(T) := sup\\r\\nπ∈CRn\\r\\nXπ(T) = sup\\r\\nξ∈∆\\r\\nN−1,n\\r\\n+\\r\\nXξ(T). (4.35)\\r\\nThis X∗(T) represents the maximal wealth at time T, achievable over all constant rebalanced\\r\\nportfolios among the top n stocks by rank. We show in the following that a F(T)-measurable\\r\\nrandom vector of weights π\\r\\n∗\\r\\n(T) ≡ ξ\\r\\n∗\\r\\nexists, which attains the supremum in (4.35), namely, that\\r\\nX∗(T) = Xπ∗(T)(T) = Xξ\\r\\n∗ (T) holds.\\r\\nLemma 4.19. For a fixed T > 0, the mapping ∆\\r\\nN−1,n\\r\\n+ ∋ ξ 7→ Xξ(T) ∈ R is continuous.\\r\\nProof. For ξ, ζ ∈ ∆\\r\\nN−1,n\\r\\n+ , we have\\r\\nlog Xξ(T) − log Xζ (T) = log Xξ(T)\\r\\nXµe(T)\\r\\n− log Xζ (T)\\r\\nXµe(T)\\r\\n=\\r\\nXn\\r\\nk=1\\r\\nlog \\x12\\r\\nµ(k)(T)\\r\\nµ(k)(0) \\x13(ξk−ζk)\\r\\n(4.36)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ T\\r\\n0\\r\\nXn−1\\r\\nk=1\\r\\n\\x12\\r\\nξk − ζk\\r\\nµ(k)(t)\\r\\n−\\r\\nξk+1 − ζk+1\\r\\nµ(k+1)(t)\\r\\n\\x13\\r\\ndLµ(k)−µ(k+1) (t) −\\r\\n1\\r\\n2\\r\\nZ T\\r\\n0\\r\\nξn − ζn\\r\\nµ(n)(t)\\r\\ndLµ(n)−µ(n+1) (t)\\r\\n−\\r\\n1\\r\\n2\\r\\nZ T\\r\\n0\\r\\nXn\\r\\nk=1\\r\\nXn\\r\\nℓ=1\\r\\nξkξℓ − ζkζℓ\\r\\nµ(k)(t)µ(ℓ)(t)\\r\\nd[µ(k), µ(ℓ)](t) + 1\\r\\n2\\r\\nZ T\\r\\n0\\r\\nXn\\r\\nk=1\\r\\nξkζk\\r\\nµ\\r\\n2\\r\\n(k)\\r\\n(t)\\r\\nd[µ(k), µ(k)](t).\\r\\nIn the last equality, we used the master formula (4.29) twice, and applied it to the functions of the\\r\\nform (4.33) for ξ and ζ, respectively. Since the functions µ(k)(·), Lµ(k)−µ(k+1) (·), [µ(k), µ(ℓ)](·)\\r\\nfor 1 ≤ k, ℓ ≤ n on the right-hand side (4.36) are all continuous, they are bounded on the\\r\\ncompact interval [0, T]. Thus, we obtain the estimate\\r\\n\\x0c\\r\\n\\x0c\\r\\nlog Xξ(T) − log Xζ (T)\\r\\n\\x0c\\r\\n\\x0c ≤ ||ξ − ζ||KT for\\r\\nsome positive constant KT , which depends on min0≤t≤T µ(k)(t), Lµ(k)−µ(k+1) (T), [µ(k)\\r\\n, µ(k)](T)\\r\\nfor k = 1, · · · , n, and this proves the continuity.\\r\\n43\\nDefinition 4.20 (Best retrospectively chosen vector of weights). The continuity shown in\\r\\nLemma 4.19 shows that there exists a vector ξ\\r\\n∗ ≡ π∗\\r\\n(T) ∈ ∆\\r\\nN−1,n\\r\\n+ which attains the supremum\\r\\nin (4.35) for a fixed T ∈ (0,∞). We call this F(T)-measurable, ∆N−1,n\\r\\n+ -valued random variable\\r\\nπ\\r\\n∗\\r\\n(T) the best retrospectively chosen vector of weights among the top n stocks for the given\\r\\nT ∈ (0,∞).\\r\\nEven though π\\r\\n∗\\r\\n(T) meant to outperform all constant rebalanced portfolios among the top\\r\\nn stocks by rank at T > 0, constructing it requires the knowledge of stock prices over the entire\\r\\ninterval [0, T], that is, ahead of time. Cover (1991) introduced a remarkable way to construct\\r\\na portfolio, called “universal portfolio”, depending only on past stock prices, whose long-run\\r\\nperformance is almost as good as that of the best retrospectively chosen vector of weights.\\r\\nCover’s idea of building the universal portfolio, was to determine its weights by averaging the\\r\\nperformances of all constant portfolio weights, at any time t ≥ 0.\\r\\nDefinition 4.21 (universal portfolio). With the notation ∆N−1,n\\r\\n+ of (4.31), the portfolio ˆπ,\\r\\ndefined as\\r\\nπˆpk(t)(t) :=\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\nξkXξ(t)dξ\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\nXξ(t)dξ for t ≥ 0, k = 1, · · · , N, (4.37)\\r\\nis called universal portfolio among the top n stocks.\\r\\nFrom the notation ∆N−1,n\\r\\n+ , we have ˆπpk(t)\\r\\n(t) = 0 for all t ≥ 0 for k > n; i.e., ˆπ invests only\\r\\nin the top n stocks, thus it belongs to I(R) ∩ T (n) ∩ ∆N−1, the collections of stock portfolios\\r\\namong the top n stocks. We next compute the wealth of the universal portfolio.\\r\\nProposition 4.22. The wealth process Xπˆ is given as\\r\\nXπˆ(t) =\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\nXξ(t)dξ\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\ndξ , for t ≥ 0. (4.38)\\r\\nProof. Let Z(t) denote the right-hand side of (4.38). We have\\r\\ndZ(t)\\r\\nZ(t)\\r\\n=\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\ndXξ(t)dξ\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\nXξ(t)dξ =\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\nXξ(t)\\r\\nPN\\r\\ni=1\\r\\nPn\\r\\nk=1 ξk1{ui(t)=k}dRi(t)dξ\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\nXξ(t)dξ\\r\\n=\\r\\nX\\r\\nN\\r\\ni=1\\r\\nXn\\r\\nk=1\\r\\nπˆpk(t)(t)1{ui(t)=k}dRi(t) = X\\r\\nN\\r\\ni=1\\r\\nπˆi(t)dRi(t) = dXπˆ(t)\\r\\nXπˆ(t)\\r\\n.\\r\\nHere, the second, third and last equalities are from (4.34), (4.37), and (2.22), respectively. Since\\r\\nXπˆ(0) = Z(0) = 1, the result follows.\\r\\nWe are now ready to compare the long-run performance of the universal portfolio with the\\r\\nbest retrospectively chosen vector of weights.\\r\\nTheorem 4.23. Suppose that the portfolio µ, defined in (4.3), satisfies\\r\\nµ(1)(t) ≥ · · · ≥ µ(n)(t) ≥ δ, for all t ≥ 0 for some δ > 0, (4.39)\\r\\n44\\nlim sup\\r\\nT→∞\\r\\n1\\r\\nT\\r\\n[µ(k), µ(k)](T) < ∞, lim sup\\r\\nT→∞\\r\\n1\\r\\nT\\r\\nL\\r\\nµ(k)−µ(k+1) (T) < ∞, for k = 1, · · · , n. (4.40)\\r\\nThen, the best retrospectively chosen vector of weights and the universal portfolio have the same\\r\\nasymptotic growth rate; that is,\\r\\nlim\\r\\nT→∞\\r\\n1\\r\\nT\\r\\n\\x10\\r\\nlog Xπ∗(T)(T) − log Xπˆ(T)\\r\\n\\x11\\r\\n= 0, (4.41)\\r\\nwhere π\\r\\n∗\\r\\n(T) and πˆ are as in Definitions 4.20 and 4.21, respectively.\\r\\nProof. Since Xπ∗(T)(T) ≥ Xξ(T) holds for every ξ ∈ ∆\\r\\nN−1,n\\r\\n+ for every T ≥ 0, the inequality “≥”\\r\\nof (4.41) is obvious from (4.38).\\r\\nWe now show the reverse inequality. Let ξ\\r\\n∗ ∈ ∆\\r\\nN−1,n\\r\\n+ be the corresponding vector of weights\\r\\nπ\\r\\n∗\\r\\n(T) as in Definition 4.20. For any ξ ∈ ∆\\r\\nN−1,n\\r\\n+ satisfying ||ξ\\r\\n∗ −ξ|| ≤ η for some η > 0, we have\\r\\nthe estimate\\r\\n1\\r\\nT\\r\\n\\x10\\r\\nlog Xξ(T) − log Xξ\\r\\n∗ (T)\\r\\n\\x11\\r\\n≥ −\\r\\nη\\r\\nT\\r\\n\\x10an\\r\\nδ\\r\\nmax\\r\\n1≤k≤n\\r\\nL\\r\\nµ(k)−µ(k+1) (T) + bn\\r\\nδ\\r\\n2 max\\r\\n1≤k≤n\\r\\n[µ(k), µ(k)](T)\\r\\n\\x11\\r\\n=: −\\r\\nη\\r\\nT\\r\\nKT ,\\r\\nin the same manner as in the proof of Lemma 4.19, for some positive constants an and bn\\r\\ndepending on n. Due to the condition (4.40), we can take η sufficiently small such that η\\r\\nT KT ≤ ǫ\\r\\nholds for every T ≥ 1, for any given ǫ > 0. To summarize, for any given ǫ > 0, there exists η > 0\\r\\nsuch that\\r\\n1\\r\\nT\\r\\n\\r\\nlog Xξ(T) − log Xξ\\r\\n∗ (T)\\r\\n\\x01\\r\\n≥ −ǫ (4.42)\\r\\nholds for every ξ ∈ B(ξ\\r\\n∗\\r\\n, η) and for every T ≥ 1. Here, B(ξ\\r\\n∗\\r\\n, η) is the intersection of ∆N−1,n\\r\\n+\\r\\nand || · ||-ball in R\\r\\nN centered at ξ∗ with radius η. We denote VB(ξ\\r\\n∗,η) and V∆\\r\\nN−1,n\\r\\n+\\r\\nthe volume\\r\\nof B(ξ\\r\\n∗\\r\\n, η) and the volume of the subset ∆N−1,n\\r\\n+ of R\\r\\nN , respectively.\\r\\nFrom (4.38) and Jensen’s inequality, we have\\r\\n\\x12\\r\\nXπˆ(T)\\r\\nXπ∗(T)(T)\\r\\n\\x13 1\\r\\nT\\r\\n=\\r\\n\\x12\\r\\nR\\r\\n∆\\r\\nN−1,n\\r\\n+\\r\\nXξ(T)dξ\\r\\nXξ\\r\\n∗ (T) V∆\\r\\nN−1,n\\r\\n+\\r\\n\\x13 1\\r\\nT\\r\\n≥\\r\\n\\x12R\\r\\nB(ξ\\r\\n∗,η) Xξ(T)dξ\\r\\nXξ\\r\\n∗ (T) V∆\\r\\nN−1,n\\r\\n+\\r\\n\\x13 1\\r\\nT\\r\\n≥\\r\\n\\r\\nVB(ξ∗,η)\\r\\n\\x01 1\\r\\nT −1\\r\\nR\\r\\nB(ξ\\r\\n∗,η) Xξ(T)\\r\\n1\\r\\nT dξ\\r\\n\\r\\nXξ\\r\\n∗ (T)\\r\\n\\x01 1\\r\\nT\\r\\n\\r\\nV∆\\r\\nN−1,n\\r\\n+\\r\\n\\x01 1\\r\\nT\\r\\n=\\r\\n\\r\\nVB(ξ∗,η)\\r\\n\\x01 1\\r\\nT −1\\r\\n\\r\\nV∆\\r\\nN−1,n\\r\\n+\\r\\n\\x01 1\\r\\nT\\r\\nZ\\r\\nB(ξ\\r\\n∗,η)\\r\\n\\x12\\r\\nXξ(T)\\r\\nXξ\\r\\n∗ (T)\\r\\n\\x13 1\\r\\nT\\r\\ndξ ≥\\r\\n\\x12\\r\\nVB(ξ∗,η)\\r\\nV∆\\r\\nN−1,n\\r\\n+\\r\\n\\x13 1\\r\\nT\\r\\ne\\r\\n−ǫ\\r\\n,\\r\\nwhere the last inequality is from (4.42). Taking logarithms, then letting T → ∞ for any given\\r\\nǫ > 0, the desired inequality follows.\\r\\n45\\n5 Conclusion\\r\\nMost of the results in Section 3, including the main Theorem 3.19, a foundational result of\\r\\nequity market structure and of the study of arbitrage in open markets, can be formulated quite\\r\\nsimply in terms of the local characteristics αe and ec of the open market, defined in (2.40), (2.41).\\r\\nIn particular, the supermartingale num´eraire portfolio ρ in the top n open market, if it exists,\\r\\nshould satisfy the equation αe = ecρ of (3.11). From this equation, we were able to conclude\\r\\nthat the supermartingale num´eraire portfolio ρ in the open market takes the form of ρ = Dec\\r\\n†αe.\\r\\nHere, multiplying by the diagonal matrix D of (2.47) makes the portfolio invest only in the top\\r\\nn stocks, while maintaining its supermartingale num´eraire property.\\r\\nHowever, as foretold in the introductory part of Section 4, we cannot use this technique to\\r\\ndeal with stock portfolios; multiplying by D a stock portfolio in order to make it invest only\\r\\nin the top n stocks, destroys its self-financing property. For example, a unit vector π := e\\r\\n1 =\\r\\n(1, 0, · · · , 0) is a stock portfolio which invests all capital into the first stock, but Dπ is not a stock\\r\\nportfolio as it invests all wealth into the money market whenever the first stock fails to belong\\r\\nto the top n market. Thus, for stock portfolios in open markets, a different approach is offered.\\r\\nFernholz’s functional generation of stock portfolios with ranked market weights, under the extra\\r\\nconditions (A) and (B) of Definition 4.11, provides a systematic way to construct stock portfolios\\r\\nthat invest only in the top n open market. This approach also yields the ‘master formula’ in\\r\\nCorollary (4.15), which allows comparing these portfolios with the top n market portfolio, µe.\\r\\nAs an application of this formula, we could prove that Cover’s result on the universal portfolio\\r\\nis also valid in open markets.\\r\\nNonetheless, there are a lot of limitations when considering stock portfolios in open markets.\\r\\nFirst, the balance condition (4.24) significantly restricts the class of generating functions in\\r\\nopen markets. Moreover, the local time terms which appear on the right-hand side of the\\r\\nmaster formula (4.29), make it very difficult to find stock portfolios in open markets which\\r\\noutperform µe. These difficulties are an inevitable price to pay for dealing with stock portfolios\\r\\nin open markets.\\r\\n46\\nReferences\\r\\nBanner, A. D. and Ghomrasni, R. (2008). Local times of ranked continuous semimartingales.\\r\\nStochastic Process. Appl., 118(7):1244–1253.\\r\\nCover, T. (1991). Universal portfolios. Mathematical Finance, 1(1):1–29.\\r\\nCuchiero, C., Schachermayer, W., and Wong, T.-K. L. (2019). Cover’s universal portfolio,\\r\\nstochastic portfolio theory, and the num´eraire portfolio. Mathematical Finance, 29(3):773–\\r\\n803.\\r\\nFernholz, E. R. (2002). Stochastic Portfolio Theory, volume 48 of Applications of Mathematics\\r\\n(New York). Springer-Verlag, New York. Stochastic Modelling and Applied Probability.\\r\\nFernholz, R. (1999). Portfolio generating functions. In Avellaneda, M., editor, Quantitative\\r\\nAnalysis in Financial Markets. World Scientific.\\r\\nFernholz, R. and Karatzas, I. (2009). Stochastic Portfolio Theory: an overview. In Bensoussan,\\r\\nA., editor, Handbook of Numerical Analysis, volume Mathematical Modeling and Numerical\\r\\nMethods in Finance. Elsevier.\\r\\nJamshidian, F. (1992). Asymptotically optimal portfolios. Mathematical Finance, 2(2):131–150.\\r\\nKaratzas, I. and Kardaras, C. (2020). Portfolio Theory and Arbitrage. To be published.\\r\\nKaratzas, I. and Ruf, J. (2017). Trading strategies generated by Lyapunov functions. Finance\\r\\nand Stochastics, 21(3):753–787.\\r\\nKaratzas, I. and Shreve, S. E. (1991). Brownian Motion and Stochastic Calculus, volume 113 of\\r\\nGraduate Texts in Mathematics. Springer-Verlag, New York, second edition.\\r\\nLarsen, K. and Zitkovi´c, G. (2007). Stability of utility-maximization i ˇ n incomplete markets.\\r\\nStochastic Process. Appl., 117(11):1642–1662.\\r\\nSchweizer, M. (1995). On the minimal martingale measure and the F¨ollmer-Schweizer decom\\x02position. Stochastic Analysis and Applications, 13(5):573–599.\\r\\nVervuurt, A. and Karatzas, I. (2015). Diversity-weighted portfolios with negative parameter.\\r\\nAnn. Finance, 11(3-4):411–432.\\r\\n47'},\n",
       " {'name': '1810.10845v1.pdf',\n",
       "  'content': 'Forecasting of Jump Arrivals in Stock Prices: New\\r\\nAttention-based Network Architecture using Limit Order\\r\\nBook Data\\r\\nYmir M¨akinena, Juho Kanniainenb,∗, Moncef Gabbouja, Alexandros Iosifidisc\\r\\na Laboratory of Signal Processing, Tampere University of Technology, Finland.\\r\\nbLaboratory of Industrial and Information Management, Tampere University of Technology, Finland.\\r\\ncDepartment of Engineering, Electrical and Computer Engineering, Aarhus University, Denmark\\r\\nAbstract\\r\\nThe existing literature provides evidence that limit order book data can be used to predict\\r\\nshort-term price movements in stock markets. This paper proposes a new neural network\\r\\narchitecture for predicting return jump arrivals in equity markets with high-frequency\\r\\nlimit order book data. This new architecture, based on Convolutional Long Short-Term\\r\\nMemory with Attention, is introduced to apply time series representation learning with\\r\\nmemory and to focus the prediction attention on the most important features to improve\\r\\nperformance. The data set consists of order book data on five liquid U.S. stocks. The use\\r\\nof the attention mechanism makes it possible to analyze the importance of the inclusion\\r\\nlimit order book data and other input variables. By using this mechanism, we provide\\r\\nevidence that the use of limit order book data was found to improve the performance of\\r\\nthe proposed model in jump prediction, either clearly or marginally, depending on the\\r\\nunderlying stock. This suggests that path-dependence in limit order book markets is a\\r\\nstock specific feature. Moreover, we find that the proposed approach with an attention\\r\\nmechanism outperforms the multi-layer perceptron network as well as the convolutional\\r\\nneural network and Long Short-Term memory model.\\r\\nKeywords: Jumps, Limit Order Book Data, Neural Networks, Convolutional\\r\\nNetworks, Long Short-Term Memory, Attention Mechanism\\r\\n1. Introduction\\r\\nNowadays, many exchanges, such as the New York Stock Exchange (NYSE) and var\\x02ious NASDAQ exchanges, are using systems driven by limit order submissions. Limit\\r\\norders are submissions to the system that contain a price and the desired quantity to buy\\r\\nor sell. The Limit Order Book (LOB) markets operate in very high frequencies, where\\r\\ndelays generally range from milliseconds to several nanoseconds for machines located\\r\\n∗Corresponding author\\r\\nEmail addresses: (Ymir M¨akinen), juho.kanniainen@tut.fi (Juho Kanniainen),\\r\\nmoncef.gabbouj@tut.fi (Moncef Gabbouj), alexandros.iosifidis@eng.au.dk (Alexandros Iosifidis)\\r\\nPreprint submitted to arXiv September 17, 2021\\r\\narXiv:1810.10845v1 [q-fin.TR] 25 Oct 2018\\nnear the exchange. This, along with the possibility to obtain event data from exchanges,\\r\\nyields huge amounts of data, which has created new opportunities for data processing.\\r\\nThis enables market analysis on a completely new level on many interesting questions\\r\\n(see, for example Toth et al., 2015; Chiarella et al., 2015), but has also brought unique\\r\\nchallenges for both theory and computational methods (Cont, 2011). In the recent liter\\x02ature, both tractable models and data-driven approach—that is, machine learning—have\\r\\nbeen introduced to predict price movements with LOB data (Cont et al., 2010; Cont,\\r\\n2011; Cont and De Larrard, 2012; Kercheval and Zhang, 2015; Ntakaris et al., 2018;\\r\\nTsantekidis et al., 2017b,a; Passalis et al., 2017; Dixon, 2018; Tran et al., 2018; Sirignano\\r\\nand Cont, 2018). Overall, the existing literature provides evidence that limit order book\\r\\ndata can be used to predict price movements in stock markets.\\r\\nEven though stock prices movements have been predicted using LOB data in general,\\r\\nless research is published about the use of LOB data to predict the arrival of jumps in\\r\\nstock prices. Stock price jumps are significant discontinuities in the price path so that\\r\\nrealized return at that time is much greater than usual continuous innovations. In the\\r\\nliterature, there is strong empirical evidence on the existence of return jumps in stock\\r\\nmarkets (see, e.g., Eraker, 2004; Lee, 2012; Yang and Kanniainen, 2017, and references\\r\\ntherein). Economically, the return jumps reflect information arrivals (Lee, 2012; Bradley\\r\\net al., 2014; Kanniainen and Yue, 2017), and therefore the jumps in stock prices are\\r\\nalso related to the predictability of information releases. Moreover, return jumps are\\r\\nfundamentally important in option pricing (Cont and Tankov, 2003).\\r\\nThe main research question that this work addresses is as follows: How well can the\\r\\narrival of jumps in equity returns be predicted using high-frequency limit order book\\r\\n(LOB) data with advanced machine learning techniques? The consequent question is if\\r\\nprice jumps can be foreseen in the order book data. These questions are motivated by\\r\\nthe fact that market makers, i.e. liquidity providers, can have prior information about\\r\\nforthcoming—scheduled or non-scheduled—news arrivals that will be realized as large\\r\\nprice movements and play against the market makers. Sophisticated market makers\\r\\ndo not want to provide liquidity because limit orders can be understood as options to\\r\\ntrade the underlying security at a given price and they suffer from adverse selection (see\\r\\nCopeland and Galai, 1983). As Foucault et al. (2007) argue, speculators may exercise\\r\\nthese options, that is, pick off limit orders, if limit orders become stale after the arrival of\\r\\nnew information. For this reason, sophisticated market makers do not want to take a risk\\r\\nthat their limit orders are on the wrong side of the book to be exploited by fast traders\\r\\nright after the price jumps, which is seen as low limit order book liquidity Siikanen et al.\\r\\n(2017). Moreover, if market makers were capable to predict not only the location but also\\r\\nthe direction of the forthcoming jump, then limit order book can become asymmetrically\\r\\nilliquid.\\r\\nThis kind of situation is demonstrated in Figure 1, which illustrates the order book\\r\\nstates around a jump in mid-price for Apple on 9th of June, 2014, where a positive jump\\r\\nwas detected between 9:33-9:34am. The figure provides snapshots for 1 minute and 1\\r\\nsecond before the beginning of the 1-minute jump interval and the third snapshot plot 1\\r\\nsecond after the end of the same 1-minute jump interval. It demonstrates the following:\\r\\n– 1 minute before the beginning of the jump interval: The order book is rather\\r\\nasymmetric, though quite thin (thin and widespread) and it is relatively expensive\\r\\nto trade a large number of shares by market order.\\r\\n2\\n– 1 second before the beginning of the the jump interval: Order book has become\\r\\nasymmetric so that the order book is very illiquid on ask side while remaining\\r\\nrelatively liquid on bid side. This can mean that liquidity providers had a hunch\\r\\non shortly arriving positive mid-price jump. In this case, even small trades on ask\\r\\nside can induce large price movements up.\\r\\n– 1 second after the end of jump interval (and 1 minute 1 second after the beginning\\r\\nof the interval): The liquidity provides have come back on ask side and the ask-side\\r\\nliquidity has recovered.\\r\\n100000 0 100000\\r\\nQuantity\\r\\n92.2\\r\\n92.4\\r\\n92.6\\r\\n92.8\\r\\n93.0\\r\\n93.2\\r\\nPrice\\r\\n-1 min\\r\\n100000 0 100000\\r\\nQuantity\\r\\n92.2\\r\\n92.4\\r\\n92.6\\r\\n92.8\\r\\n93.0\\r\\n93.2\\r\\nPrice\\r\\n-1 s\\r\\n100000 0 100000\\r\\nQuantity\\r\\n92.2\\r\\n92.4\\r\\n92.6\\r\\n92.8\\r\\n93.0\\r\\n93.2\\r\\nPrice\\r\\n+1 min 1 s\\r\\nFigure 1: Three snapshots on Apple’s order book on 9th of June, 2014, around a jump detected between\\r\\n9:33-9:34am. A plot on left shows the state of the order book one minute before the beginning of the\\r\\njump interval at 9:32am. A plot in the middle shows the state of the order book just a second before\\r\\nthe beginning of the interval at 9:29:59am. The third plot on right draws the order book a second after\\r\\nthe end of the interval at 9:34:01am. The side left of the red line at zero quantity contains the bids (i.e.\\r\\nbid orders are presented as negative quantity), referred to as the bid side of the book, and the right side\\r\\ncontains asks (i.e. positive quantity), referred to as the ask side. The black dotted lines present the\\r\\nmid-prices. The data is provided by Nasdaq US.\\r\\nThis example also begs the question about the root cause: if market makers antici\\x02pated the price jump based on market fundamentals and thus delivered no liquidity on\\r\\nask side or, alternatively, if the price movement was introduced by microstructure noise\\r\\nso that the order book illiquidity on the ask side was not based on market fundamentals.\\r\\nIn this paper, we keep both explanation possible. In fact, the root cause is rather irrel\\x02evant – the aim of this paper is to build neural network models to predict price jumps\\r\\nand thus answer the question if price jumps can be foreseen in the order book data, were\\r\\nthe root cause one or other.\\r\\nMethodologically, we have two-class prediction problem: Whether or not there is a\\r\\njump within the next minute. The output data consists of minute-by-minute observations\\r\\nabout the location and sign of detected jumps in stock prices. The input data is extracted\\r\\nfrom the reconstructed order books. We do not only use the ’raw’ data, i.e. prices and\\r\\nquantities on different levels, but also hand-crafted features that are extracted from\\r\\n3\\nmillisecond-level observations over the past 120 minutes. Regarding jumps, this paper\\r\\nfollows the existing literature to define jumps as large price movements that cannot be\\r\\nexplained by Brownian motion. As a preliminary step, the locations of return jumps\\r\\nare detected from the high-frequency mid-price data utilizing the nonparametric jump\\r\\ndetection test of (Lee and Mykland, 2008). Then, after preprocessing the limit order\\r\\nbook data, various neural network methods are applied to predict the locations of jumps\\r\\nusing real-time features on high-frequency limit order data.\\r\\nIn this paper, machine learning refers to a group of methods characterized by their\\r\\nlearning property, which allows the system to adjust its parameters by itself. Different\\r\\nmachine learning methods, especially neural networks, the first of which introduced in\\r\\nthe 1950s (Rosenblatt, 1957), have been becoming increasingly popular within the last\\r\\ndecade. Neural networks have been shown to be one of the few methods that is broadly\\r\\nsuccessful in time series prediction (Graves, 2012), although financial time series are\\r\\ngenerally regarded as very difficult to predict (Kara et al., 2011). In this paper, we use\\r\\nnot only the standard multi-layer perceptron network (MLP) but also a convolutional\\r\\nneural network (CNN) and a Long Short-Term Memory (LSTM) network, both of which\\r\\nhave been especially successful in predicting stock price movements (Tsantekidis et al.,\\r\\n2017b,a). Moreover, a new network model is developed by combining convolutional and\\r\\nLong Short-Term Memory (LSTM) layers as well as the attention model proposed by\\r\\nZhou et al. (2016). The proposed convolutional LSTM attention model (CNN–LSTM–\\r\\nAttention) aims to utilize LSTM for time series memory, convolution (CNN), and the\\r\\nattention model for reducing the input size, increasing locality, and focusing on the\\r\\nmost important features to improve prediction results. In addition to the main question\\r\\nabove, we also consider which method (MLP, CNN, LSTM, CNN-LSTM-Attention) is\\r\\nbest for predicting jumps with LOB data. The performance of the proposed CNN-LSTM\\x02Attention network is of particular interest, as it offers a new combination of methods that\\r\\nis jointly optimized for jump prediction.\\r\\nTo analyze the predictability and performance of the selected networks, a dataset of\\r\\nhigh frequency LOB data from several top NASDAQ stocks is employed for both training\\r\\nand testing the proposed methods. The stocks that are used are GOOG (Google), MSFT\\r\\n(Microsoft), AAPL (Apple), INTC (Intel), and FB (Facebook).1\\r\\nThe rest of the paper is organized as follows. Section 2 introduces both output\\r\\n(detected jump locations) and input data (real-time order book features). Then, Section\\r\\n3 presents the network models used in this paper, including the new network architecture\\r\\ncalled the CNN-LSTM-Attention model. Section 4 provides the empirical results, and,\\r\\nfinally, Section 5 concludes this work.\\r\\n2. Data\\r\\n2.1. Data sets\\r\\nThis research is conducted using NASDAQ’s “TotalView-ITCH” limit order data.\\r\\nThe data consist of ultra-high-frequency (on millisecond bases) information regarding\\r\\n1We emphasize that the proposed methods are applicable for any security for which limit order book\\r\\ndata is available. At the same time, the methods are not applicable to predict jumps in Foreign Exchange\\r\\nMarkets (Bates, 1996) and other markets where such a limit order book is not publicly available or to\\r\\nanalyze the processes related to the real investments (Dixit et al., 1994; Kanniainen, 2009) or other\\r\\nassets whose value processes are not observable.\\r\\n4\\nthe limit orders, cancellations, and trades executed through NASDAQ’s system. The\\r\\ndata contain prices and quantities of orders as well as their linked partial and full trades\\r\\nand cancellations. The data are further transformed into two data sets:\\r\\n(i) Output data: Minute-by-minute data about detected jumps in stock prices. This is\\r\\nbased on mid-price observations from which jumps are detected, so each one-minute\\r\\ntime period is classified as either having a jump or not.\\r\\n(ii) Event-by-event input data about the state of the order book. These data are ex\\x02tracted from millisecond-level observations about order book events. The resulting\\r\\ndata contain both bid and ask prices as well as their quantities for the ten best\\r\\nlevels on both sides of the book.\\r\\nThe order-driven market systems work in a way such that investors may place either\\r\\nask or bid orders of their desired price, and the system will match eligible orders to create\\r\\na trade. Orders may be either limit or market orders. Limit orders are placed in the\\r\\nlist of orders at a specified price. Market orders are immediately executed with the limit\\r\\norder of the best price if such exists. In a way, this resembles a queue system, especially\\r\\nwhen orders of identical prices are submitted. A limit order that has not been executed\\r\\ncan also be cancelled at any time. Both trades and cancels can also be partial, meaning\\r\\nthat a part of the limit order will be left in the book after execution (Cont et al., 2010).\\r\\nStock Orders Trades Cancels\\r\\nAAPL 1963.37 181.33 1870.52\\r\\nFB 1665.53 136.32 1563.80\\r\\nINTC 848.58 71.38 823.11\\r\\nMSFT 1304.75 95.22 1272.25\\r\\nGOOG 480.34 27.86 462.20\\r\\nTable 1: Average number of order submissions, trades, and cancels for each stock over a minute. Trades\\r\\nand cancels also include partial executions and cancellations of orders.\\r\\nTo ensure a continuous order flow, several well-known liquid stocks are selected for the\\r\\nstudy. These are GOOG (Google), MSFT (Microsoft), AAPL (Apple)2, INTC (Intel),\\r\\nand FB (Facebook). All of the selected stocks have large amounts of orders and trades\\r\\neach day. Table 1 shows the average numbers of order submissions, cancellations, and\\r\\ntrades over one minute.\\r\\nThe data are divided into two categories: training data and test data. The training\\r\\ndata are those used to learn the problem, that is, the data fed to the networks in the\\r\\ntraining phase to adjust the weights through the optimization algorithm. The train\\x02ing data consist of the series of observations in fifty-days periods. Fifteen percent of\\r\\n2The price data for AAPL was adjusted slightly. On June 6, 2014 at 5pm, Apple issued 10 800 000\\r\\n000 new shares, effectively splitting each existing common share into seven separate parts. As this was\\r\\nin the middle of the observed period and caused no difference in individual investors’ wealth in terms of\\r\\nowned stock, all stock prices prior to the split are divided by seven to make the true value of the owned\\r\\nstock continuous.\\r\\n5\\nthe training data is selected as validation data before starting the training of a model.\\r\\nValidation and test data are intended to evaluate the performance of the system. This\\r\\ncannot be done with the training data alone, as the model will easily be overfitted, which\\r\\nsharply reduces the performance outside the training dataset because the trained model\\r\\nis no longer generalizable (Webb and Copsey, 2011). The difference between test and\\r\\nvalidation data is that validation data is constantly used in model selection and adjust\\x02ment during the training phase. After selecting the best model, test data are used to\\r\\nevaluate the model’s performance. Thus, validation data are kept separately from test\\r\\ndata to ensure that the model is not developed solely to be able to classify the test data;\\r\\nmoreover, this provides an objective view on the performance of the system.\\r\\nIn all datasets, observations are picked every minute, but the amount of jump samples\\r\\nis increased by duplicating the jump observations. Specifically, the beginning of the\\r\\nduplicated sample is shifted by several seconds to ensure there are no identical samples.\\r\\nThe time intervals of the data sets are presented in Table 2. Validation data are selected\\r\\nin a way such that the duplicated samples belong to either the training or validation data\\r\\nsets. The data are divided into training sets based on the day of the observation. A total\\r\\nof 360 days, spanning about one and half years, are selected from 2014-2015. The data\\r\\nare divided so that first there are 50 days of training data, followed by 10 days of test\\r\\ndata. The next set contains the first 50 days as well as the following 50, and it is tested\\r\\non the following 10 days after both sets. This pattern is followed through the whole\\r\\ndataset so that the seventh test set trains on 350 days and tests with the last 10 of 360.\\r\\nAdditionally, the training data are presented in a window such that the model is trained\\r\\non the newest 50 samples at a time starting from the beginning of the observation period\\r\\n(but not reset between sets).\\r\\nData set Training days Test days\\r\\n1. 1-50 51-60\\r\\n2. 1-100 101-110\\r\\n3. 1-150 151-160\\r\\n4. 1-200 201-210\\r\\n5. 1-250 251-260\\r\\n6. 1-300 301-310\\r\\n7. 1-350 351-360\\r\\nTable 2: Division of data into sets used in training in 50 daylong sequences.\\r\\n2.2. Detected Jumps (output data)\\r\\nTo detect jumps in stock prices, we use an algorithm proposed by Lee and Mykland\\r\\n(2008). As jumps are predicted short term, samples are collected every minute for the\\r\\nduration of the observation period. This gives a one-minute window in which a jump\\r\\nmay occur, allowing these samples to be classified as either having or not having a jump\\r\\nin the following one-minute period. We run the jump detection algorithm for the entire\\r\\nsampling period for the collection of necessary amounts of jump samples. The length of\\r\\nthe data window used for the estimation of bipower variation is 600 minutes.\\r\\n6\\nThe frequencies of detected jumps are presented in Table 3. On average, there are\\r\\naround three jumps per day per stock. However, jumps are not evenly divided between\\r\\ndays. Instead, the days that have jumps tend to have a larger number of jumps on\\r\\naverage. A sample distribution of jumps per day counts is shown in Figure 2. Moreover,\\r\\nduring a single trading day, jumps tend to be heavily skewed towards morning hours, as\\r\\nobserved, for example, by Lee and Mykland (2008). The vast majority of detected jumps\\r\\noccurred within the first half hour of the trading day, with only occasional jumps after\\r\\nthe first 1.5 hours for all stocks. Additionally, all stocks had a slight increase in quantity\\r\\nat 2 pm, where the time period between 14:00 and 14:05 contained around four times as\\r\\nmany jumps as between 13:55 and 14:00. The jumps at this time occur during multiple\\r\\ndays thorough the whole observation period. The distribution of jumps according to the\\r\\ntime of day counted for the whole observation period is presented in Figure 3.\\r\\nTraining period AAPL FB GOOG MSFT INTC Average\\r\\n1-50 164 182 155 160 149 162\\r\\n51-100 200 177 131 161 150 164\\r\\n101-150 172 192 102 152 165 157\\r\\n151-200 161 171 125 132 170 152\\r\\n201-250 178 181 136 149 155 160\\r\\n251-300 172 186 111 128 155 150\\r\\n301-350 184 182 109 122 139 147\\r\\nAverage 176 182 124 143 155 156\\r\\nTest period AAPL FB GOOG MSFT INTC Average\\r\\n51-60 37 35 42 37 38 38\\r\\n101-110 54 55 37 27 17 38\\r\\n151-160 49 38 38 26 27 36\\r\\n201-210 32 41 26 28 18 29\\r\\n251-260 39 32 33 26 27 31\\r\\n301-310 35 35 12 24 18 25\\r\\n351-360 26 19 16 5 13 16\\r\\nAverage 39 36 29 25 23 30\\r\\nTable 3: The frequencies of jumps in the training and test datasets by stock and set. A total of 5537\\r\\njumps across 362 − 2 days were observed.\\r\\nThe data from the stocks are used to construct training and test sets by time and\\r\\nstock, as presented in Table 3. Jumps at the very first observation of the public market\\r\\nopening (9.30) were not taken into account. Additionally, jumps from the first two\\r\\ndays were not detected due to the insufficient amount of previous observations to satisfy\\r\\nthe window size requirement of the jump detection algorithm. This also means that\\r\\nthe training sets presented in Table 2 skip the first two days of the sequence to avoid\\r\\nlabeling possible jump samples as non-jumps due to the undetectability of jumps during\\r\\n7\\n0 5 10 15 20 25 30\\r\\nNumber of jumps\\r\\n0\\r\\n10\\r\\n20\\r\\n30\\r\\n40\\r\\n50\\r\\nDays\\r\\nDays / jump count\\r\\nFigure 2: Jumps per day counts for AAPL. Around 12% of days had no jumps, and around 19% of days\\r\\nhad more than five jumps, with the median being three jumps.\\r\\n0 1 2 3 4 5 6\\r\\nTime (hours)\\r\\n0\\r\\n500\\r\\n1000\\r\\n1500\\r\\n2000\\r\\nJumps\\r\\nJumps by time of day\\r\\nFigure 3: Total amount of jumps at times of the day in 10 minute periods starting from the beginning\\r\\nof the trading day at 9.30 am. All stocks are distributed similarly, shown by the different colors, from\\r\\ntop to bottom: AAPL, FB, INTC, MSFT, GOOG.\\r\\nthe beginning of the price sequence. Thus, day 1 in the table is really day 3 of the price\\r\\nobservation period.\\r\\n2.3. Order book state data (input data)\\r\\nThe inputs use LOB data, which is reconstructed from the order book event data.\\r\\nThe LOB contains both ask and bid prices as well as their quantities for the ten best\\r\\nlevels on both sides of the book. This is done simply by checking active orders at a\\r\\ncertain time, which can be then ordered by price to obtain the ten best levels so that\\r\\n8\\nthe lowest ask and the highest bid are on the first level, and subsequent levels are filled\\r\\nby existing prices next in the order. The quantity is the sum of quantities for orders of\\r\\nthat price, and quantities at levels with multiple orders are the sum of all active orders\\r\\nat that level. The method of constructing the book also means that empty levels cannot\\r\\nexist between two defined prices. Instead, completely empty ticks are left off unless there\\r\\nsimply are not enough orders to fill the ten levels, in which case the levels last in order\\r\\nare filled with prices and quantities of 0.\\r\\nTo get the best view of the state of the order book, we follow Kercheval and Zhang\\r\\n(2015) to extract 144 indicators from the data: a) the basic set of features containing the\\r\\nraw LOB data over ten levels, with both sides containing price and volume values for bid\\r\\nand ask orders, b) the time-insensitive set of features describing the state of the LOB,\\r\\nexploiting past information, and c) the time-sensitive features describing the information\\r\\nedge in the raw data by taking time into account. The time-insensitive set contains\\r\\nfurther information about the spreads, differences, and means. The time-sensitive set\\r\\ncontains features that indicate changes in the data in time, such as derivatives, accel\\x02erations, and intensities. These features, provided in Table 4, are used also in Ntakaris\\r\\net al. (2018); Tsantekidis et al. (2017b,a); Passalis et al. (2017); Tran et al. (2018).\\r\\nFeature Set Description Details\\r\\na) Basic v1 = {P\\r\\nask\\r\\ni\\r\\n, V ask\\r\\ni\\r\\n, Pbid\\r\\ni\\r\\n, V bid\\r\\ni }\\r\\nn\\r\\ni=1 10-level LOB Data, i = 1 . . . n\\r\\nb) Time- v2 = {(P\\r\\nask\\r\\ni − P\\r\\nbid\\r\\ni\\r\\n),(P\\r\\nask\\r\\ni + P\\r\\nbid\\r\\ni\\r\\n)/2}\\r\\nn\\r\\ni=1 Spread & Mid-Price\\r\\nInsensitive v3 = {|P\\r\\nask\\r\\ni+1 − P\\r\\nask\\r\\ni\\r\\n|, |P\\r\\nbid\\r\\ni+1 − P\\r\\nbid\\r\\ni\\r\\n|}n−1\\r\\ni=1 Price differences\\r\\nv4 = {\\r\\n1\\r\\nn\\r\\nPn\\r\\ni=1\\r\\nP\\r\\nask\\r\\ni\\r\\n,\\r\\n1\\r\\nn\\r\\nPn\\r\\ni=1\\r\\nP\\r\\nbid\\r\\ni\\r\\n,\\r\\n1\\r\\nn\\r\\nPn\\r\\ni=1\\r\\nV\\r\\nask\\r\\ni\\r\\n,\\r\\n1\\r\\nn\\r\\nPn\\r\\ni=1\\r\\nV\\r\\nbid\\r\\ni } Price & Volume means\\r\\nv5 = {\\r\\n1\\r\\nn\\r\\nPn\\r\\ni=1(P\\r\\nask\\r\\ni − P\\r\\nbid\\r\\ni\\r\\n),\\r\\n1\\r\\nn\\r\\nPn\\r\\ni=1(V\\r\\nask\\r\\ni − V\\r\\nbid\\r\\ni\\r\\n)} Accumulated differences\\r\\nc) Time- v6 = {dPask\\r\\ni /dt, dPbidi /dt, dV ask\\r\\ni /dt, dV bid\\r\\ni /dt}\\r\\nn\\r\\ni=1 Price & Volume derivatives\\r\\nSensitive v7 = {λ\\r\\nla\\r\\n∆t\\r\\n, λlb\\r\\n∆t\\r\\n, λma\\r\\n∆t\\r\\n, λmb\\r\\n∆t\\r\\n, λca\\r\\n∆t\\r\\n, λcb\\r\\n∆t} Average intensity per type\\r\\nv8 = {1λ\\r\\nla∆t>λla∆T\\r\\n, 1λ\\r\\nlb∆t>λlb∆T\\r\\n, 1λma ∆t >λma ∆T\\r\\n, 1λmb ∆t >λmb ∆T} Relative intensity indicators\\r\\nv9 = {dλma/dt, dλlb/dt, dλmb/dt, dλla/dt} Accelarations\\r\\nd) Clock time v10 = {b t\\r\\n60 c} Time, rounded to hours\\r\\nTable 4: Feature Sets. In the table, P stands for prices and V for volumes. In addition, λ denoted the\\r\\nintensity of a given order book event.\\r\\nIn addition to the LOB data, some of the time-sensitive features presented in Kercheval\\r\\nand Zhang (2015) require calculating intensities, that is, the number of arriving orders\\r\\nor cancellations of a certain type, which cannot be directly calculated from the con\\x02structed book and instead must be counted from the original event data. The intensities\\r\\nare separated into ask and bid, and the orders are categorized based on whether they\\r\\nare limit or market orders. The intensities at each step are calculated directly from the\\r\\norder flow data and attached to the corresponding order book data of the step. Within\\r\\nmarket hours, both the limit order book state and the intensities are calculated every\\r\\nsecond, yielding a total of 23,400 observations per day. Data from non-trading hours\\r\\nare discarded due to different trading mechanisms, and the data used over multiple days\\r\\nis treated as a continuous sequence. In addition to some of the suggested features, ap\\x02proximate times of the observations are included to account for the differences in stock\\r\\nbehavior at different points during the day. The timestamps are rounded to the nearest\\r\\n9\\nhour to avoid converging to the local minima of purely time-based classification.\\r\\nFor the data sets (Table 2), samples are extracted by a one-minute moving window\\r\\nthorough the training set, creating one sample per minute, for a total of 390 samples a\\r\\nday. Positive samples are defined as those with a jump right after the last observation,\\r\\nthat is, during the next minute, which is not included in the window. Negative samples\\r\\nare only collected from the moving window; for positive samples, the window is shifted\\r\\nslightly multiple times to generate more positive samples due to the large difference\\r\\nin the sample sizes. As the data are collected every second, it is possible to shift the\\r\\nwindow small enough amounts to not include the jump while creating slightly different\\r\\ndata for the samples to increase variety and to preserve the original classification of a\\r\\njump existing within the next minute. To ensure that possible periodical changes in the\\r\\norder books will not affect the classification results due to only positive samples being\\r\\nshifted, negative samples are also shifted randomly.\\r\\nAll collected samples contain 120 steps sampled at a one-minute interval. These\\r\\nsamples are then normalized using the z-score to eliminate the irrelevant noise due to,\\r\\nfor example, different starting prices: xnormalized = (x − x¯) /σx¯, where x is the feature\\r\\nvector to be normalized, ¯x is its mean, and σx¯ the standard deviation (Cheadle et al.,\\r\\n2003). The features are normalized sample-wise one feature at a time: x is then a vector\\r\\nof length 120 containing all observations of a single feature in a sample, for example, all\\r\\nof the ask level 5 volumes. Separate normalization for different features is necessary due\\r\\nto the vastly different behaviors and scales between both different levels and volumes\\r\\nas well as their indicators. Including different indicators calculated from the limit order\\r\\nbook, such as the price differences, allows for the preservation of information regarding\\r\\nthe relations between different values, even after normalization.\\r\\nThe data are normalized sample-by sample due to the changes in price behavior that\\r\\noccur even during a single day. A relatively short normalization window is also needed to\\r\\navoid larger scale price dependence. If, for example, the data were normalized over the\\r\\nfull-time period, the main differences between prices in observations would come from\\r\\nthe long time drift instead of the price changes in the recent past. As long-term changes\\r\\nare unlikely to be the main determining factor of jump occurrence in minute-level data,\\r\\nthe normalization period should be short enough to avoid learning from them.\\r\\nAdditionally, in the used data, the most important factors seem to be changes that\\r\\noccur in the hours right before the jump. Changes within this timespan have also been\\r\\nnoted for bigger jumps associated with company announcements, where changes in liq\\x02uidity often start over an hour before the price jump (Siikanen et al., 2017,b). The\\r\\nnormalization done within the sample also requires a sufficiently big observation win\\x02dow, as it needs to be large enough to capture the element of change. There is also a\\r\\nfairly signficant chance that a jump has already occurred on the same day at the time\\r\\nanother prediction is made, lessening the impact of price changes compared to the pre\\x02vious data. Additionally, since all samples are of equal length, for the first two hours of\\r\\nthe day, the window must include samples collected from the previous day.\\r\\n3. Neural Network Models\\r\\nNeural networks are learning systems that are modeled based on the structure of the\\r\\nhuman brain: large amounts of individual units, called neurons, process the information\\r\\n10\\nfed through the network. They then adjust their inner weights based on the informa\\x02tion provided, making the system “learn”. Methodwise, price jump prediction can be\\r\\nseen as a similar problem to mid-price prediction (Kercheval and Zhang, 2015; Ntakaris\\r\\net al., 2018; Tsantekidis et al., 2017b,a; Passalis et al., 2017; Tran et al., 2018; Sirig\\x02nano and Cont, 2018), although it has its own problems due to the small proportion of\\r\\ntime-intervals with jumps versus without jumps. The methods used in this work are the\\r\\nstandard MLP, LSTM, and convolutional networks, which are chosen due to their suc\\x02cess in the prediction and classification of other time series Yang et al. (2015); Xingjian\\r\\net al. (2015); Greff et al. (2017). Moreover, a new network model is developed by com\\x02bining convolutional and LSTM layers as well as the attention model proposed by Zhou\\r\\net al. (2016). The proposed convolutional Long Short-Term Memory Attention model\\r\\n(CNN-LSTM-Attention) aims to utilize LSTM for time series memory and CNN and the\\r\\nattention model for reducing the input size, increasing locality, and focusing on the most\\r\\nimportant features to improve prediction results.\\r\\n3.1. Multi-layer perceptron\\r\\nPerhaps the most common type of neural network is the MLP, which is a feed-forward\\r\\nneural network formed by layers of neurons stacked in a hierarchical manner. It receives\\r\\nthe data vectors in the input layer, and then the information is propagated throughout\\r\\nthe hidden layers, providing a response at the output layer. Each layer is formed by a set\\r\\nof neurons, each receiving an input from the neurons of the preceding layer, and provides\\r\\na nonlinear response of the form\\r\\nbh = θh\\r\\n X\\r\\nI\\r\\ni=1\\r\\nwihxi\\r\\n!\\r\\n, (1)\\r\\nwhere I is the number of neurons in the previous layer, each providing an input xi, and\\r\\nwij is the weight connecting the i-th neuron in the preceding layer to the j-th neuron\\r\\nof the current layer. θ·is a nonlinear (piece-wise) differentiable function, which is used\\r\\nto nonlinearly scale the response of the neuron. The output neuron works exactly as\\r\\nthe hidden layer neurons, although they may use a different activation (e.g., to lead to\\r\\nprobability-like responses).\\r\\nThe optimal size of the hidden layer is defined by the data used, whereas the output\\r\\nlayer size is defined by the number of output classes (Graves, 2012; Jefferson et al., 1995).\\r\\nMulti-class classification is performed by following a competitive training approach, that\\r\\nis, the output neuron with the highest response indicates the predicted class label (Chollet\\r\\nand Others, 2015).\\r\\nThe training of a network consists of two phases, forward pass and backward pass.\\r\\nIn forward pass, training vectors are introduced to the network and its responses are\\r\\nobtained. These responses are used in combination with the provided annotations (i.e.,\\r\\ntarget vectors indicating the optimal response for each training vector) to define the\\r\\nnetwork’s error with respect to a loss function. This error is then used in the backward\\r\\npass to update the parameters of the network. This is achieved by exploiting the (piece\\x02wise) differentiable property of the neurons’ activation functions, following a gradient\\r\\ndescent learning approach called error backpropagation. We use an advanced version of\\r\\nthis parameter update approach, called Adam Kingma and Ba (2014), which adaptively\\r\\ndefines the hyper-parameters of each update step based on the input vectors.\\r\\n11\\nFor classification problems and networks giving probability-like responses, the crossen\\x02tropy loss function is commonly used. It determines the entropy between sets by mea\\x02suring the average number of bits needed to identify an event drawn from a set. For\\r\\ndiscrete sets p and q, where piis the true label and qiis the current predicted value,\\r\\nbinary crossentropy can be defined as\\r\\nH(p, q) = −\\r\\nX\\r\\ni\\r\\npilog(qi). (2)\\r\\nIt can be shown that when choosing between distributions q, which estimate the true dis\\x02tribution p, minimizing cross-entropy leads to choosing the best estimate by maximizing\\r\\nthe overall entropy (Shore and Johnson, 1980). Thus, it is a suitable loss function to be\\r\\nminimized, and often portrays the true loss better than simple error measures.\\r\\n3.2. Recurrent neural networks and Long Short-Term Memory\\r\\nIn this paper, the Long Short-Term Memory (LSTM) model is used to accumulate\\r\\nfeatures in time-domain and to simulate memory, by passing the previous signals through\\r\\nthe same nodes. LSTM can be seen as a special case of recurrent neural networks (RNN)\\r\\nin which the connections between neurons allow directedly cyclical connections. In a\\r\\nbasic recurrent network, neurons form connections inside the same layer, creating a net\\r\\nof one-way connections. In the simplest form, this means a standard neural network but\\r\\nwith a feedback loop. The connections in the basic RNN are weighted as in a standard\\r\\nMLP. RNNs address the temporal relationships in their inputs by maintaining an internal\\r\\nstate due to the recursive property, a quality especially suitable for time series data (Giles\\r\\net al., 2001).\\r\\nLSTM was first proposed by Hochreiter and Schmidhuber (1997) and it was developed\\r\\nto combat the problem of keeping error signals in proportion when flowing backward in\\r\\ntime (especially for long time dependencies) by making use of both short-term memory,\\r\\nbased on the recurrent connections, and long-term memory, represented by the slowly\\r\\nchanging weights. A constant error signal flow is ensured by connecting the neurons to\\r\\nthemselves. LSTM introduced the concept of a memory cell to control the memory flow\\r\\nof a network. A memory cell is a singular neural unit with the addition of multiplicative\\r\\ninput and output gates. These are created to protect the neuron from changes triggered\\r\\nby irrelevant inputs and to protect other units from the irrelevant information currently\\r\\nstored within the neuron. Each memory cell has a fixed self-connection and processes\\r\\ninput from multiple input sources to create the output signals. Memory cells that share\\r\\nthe same input and output gates form memory cell blocks (Hochreiter and Schmidhuber,\\r\\n1997).\\r\\nTraining an LSTM network is done using a modified version of backpropagation,\\r\\nwhere a single step involves a forward pass and the update of all units through the\\r\\ncomputation of error signals for all weights, which are passed backwards in the network\\r\\n(backward pass). The activation of the input gate y\\r\\nin and output gate yout are defined\\r\\nas\\r\\ny\\r\\noutj\\r\\n(t) = foutj(\\r\\nX\\r\\nm\\r\\nwoutjmy\\r\\nm(t − 1)), (3)\\r\\ny\\r\\ninj\\r\\n(t) = finj(\\r\\nX\\r\\nm\\r\\nwinjmy\\r\\nm(t − 1)), (4)\\r\\n12\\nwhere j is the memory block index and v is a cell inside the memory block j, so that c\\r\\nv\\r\\nj\\r\\nmarks the v-th cell of the j-th memory block and wlm is the weight for the connection\\r\\nbetween units m and l. Input gates are defined as in and output gates as out. The\\r\\nloop sums all the source units defined by the network. The function f is a differentiable\\r\\nfunction for the gates, such as the logistic sigmoid\\r\\nf(x) = 1\\r\\n1 + e−x\\r\\n, (5)\\r\\nwhere x ∈ [0, 1]. The input is further squashed by a differentiable function g(·) (Gers\\r\\net al., 2000).\\r\\nGers et al. (2000) further adds to the LSTM model by including an additional gate,\\r\\nthe “forget gate”. The forget gate allows the LSTM cell to reset itself at appropriate\\r\\ntimes, releasing resources to use. The LSTM layer outputs either a one-dimensional\\r\\nvector of activations for each feature or a two-dimensional structure with a value for\\r\\neach feature at each processed time step. With an LSTM layer connected to a dense\\r\\nlayer, the former is needed, as the dense layer expects one-dimensional input. However,\\r\\nsome models, such as the attention model proposed by Zhou et al. (2016), require multi\\x02dimensional LSTM output when applied to the LSTM layer, as its purpose is to calculate\\r\\na weighting value for each time step.\\r\\n3.3. Convolutional neural networks\\r\\nConvolutional neural networks (CNN) can be used to capture patterns in time and\\r\\nfeature space. Convolution neurons combine information from neighboring observations\\r\\nin the feature and/or time dimensions and each neuron identifies different pattern in the\\r\\ninput time-series.\\r\\nCNNs mimic the way the visual system processes visual data. Specific neurons are\\r\\nonly concerned with specific parts of the input, simultaneously making the position of\\r\\nspecific features less relevant, as long as they are in a certain relation to the other features.\\r\\nEven though they were originally proposed for image recognition tasks, CNNs have found\\r\\nuses in speech classification and time series prediction tasks. The convolutional network\\r\\ncombines the principles of the importance of locality in data points, shared weights\\r\\nbetween points, and possible subsampling. (LeCun and Bengio, 1995) CNNs have been\\r\\nespecially successful in the domain of image processing, providing, for example, a winning\\r\\nbest entry in the popular ImageNet image classification challenge (Krizhevsky et al.,\\r\\n2012) and the ImageNet feature localization challenge (Sermanet et al., 2013). In a\\r\\nCNN, the images are first normalized, resized, and approximately centered. After the\\r\\ninput layer, each unit in a single layer receives inputs from a certain set of inputs in its\\r\\nneighborhood from the previous layer, making the receptive fields localized. This allows\\r\\nthe extraction of certain local features, which can then be combined (LeCun and Bengio,\\r\\n1995).\\r\\nEach convolutional layer is followed by an additional “pooling layer” to perform local\\r\\naveraging and/or subsampling. This reduces the resolution of the input at every step and\\r\\nreduces the network’s sensitivity to shifts and distortions (LeCun and Bengio, 1995). A\\r\\nsimple CNN-pooling combination is shown in Figure 4. Pooling can also be done using\\r\\nthe maximums of the input window, drawing attention to more pronounced features\\r\\nwhile reducing the resolution. This is called max pooling and is also often done between\\r\\n13\\nsample\\tin\\twindow apply\\tconvolution apply\\tresult max\\tpool\\r\\nFigure 4: 2d convolution with max pooling. A single 2d CNN layer, taking the convolution neigh\\x02borhood, applying the convolution kernel and reducing dimensionality with max pooling. (Adapted from\\r\\n(Sermanet et al., 2013))\\r\\nconvolutions (Scherer et al., 2010). Convolutional and pooling layers are usually repeated\\r\\nuntil the feature maps convolute to a singular output for all possible classification results\\r\\n(LeCun and Bengio, 1995), or they may be connected to regular dense (MLP) network\\r\\nlayers to produce the final output (Krizhevsky et al., 2012).\\r\\nTime series analysis with convolutional neural networks works much the same as\\r\\nin images, although the dimensionalities of the inputs are naturally different. Locality\\r\\nof the fields works well with time series, as the observations are dependent on time;\\r\\nthe same observation can be followed by different results at different times, and the\\r\\nsurroundings of the observation can be used to generate a better estimate (L¨angkvist\\r\\net al., 2014). Convolutions can also be applied to one-dimensional time series data,\\r\\nallowing the convolution for both single- and multi-parameter problems (Di Persio and\\r\\nHonchar, 2016). An example of feature-dimension time series convolution is presented in\\r\\nFigure 5.\\r\\n3.4. Dropout\\r\\nDropout layers, first proposed by Hinton et al. (2012), improve classification results\\r\\nby preventing complex co-adaptations of the training data. On each introduction of a\\r\\ntraining sample, hidden units are randomly omitted, according to a probability distribu\\x02tion, thus “dropping out” the unit activations from the information flow. As they may\\r\\nnot be present, this means hidden units cannot rely on the presence of any other hidden\\r\\nunit at any time, making the network more robust as it cannot depend on any single\\r\\npassed value.\\r\\n14\\ntime\\r\\nfeatures\\r\\nconvolution pooling\\r\\ninput\\r\\n…\\r\\nconnect\\tfurther\\r\\nFigure 5: 1d convolution with pooling. A single 1d CNN layer, convoluting in feature dimension,\\r\\napplying the convolution kernel and reducing dimensionality with unspecified pooling. (Adapted from\\r\\n(Hu et al., 2014))\\r\\nThe probability of dropping out any one unit is predefined; Hinton et al. (2012)\\r\\nproposes a dropout threshold of 0.5. This means that generally only half of the units\\r\\nare present at any iteration of the training, and thus even if they fully (over)fit into a\\r\\ngiven training sample, the entire network will not. Dropout can be introduced with any\\r\\nconnection, for example, between layers, or inside the recurrent connections of an LSTM\\r\\nlayer.\\r\\n3.5. Attention model\\r\\nAttention is a mechanism that has been recently used in sentence classification, trans\\x02lation (Bahdanau et al., 2014), and generation (Graves, 2013). An attention mechanism\\r\\ngenerates an output by focusing on relevant elements of the input. That is, the attention\\r\\nmodel gives weights to the elements of the input sequence based on both the location\\r\\nand the contents of the sequence, supporting the possibility that observations at specific\\r\\nspots could have a greater importance in determining the results. Thus, the attention\\r\\nmodel could be used to weight different words in a sentence to find relations between\\r\\nthem (Zhou et al., 2016) or to weigh different time steps in a time series, for example, in\\r\\nspeech recognition (Chorowski et al., 2015).\\r\\nIn this paper, we employ the attention layer proposed by Zhou et al. (2016) for sen\\x02tence relation classification, with LOB data. Here, the steps are the timesteps of the LOB\\r\\nobservations processed by the recurrent layer. In this model, the output representation\\r\\n15\\nr is formed by a weighted sum of several output vectors:\\r\\nM = tanh(H)\\r\\nα = softmax(w\\r\\nTM)\\r\\nr = HαT,\\r\\nwhere H is the attention layer input matrix consisting of the recurrent layer’s output\\r\\nvectors [h1, h2, ..., hT ], and H ∈ Rd\\r\\nw×L, where d\\r\\nw is the dimension of the observation\\r\\nvectors. w is a trained parameter vector and w\\r\\nT\\r\\nits transpose; L is the length of the\\r\\nsequence (Zhou et al., 2016). The sof tmax is a normalized exponential function that\\r\\nsquashes the inputs to output probability-like responses in the range [0, 1]:\\r\\nsoftmax(zi) = e\\r\\nzi\\r\\nP\\r\\nj\\r\\ne\\r\\nzj\\r\\n,\\r\\nwhere the activation is calculated in an element-wise manner (Mikolov et al., 2015). The\\r\\nfinal output of the attention layer is calculated from the representations with\\r\\nh\\r\\n∗ = tanh(r).\\r\\nZhou et al. (2016) also includes a softmax dense layer, which takes the attention output\\r\\nh\\r\\n∗\\r\\nto calculate the final classification result (Zhou et al., 2016).\\r\\nIn this work, the attention layer is connected directly into the unconvoluted input,\\r\\nfollowed by the convolution and LSTM layers. Additionally, in place of time steps, the\\r\\nattention model is applied on the feature dimension. That is, all features are weighted,\\r\\nand the same weight for a single feature is repeated and thus applied to all of the time\\r\\nsteps within the sample. This allows for selecting the features that are most relevant in\\r\\nany given sample.\\r\\n3.6. Implementation\\r\\nThe neural networks were built using several Python libraries. The main library\\r\\nused was Keras, a high-level, open-source framework for building multilayer networks\\r\\nfocused on enabling fast experimentation (Chollet and Others, 2015). Keras, however,\\r\\ndoes not provide the network structure but rather an interface for building it. Thus,\\r\\nTensorFlow, an implementation for executing different machine learning algorithms, was\\r\\nused as the Keras backend. Tensorflow is a flexible system, allowing the utilization of\\r\\ngraphics processing units for speeding up the computation (Abadi et al., 2015). The\\r\\nKeras’ Model provides a simple framework to which layers can be added in a straight\\x02forward manner, and their connections to other layers can be specified. This allows the\\r\\nbuilding of both simple sequential networks as well as more branching approaches. As\\r\\nKeras provides premade definitions for many different layer types, experimenting with\\r\\ndifferent configurations is fairly simple.\\r\\nThe MLP network consisted of two leaky ReLu layers of 40 neurons each. The MLP\\r\\nnetwork structure is presented in Figure 6. The CNN model for predicting stock price\\r\\nmovements proposed by Tsantekidis et al. (2017b) is illustrated in Figure 7. It consists\\r\\nof eight layers. The first layer is a 2D convolution with 16 filters of size (4,40), followed\\r\\nby an 1D convolution with 16 four-long filters and a max pool of two. This is followed\\r\\n16\\nInput\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nFigure 6: Layer structure of the MLP network used.\\r\\nby 2 additional 1D convolutions with 32 size 3 filters, and one additional size 2 max\\r\\npooling layer. Furthermore, there are two fully connected dense layers, the first one\\r\\nwith 32 neurons and the following one with 3 neurons. The output layer is modified to\\r\\ncontain only a single output neuron to act as a two-class classifier. Additionally, while\\r\\nthe network was designed to use only the 40 pure limit order book data features, it was\\r\\nmodified in size to test it with the extra features used in this research. However, the\\r\\noriginal 40-feature network was selected for further analysis due to better results. The\\r\\ndifferences may have been due to the 2D convolution, which mixes features in both time\\r\\nand feature axes.\\r\\nInput\\r\\n2D\\tconvolution\\t (4,\\t40),\\t 16\\tfilters\\r\\n1D\\tconvolution\\t (4,),\\t 16\\tfilters\\r\\nMax\\tpooling\\t (2,)\\r\\n1D\\tconvolution\\t (3,),\\t 32\\tfilters\\r\\n1D\\tconvolution\\t (3,),\\t 32\\tfilters\\r\\nMax\\tpooling\\t (2,)\\r\\nDense,\\t32\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nInput\\r\\nAttention\\r\\n1D\\tconvolution\\t (5,),\\t 32\\tfilters\\r\\nMax\\tpooling\\t 1D\\t(2,)\\r\\nLSTM,\\t40\\tneurons\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nMerge Input\\r\\nLSTM,\\t40\\tneurons\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nFigure 7: Layer structure of the convolutional network used.\\r\\n17\\nAnother network is the LSTM network for stock price prediction presented in Tsan\\x02tekidis et al. (2017b). The LSTM network structure is shown in Figure 8. The network\\r\\nconsists of an LSTM layer with 40 hidden neurons followed by a fully connected Leaky\\r\\nReLu unit defined in Maas et al. (2013).\\r\\nut\\r\\n4,\\t40),\\t 16\\tfilters\\r\\n(4,),\\t 16\\tfilters\\r\\ning\\t (2,)\\r\\n(3,),\\t 32\\tfilters\\r\\n(3,),\\t 32\\tfilters\\r\\ning\\t (2,)\\r\\n\\tneurons\\r\\n\\tneuron\\r\\nInput\\r\\nAttention\\r\\n1D\\tconvolution\\t (5,),\\t 32\\tfilters\\r\\nMax\\tpooling\\t 1D\\t(2,)\\r\\nLSTM,\\t40\\tneurons\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nMerge Input\\r\\nLSTM,\\t40\\tneurons\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nFigure 8: Layer structure of the LSTM network used.\\r\\nThe CNN-LSTM-Attention network is the most sophisticated model in this paper and\\r\\nit is designed to learn the most important patterns through feature and time domains\\r\\nfor jump prediction and to optimally weight the different features to predict jumps. It\\r\\nis constructed as follows. The first layer connected after the input is the attention layer,\\r\\ncomposed of multiple Keras components: A regular dense layer with tanh activation is\\r\\ncreated with a weight for each time step, flattened to one dimension, to which softmax\\r\\nactivation is further applied. This layer is repeated once for each step to apply the\\r\\nattention to full time steps. The dimensions are then switched to match the original\\r\\ninput shape and merged together by multiplying the activations from the attention model\\r\\nand the input values from the original input layer. This gives each feature its own weight\\r\\nsuch that the same feature is weighted the same across all given time steps within a\\r\\nsample.\\r\\nThe resulting attention mechanism output is a matrix of the original input size, which\\r\\nis passed forward to a 1D convolutional layer with 32 size 5 filters. The convolution output\\r\\nis further processed with a max pool of size 2, and the max pooled activations are passed\\r\\nto an LSTM layer with 40 relu neurons. The LSTM also includes a dropout of 0.5 both\\r\\ninside regular and recurrent connections. After the LSTM, there is a regular dense fully\\r\\nconnected layer of the same size and, finally, the singular output neuron with sigmoid\\r\\nactivation. This means that the output is a single value in the range [0, 1], which is\\r\\nthen rounded to obtain class prediction. The proposed network structure is illustrated\\r\\nin Figure 9.\\r\\n18\\nInput\\r\\n2D\\tconvolution\\t (4,\\t40),\\t 16\\tfilters\\r\\n1D\\tconvolution\\t (4,),\\t 16\\tfilters\\r\\nMax\\tpooling\\t (2,)\\r\\n1D\\tconvolution\\t (3,),\\t 32\\tfilters\\r\\n1D\\tconvolution\\t (3,),\\t 32\\tfilters\\r\\nMax\\tpooling\\t (2,)\\r\\nDense,\\t32\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nInput\\r\\nAttention\\r\\n1D\\tconvolution\\t (5,),\\t 32\\tfilters\\r\\nMax\\tpooling\\t 1D\\t(2,)\\r\\nLSTM,\\t40\\tneurons\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nMerge Input\\r\\nLSTM,\\t40\\tneurons\\r\\nDense,\\t40\\tneurons\\r\\nDense,\\t1\\tneuron\\r\\nFigure 9: Layer structure of the CNN-LSTM-Attention network. Additionally, the attention layer\\r\\nconsists of repeated single neuron layers to apply activations on a time-step basis.\\r\\n4. Results\\r\\n4.1. Performance Measures\\r\\nThe network performance was assessed with several metrics. The main target is F1\\r\\nscore, which is defined as the harmonic mean of precision and recall:\\r\\nF1 =\\r\\n2\\r\\n1\\r\\nrecall +\\r\\n1\\r\\nprecision\\r\\n(6)\\r\\nRecall is defined as\\r\\nrecall =\\r\\ntp\\r\\ntp + fn (7)\\r\\nand precision as\\r\\nprecision =\\r\\ntp\\r\\ntp + f p (8)\\r\\nwhere tp is true positives, the number of jump samples correctly classified as jumps;\\r\\nfn is false negatives, jumps incorrectly classified as negative samples; and f p is false\\r\\npositives, negative samples incorrectly classified as jumps. Thus, recall is the portion of\\r\\njumps classified as jumps, and precision is the portion of real jump samples in samples\\r\\nclassified as jumps (Lipton et al., 2014). High recall implies that a majority of jumps\\r\\ncan be detected, whereas high precision means that jumps can be detected without also\\r\\nclassifying many non-jump samples as jumps.\\r\\nIt should be noted that neither precision nor recall consider the number of true\\r\\nnegatives. This also makes F1 independent of the ratio of accurately classified negatives,\\r\\nand instead focuses heavily on correctly classifying the positives. Thus, F1 provides a\\r\\nmeasure that is both non-linear and non-symmetricF1 is commonly used in cases where\\r\\n19\\nthe positive class is rare compared to the negatives (Lipton et al., 2014). As the portion\\r\\nof jumps in the data is very small, F1 is a suitable metric. Successfully predicting no\\r\\njump is also a less informative result than successfully predicting one given a relatively\\r\\nsmall number of false positives, as very good accuracy could be achieved by just always\\r\\npredicting that there will be no jump. Measuring the results with F1 also avoids this\\r\\nscenario, as correct negatives do not affect the score.\\r\\nAnother used metric is Cohen’s Kappa:\\r\\nκ =\\r\\npo − pc\\r\\n1 − pc\\r\\n, (9)\\r\\nfor which po and pc can be calculated from the confusion matrix (Cohen, 1960). As\\r\\nthe Kappa also takes into account the agreement by chance, it can be seen as a more\\r\\nrobust measurement for agreement. There is no single interpretation for what can be\\r\\nconsidered a good Kappa value, and thus it depends on the type of the problem analyzed.\\r\\nFleiss et al. (2003, p. 604) presents intervals such that values greater than 0.75 signify\\r\\nexcellent agreement, values above 0.40 a fair or good agreement, and values under 0.40\\r\\na poor agreement. Conversely, Landis and Koch (1977) suggest that values of 0.21-0.40\\r\\nare already fair, 0.41-0.60 are moderate, 0.61-0.80 are substantial, and values above that\\r\\nare almost perfect.\\r\\n4.2. Main results\\r\\nPerformance of the networks used is presented by averaging the scores across all\\r\\nstocks and sets in Table 5. Because of the unbalanced data, we consider F1 as the\\r\\nmost appropriate performance measure, and therefore F1 values are in a bold font in\\r\\nthe table. The table shows that of all the tested networks, the CNN–LSTM–Attention\\r\\nmodel achieves the highest average F1 for all samples (around 0.72). The second best\\r\\nnetwork is the pure LSTM (0.69), followed by the CNN (0.66) and the CNN-LSTM-v10,\\r\\nwhere no information other than the time of day (feature v10) is used. Finally, the MLP\\r\\nachieved an average F1 of 0.53. All of the models clearly outperform a random classifier,\\r\\nfor which F1 is 0.32.\\r\\nAdditionally, when comparing average F1s by stocks, according to Table 6, the CNN\\x02LSTM-Attention model again performs the best. With all of the tested network models,\\r\\nthe resulting F1s are above those of a random classifier. MLP is somewhat worse, possibly\\r\\ndue to being unable to deal with fairly large time series input data without overlearning,\\r\\nbut it still clearly better than random choice. The scores implies that at least a part of\\r\\nthe jumps in the data are predictable with a reasonable level of confidence. Additionally,\\r\\nwhen interpreting the Kappa scores, the scores of both LSTM models can be seen as at\\r\\nleast good if not very good. Also the CNN-LSTM-v10 works surprisingly well, given that\\r\\njumps are predicted using information on the time of day only. Therefore, LOB data\\r\\ncan be said to be useful for predicting price jumps, especially for Apple, Facebook, and\\r\\nMicrosoft, but sometimes it gives a rather marginal advantage (see Google and Intel in\\r\\nTable 6).\\r\\nAccording to Tables 5 and 6, the most promising model is CNN-LSTM-Attention, and\\r\\nhence we zoom into the F1 scores of CNN-LSTM-Attention model over the stocks and\\r\\ndata sets in Table 7. The average F1 for all CNN-LSTM-Attention model sets is 0.71, with\\r\\nsome variation between both different time periods and different stocks. The variation is\\r\\n20\\nPrecision Recall F1 Cohen’s Kappa\\r\\nCNN-LSTM-A 0.66 0.80 0.72 0.62\\r\\nLSTM 0.73 0.66 0.69 0.60\\r\\nCNN 0.66 0.66 0.66 0.55\\r\\nMLP 0.78 0.41 0.53 0.44\\r\\nRandom 0.24 0.50 0.32 0.00\\r\\nCNN-LSTM-v10 0.57 0.79 0.66 0.54\\r\\nTable 5: Overall precision, recall, F1, and Cohen’s Kappa scores for all four networks and a random\\r\\nclassifier. As F1 is the most appropriated performance measure because of unbalanced data, it is em\\x02phasized in a bold font in the table. CNN-LSTM-v10 denotes CNN-LSTM using no other information\\r\\nbut the time of day (feature v10).\\r\\nmost likely due to different stock price and jump dynamics for different securities.3 As\\r\\ntrue negatives are not accounted for in the calculation of F1, and having less jumps to\\r\\ndetect directly lowers precision if the ratio of detected jumps remains the same, fewer\\r\\njumps to detect tends to cause a lower score unless the amount of jumps that are easily\\r\\npredictable remains the same. Intel (INTC) jumps were predicted the best (0.78 F1)\\r\\nwhile Microsoft had the worst score, with a difference of around ten percentage points.\\r\\nThe corresponding results for other models, the LSTM model proposed in Tsantekidis\\r\\net al. (2017a), the convolutional model proposed in Tsantekidis et al. (2017b), and a\\r\\nregular two-layer MLP network, are provided in the Appendix (Tables 8, 9, 10).\\r\\n4.3. Attention layer\\r\\nEven though the attention model was originally proposed in Zhou et al. (2016) for\\r\\nuse in the time dimension, it can also be used to highlight important features. That is,\\r\\ninstead of focusing on time steps where interesting things occur, the network focuses on\\r\\nfeatures that are particularly interesting at that time point. This was suitable due to the\\r\\nlarge number of features used.\\r\\nFour samples were selected for qualitative analysis of the attention mechanism through\\r\\nthe activations of the layer created for a given sample: one true positive and true negative\\r\\nas well as one false positive and false negative. As expected, all of these samples placed\\r\\nsome importance on the time feature v10 (see Table 4), as its feasibility had already\\r\\nbeen studied while developing the network. The attention was checked by comparing the\\r\\nattention activations from the unmerged layer. Other than that, the attention was given\\r\\nto quite different features between the four samples.\\r\\nBoth true and false positives included some volumes from the top levels of the order\\r\\nbook from the basic set (v1) of the features. Still, neither included any price information\\r\\nfrom the books in the attention, even though the quantities were regarded as important.\\r\\nBoth also included several derivatives from the set of features v6; the true positive had\\r\\n3\\r\\nIn Table 7. the scores are first calculated individually for each set and stock and then averaged over\\r\\nthe sets. As the number of samples is not constant across the sets, the average of individually calculated\\r\\nset scores is not exactly equal to the score calculated directly across all samples. Because of this, there\\r\\nare minor deviations between the values presented in Tables 5, 6 and 7.\\r\\n21\\nPrecision Recall F1 Cohen’s Kappa\\r\\nAAPL CNN-LSTM-A 0.67 0.74 0.71 0.61\\r\\nLSTM 0.70 0.54 0.61 0.50\\r\\nCNN 0.62 0.57 0.59 0.47\\r\\nMLP 0.73 0.35 0.47 0.37\\r\\nRandom 0.24 0.50 0.33 0.00\\r\\nCNN-LSTM-v10 0.65 0.72 0.68 0.58\\r\\nFB CNN-LSTM-A 0.67 0.81 0.73 0.63\\r\\nLSTM 0.73 0.67 0.70 0.60\\r\\nCNN 0.69 0.70 0.69 0.59\\r\\nMLP 0.80 0.41 0.54 0.45\\r\\nRandom 0.25 0.50 0.33 0.00\\r\\nCNN-LSTM-v10 0.48 0.90 0.62 0.44\\r\\nGOOG CNN-LSTM-A 0.60 0.83 0.69 0.59\\r\\nLSTM 0.69 0.64 0.66 0.58\\r\\nCNN 0.62 0.77 0.69 0.59\\r\\nMLP 0.75 0.36 0.48 0.40\\r\\nRandom 0.21 0.50 0.30 0.00\\r\\nCNN-LSTM-v10 0.57 0.82 0.68 0.57\\r\\nMSFT CNN-LSTM–A 0.62 0.77 0.69 0.59\\r\\nLSTM 0.72 0.65 0.68 0.60\\r\\nCNN 0.66 0.66 0.66 0.56\\r\\nMLP 0.76 0.42 0.54 0.46\\r\\nRandom 0.22 0.50 0.30 0.00\\r\\nCNN-LSTM-v10 0.49 0.70 0.58 0.44\\r\\nINTC CNN-LSTM-A 0.72 0.85 0.78 0.69\\r\\nLSTM 0.77 0.77 0.77 0.69\\r\\nCNN 0.71 0.63 0.67 0.56\\r\\nMLP 0.84 0.48 0.61 0.52\\r\\nRandom 0.26 0.50 0.34 0.00\\r\\nCNN-LSTM-v10 0.72 0.82 0.77 0.68\\r\\nTable 6: Overall precision, recall, F1, and Cohen’s Kappa scores by network and stock for all four\\r\\nnetworks tested. The CNN-LSTM-Attention network has the best total average Recall, F1, and Kappa\\r\\nscores. The MLP has a higher precision with the cost of greatly reduced recall–the given class labels\\r\\nare skewed towards negative. Random is a fully random classifier that is included as a benchmark. By\\r\\ndefinition, random recall is 0.50 and Kappa is 0. As F1 is the most appropriated performance measure\\r\\nbecause of unbalanced data, it is emphasized in a bold font in the table. CNN-LSTM-v10 denotes\\r\\nCNN-LSTM using no other information other than the time of day (feature v10).\\r\\nonly quantities from both sides, whereas the false positive also included ask prices. In\\x02terestingly, the real positive sample also focused on a single, level 4 ask price difference\\r\\nfrom feature set v3, none other of which were featured in any other inspected samples.\\r\\nThe negative samples were fairly different from the positives, although both had\\r\\nfocused on the time feature as well as some derivative from feature set v6. However,\\r\\nthe derivatives of the negative samples are purely quantity derivatives for ask or bid\\r\\nvolumes. The false negative and true negative were also fairly similar to each other in\\r\\n22\\nSet/stock AAPL FB GOOG MSFT INTC Average\\r\\nSet 1 0.76 0.63 0.68 0.68 0.74 0.70\\r\\nSet 2 0.70 0.67 0.47 0.72 0.84 0.68\\r\\nSet 3 0.66 0.72 0.69 0.68 0.74 0.70\\r\\nSet 4 0.65 0.85 0.69 0.68 0.81 0.73\\r\\nSet 5 0.75 0.81 0.78 0.66 0.79 0.76\\r\\nSet 6 0.72 0.75 0.77 0.76 0.74 0.75\\r\\nSet 7 0.70 0.69 0.74 0.53 0.77 0.68\\r\\nAverage 0.70 0.73 0.69 0.67 0.78 0.71\\r\\nTable 7: F1 scores by set and by stock for CNN-LSTM-Attention network.\\r\\nregards to attention, which makes sense since the end classification result was the same.\\r\\nBoth focused on the basic set for both ask and bid as well as quantity and price. They\\r\\nalso focused on several mid prices, which was not the case with either positive sample.\\r\\nInterestingly, in addition to these, the true negative sample had also selected cancel\\r\\nintensities for both ask and bid.\\r\\nFor all of the inspected samples, the focus was clearly on several specific feature sets,\\r\\nwith some features not included in the attentions at all. However, ask and bid values\\r\\nwere fairly equally present as well as values from across the 10 levels of the order books.\\r\\nQuantity-dependent values were regarded as especially important in multiple feature sets,\\r\\nwhich indicates the relation between liquidity and jumps (see Siikanen et al., 2017b,?,\\r\\nfor the relation between order book liquidity and news announcements). Still, the full\\r\\ntest set was not inspected, so even though this implies some relevance for the values to\\r\\nwhich attention was paid, the remaining ones would need to be thoroughly inspected to\\r\\ndraw conclusions regarding their usefulness in jump prediction.\\r\\n4.4. Prediction of the direction of jumps\\r\\nThis study focused on a two-class prediction problem, where the class information\\r\\nrelated to whether or not the jump statistic would exceed a certain threshold within\\r\\nthe next minute independently of the direction of the jump. However, the problem can\\r\\nalso be formulated as a three-class classification problem, where c0 is no jump, c1 is\\r\\nan upwards jump, and c2 is a downwards jump. This was done with the same model\\r\\nand the same input data, but the output layer was changed to a three-neuron block.\\r\\nStill, this formulation proved much more challenging than only predicting the arrical of\\r\\na jump, and the results did not significantly differ from random selection with regard to\\r\\nthe jump direction. Results for differentiating between the jump classes c1 and c2 with\\r\\nthe CNN-LSTM-Attention network are presented in Table 12 in Appendix.\\r\\nThe better predictability of the arrival of a jump (than the sign of a jump) is consis\\x02tent with the literature, which has provided evidence about the relation between jumps\\r\\nand news announcements and analysts’ recommendations, whose arrival time, but not\\r\\ndirection, is often predictable. Lee (2012); Bradley et al. (2014) This result is also consis\\x02tent with the definition of the stock price model with jumps: the jump term consists of\\r\\ntwo separate parts, the counting process of the occurrence of a jump and the jump size,\\r\\n23\\nwhich is independent and identically distributed . Thus, it is possible that the counting\\r\\nprocess is predictable, while the size and thus the direction of the jump is not. However,\\r\\nthis study did not focus on such a prediction case, and so it may be possible to also\\r\\npredict the direction of the jump with some accuracy, albeit likely much lower than the\\r\\noccurrence of the jump, which may even be known beforehand due to a prescheduled\\r\\nrelease of information regarding the stock.\\r\\nAnother possible direction for jump prediction is to not only predict jumps but to\\r\\npredict the jump statistic for each time step. This means that the prediction would\\r\\nbe a regression problem in which the jumps are only implied by the statistics that are\\r\\nabove the threshold by interpreting the output of the network. However, this method\\r\\nrequires more accurate predictions of price movements considering the non-jump price\\r\\nprocess σ(t)dW(t), and thus it also requires the prediction of the stock price process.\\r\\nOf course, the task can be made easier by using only the absolute value of the jump\\r\\nstatistic, disregarding the direction of the movement. Even so, this task was much more\\r\\ndifficult than simple jump prediction, at least with the tested methods. This is most\\r\\nlikely due to the combination of more exact prediction requirements of a regression task,\\r\\nthe limitations of the chosen methods, and the need to predict the magnitude of the\\r\\nnormal stock price process to get the correct statistics in the samples without jumps.\\r\\nThis question will be addressed further in our future research.\\r\\n5. Conclusions\\r\\nIn this work, a new CNN-LSTM-Attention model was developed to predict jumps\\r\\nfrom LOB data. The problem was also tested on several existing neural network models\\r\\nfor stock price prediction. The networks were both trained and tested for five separate\\r\\nstocks for a total of 360 days. The developed CNN-LSTM-Attention model performed\\r\\nthe best in regard to F1 for all stocks and, on average, also Cohen’s Kappa. Overall,\\r\\nthe F1 scores achieved with the other tested networks provide evidence that the use of\\r\\nlimit order book data was found to improve the performance of the proposed model in\\r\\njump prediction, either clearly or marginally, depending on the underlying stock. This\\r\\nsuggests that path-dependence in limit order book markets is a stock specific feature.\\r\\nPredicting the direction of such jumps is much more difficult as is the jump statistic in\\r\\ngeneral, which in line with the efficient market hypothesis.\\r\\nMoreover, we find that the proposed approach with an attention mechanism outper\\x02forms the multi-layer perceptron network as well as the convolutional neural network and\\r\\nLong Short-Term memory model.\\r\\nThis research focused mainly on predicting the existence of a jump in the near fu\\x02ture. Predicting the jump statistic—rather than if the statistics exceeded a threshold\\r\\nor not—would be especially interesting even outside the context of the jumps, as it is\\r\\nin essence a statistic of future returns of the stock. Thus, it could also be of use in\\r\\nthose contexts, even if the absolute value is used and the stock price direction is thus\\r\\nnot considered. Additionally, the definition of jump itself includes some randomness due\\r\\nto the unbounded nature of the normal distribution, which would be eliminated from\\r\\nthe learning process if it only considered the jump statistic that is directly based on the\\r\\nstock price and thus does not include uncertainty in its interpretation. This study also\\r\\nfocused on several known network types, LSTM and CNN, to provide a basis for the\\r\\nmodel, and it might be beneficial to apply other types of classifiers to this type of a\\r\\n24\\nproblem. This problem about predicting the statistics as a non-linear regression problem\\r\\nwill be addressed in our future research.\\r\\nAnother area of interest is the feature attention model used in this study, as such\\r\\nmodels can also be also as indicators as to which types of measures can be used to\\r\\npredict price jumps in general and which features of the book affect the formation of\\r\\nthe jump. However, the attentions were only studied based on several sample series,\\r\\nand so more in-depth research on these features would be particularly interesting. The\\r\\nresults of the attention study could also be used to further develop the input data of the\\r\\npredictor network by expanding on the features that the network considers as relevant\\r\\nto classification. At the same time, performance in terms of results and computational\\r\\ncomplexity could be improved by leaving out features that do not yield significant weights\\r\\nfor any correctly classified samples.\\r\\nReferences\\r\\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G., Davis, A., Dean,\\r\\nJ., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Kaiser, L.,\\r\\nKudlur, M., Levenberg, J., Man, D., Monga, R., Moore, S., Murray, D., Shlens, J., Steiner, B.,\\r\\nSutskever, I., Tucker, P., Vanhoucke, V., Vasudevan, V., Vinyals, O., Warden, P., Wicke, M., Yu,\\r\\nY., Zheng, X., 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\\r\\nSystems. doi:10.1038/nn.3331, arXiv:1603.04467.\\r\\nBahdanau, D., Cho, K., Bengio, Y., 2014. Neural machine translation by jointly learning to align and\\r\\ntranslate. arXiv: 1409.0473 .\\r\\nBates, D.S., 1996. Jumps and stochastic volatility: Exchange rate processes implicit in deutsche mark\\r\\noptions. The Review of Financial Studies 9, 69–107.\\r\\nBradley, D., Clarke, J., Lee, S., Ornthanalai, C., 2014. Are analysts recommendations informative?\\r\\nintraday evidence on the impact of time stamp delays. The Journal of Finance 69, 645–673.\\r\\nCheadle, C., Vawter, M.P., Freed, W.J., Becker, K.G., 2003. Analysis of microarray data using Z score\\r\\ntransformation. The Journal of Molecular Diagnostics 5, 73–81. doi:10.1016/S1525-1578(10)\\r\\n60455-2.\\r\\nChiarella, C., He, X.Z., Wei, L., 2015. Learning, information processing and order submission in limit\\r\\norder markets. Journal of Economic Dynamics and Control 61, 245–268.\\r\\nChollet, F., Others, 2015. Keras.\\r\\nChorowski, J., Bahdanau, D., Serdyuk, D., 2015. Attention-based models for speech recognition, in:\\r\\nAdvances in Neural Information Processing Systems, pp. 577–585.\\r\\nCohen, J., 1960. A coefficient of agreement for nominal scales. Educational and psychological measure\\x02ment 20, 37–46.\\r\\nCont, R., 2011. Statistical modeling of high-frequency financial data. IEEE Signal Processing Magazine\\r\\n28, 16–25. doi:10.1109/MSP.2011.941548.\\r\\nCont, R., De Larrard, A., 2012. Order book dynamics in liquid markets: limit theorems and diffusion\\r\\napproximations. SSRN Working Paper .\\r\\nCont, R., Stoikov, S., Talreja, R., 2010. A Stochastic Model for Order Book Dynamics. Operations\\r\\nResearch 58, 549–563. doi:10.1287/opre.1090.0780.\\r\\nCont, R., Tankov, P., 2003. Financial modelling with jump processes. volume 2. CRC press.\\r\\nCopeland, T., Galai, D., 1983. Information effects on the bid-ask spread. the Journal of Finance 38,\\r\\n1457–1469.\\r\\nDi Persio, L., Honchar, O., 2016. Artificial neural networks architectures for stock price prediction:\\r\\nComparisons and applications. International Journal of Circuits, Systems and Signal Processing\\r\\n10, 403–413.\\r\\nDixit, A.K., Dixit, R.K., Pindyck, R.S., Pindyck, R., 1994. Investment under uncertainty. Princeton\\r\\nuniversity press.\\r\\nDixon, M., 2018. A high-frequency trade execution model for supervised learning. High Frequency\\r\\nforthcoming.\\r\\nEraker, B., 2004. Do stock prices and volatility jump? reconciling evidence from spot and option prices.\\r\\nThe Journal of Finance 59, 1367–1403.\\r\\n25\\nFleiss, J., Levin, B., Cho Paik, M., 2003. Statistical Methods for Rates and Proportions. John Wiley &\\r\\nSons. doi:10.1198/tech.2004.s812, arXiv:arXiv:1011.1669v3.\\r\\nFoucault, T., Moinas, S., Theissen, E., 2007. Does anonymity matter in electronic limit order markets?\\r\\nReview of Financial Studies 20, 1707–1747.\\r\\nGers, F.A., Schmidhuber, J., Cummins, F., 2000. Learning to Forget: Continual Prediction with LSTM.\\r\\nNeural Computation 12, 2451–2471. doi:10.1162/089976600300015015, arXiv:arXiv:1011.1669v3.\\r\\nGiles, C.L., Lawrence, S., Tsoi, A.C.A., 2001. Noisy Time Series Prediction using Recurrent Neural Net\\x02works and Grammatical Inference. Machine Learning 44, 161–183. doi:10.1023/A:1010884214864.\\r\\nGraves, A., 2012. Supervised Sequence Labelling with Recurrent Neural Networks. volume 385. doi:10.\\r\\n1007/978-3-642-24797-2, arXiv:arXiv:1308.0850v1.\\r\\nGraves, A., 2013. Generating Sequences With Recurrent Neural Networks. arXiv preprint\\r\\narXiv:1308.0850 arXiv:1308.0850.\\r\\nGreff, K., Srivastava, R.K., Koutn´ık, J., Steunebrink, B.R., Schmidhuber, J., 2017. Lstm: A search\\r\\nspace odyssey. IEEE transactions on neural networks and learning systems 28, 2222–2232.\\r\\nHinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R., 2012. Improving neural\\r\\nnetworks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 , 1–18.\\r\\nHochreiter, S., Schmidhuber, J., 1997. Long Short-Term Memory. Neural Computation 9, 1735–1780.\\r\\ndoi:10.1162/neco.1997.9.8.1735, arXiv:1206.2944.\\r\\nHu, B., Lu, Z., Li, H., Chen, Q., 2014. Convolutional neural network architectures for matching natural\\r\\nlanguage sentences, in: Advances in neural information processing systems, pp. 2042–2050.\\r\\nJefferson, M., Pendleton, N., Lucas, S., Horan, M., Tarassenko, L., 1995. Neural networks. doi:10.1016/\\r\\nS0140-6736(95)92880-4, arXiv:arXiv:1411.3159v1.\\r\\nKanniainen, J., 2009. Can properly discounted projects follow geometric brownian motion? Mathemat\\x02ical methods of operations research 70, 435.\\r\\nKanniainen, J., Yue, Y., 2017. The arrival of news and jumps in stock prices. SSRN Working Paper .\\r\\nKara, Y., Acar, M., Kaan, O., 2011. Expert Systems with Applications Predicting direction of stock ¨\\r\\nprice index movement using artificial neural networks and support vector machines : The sample\\r\\nof the Istanbul Stock Exchange. Expert Systems With Applications 38, 5311–5319. doi:10.1016/\\r\\nj.eswa.2010.10.027.\\r\\nKercheval, A.N., Zhang, Y., 2015. Modelling high-frequency limit order book dynamics with support\\r\\nvector machines. Quantitative Finance 15, 1315–1329. doi:10.1080/14697688.2015.1032546.\\r\\nKingma, D.P., Ba, J., 2014. Adam: A Method for Stochastic Optimization. arXiv preprint\\r\\narXiv:1412.6980 arXiv:1412.6980.\\r\\nKrizhevsky, A., Sutskever, I., Hinton, G., 2012. ImageNet Classification with Deep Convolutional Neural\\r\\nNetworks .\\r\\nLandis, J.R., Koch, G.G., 1977. The Measurement of Observer Agreement for Categorical Data. Bio\\x02metrics 33, 159. doi:10.2307/2529310.\\r\\nL¨angkvist, M., Karlsson, L., Loutfi, A., 2014. A review of unsupervised feature learning and deep\\r\\nlearning for time-series modeling. Pattern Recognition Letters .\\r\\nLeCun, Y., Bengio, Y., 1995. Convolutional networks for images, speech, and time series. The\\r\\nhandbook of brain theory and neural networks 3361, 255–258. doi:10.1109/IJCNN.2004.1381049,\\r\\narXiv:arXiv:1011.1669v3.\\r\\nLee, S., 2012. Jumps and information flow in financial markets. Review of Financial Studies 25, 439–479.\\r\\nLee, S.S., Mykland, P.a., 2008. Jumps in Real-Time Financial Markets: A New Nonparametric Test and\\r\\nJump Dynamics. The Review of Financial Studies 21, 2535–2563. doi:10.2139/ssrn.891611.\\r\\nLipton, Z.C., Elkan, C., Naryanaswamy, B., 2014. Optimal thresholding of classifiers to maximize\\r\\nF1 measure, in: Joint European Conference on Machine Learning and Knowledge Discovery in\\r\\nDatabases., pp. 225–239. doi:10.1007/978-3-662-44851-9_15, arXiv:1402.1892.\\r\\nMaas, A.L., Hannun, A.Y., Ng, A.Y., 2013. Rectifier Nonlinearities Improve Neural Network Acoustic\\r\\nModels. Proceedings of the 30 th International Conference on Machine Learning 28, 6.\\r\\nMikolov, T., Karafiat, M., Burget, L., Cernocky, J., Khudanpur, S., 2015. Recurrent Neural Network\\r\\nbased Language Model. Proceedings of the Annual Conference of the International Speech Com\\x02munication Association, INTERSPEECH , 1045–1048.\\r\\nNtakaris, A., Magris, M., Kanniainen, J., Gabbouj, M., Iosifidis, A., 2018. Benchmark dataset for\\r\\nmid-price prediction of limit order book data. forthcoming in Journal of Forecasting .\\r\\nPassalis, N., Tsantekidis, A., Tefas, A., Kanniainen, J., Gabbouj, M., Iosifidis, A., 2017. Time-series\\r\\nclassification using neural bag-of-features, in: Signal Processing Conference (EUSIPCO), 2017 25th\\r\\nEuropean, IEEE. pp. 301–305.\\r\\nRosenblatt, F., 1957. The Perceptron - A Perceiving and Recognizing Automaton. Technical Report.\\r\\n26\\ndoi:85-460-1.\\r\\nScherer, D., M¨uller, A., Behnke, S., 2010. Evaluation of pooling operations in convolutional architectures\\r\\nfor object recognition. Artificial Neural Networks ICANN 2010 .\\r\\nSermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., 2013. OverFeat: Inte\\x02grated Recognition, Localization and Detection using Convolutional Networks. arXiv preprint\\r\\narXiv doi:10.1109/CVPR.2015.7299176, arXiv:1312.6229.\\r\\nShore, J.E., Johnson, R.W., 1980. Axiomatic Derivation of the Principle of Maximum Entropy and\\r\\nthe Principle of Minimum Cross-Entropy. IEEE Transactions on Information Theory 26, 26–37.\\r\\ndoi:10.1109/TIT.1980.1056144.\\r\\nSiikanen, M., Kanniainen, J., Luoma, A., 2017b. What drives the sensitivity of limit order books to\\r\\ncompany announcement arrivals? Economics Letters 159, 65–68.\\r\\nSiikanen, M., Kanniainen, J., Valli, J., 2017. Limit order books and liquidity around scheduled and\\r\\nnon-scheduled announcements: Empirical evidence from nasdaq nordic. Finance Research Letters\\r\\n21, 264–271.\\r\\nSirignano, J., Cont, R., 2018. Universal features of price formation in financial markets: perspectives\\r\\nfrom deep learning. SSRN Working Paper .\\r\\nToth, B., Palit, I., Lillo, F., Farmer, J.D., 2015. Why is equity order flow so persistent? Journal of\\r\\nEconomic Dynamics and Control 51, 218–239.\\r\\nTran, D.T., Iosifidis, A., Kanniainen, J., Gabbouj, M., 2018. Temporal attention augmented bilinear net\\x02work for financial time-series data analysis. Forthcoming in IEEE Transactions on Neural Networks\\r\\nand Learning Systems .\\r\\nTsantekidis, A., Passalis, N., Tefas, A., Kanniainen, J., Gabbouj, M., Iosifidis, A., 2017a. Forecasting\\r\\nstock prices from the limit order book using convolutional neural networks, in: Business Informatics\\r\\n(CBI), 2017 IEEE 19th Conference on, IEEE. pp. 7–12.\\r\\nTsantekidis, A., Passalis, N., Tefas, A., Kanniainen, J., Gabbouj, M., Iosifidis, A., 2017b. Using deep\\r\\nlearning to detect price change indications in financial markets, in: Signal Processing Conference\\r\\n(EUSIPCO), 2017 25th European, IEEE. pp. 2511–2515.\\r\\nWebb, A.R., Copsey, K.D., 2011. Statistical Pattern Recognition. John Wiley & Sons. doi:10.1002/\\r\\n9781119952954.\\r\\nXingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c., 2015. Convolutional lstm\\r\\nnetwork: A machine learning approach for precipitation nowcasting, in: Advances in neural infor\\x02mation processing systems, pp. 802–810.\\r\\nYang, H., Kanniainen, J., 2017. Jump and volatility dynamics for the s&p 500: Evidence for infinite\\x02activity jumps with non-affine volatility dynamics from stock and option markets. Review of\\r\\nFinance 21, 811–844.\\r\\nYang, J., Nguyen, M.N., San, P.P., Li, X., Krishnaswamy, S., 2015. Deep convolutional neural networks\\r\\non multichannel time series for human activity recognition., in: IJCAI, pp. 3995–4001.\\r\\nZhou, P., Shi, W., Tian, J., Qi, Z., Li, B., Hao, H., 2016. Attention-based bidirectional long short\\x02term memory networks for relation classification. Proceedings of the 54th Annual Meeting of the\\r\\nAssociation for Computational Linguistics (Volume 2: Short Papers) .\\r\\nAppendix\\r\\n27\\nSet/stock AAPL FB GOOG MSFT INTC Average\\r\\nSet 1 0.71 0.59 0.61 0.71 0.77 0.68\\r\\nSet 2 0.36 0.63 0.56 0.82 0.83 0.64\\r\\nSet 3 0.64 0.74 0.61 0.73 0.72 0.69\\r\\nSet 4 0.59 0.72 0.68 0.64 0.81 0.69\\r\\nSet 5 0.69 0.77 0.74 0.56 0.75 0.70\\r\\nSet 6 0.67 0.71 0.74 0.65 0.79 0.71\\r\\nSet 7 0.60 0.70 0.72 0.52 0.75 0.66\\r\\nAverage 0.61 0.70 0.67 0.66 0.77 0.68\\r\\nTable 8: F1 scores by set and by stock for LSTM network.\\r\\nSet/stock AAPL FB GOOG MSFT INTC Average\\r\\nSet 1 0.57 0.65 0.69 0.68 0.74 0.67\\r\\nSet 2 0.48 0.57 0.56 0.78 0.76 0.63\\r\\nSet 3 0.60 0.69 0.62 0.69 0.68 0.66\\r\\nSet 4 0.60 0.82 0.68 0.70 0.38 0.64\\r\\nSet 5 0.59 0.77 0.77 0.55 0.62 0.66\\r\\nSet 6 0.73 0.72 0.78 0.46 0.74 0.69\\r\\nSet 7 0.63 0.47 0.69 0.62 0.61 0.60\\r\\nAverage 0.60 0.67 0.68 0.64 0.64 0.65\\r\\nTable 9: F1 scores by set and by stock for CNN network.\\r\\nSet/stock AAPL FB GOOG MSFT INTC Average\\r\\nSet 1 0.50 0.51 0.50 0.23 0.47 0.44\\r\\nSet 2 0.21 0.19 0.46 0.79 0.69 0.47\\r\\nSet 3 0.43 0.36 0.49 0.72 0.60 0.52\\r\\nSet 4 0.45 0.63 0.63 0.40 0.65 0.55\\r\\nSet 5 0.56 0.78 0.47 0.45 0.59 0.57\\r\\nSet 6 0.63 0.65 0.18 0.43 0.69 0.52\\r\\nSet 7 0.57 0.66 0.54 0.57 0.62 0.59\\r\\nAverage 0.48 0.54 0.47 0.51 0.62 0.52\\r\\nTable 10: F1 scores by set and by stock for MLP network.\\r\\nSet/stock AAPL FB GOOG MSFT INTC Average\\r\\nSet 1 0.68 0.60 0.64 0.56 0.73 0.64\\r\\nSet 2 0.59 0.54 0.35 0.26 0.87 0.52\\r\\nSet 3 0.71 0.57 0.72 0.76 0.71 0.70\\r\\nSet 4 0.64 0.61 0.61 0.71 0.75 0.66\\r\\nSet 5 0.75 0.74 0.76 0.70 0.79 0.75\\r\\nSet 6 0.76 0.76 0.82 0.77 0.78 0.78\\r\\nSet 7 0.69 0.66 0.71 0.51 0.71 0.66\\r\\nAverage 0.69 0.64 0.66 0.61 0.76 0.67\\r\\nTable 11: F1 scores by set and by stock for v10 network.\\r\\n28\\nSet Method Precision Recall Cohen’s Kappa F1\\r\\nAAPL CNN-LSTM-Att 0.45 0.49 0.08 0.47\\r\\nrandom 0.41 0.50 0.00 0.45\\r\\nFB CNN-LSTM-Att 0.58 0.64 0.08 0.61\\r\\nrandom 0.55 0.50 0.00 0.52\\r\\nGOOG CNN-LSTM-Att 0.45 0.59 0.00 0.51\\r\\nrandom 0.45 0.50 0.00 0.47\\r\\nMSFT CNN-LSTM-Att 0.54 0.50 0.07 0.52\\r\\nrandom 0.50 0.50 0.00 0.50\\r\\nINTC CNN-LSTM-Att 0.50 0.51 -0.01 0.51\\r\\nrandom 0.51 0.50 0.00 0.50\\r\\nAverage CNN-LSTM-Att 0.51 0.55 0.05 0.53\\r\\nrandom 0.48 0.50 0.00 0.49\\r\\nTable 12: Results of the CNN-LSTM-Attention model for separated jump direction prediction, with\\r\\nupward jumps as positive samples and downward jumps as negative samples. The number of samples\\r\\nin the two classes is not equal, but the dominant direction depends on the set. The network is slightly\\r\\nbetter than a random classifier on average, although not consistently, and the results vary due to changing\\r\\nproportions of jump directions between different stocks. It should be noted that, in some sets, the number\\r\\nof upward jumps is large enough to make always choosing positive the best choice, even according to\\r\\nF1, as the changing imbalance between class sizes means that this option cannot be eliminated with the\\r\\nchoice of positive and negative labels.\\r\\n29'},\n",
       " {'name': '1907.00235v3.pdf',\n",
       "  'content': 'Enhancing the Locality and Breaking the Memory\\r\\nBottleneck of Transformer on Time Series Forecasting\\r\\nShiyang Li\\r\\nshiyangli@ucsb.edu\\r\\nXiaoyong Jin\\r\\nx_jin@ucsb.edu\\r\\nYao Xuan\\r\\nyxuan@ucsb.edu\\r\\nXiyou Zhou\\r\\nxiyou@ucsb.edu\\r\\nWenhu Chen\\r\\nwenhuchen@ucsb.edu\\r\\nYu-Xiang Wang\\r\\nyuxiangw@cs.ucsb.edu\\r\\nXifeng Yan\\r\\nxyan@cs.ucsb.edu\\r\\nUniversity of California, Santa Barbara\\r\\nAbstract\\r\\nTime series forecasting is an important problem across many domains, including\\r\\npredictions of solar plant energy output, electricity consumption, and traffic jam\\r\\nsituation. In this paper, we propose to tackle such forecasting problem with\\r\\nTransformer [1]. Although impressed by its performance in our preliminary study,\\r\\nwe found its two major weaknesses: (1) locality-agnostics: the point-wise dot\\x02product self-attention in canonical Transformer architecture is insensitive to local\\r\\ncontext, which can make the model prone to anomalies in time series; (2) memory\\r\\nbottleneck: space complexity of canonical Transformer grows quadratically with\\r\\nsequence length L, making directly modeling long time series infeasible. In\\r\\norder to solve these two issues, we first propose convolutional self-attention by\\r\\nproducing queries and keys with causal convolution so that local context can\\r\\nbe better incorporated into attention mechanism. Then, we propose LogSparse\\r\\nTransformer with only O(L(log L)\\r\\n2\\r\\n) memory cost, improving forecasting accuracy\\r\\nfor time series with fine granularity and strong long-term dependencies under\\r\\nconstrained memory budget. Our experiments on both synthetic data and real\\x02world datasets show that it compares favorably to the state-of-the-art.\\r\\n1 Introduction\\r\\nTime series forecasting plays an important role in daily life to help people manage resources and make\\r\\ndecisions. For example, in retail industry, probabilistic forecasting of product demand and supply\\r\\nbased on historical data can help people do inventory planning to maximize the profit. Although\\r\\nstill widely used, traditional time series forecasting models, such as State Space Models (SSMs) [2]\\r\\nand Autoregressive (AR) models, are designed to fit each time series independently. Besides, they\\r\\nalso require practitioners’ expertise in manually selecting trend, seasonality and other components.\\r\\nTo sum up, these two major weaknesses have greatly hindered their applications in the modern\\r\\nlarge-scale time series forecasting tasks.\\r\\nTo tackle the aforementioned challenges, deep neural networks [3–6] have been proposed as an\\r\\nalternative solution, where Recurrent Neural Network (RNN) [7–9] has been employed to model time\\r\\nseries in an autoregressive fashion. However, RNNs are notoriously difficult to train [10] because\\r\\nof gradient vanishing and exploding problem. Despite the emergence of various variants, including\\r\\nLSTM [11] and GRU [12], the issues still remain unresolved. As an example, [13] shows that\\r\\nlanguage models using LSTM have an effective context size of about 200 tokens on average but are\\r\\nonly able to sharply distinguish 50 tokens nearby, indicating that even LSTM struggles to capture\\r\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\r\\narXiv:1907.00235v3 [cs.LG] 3 Jan 2020\\nlong-term dependencies. On the other hand, real-world forecasting applications often have both\\r\\nlong- and short-term repeating patterns [7]. For example, the hourly occupancy rate of a freeway in\\r\\ntraffic data has both daily and hourly patterns. In such cases, how to model long-term dependencies\\r\\nbecomes the critical step in achieving promising performances.\\r\\nRecently, Transformer [1, 14] has been proposed as a brand new architecture which leverages attention\\r\\nmechanism to process a sequence of data. Unlike the RNN-based methods, Transformer allows the\\r\\nmodel to access any part of the history regardless of distance, making it potentially more suitable\\r\\nfor grasping the recurring patterns with long-term dependencies. However, canonical dot-product\\r\\nself-attention matches queries against keys insensitive to local context, which may make the model\\r\\nprone to anomalies and bring underlying optimization issues. More importantly, space complexity of\\r\\ncanonical Transformer grows quadratically with the input length L, which causes memory bottleneck\\r\\non directly modeling long time series with fine granularity. We specifically delve into these two\\r\\nissues and investigate the applications of Transformer to time series forecasting. Our contributions\\r\\nare three fold:\\r\\n• We successfully apply Transformer architecture to time series forecasting and perform extensive\\r\\nexperiments on both synthetic and real datasets to validate Transformer’s potential value in better\\r\\nhandling long-term dependencies than RNN-based models.\\r\\n• We propose convolutional self-attention by employing causal convolutions to produce queries and\\r\\nkeys in the self-attention layer. Query-key matching aware of local context, e.g. shapes, can help\\r\\nthe model achieve lower training loss and further improve its forecasting accuracy.\\r\\n• We propose LogSparse Transformer, with only O(L(log L)\\r\\n2\\r\\n) space complexity to break the\\r\\nmemory bottleneck, not only making fine-grained long time series modeling feasible but also\\r\\nproducing comparable or even better results with much less memory usage, compared to canonical\\r\\nTransformer.\\r\\n2 Related Work\\r\\nDue to the wide applications of forecasting, various methods have been proposed to solve the problem.\\r\\nOne of the most prominent models is ARIMA [15]. Its statistical properties as well as the well\\x02known Box-Jenkins methodology [16] in the model selection procedure make it the first attempt for\\r\\npractitioners. However, its linear assumption and limited scalability make it unsuitable for large-scale\\r\\nforecasting tasks. Further, information across similar time series cannot be shared since each time\\r\\nseries is fitted individually. In contrast, [17] models related time series data as a matrix and deal with\\r\\nforecasting as a matrix factorization problem. [18] proposes hierarchical Bayesian methods to learn\\r\\nacross multiple related count time series from the perspective of graph model.\\r\\nDeep neural networks have been proposed to capture shared information across related time series\\r\\nfor accurate forecasting. [3] fuses traditional AR models with RNNs by modeling a probabilistic\\r\\ndistribution in an encoder-decoder fashion. Instead, [19] uses an RNN as an encoder and Multi-layer\\r\\nPerceptrons (MLPs) as a decoder to solve the so-called error accumulation issue and conduct multi\\x02ahead forecasting in parallel. [6] uses a global RNN to directly output the parameters of a linear\\r\\nSSM at each step for each time series, aiming to approximate nonlinear dynamics with locally linear\\r\\nsegments. In contrast, [9] deals with noise using a local Gaussian process for each time series while\\r\\nusing a global RNN to model the shared patterns. [20] tries to combine the advantages of AR models\\r\\nand SSMs, and maintain a complex latent process to conduct multi-step forecasting in parallel.\\r\\nThe well-known self-attention based Transformer [1] has recently been proposed for sequence\\r\\nmodeling and has achieved great success. Several recent works apply it to translation, speech,\\r\\nmusic and image generation [1, 21–23]. However, scaling attention to extremely long sequences is\\r\\ncomputationally prohibitive since the space complexity of self-attention grows quadratically with\\r\\nsequence length [21]. This becomes a serious issue in forecasting time series with fine granularity\\r\\nand strong long-term dependencies.\\r\\n2\\n3 Background\\r\\nProblem definition Suppose we have a collection of N related univariate time series {zi,1:t0 }\\r\\nN\\r\\ni=1,\\r\\nwhere zi,1:t0 , [zi,1, zi,2, · · · , zi,t0] and zi,t ∈ R denotes the value of time series i at time t\\r\\n1\\r\\n. We\\r\\nare going to predict the next τ time steps for all time series, i.e. {zi,t0+1:t0+τ }\\r\\nN\\r\\ni=1. Besides, let\\r\\n{xi,1:t0+τ }\\r\\nN\\r\\ni=1 be a set of associated time-based covariate vectors with dimension d that are assumed\\r\\nto be known over the entire time period, e.g. day-of-the-week and hour-of-the-day. We aim to model\\r\\nthe following conditional distribution\\r\\np(zi,t0+1:t0+τ |zi,1:t0, xi,1:t0+τ ; Φ) =\\r\\ntY0+τ\\r\\nt=t0+1\\r\\np(zi,t|zi,1:t−1, xi,1:t; Φ).\\r\\nWe reduce the problem to learning a one-step-ahead prediction model p(zt|z1:t−1, x1:t; Φ)\\r\\n2\\r\\n, where\\r\\nΦ denotes the learnable parameters shared by all time series in the collection. To fully utilize both\\r\\nthe observations and covariates, we concatenate them to obtain an augmented matrix as follows:\\r\\nyt , [zt−1 ◦ xt] ∈ R\\r\\nd+1\\r\\n, Yt = [y1, · · · , yt]\\r\\nT ∈ Rt×(d+1)\\r\\n,\\r\\nwhere [· ◦ ·] represents concatenation. An appropriate model zt ∼ f(Yt) is then explored to predict\\r\\nthe distribution of zt given Yt.\\r\\nTransformer We instantiate f with Transformer 3 by taking advantage of the multi-head self\\x02attention mechanism, since self-attention enables Transformer to capture both long- and short-term\\r\\ndependencies, and different attention heads learn to focus on different aspects of temporal patterns.\\r\\nThese advantages make Transformer a good candidate for time series forecasting. We briefly introduce\\r\\nits architecture here and refer readers to [1] for more details.\\r\\nIn the self-attention layer, a multi-head self-attention sublayer simultaneously transforms Y 4into H\\r\\ndistinct query matrices Qh = YWQ\\r\\nh\\r\\n, key matrices Kh = YWK\\r\\nh\\r\\n, and value matrices Vh = YWV\\r\\nh\\r\\nrespectively, with h = 1, · · · , H. Here WQ\\r\\nh\\r\\n,WK\\r\\nh ∈ R\\r\\n(d+1)×dk and WV\\r\\nh ∈ R\\r\\n(d+1)×dv are learnable\\r\\nparameters. After these linear projections, the scaled dot-product attention computes a sequence of\\r\\nvector outputs:\\r\\nOh = Attention(Qh, Kh, Vh) = softmax \\x12\\r\\nQhKT\\r\\n√\\r\\nh\\r\\ndk\\r\\n· M\\r\\n\\x13\\r\\nVh.\\r\\nNote that a mask matrix M is applied to filter out rightward attention by setting all upper triangular\\r\\nelements to −∞, in order to avoid future information leakage. Afterwards, O1, O2, · · · , OH are\\r\\nconcatenated and linearly projected again. Upon the attention output, a position-wise feedforward\\r\\nsublayer with two layers of fully-connected network and a ReLU activation in the middle is stacked.\\r\\n4 Methodology\\r\\n4.1 Enhancing the locality of Transformer\\r\\nPatterns in time series may evolve with time significantly due to various events, e.g. holidays and\\r\\nextreme weather, so whether an observed point is an anomaly, change point or part of the patterns\\r\\nis highly dependent on its surrounding context. However, in the self-attention layers of canonical\\r\\nTransformer, the similarities between queries and keys are computed based on their point-wise values\\r\\nwithout fully leveraging local context like shape, as shown in Figure 1(a) and (b). Query-key matching\\r\\nagnostic of local context may confuse the self-attention module in terms of whether the observed\\r\\nvalue is an anomaly, change point or part of patterns, and bring underlying optimization issues.\\r\\nWe propose convolutional self-attention to ease the issue. The architectural view of proposed\\r\\nconvolutional self-attention is illustrated in Figure 1(c) and (d). Rather than using convolution of\\r\\n1Here time index t is relative, i.e. the same t in different time series may represent different actual time point.\\r\\n2\\r\\nSince the model is applicable to all time series, we omit the subscript i for simplicity and clarity.\\r\\n3By referring to Transformer, we only consider the autoregressive Transformer-decoder in the following.\\r\\n4At each time step the same model is applied, so we simplify the formulation with some abuse of notation.\\r\\n3\\n Masked Multi-Head Attention Masked Multi-Head Attention\\r\\nQ V K\\r\\nConv, 1 Conv, 1 Conv, 1 Conv, k Conv, 1 Conv, k\\r\\nQ V K\\r\\n(a) (b) (c) (d)\\r\\nFigure 1: The comparison between canonical and our convolutional self-attention layers. “Conv,\\r\\n1” and “Conv, k” mean convolution of kernel size {1, k} with stride 1, respectively. Canonical\\r\\nself-attention as used in Transformer is shown in (b), may wrongly match point-wise inputs as shown\\r\\nin (a). Convolutional self-attention is shown in (d), which uses convolutional layers of kernel size k\\r\\nwith stride 1 to transform inputs (with proper paddings) into queries/keys. Such locality awareness\\r\\ncan correctly match the most relevant features based on shape matching in (c).\\r\\nMon Tue Wed Thu Fri Sat Sun Mon\\r\\n0.00\\r\\n0.05\\r\\n0.10\\r\\n0.15\\r\\n0.20 occupancy rate\\r\\nattn score in layer 2\\r\\nattn score in layer 6\\r\\nattn score in layer 10\\r\\nFigure 2: Learned attention patterns from a 10-layer canonical Transformer trained on traffic-f\\r\\ndataset with full attention. The green dashed line indicates the start time of forecasting and the\\r\\ngray dashed line on its left side is the conditional history. Blue, cyan and red lines correspond to\\r\\nattention patterns in layer 2, 6 and 10, respectively, for a head when predicting the value at the time\\r\\ncorresponding to the green dashed line. a) Layer 2 tends to learn shared patterns in every day. b)\\r\\nLayer 6 focuses more on weekend patterns. c) Layer 10 further squeezes most of its attention on only\\r\\nseveral cells in weekends, causing most of the others to receive little attention.\\r\\nkernel size 1 with stride 1 (matrix multiplication), we employ causal convolution of kernel size k\\r\\nwith stride 1 to transform inputs (with proper paddings) into queries and keys. Note that causal\\r\\nconvolutions ensure that the current position never has access to future information. By employing\\r\\ncausal convolution, generated queries and keys can be more aware of local context and hence, compute\\r\\ntheir similarities by their local context information, e.g. local shapes, instead of point-wise values,\\r\\nwhich can be helpful for accurate forecasting. Note that when k = 1, the convolutional self-attention\\r\\nwill degrade to canonical self-attention, thus it can be seen as a generalization.\\r\\n4.2 Breaking the memory bottleneck of Transformer\\r\\nTo motivate our approach, we first perform a qualitative assessment of the learned attention patterns\\r\\nwith a canonical Transformer on traffic-f dataset. The traffic-f dataset contains occupancy\\r\\nrates of 963 car lanes of San Francisco bay area recorded every 20 minutes [6]. We trained a 10-layer\\r\\ncanonical Transformer on traffic-f dataset with full attention and visualized the learned attention\\r\\npatterns. One example is shown in Figure 2. Layer 2 clearly exhibited global patterns, however, layer\\r\\n6 and 10, only exhibited pattern-dependent sparsity, suggesting that some form of sparsity could be\\r\\nintroduced without significantly affecting performance. More importantly, for a sequence with length\\r\\nL, computing attention scores between every pair of cells will cause O(L\\r\\n2\\r\\n) memory usage, making\\r\\nmodeling long time series with fine granularity and strong long-term dependencies prohibitive.\\r\\nWe propose LogSparse Transformer, which only needs to calculate O(log L) dot products for each\\r\\ncell in each layer. Further, we only need to stack up to O(log L) layers and the model will be able to\\r\\naccess every cell’s information. Hence, the total cost of memory usage is only O(L(log L)\\r\\n2\\r\\n). We\\r\\ndefine I\\r\\nk\\r\\nl\\r\\nas the set of indices of the cells that cell l can attend to during the computation from kth\\r\\n4\\n(a). Full Self Attention (b). LogSparse Self Attention\\r\\n(d). Restart Attention + LogSparse Self Attention (c). Local Attention + LogSparse Self Attention\\r\\nLogSparse Attention Range Local Attention Range LogSparse Attention Range LogSparse Attention Range\\r\\nSelf LogSparse Attention Range Self\\r\\nSelf Self\\r\\nFigure 3: Illustration of different attention mechanism between adjacent layers in Transformer.\\r\\nlayer to (k + 1)th layer. In the standard self-attention of Transformer, I\\r\\nk\\r\\nl = {j : j ≤ l}, allowing\\r\\nevery cell to attend to all its past cells and itself as shown in Figure 3(a). However, such an algorithm\\r\\nsuffers from the quadratic space complexity growth along with the input length. To alleviate such an\\r\\nissue, we propose to select a subset of the indices I\\r\\nk\\r\\nl ⊂ {j : j ≤ l} so that |I\\r\\nk\\r\\nl\\r\\n| does not grow too\\r\\nfast along with l. An effective way of choosing indices is |I\\r\\nk\\r\\nl\\r\\n| ∝ log L.\\r\\nNotice that cell l is a weighted combination of cells indexed by I\\r\\nk\\r\\nl\\r\\nin kth self-attention layer and can\\r\\npass the information of cells indexed by I\\r\\nk\\r\\nl\\r\\nto its followings in the next layer. Let S\\r\\nk\\r\\nl\\r\\nbe the set which\\r\\ncontains indices of all the cells whose information has passed to cell l up to kth layer. To ensure that\\r\\nevery cell receives the information from all its previous cells and itself, the number of stacked layers\\r\\n˜kl should satisfy that S\\r\\nk˜l\\r\\nl = {j : j ≤ l} for l = 1, · · · , L. That is, ∀l and j ≤ l, there is a directed\\r\\npath Pjl = (j, p1, p2, · · · , l) with ˜kl edges, where j ∈ I\\r\\n1\\r\\np1\\r\\n, p1 ∈ I\\r\\n2\\r\\np2\\r\\n, · · · , pk˜l−1 ∈ I\\r\\nk˜l\\r\\nl\\r\\n.\\r\\nWe propose LogSparse self-attention by allowing each cell only to attend to its previous cells\\r\\nwith an exponential step size and itself. That is, ∀k and l, I\\r\\nk\\r\\nl = {l − 2\\r\\nblog2lc\\r\\n, l − 2\\r\\nblog2lc−1\\r\\n, l −\\r\\n2\\r\\nblog2lc−2\\r\\n, ..., l − 2\\r\\n0\\r\\n, l}, where b·c denotes the floor operation, as shown in Figure 3(b).5\\r\\nTheorem 1. ∀l and j ≤ l, there is at least one path from cell j to cell l if we stack blog2lc+ 1 layers.\\r\\nMoreover, for j < l, the number of feasible unique paths from cell j to cell l increases at a rate of\\r\\nO(blog2(l − j)c!).\\r\\nThe proof, deferred to Appendix A.1, uses a constructive argument.\\r\\nTheorem 1 implies that despite an exponential decrease in the memory usage (from O(L\\r\\n2\\r\\n) to\\r\\nO(Llog2 L)) in each layer, the information could still flow from any cell to any other cell provided\\r\\nthat we go slightly “deeper” — take the number of layers to be blog2 Lc + 1. Note that this implies\\r\\nan overall memory usage of O(L(log2 L)\\r\\n2\\r\\n) and addresses the notorious scalability bottleneck of\\r\\nTransformer under GPU memory constraint [1]. Moreover, as two cells become further apart, the\\r\\nnumber of paths increases at a rate of super-exponential in log2(l − j), which indicates a rich\\r\\ninformation flow for modeling delicate long-term dependencies.\\r\\nLocal Attention We can allow each cell to densely attend to cells in its left window of size\\r\\nO(log2 L) so that more local information, e.g. trend, can be leveraged for current step forecasting.\\r\\nBeyond the neighbor cells, we can resume our LogSparse attention strategy as shown in Figure 3(c).\\r\\nRestart Attention Further, one can divide the whole input with length L into subsequences and set\\r\\neach subsequence length Lsub ∝ L. For each of them, we apply the LogSparse attention strategy.\\r\\nOne example is shown in Figure 3(d).\\r\\nEmploying local attention and restart attention won’t change the complexity of our sparse attention\\r\\nstrategy but will create more paths and decrease the required number of edges in the path. Note that\\r\\none can combine local attention and restart attention together.\\r\\n5Applying other bases is trivial so we don’t discuss other bases here for simplicity and clarity.\\r\\n5\\n5 Experiments\\r\\n5.1 Synthetic datasets\\r\\nTo demonstrate Transformer’s capability to capture long-term dependencies, we conduct experiments\\r\\non synthetic data. Specifically, we generate a piece-wise sinusoidal signals\\r\\nf(x) =\\r\\n\\uf8f1\\r\\n\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f3\\r\\nA1 sin(πx/6) + 72 + Nx x ∈ [0, 12),\\r\\nA2 sin(πx/6) + 72 + Nx x ∈ [12, 24),\\r\\nA3 sin(πx/6) + 72 + Nx x ∈ [24, t0),\\r\\nA4 sin(πx/12) + 72 + Nx x ∈ [t0, t0 + 24),\\r\\nwhere x is an integer, A1, A2, A3 are randomly generated by uniform distribution on [0, 60], A4 =\\r\\nmax(A1, A2) and Nx ∼ N (0, 1). Following the forecasting setting in Section 3, we aim to predict\\r\\nthe last 24 steps given the previous t0 data points. Intuitively, larger t0 makes forecasting more\\r\\ndifficult since the model is required to understand and remember the relation between A1 and A2\\r\\nto make correct predictions after t0 − 24 steps of irrelevant signals. Hence, we create 8 different\\r\\ndatasets by varying the value of t0 within {24, 48, 72, 96, 120, 144, 168, 192}. For each dataset, we\\r\\ngenerate 4.5K, 0.5K and 1K time series instances for training, validation and test set, respectively.\\r\\nAn example time series with t0 = 96 is shown in Figure 4(a).\\r\\nIn this experiment, we use a 3-layer canonical Transformer with standard self-attention. For com\\x02parison, we employ DeepAR [3], an autoregressive model based on a 3-layer LSTM, as our baseline.\\r\\nBesides, to examine if larger capacity could improve performance of DeepAR, we also gradually\\r\\nincrease its hidden size h as {20, 40, 80, 140, 200}. Following [3, 6], we evaluate both methods using\\r\\nρ-quantile loss Rρ with ρ ∈ (0, 1),\\r\\nRρ(x, xˆ) =\\r\\n2\\r\\nP\\r\\ni,t Dρ(x\\r\\n(i)\\r\\nt\\r\\n, xˆ\\r\\n(i)\\r\\nt\\r\\n)\\r\\nP\\r\\ni,t |x\\r\\n(i)\\r\\nt\\r\\n|\\r\\n, Dρ(x, xˆ) = (ρ − I{x≤xˆ})(x − xˆ),\\r\\nwhere xˆ is the empirical ρ-quantile of the predictive distribution and I{x≤xˆ} is an indicator function.\\r\\nFigure 4: (a) An example time series with t0 = 96.\\r\\nBlack line is the conditional history while red\\r\\ndashed line is the target. (b) Performance compar\\x02ison between DeepAR and canonical Transformer\\r\\nalong with the growth of t0. The larger t0 is, the\\r\\nlonger dependencies the models need to capture\\r\\nfor accurate forecasting.\\r\\nFigure 4(b) presents the performance of DeepAR\\r\\nand Transformer on the synthetic datasets.\\r\\nWhen t0 = 24, both of them perform very well.\\r\\nBut, as t0 increases, especially when t0 ≥ 96,\\r\\nthe performance of DeepAR drops significantly\\r\\nwhile Transformer keeps its accuracy, suggest\\x02ing that Transformer can capture fairly long\\x02term dependencies when LSTM fails to do so.\\r\\n5.2 Real-world datasets\\r\\nWe further evaluate our model on several real\\x02world datasets. The electricity-f (fine)\\r\\ndataset consists of electricity consumption of\\r\\n370 customers recorded every 15 minutes and\\r\\nthe electricity-c (coarse) dataset is the\\r\\naggregated electricity-f by every 4 points,\\r\\nproducing hourly electricity consumption. Sim\\x02ilarly, the traffic-f (fine) dataset contains\\r\\noccupancy rates of 963 freeway in San Francisco\\r\\nrecorded every 20 minutes and the traffic-c\\r\\n(coarse) contains hourly occupancy rates by\\r\\naveraging every 3 points in traffic-f. The\\r\\nsolar dataset6contains the solar power pro\\x02duction records from January to August in 2006,\\r\\nwhich is sampled every hour from 137 PV plants\\r\\nin Alabama. The wind7 dataset contains daily\\r\\n6\\r\\nhttps://www.nrel.gov/grid/solar-power-data.html\\r\\n7\\r\\nhttps://www.kaggle.com/sohier/30-years-of-european-wind-generation\\r\\n6\\nTable 1: Results summary (R0.5/R0.9-loss) of all methods. e-c and t-c represent electricity-c\\r\\nand traffic-c, respectively. In the 1st and 3rd row, we perform rolling-day prediction of 7 days\\r\\nwhile in the 2nd and 4th row, we directly forecast 7 days ahead. TRMF outputs points predictions, so\\r\\nwe only report R0.5. \\x05 denotes results from [6].\\r\\nARIMA ETS TRMF DeepAR DeepState Ours\\r\\ne-c1d 0.154/0.102 0.101/0.077 0.084/- 0.075\\x05/0.040\\x050.083\\x05/0.056\\x050.059/0.034\\r\\ne-c7d 0.283\\x05/0.109\\x050.121\\x05/0.101\\x050.087/- 0.082/0.053 0.085\\x05/0.052\\x050.070/0.044\\r\\nt-c1d 0.223/0.137 0.236/0.148 0.186/- 0.161\\x05/0.099\\x050.167\\x05/0.113\\x050.122/0.081\\r\\nt-c7d 0.492\\x05/0.280\\x050.509\\x05/0.529\\x050.202/- 0.179/0.105 0.168\\x05/0.114\\x050.139/0.094\\r\\n0 2 4 6 8 10 12 14\\r\\niter. (1e4)\\r\\n3.8\\r\\n3.6\\r\\n3.4\\r\\n3.2\\r\\n3.0\\r\\n2.8\\r\\n2.6\\r\\n2.4\\r\\ntraining loss (NLL)\\r\\nk=1\\r\\nk=3\\r\\nk=9\\r\\n0 2 4 6 8 10 12 14\\r\\niter. (1e4)\\r\\n7.0\\r\\n7.2\\r\\n7.4\\r\\n7.6\\r\\n7.8\\r\\ntraining loss (NLL)\\r\\nk=1\\r\\nk=3\\r\\nk=9\\r\\nFigure 5: Training curve comparison (with proper smoothing) among kernel size k ∈ {1, 3, 9} in\\r\\ntraffic-c (left) and electricity-c (right) dataset. Being aware of larger local context size, the\\r\\nmodel can achieve lower training error and converge faster.\\r\\nestimates of 28 countries’ energy potential from 1986 to 2015 as a percentage of a power plant’s\\r\\nmaximum output. The M4-Hourly contains 414 hourly time series from M4 competition [24].\\r\\nLong-term and short-term forecasting We first show the effectiveness of canonical Trans\\x02former equipped with convolutional self-attention in long-term and short-term forecasting in\\r\\nelectricity-c and traffic-c dataset. These two datasets exhibit both hourly and daily sea\\x02sonal patterns. However, traffic-c demonstrates much greater difference between the patterns of\\r\\nweekdays and weekends compared to electricity-c. Hence, accurate forecasting in traffic-c\\r\\ndataset requires the model to capture both long- and short-term dependencies very well. As baselines,\\r\\nwe use classical forecasting methods auto.arima, ets implemented in R’s forecast package and\\r\\nthe recent matrix factorization method TRMF [17], a RNN-based autoregressive model DeepAR and a\\r\\nRNN-based state space model DeepState [6]. For short-term forecasting, we evaluate rolling-day\\r\\nforecasts for seven days ( i.e., prediction horizon is one day and forecasts start time is shifted by one\\r\\nday after evaluating the prediction for the current day [6]). For long-term forecasting, we directly\\r\\nforecast 7 days ahead. As shown in Table 1, our models with convolutional self-attention get betters\\r\\nresults in both long-term and short-term forecasting, especially in traffic-c dataset compared to\\r\\nstrong baselines, partly due to the long-term dependency modeling ability of Transformer as shown\\r\\nin our synthetic data.\\r\\nConvolutional self-attention In this experiment, we conduct ablation study of our proposed convo\\x02lutional self-attention. We explore different kernel size k ∈ {1, 2, 3, 6, 9} on the full attention model\\r\\nand fix all other settings. We still use rolling-day prediction for seven days on electricity-c and\\r\\ntraffic-c datasets. The results of different kernel sizes on both datasets are shown in Table 2. On\\r\\nelectricity-c dataset, models with kernel size k ∈ {2, 3, 6, 9} obtain slightly better results in\\r\\nterm of R0.5 than canonical Transformer but overall these results are comparable and all of them\\r\\nperform very well. We argue it is because electricity-c dataset is less challenging and covariate\\r\\nvectors have already provided models with rich information for accurate forecasting. Hence, being\\r\\naware of larger local context may not help a lot in such cases. However, on much more challenging\\r\\ntraffic-c dataset, the model with larger kernel size k can make more accurate forecasting than\\r\\nmodels with smaller ones with as large as 9% relative improvement. These consistent gains can be\\r\\nthe results of more accurate query-key matching by being aware of more local context. Further, to\\r\\nverify if incorporating more local context into query-key matching can ease the training, we plot the\\r\\n7\\nTable 2: Average R0.5/R0.9-loss of different kernel sizes for rolling-day prediction of 7 days.\\r\\nk = 1 k = 2 k = 3 k = 6 k = 9\\r\\nelectricity-c1d 0.060/0.030 0.058/0.030 0.057/0.031 0.057/0.031 0.059/0.034\\r\\ntraffic-c1d 0.134/0.089 0.124/0.085 0.123/0.083 0.123/0.083 0.122/0.081\\r\\ntraining loss of kernel size k ∈ {1, 3, 9} in electricity-c and traffic-c datasets. We found that\\r\\nTransformer with convolutional self-attention also converged faster and to lower training errors, as\\r\\nshown in Figure 5, proving that being aware of local context can ease the training process.\\r\\nSparse attention Further, we compare our proposed LogSparse Transformer to the full attention\\r\\ncounterpart on fine-grained datasets, electricity-f and traffic-f. Note that time series in\\r\\nthese two datasets have much longer periods and are noisier comparing to electricity-c and\\r\\ntraffic-c. We first compare them under the same memory budget. For electricity-f dataset,\\r\\nwe choose Le1 = 768 with subsequence length Le1/8 and local attention length log2(Le1/8) in each\\r\\nsubsequence for our sparse attention model and Le2 = 293 in the full attention counterpart. For\\r\\ntraffic-f dataset, we select Lt1 = 576 with subsequence length Lt1/8 and local attention length\\r\\nlog2(Lt1/8) in each subsequence for our sparse attention model, and Lt2 = 254 in the full attention\\r\\ncounterpart. The calculation of memory usage and other details can be found in Appendix A.4. We\\r\\nconduct experiments on aforementioned sparse and full attention models with/without convolutional\\r\\nself-attention on both datasets. By following such settings, we summarize our results in Table 3\\r\\n(Upper part). No matter equipped with convolutional self-attention or not, our sparse attention models\\r\\nachieve comparable results on electricity-f but much better results on traffic-f compared\\r\\nto its full attention counterparts. Such performance gain on traffic-f could be the result of the\\r\\ndateset’s stronger long-term dependencies and our sparse model’s better capability of capturing these\\r\\ndependencies, which, under the same memory budget, the full attention model cannot match. In\\r\\naddition, both sparse and full attention models benefit from convolutional self-attention on challenging\\r\\ntraffic-f, proving its effectiveness.\\r\\nTo explore how well our sparse attention model performs compared to full attention model with\\r\\nthe same input length, we set Le2 = Le1 = 768 and Lt2 = Lt1 = 576 on electricity-f and\\r\\ntraffic-f, respectively. The results of their comparisons are summarized in Table 3 (Lower part).\\r\\nAs one expects, full attention Transformers can outperform our sparse attention counterparts no matter\\r\\nthey are equipped with convolutional self-attention or not in most cases. However, on traffic-f\\r\\ndataset with strong long-term dependencies, our sparse Transformer with convolutional self-attention\\r\\ncan get better results than the canonical one and, more interestingly, even slightly outperform its full\\r\\nattention counterpart in term of R0.5, meaning that our sparse model with convolutional self-attention\\r\\ncan capture long-term dependencies fairly well. In addition, full attention models under length\\r\\nconstraint consistently obtain gains from convolutional self-attention on both electricity-f and\\r\\ntraffic-f datasets, showing its effectiveness again.\\r\\nTable 3: Average R0.5/R0.9-loss comparisons between sparse attention and full attention models\\r\\nwith/without convolutional self-attention by rolling-day prediction of 7 days. “Full” means models\\r\\nare trained with full attention while “Sparse” means they are trained with our sparse attention strategy.\\r\\n“+ Conv” means models are equipped with convolutional self-attention with kernel size k = 6.\\r\\nConstraint Dataset Full Sparse Full + Conv Sparse + Conv\\r\\nMemory electricity-f1d 0.083/0.051 0.084/0.047 0.078/0.048 0.079/0.049\\r\\ntraffic-f1d 0.161/0.109 0.150/0.098 0.149/0.102 0.138/0.092\\r\\nLength electricity-f1d 0.082/0.047 0.084/0.047 0.074/0.042 0.079/0.049\\r\\ntraffic-f1d 0.147/0.096 0.150/0.098 0.139/0.090 0.138/0.092\\r\\nFurther Exploration In our last experiment, we evaluate how our methods perform on datasets\\r\\nwith various granularities compared to our baselines. All datasets except M4-Hourly are evaluated\\r\\nby rolling window 7 times since the test set of M4-Hourly has been provided. The results are shown\\r\\nin Table 4. These results further show that our method achieves the best performance overall.\\r\\n8\\nTable 4: R0.5/R0.9-loss of datasets with various granularities. The subscript of each dataset presents\\r\\nthe forecasting horizon (days). TRMF is not applicable for M4-Hourly2d and we leave it blank. For\\r\\nother datasets, TRMF outputs points predictions, so we only report R0.5.\\r\\n\\x05 denotes results from [10].\\r\\nelectricity-f1d traffic-f1d solar1d M4-Hourly2d wind30d\\r\\nTRMF 0.094/- 0.213/- 0.241/- -/- 0.311/-\\r\\nDeepAR 0.082/0.063 0.230/0.150 0.222/0.093 0.090\\x05/0.030\\x050.286/0.116\\r\\nOurs 0.074/0.042 0.139/0.090 0.210 /0.082 0.067 /0.025 0.284/0.108\\r\\n6 Conclusion\\r\\nIn this paper, we propose to apply Transformer in time series forecasting. Our experiments on\\r\\nboth synthetic data and real datasets suggest that Transformer can capture long-term dependencies\\r\\nwhile LSTM may suffer. We also showed, on real-world datasets, that the proposed convolutional\\r\\nself-attention further improves Transformer’ performance and achieves state-of-the-art in different\\r\\nsettings in comparison with recent RNN-based methods, a matrix factorization method, as well as\\r\\nclassic statistical approaches. In addition, with the same memory budget, our sparse attention models\\r\\ncan achieve better results on data with long-term dependencies. Exploring better sparsity strategy in\\r\\nself-attention and extending our method to better fit small datasets are our future research directions.\\r\\nReferences\\r\\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\r\\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\\r\\nsystems, pages 5998–6008, 2017.\\r\\n[2] James Durbin and Siem Jan Koopman. Time series analysis by state space methods. Oxford university\\r\\npress, 2012.\\r\\n[3] Valentin Flunkert, David Salinas, and Jan Gasthaus. Deepar: Probabilistic forecasting with autoregressive\\r\\nrecurrent networks. arXiv preprint arXiv:1704.04110, 2017.\\r\\n[4] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\r\\n[5] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In\\r\\nAdvances in neural information processing systems, pages 3104–3112, 2014.\\r\\n[6] Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim\\r\\nJanuschowski. Deep state space models for time series forecasting. In Advances in Neural Information\\r\\nProcessing Systems, pages 7785–7794, 2018.\\r\\n[7] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal\\r\\npatterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research &\\r\\nDevelopment in Information Retrieval, pages 95–104. ACM, 2018.\\r\\n[8] Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensor-train\\r\\nrnns. arXiv preprint arXiv:1711.00073, 2017.\\r\\n[9] Danielle C Maddix, Yuyang Wang, and Alex Smola. Deep factors with gaussian processes for forecasting.\\r\\narXiv preprint arXiv:1812.00098, 2018.\\r\\n[10] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\\r\\nnetworks. In International conference on machine learning, pages 1310–1318, 2013.\\r\\n[11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\\r\\n1997.\\r\\n[12] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of\\r\\nneural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\\r\\n[13] Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural\\r\\nlanguage models use context. arXiv preprint arXiv:1805.04623, 2018.\\r\\n9\\n[14] Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model\\r\\nfor natural language inference. arXiv preprint arXiv:1606.01933, 2016.\\r\\n[15] George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. Journal of the\\r\\nRoyal Statistical Society. Series C (Applied Statistics), 17(2):91–109, 1968.\\r\\n[16] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis:\\r\\nforecasting and control. John Wiley & Sons, 2015.\\r\\n[17] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for high\\x02dimensional time series prediction. In Advances in neural information processing systems, pages 847–855,\\r\\n2016.\\r\\n[18] Nicolas Chapados. Effective bayesian modeling of groups of related count time series. arXiv preprint\\r\\narXiv:1405.3738, 2014.\\r\\n[19] Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi-horizon quantile\\r\\nrecurrent forecaster. arXiv preprint arXiv:1711.11053, 2017.\\r\\n[20] Xiaoyong Jin, Shiyang Li, Yunkai Zhang, and Xifeng Yan. Multi-step deep autoregressive fore\\x02casting with latent states. URL http://roseyu.com/time-series-workshop/submissions/2019/timeseries\\x02ICML19_paper_19.pdf, 2019.\\r\\n[21] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, Andrew M\\r\\nDai, Matthew D Hoffman, and Douglas Eck. An improved relative self-attention mechanism for transformer\\r\\nwith application to music generation. arXiv preprint arXiv:1809.04281, 2018.\\r\\n[22] Daniel Povey, Hossein Hadian, Pegah Ghahremani, Ke Li, and Sanjeev Khudanpur. A time-restricted\\r\\nself-attention layer for asr. In 2018 IEEE International Conference on Acoustics, Speech and Signal\\r\\nProcessing (ICASSP), pages 5874–5878. IEEE, 2018.\\r\\n[23] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\\r\\nTran. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\\r\\n[24] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: Results,\\r\\nfindings, conclusion and way forward. International Journal of Forecasting, 34(4):802–808, 2018.\\r\\n[25] Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when\\r\\ncan deep-but not shallow-networks avoid the curse of dimensionality: a review. International Journal of\\r\\nAutomation and Computing, 14(5):503–519, 2017.\\r\\n[26] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern\\r\\nrecognition unaffected by shift in position. Biological cybernetics, 36(4):193–202, 1980.\\r\\n[27] Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory\\r\\nperspective. Analysis and Applications, 14(06):829–848, 2016.\\r\\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\r\\narXiv:1412.6980, 2014.\\r\\n[29] Aäron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\\r\\nKalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.\\r\\nSSW, 125, 2016.\\r\\n[30] Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. Conditional time series forecasting with\\r\\nconvolutional neural networks. arXiv preprint arXiv:1703.04691, 2017.\\r\\n[31] Scott Gray, Alec Radford, and Diederik P. Kingma. Gpu kernels for block-sparse weights. arXiv preprint\\r\\narXiv:1711.09224, 2017.\\r\\n[32] Tim Cooijmans, Nicolas Ballas, César Laurent, Çaglar Gülçehre, and Aaron Courville. Recurrent batch ˘\\r\\nnormalization. arXiv preprint arXiv:1603.09025, 2016.\\r\\n[33] Rose Yu, Yaguang Li, Cyrus Shahabi, Ugur Demiryurek, and Yan Liu. Deep learning: A generic approach\\r\\nfor extreme condition traffic forecasting. In Proceedings of the 2017 SIAM International Conference on\\r\\nData Mining, pages 777–785. SIAM, 2017.\\r\\n[34] Guoqiang Zhang, B Eddy Patuwo, and Michael Y Hu. Forecasting with artificial neural networks:: The\\r\\nstate of the art. International journal of forecasting, 14(1):35–62, 1998.\\r\\n10\\n[35] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\r\\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\\r\\n[36] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\\r\\nShazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.\\r\\n[37] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under\\x02standing by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/research\\x02covers/languageunsupervised/language understanding paper. pdf, 2018.\\r\\n[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec\\x02tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\r\\n[39] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\r\\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\\r\\n[40] Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting with\\r\\nneural networks at uber. In International Conference on Machine Learning, number 34, pages 1–5, 2017.\\r\\n11\\nA Supplementary Materials\\r\\nA.1 Proof of Theorem 1\\r\\nProof. According to the attention strategy in LogSparse Transformer, in each layer, cell l could attend to the\\r\\ncells with indicies in I\\r\\nk\\r\\nl = {l − 2\\r\\nblog2 lc\\r\\n, l − 2\\r\\nblog2 lc−1\\r\\n, l − 2\\r\\nblog2 lc−2\\r\\n, · · · , l − 2\\r\\n0\\r\\n, l}. To ensure that every\\r\\ncell receives the information from all its previous cells and itself, the number of stacked layers k˜l should satisfy\\r\\nthat S\\r\\nk˜\\r\\nl\\r\\nl = {j : j ≤ l} for l = 1, · · · , L. That is, ∀ l and j ≤ l, there is a directed path Pjl = (j, p1, p2, · · · , l)\\r\\nwith k˜l edges, where j ∈ I\\r\\n1\\r\\np1\\r\\n, p1 ∈ I\\r\\n2\\r\\np2\\r\\n, · · · , pk˜\\r\\nl−1 ∈ I\\r\\nk˜\\r\\nl\\r\\nl\\r\\n. We prove the theorem by constructing a path\\r\\nfrom cell j to cell l, with length (number of edges) no larger than blog2lc + 1. Case j = l is trivial, we only\\r\\nneed to consider j < l here. Consider the binary representation of l − j, l − j =\\r\\nPblog2(l−j)c\\r\\nm=0 bm2\\r\\nm, where\\r\\nbm ∈ {0, 1}. Suppose {msub} is the subsequence {m|0 ≤ m ≤ blog2(l − j)c , bm = 1} and mp is the pth\\r\\nelement of {msub}. A feasible path from j to l is Pjl = {j, j + 2m0, j + 2m0 + 2m1, · · · , l}. The length of\\r\\nthis path is |{msub}|, which is no larger than blog2(l − j)c + 1. So\\r\\nmin {k˜l|S\\r\\nk˜\\r\\nl\\r\\nl = {j : j ≤ l}} ≤ max\\r\\n{j|j<l}\\r\\nblog2(l − j)c + 1 ≤ blog2lc + 1.\\r\\nFurthermore, by reordering {msub}, we can generate multiple different paths from cell j to cell l. The number\\r\\nof feasible paths increases at a rate of O(blog2(l − j)c!) along with l.\\r\\nA.2 Training\\r\\nTable 5: Dataset statistics. \"T\", \"M\" and \"S\" represent the length, number and sample rate of the time\\r\\nseries, respectively.\\r\\nelectricity-c electricity-f traffic-c traffic-f wind solar M4-Hourly\\r\\nT 32304 129120 4049 12435 10957 5832 748/1008\\r\\nM 370 370 963 963 28 137 414\\r\\nS 1 hour 15 mins 1 hour 20 mins 1 day 1 hour 1 hour\\r\\nTo learn the model, we are given a time series dataset {zi,1:T }\\r\\nM\\r\\ni=1 and its associated covariates {xi,1:T }\\r\\nM\\r\\ni=1,\\r\\nwhere T is the length of all available observations and M is the number of different time series. The dataset\\r\\nstatistics is shown as Table 5. Following [3], we create training instances by selecting windows with fixed history\\r\\nlength t0 and forecasting horizon τ but varying the start point of forecasting from each of the original long\\r\\ntime series. As a follow-up of [3], we sample training windows through weight sampling strategy in [3]. Note\\r\\nthat during selecting training windows, data in the test set can never be accessed. As a result, we get a training\\r\\ndataset with N sliding windows {zi,1:t0+τ , xi,1:t0+τ }\\r\\nN\\r\\ni=1.\\r\\nFor positional encoding in Transformer, we use learnable position embedding. For covariates, following [3], we\\r\\nuse all or part of year, month, day-of-the-week, hour-of-the-day, minute-of-the-hour, age and time-series-ID\\r\\naccording to the granularities of datasets. age is the distance to the first observation in that time series [3]. Each\\r\\nof them except time series ID has only one dimension and is normalized to have zero mean and unit variance (if\\r\\napplicable). For time-series-ID, it has the same dimension as position embedding through ID embedding matrix\\r\\nso that they can be summed up (with broadcasting). The summation is then concatnated with aforementioned\\r\\nother covariates as the input of 1st layer in Transformer.\\r\\nDeepAR [3] uses an encoder-decoder fashion, where the encoder is the same as the decoder and the final hidden\\r\\nstate of the encoder is used to initialize the hidden state of the decoder. Such an architecture can be seen as a\\r\\ndecoder-only network as the encoder and decoder are the same, where the objective is to predict the distribution\\r\\nof next point according to current input and last hidden state. Inspired by this observation, we use Transformer\\r\\ndecoder-only mode [36] to model time series. Similar to [37], a fully-connected layer on the top of Transformer\\r\\nis stacked, which outputs the parameters of the probability distribution after scaling for the next time point with\\r\\nappropriate transformations. For example, for parameters requiring positivity, a softplus activation is applied. We\\r\\nuse the same scale handling technique as in [3] to scale our input and output of our models. We refer readers to\\r\\n[3] for more details of scale handling. In our experiments, we use Gaussian likelihood since our training datasets\\r\\nare real-valued data. Note that one can also use other likelihood models, e.g. negative-binomial likelihood for\\r\\npositive count data. In synthetic datasets, we only count log-likelihood from t0 + 1 to t0 + τ . On real-world\\r\\ndatasets, we not only count log-likelihood from t0 + 1 to t0 + τ , but also include the log-likelihood from 1 to t0,\\r\\nsimilar to training in [3] and pre-training in [37].\\r\\nDuring training, we use vanilla Adam optimizer [28] with early stopping except experiments on electricity-f\\r\\nand traffic-f to maximize the log-likelihood of each training instance. Our preliminary study show that\\r\\n12\\ntraining on these two datasets are very unstable with Adam. Rather, we found that BERTAdam [38]\\r\\n8\\r\\n, a variant\\r\\nof Adam with warmup and learning rate annealing, can stabilize the training process on these two datasets.\\r\\nFor electricity-c and traffic-c, we take 500K training windows while for electricity-f and\\r\\ntraffic-f, we select 125K and 200K training windows, respectively. For wind, M4-Hourly and solar,\\r\\nwe choose 10K, 50K and 50K training windows, respectively. The window selection strategy is described above.\\r\\nFor our Transformer models, all of them use H = 8 heads and the dimension of position embedding and time\\r\\nseries ID embedding are all 20. All of our models have 3 layers except experiments on electricity-f and\\r\\ntraffic-f, where our models use 6 and 10 layers, respectively. The data before the forecast start time is used\\r\\nas the training set and split into two partitions. For each experiment on real-world datasets, we train our model\\r\\non the first partition of the training set containing 90% of the data 5 times with different random seeds and\\r\\nwe pick the one that has the minimal negative log-likelihood on the remaining 10%. The results on test set\\r\\ncorresponding to minimal negative log-likelihood on the remaining 10% are reported. All models are trained on\\r\\nGTX 1080 Ti GPUs.\\r\\nA.3 Evaluation\\r\\nFollowing the experimental settings in [6], one week data from 9/1/2014 00:00 (included) 9on electricity-c\\r\\nand 6/15/2008 17:00 (included) 10 on traffic-c is left as test sets. For electricity-f and traffic-f\\r\\ndatasets, one week data from 8/31/2014 00:15 (included) and 6/15/2008 17:00 (included) 11 is left as test sets,\\r\\nrespectively. For solar, we leave the last 7 days in August as test set. For wind, last 210 days in year 2015 are\\r\\nleft as test set. For M4-Hourly, its training and test set are already provided. After training on previous settings,\\r\\nwe evaluate our models on aforementioned test intervals and report standard quantile loss (R0.5 and R0.9) on\\r\\nthe test sets.\\r\\nA.4 Implementation of sparse attention and its memory cost\\r\\nDuring the implementation of our sparse attention, ∀l ≤ |I\\r\\nk\\r\\nL|, one can allow such cell l to densely attend all its\\r\\npast cells and itself without increasing space usage as query-key matching are parallelly computed in reality and\\r\\nmaximum number of cells that a cell can attend is reached by cell L.\\r\\nOur current implementation of LogSparse attention is via a mask matrix and its relative memory usage is\\r\\ncalculated ideally from the attention matrix, which is the memory bottleneck of Transformer.\\r\\nFor electricity-f dataset, we choose Le1 = 768 with subsequence length L\\r\\ne1\\r\\nsub = Le1 /8 = 96 and local\\r\\nattention length L\\r\\ne1\\r\\nloc = dlog2\\r\\n(L\\r\\ne1\\r\\nsub)e = 7 in each subsequence for our sparse attention model, and Le2 = 293\\r\\nin its full attention counterpart. We stack the same layers on both sparse attention and full attention models.\\r\\nHence, we can make sure that their whole memory usage is comparable if their memory usage is comparable in\\r\\nevery layer. In sparse attention equipped with local attention, every cell attends to 2 ∗ L\\r\\ne1\\r\\nloc = 14 cells in each\\r\\nsubsequence at most, causing a cell attend to 2∗L\\r\\ne1\\r\\nloc ∗Le1 /Le1sub = 14∗8 = 112 cells at most in total. Therefore,\\r\\nwe get the memory usage of sparse attention in each layer is Le1 ∗2∗L\\r\\ne1\\r\\nloc ∗Le1 /Le1sub = 768∗112 = L\\r\\n2\\r\\ne2 ≈ 293.\\r\\nFollowing such setting, the memory usage of the sparse attention model is comparable to that of the full attention\\r\\nmodel. For traffic-f dataset, one can follow the same procedure to check the memory usage.\\r\\nA.5 Visualization of attention matrix\\r\\nHere we show an example of learned attention patterns in the masked attention matrix of a head within canonical\\r\\nTransformer’s last layer on traffic-c dataset. Figure 6 (a) is a time series window containing 8 days in\\r\\ntraffic-c . The time series obviously demonstrates both hourly and daily patterns. From its corresponding\\r\\nmasked attention matrix, as shown in Figure 6 (b), we can see that for points in weekdays, they heavily attend to\\r\\nprevious cells (including itself) at the same time in weekdays while points on weekends tend to only attend to\\r\\nprevious cells (including itself) at the same time on weekends. Hence, the model automatically learned both\\r\\nhourly and daily seasonality, which is the key to accurate forecasting.\\r\\n8\\r\\nhttps://github.com/nlpdata/mrc_bert_baseline/blob/master/bert/optimization.py\\r\\n9Value in 00:00 is the aggregation of original value in 00:15, 00:30, 00:45 and 01:00.\\r\\n10Value in 17:00 is the mean of original value in 17:00, 17:10, 17:20, 17:30, 17:40 and 17:50.\\r\\n11Value in 17:00 is the mean of original value in 17:00 and 17:10.\\r\\n13\\nSun Mon Tue Wed Thu Fri Sat Sun\\r\\n0.00\\r\\n0.05\\r\\n0.10\\r\\n0.15\\r\\n0.20\\r\\noccupancy rate\\r\\n(a)\\r\\nSun Mon Tue Wed Thu Fri Sat Sun\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n(b)\\r\\nFigure 6: (a): An example time series window in traffic-c dataset. (b): Corresponding learned\\r\\nattention patterns in the masked attention matrix of a head within the last layer.\\r\\n14'},\n",
       " {'name': '2410.04803v3.pdf',\n",
       "  'content': 'TIMER-XL: LONG-CONTEXT TRANSFORMERS FOR\\r\\nUNIFIED TIME SERIES FORECASTING\\r\\nYong Liu∗, Guo Qin∗, Xiangdong Huang, Jianmin Wang, Mingsheng LongB\\r\\nSchool of Software, BNRist, Tsinghua University, Beijing 100084, China\\r\\n{liuyong21,qinguo24}@mails.tsinghua.edu.cn\\r\\n{huangxdong,jimwang,mingsheng}@tsinghua.edu.cn\\r\\nABSTRACT\\r\\nWe present Timer-XL, a generative Transformer for unified time series forecasting.\\r\\nTo uniformly predict 1D and 2D time series, we generalize next token prediction,\\r\\npredominantly adopted for causal generation of 1D sequences, to multivariate next\\r\\ntoken prediction. The proposed paradigm uniformly formulates various forecasting\\r\\nscenarios as a long-context generation problem. We opt for the generative Trans\\x02former, which can capture global-range and causal dependencies while providing\\r\\ncontextual flexibility, to implement unified forecasting on univariate series char\\x02acterized by non-stationarity, multivariate time series with complicated dynamics\\r\\nand correlations, and covariate-informed contexts that include both endogenous\\r\\nand exogenous time series. Technically, we propose a universal TimeAttention to\\r\\nfacilitate generative Transformers on multiple time series, which can effectively\\r\\ncapture fine-grained intra- and inter-series dependencies of flattened time series\\r\\ntokens (patches), and is further enhanced by deftly designed position embeddings\\r\\nfor the temporal and variable dimensions. Timer-XL achieves state-of-the-art per\\x02formance across challenging forecasting benchmarks through a unified approach.\\r\\nBased on large-scale pre-training, Timer-XL also demonstrates notable zero-shot\\r\\nperformance, making it a promising architecture for large time series models.\\r\\n1 INTRODUCTION\\r\\nTransformers have contributed significantly to the fields of natural language and computer vision (Rad\\x02ford et al., 2018; Dosovitskiy et al., 2020), and been extensively applied in time series forecasting,\\r\\nbecoming the foundation of specialized forecasters (Zhou et al., 2021; Wu et al., 2021) and large\\r\\nmodels (Das et al., 2023). As a typical generative task, the quality of predictions heavily relies on the\\r\\ncontext (Dai et al., 2019). Reliable predictions are made by thoroughly considering endogenous tem\\x02poral variations and retrieving relevant exogenous correlations into the context (Box, 2013). Further,\\r\\nthe pre-training context length, which serves as an indicator of scaling (Kaplan et al., 2020), deter\\x02mines the maximum input and output of generative Transformers, ultimately enabling long-sequence,\\r\\nhigh-resolution, and high-frequency generation (Yin et al., 2023; Wang et al., 2024a).\\r\\nHowever, existing Transformers in the time series field crucially encounter the context bottleneck.\\r\\nAs shown in Figure 1, unlike Transformers for natural language and vision that learn dependencies\\r\\namong thousands to millions of tokens (Kirillov et al., 2023; OpenAI, 2023), time-series Transformers\\r\\ntypically work around limited contexts of up to hundreds of time series tokens (patches) (Nie et al.,\\r\\n2022). For univariate time series, a short context length leads to an insufficient perception of global\\r\\ntendencies, overlooking widespread non-stationarity in real-world time series (Hyndman, 2018). The\\r\\nexcessive reliance on stationarization, such as normalization (Kim et al., 2021), restricts the model\\r\\ncapacity and leads to overfitting of Transformers (Liu et al., 2022b). Moreover, instead of regarding\\r\\nmultivariate time series as independent channels (Nie et al., 2022), increasing Transformers explicitly\\r\\ncapture intra- and inter-channel dependencies (Zhang & Yan, 2022; Liu et al., 2023; 2024a), leading\\r\\nto an urgency of extending the context length to encompass inter-correlated variables.\\r\\nRecently, generative Transformers, which present a predominant scalable choice of large language\\r\\nmodels (Zhao et al., 2023) characterized by the decoder-only architecture, have gained increasing\\r\\n∗Equal Contribution\\r\\n1\\r\\narXiv:2410.04803v3 [cs.LG] 18 Dec 2024\\nTokens of Modality # Tokens of Context \\r\\nUnified Time Series Forecasting Answer the following mathematical questions:\\r\\nQ: If you have 12 apples and you give 5 to your \\r\\nfriend, how many apples do you have now?\\r\\nA: The answer is 7.\\r\\n106\\r\\n105\\r\\n104\\r\\n103\\r\\n102\\r\\n101\\r\\n100\\r\\n107\\r\\nGPT-4\\r\\nClaude-2\\r\\nLTM-2\\r\\nViT\\r\\nDiT\\r\\nSAM\\r\\nPatchTST\\r\\nTimer\\r\\nMoiria\\r\\nLanguage Models\\r\\nVision Models\\r\\nTime Series ModelsTimer-XL\\r\\nNon-stationary\\r\\n(a) Univariate\\r\\n(b) Multivariate\\r\\n(c) with Covariates\\r\\nYearly Context\\r\\nLarger\\r\\nContext \\r\\nOne-for-All\\r\\n Forecaster\\r\\nPast\\r\\nLong-Context Transformer \\r\\nExogenous. \\r\\nEndogenous Context\\r\\nContext\\r\\nFuture\\r\\nFuture\\r\\nFigure 1: We compare the context length (measured in the number of tokens) of Transformers in\\r\\ndifferent modalities and propose Timer-XL that increases the length to thousands of patch tokens.\\r\\nGiven the generality across contexts, Timer-XL is a versatile solution for various forecasting tasks.\\r\\nattention in the development of large time series models (Rasul et al., 2023; Ansari et al., 2024). Based\\r\\non their generalization performance and contextual flexibility, one model can accommodate varing\\r\\ninput lengths during inference (Liu et al., 2024b). Therefore, pre-training on longer contexts not only\\r\\nempowers them with the fundamental capability to incorporate more contextual information but also\\r\\nenhances the model versatility toward a one-for-all foundation model, which regards any-variate and\\r\\nany-length time series as one context. Even if previous work (Liu et al., 2024a) has achieved unified\\r\\nmodeling on flattened tokens in encoder-only Transformers (Nie et al., 2022), our empirical results\\r\\nin Figure 3 reveal that encoder-only Transformers encounter performance bottleneck especially on\\r\\nlong-context time series, while generative Transformers can mitigate performance degradation well.\\r\\nIn pursuit of unified time series forecasting, we propose multivariate next token prediction. It unifies\\r\\nforecasting tasks of Figure 1 into the patch-level generation based on long-context sequences. To\\r\\nfully leverage the global-range modeling ability and length-flexibility of generative Transformers, we\\r\\ndevelop TimeAttention that captures causal temporal dependencies in the channel-dependent approach,\\r\\npresenting a novel masking mechanism from the disentangling perspective. With incorporated relative\\r\\nposition embeddings for multivariate series, TimeAttention is aware of the chronological order of time\\r\\npoints and achieves permutation-equivariance (Zaheer et al., 2017) of variables. We propose Timer-XL\\r\\nas the extra long version of a generative time-series Transformer (Timer) (Liu et al., 2024c). We\\r\\nenlarge the context to thousands of patch tokens and achieve state-of-the-art in univariate, multivariate,\\r\\nand covariate-informed forecasting. With comparable pre-training scale and model size, Timer-XL\\r\\noutperforms advanced large models in zero-shot forecasting. Our contributions lie in three aspects:\\r\\n• We propose multivariate next token prediction and unified time series forecasting, strength\\x02ening Transformers with enriched forecasting contexts to make reliable predictions.\\r\\n• We introduce TimeAttention, a novel causal self-attention tailored for the time series modal\\x02ity on our proposed paradigm, which enables intra- and inter-series modeling with positional\\r\\nawareness, and maintains the causality and length-flexibility of generative Transformers.\\r\\n• We propose Timer-XL, a versatile Transformer for one-for-all forecasting, which mitigates\\r\\nperformance degradation in long-context time series, achieves state-of-the-art performance\\r\\nin task-specific benchmarks, and presents notable zero-shot results by pre-training.\\r\\n2 RELATED WORK\\r\\nTransformers (Vaswani et al., 2017) for time series forecasting have undergone rapid advancements.\\r\\nBased on the global-range modeling ability of tokens, Transformers have shown great power in time\\r\\nseries, rapidly standing out from RNN-, CNN- and MLP-forecasters, especially on long sequences.\\r\\nInitial Transformer-based forecasters primarily focused on long-term prediction, aiming to extend the\\r\\nforecasting horizon while mitigating the quadratic computational growth associated with increasing\\r\\nsequence length (Li et al., 2019; Zhou et al., 2021; Wu et al., 2021). However, the context (lookback)\\r\\nlength of previous models is not growing in pace, which hinders Transformers from making fully\\r\\n2\\ninformed predictions, producing oversmooth results (Liu et al., 2022b). Meanwhile, another advance\\x02ment has emerged from univariate to multivariate forecasting. Unlike natural language, time series are\\r\\nhigh-dimensional and inherently correlated (Hyndman, 2018), further requiring longer contexts that\\r\\ncontain relevant endogenous and exogenous variables. To effectively exploit the intra- and inter-series\\r\\ndependencies, tokenization of Transformers has been extensively developed in temporal-wise (Lim\\r\\net al., 2021), patch-wise (Nie et al., 2022), and variable-wise (Liu et al., 2023) approaches, with deftly\\r\\ndesigned Transformers for inter-series modeling (Zhang & Yan, 2022; Wang et al., 2024b).\\r\\nDespite the challenges caused by the limited context for making predictions, few works highlight that\\r\\nthese challenges can be uniformly tackled by long-context generative Transformers. Consequently, we\\r\\nleverage the fundamental sequence modeling capability of the vanilla Transformer, extend the context\\r\\nlength significantly, and unify various forecasting tasks into multivariate next token prediction.\\r\\nRecently, time-series Transformers have experienced the evolution from small task-specific models\\r\\nto large foundation models (Das et al., 2023; Woo et al., 2024; Ansari et al., 2024). Among them,\\r\\ndecoder-only Transformers, predominantly adopted as the backbone of large language models (Tou\\x02vron et al., 2023; OpenAI, 2023), have positioned as a scalable and generalizable choice for general\\r\\ntime series analysis (Liu et al., 2024c). By independently predicting each token based on preceding\\r\\ncontexts, decoder-only models are also multi-length forecasters (Liu et al., 2024b), avoiding resource\\x02intensive training and lookback-search. However, existing decoder-only forecasters are generally\\r\\ntrained on a single channel (Nie et al., 2022), making them inaccessible to inter-series dependencies.\\r\\nPrior work has employed encoder-only Transformers to fully capture dependencies within 2D time\\r\\nseries (Liu et al., 2024a). The incompatibility between this architecture and temporal causality may\\r\\nconstrain the flexibility and performance of Transformers. To date, the implementation of next token\\r\\nprediction and multivariate time series forecasting in a single Transformer remains a fundamental\\r\\nchallenge. Our work addresses this issue by disentangling the fine-grained token dependencies clearly\\r\\ninto variable dependencies and temporal causal masks, thereby maintaining temporal causality and\\r\\ncapturing inter-series dependencies simultaneously without altering the self-attention mechanism.\\r\\n3 APPROACH\\r\\nIn this section, we first introduce a generative Transformer to illustrate the procedure of next token\\r\\nprediction on 1D time series. As an extension, we design TimeAttention and propose Timer-XL for\\r\\nunified time series forecasting. It is applicable to univariate, multivariate, and covariate-informed\\r\\nscenarios by generalizing the context from 1D sequences to 2D time series.\\r\\n3.1 TIMER\\r\\nTimer (Liu et al., 2024c) is a generative Transformer trained by next token prediction (Bengio et al.,\\r\\n2000), which regards single-dimensional time series as non-overlapping patch tokens.\\r\\nNext Token Prediction Given an univariate time series X = {x1, . . . , xT P } of length T P, a time\\r\\nseries token is defined as P consecutive time points, also termed as the patch token:\\r\\nxi = {x(i−1)P +1, . . . , xiP } ∈ R\\r\\nP\\r\\n, i = 1, . . . , T. (1)\\r\\nThe training objective is to independently predict the next patch token to maximize the likelihood:\\r\\nP(X) = Y\\r\\nT\\r\\ni=1\\r\\np(xi+1|x≤i), (2)\\r\\nwhich is realized by a decoder-only architecture with the block number L and model dimension D:\\r\\nh\\r\\n0\\r\\ni = Wexi\\r\\n, i = 1, . . . , T,\\r\\nHl = TrmBlock(Hl−1), l = 1, . . . , L,\\r\\n{xˆi+1} = HLWd, i = 1, . . . , T.\\r\\n(3)\\r\\nFor simplicity, we omit the block index l. Timer adopts We,Wd ∈ R\\r\\nD×P that independently\\r\\nembed and project the token embeddings as H = {hi} ∈ R\\r\\nT ×D. TrmBlock includes feed-forward\\r\\nnetwork and self-attention with the temporal causal mask T ∈ R\\r\\nT ×T\\r\\n. hi ∈ R\\r\\nD is the contextual\\r\\nrepresentation of the previous i tokens. Predicted xˆi+1 are supervised by ground truth in MSE loss.\\r\\n3\\n3.2 GENERALIZE 1D SEQUENCES TO 2D TIME SERIES\\r\\nFor the enlarged context with the additional dimension, our proposed attention mechanism aims to (1)\\r\\nthoroughly capture intra- and inter-series dependencies and (2) preserve causality within the temporal\\r\\ndimension. Without loss of generality, we illustrate this with the case of multivariate forecasting.\\r\\nMultivariate Next Token Prediction Given a multivariate time series X ∈ R\\r\\nN×T P with the\\r\\nnumber of variables N, the time series token xm,i is defined as the i-th patch of the m-th variable:\\r\\nxm,i = {Xm,(i−1)P +1, . . . , Xm,iP } ∈ R\\r\\nP\\r\\n, m = 1, . . . , N, i = 1, . . . , T. (4)\\r\\nThe training objective is still to independently predict the next token. Unlike before, each prediction\\r\\nis made based on tokens of the previous time (≤ i) from all N variables:\\r\\nP(X) = Y\\r\\nN\\r\\nm=1\\r\\nY\\r\\nT\\r\\ni=1\\r\\np(xm,i+1|x:,≤i) = Y\\r\\nN\\r\\nm=1\\r\\nY\\r\\nT\\r\\ni=1\\r\\np(xm,i+1|x1,≤i, . . . , xN,≤i). (5)\\r\\nCompared with Equation 2, the multivariate context length increases from T to NT. By contrast, the\\r\\nbenefit is that this paradigm learns causal dependencies within each sequence while incorporating\\r\\nexogenous variable correlations from other sequences, making it a universal forecasting paradigm\\r\\nthat outperforms channel independence or coarse-grained variable-wise modeling experimentally.\\r\\nTechnically, we still adopt the token embedding We ∈ R\\r\\nD×P to obtain the patch-wise representation\\r\\nhm,i ∈ R\\r\\nD, which will encompass contextual information from N i tokens through Transformer\\r\\nblocks and be eventually projected by Wd ∈ R\\r\\nD×P into the predicted patch token xˆm,i+1.\\r\\nPosition Embedding Position embedding has not been sufficiently explored in time-series Trans\\x02formers. To avoid the permutation-invariance of self-attention, positional embedding is required to\\r\\nreflect the chronological order of tokens on the temporal dimension. As for the variable dimension,\\r\\nshuffling the input order of variables should not affect anything other than the output order of variables.\\r\\nFormally, the processing on multiple variables should be permutation-equivalent (Zaheer et al., 2017).\\r\\nTo meet the above requirements, we adopt RoPE (Su et al., 2024), a widely utilized position embed\\x02ding on the temporal dimension. For the variable dimension, we use two learnable scalars in each head\\r\\nto keep the permutation-equivalence of variables (Woo et al., 2024). Beyond simply incorporating\\r\\nthem together, we provide detailed ablations in Section E.3 to demonstrate the effectiveness:\\r\\nAmn,ij = h\\r\\n⊤\\r\\nm,iWqRθ,i−jW⊤k hn,j + u · 1(m = n) + v · 1(m ̸= n), (6)\\r\\nwhere Wq,Wk,Wv ∈ R\\r\\nD×dk and dk is the dimension of the query, key, and value. Rθ,t ∈ Rdk×dk\\r\\nis the rotary matrix with rotation degree t· θ, 1(·) is the indicator function, and u, v ∈ R are learnable\\r\\nparameters for the token to distinguish its endogenous and exogenous variables.\\r\\nTimeAttention In contrast to variable-wise (Liu et al., 2023) and non-causal patch-wise tokens (Nie\\r\\net al., 2022; Woo et al., 2024), our TimeAttention aims to capture causal patch-wise dependencies\\r\\nwithin and among all variables. Concretely, we sort patch tokens by flattening their 2D indices into\\r\\n1D indices in the temporal-first manner, which is illustrated in the upper left of Figure 2. Note that\\r\\nthe order of variables does not matter, since Equation 6 guarantees their permutation-equivalence.\\r\\nWe provide an intuitive example to illustrate the causal dependencies within multivariate time series:\\r\\nconsidering the 2nd token of time series A. To predict its next token, its representation h should be\\r\\nexactly dependent on the tokens-{1, 2, 4, 5}. Similarly, we provide all causal dependencies of each\\r\\ntoken in Figure 12. Based on the visualized attention mask and variable dependencies presented in\\r\\nFigure 2, where all variables are inter-correlated, we find that causal temporal dependencies in A\\r\\ncan be formally disentangled by the Kronecker product into (1) the adjacency matrix of the variable\\r\\ndependency graph C ∈ R\\r\\nN×N and (2) the causal temporal mask T ∈ RT ×T\\r\\n:\\r\\nTi,j =\\r\\n\\x1a\\r\\n1 if j ≤ i,\\r\\n0 otherwise,\\r\\nCm,n =\\r\\n\\x1a\\r\\n1 if variable m is dependent on n,\\r\\n0 otherwise.\\r\\n(7)\\r\\nLet the Kronecker product ⊗ : (R\\r\\nN×N , RT ×T\\r\\n) 7→ R\\r\\nNT ×NT take two matrices and produce a block\\r\\nmatrix. Consequently, TimeAttention is formulated as follows:\\r\\nTimeAttention(H) = Softmax \\x12\\r\\nMask(C ⊗ T ) + A\\r\\n√\\r\\ndk\\r\\n\\x13\\r\\nHWv, Mask(M) = (\\r\\n0 if Mi,j = 1,\\r\\n−∞ if Mi,j = 0.\\r\\n(8)\\r\\n4\\n1 2 3\\r\\nUnivariate\\r\\nTime Series B\\r\\nTime Series A Multivariate with Covariate B\\r\\n1 2 3\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\nMasked Token\\r\\nUnmasked Token\\r\\nDependencies\\r\\nTokens\\r\\nDependencies\\r\\nO\\r\\n\\uf8ff 1 1\\r\\n1 1 \\r\\nT\\r\\nO\\r\\nT\\r\\n\\uf8ff 1 1\\r\\n0 1\\r\\n\\r\\n=\\r\\n=\\r\\n1 2 3 4 5 6\\r\\nA-Tokens B-Tokens\\r\\n1\\r\\n2\\r\\n3\\r\\nA-Tokens B-Tokens\\r\\nC C\\r\\n4 5 6\\r\\nVariable Dependency\\r\\nVariable\\r\\nDependency\\r\\nC\\r\\nT\\r\\nDependencies\\r\\n1 2 3 4 5 6\\r\\nA B\\r\\nFigure 2: Illustration of TimeAttention. For univariate series, temporal mask T keeps the causality.\\r\\nGiven multivariate patch tokens sorted in a temporal-first order, we adopt the variable dependencies\\r\\nC, an all-one matrix, as the left-operand of Kronecker product, expanding temporal mask to a block\\r\\nmatrix, which exactly reflects dependencies of multivariate next token prediction. The formulation is\\r\\nalso generalizable to univariate and covariate-informed contexts with pre-defined variable dependency.\\r\\nAs before, token representations in H = {hm,i} ∈ R\\r\\nNT ×D will be independently processed by\\r\\nfeed-forward network and layer normalization, and fed into the next Transformer block.\\r\\nUnified Time Series Forecasting In multivariate forecasting, the variable dependency forms the\\r\\ncomplete graph, presenting an all-one matrix C. By generalizing TimeAttention on multiple sequences,\\r\\ngenerative Transformers can leverage its length-flexibility to encompass relevant covariates as well.\\r\\nIn this case, Timer-XL is adapted in two steps: (1) formulate the customized variable dependency\\r\\nas C and (2) optimize the model using the supervision of target variables. An example (target-A\\x02covariate-B) of TimeAttention is illustrated on the right of Figure 2. In a nutshell, we adopt position\\r\\nembeddings for the temporal and variable dimensions. To achieve unified time series forecasting, we\\r\\nflatten 2D time series into a unified context and capture fine-grained causal token dependencies.\\r\\n4 EXPERIMENTS\\r\\nWe conduct thorough evaluations of the performance and versatility of Timer-XL. Given that the\\r\\nlong-context forecasting paradigm receives less attention in the community, which can be concealed\\r\\ndue to the performance saturation on previous benchmarks (Wu et al., 2021; Nie et al., 2022), we delve\\r\\ninto the backbone and representation comparison on Transformers. New long-context benchmarks\\r\\nare established, which will be released for the advancement of this field. Detailed datasets, baseline\\r\\nmodels, and experimental configurations are provided in Appendix B. We also dive into commonly\\r\\nadopted techniques, such as channel independence (Nie et al., 2022) and normalization (Kim et al.,\\r\\n2021). We conclude that generative Transformers can tackle these challenges without specific designs.\\r\\n4.1 UNIVARIATE TIME SERIES FORECASTING\\r\\nSetups Due to the insufficient dataset length when extending contexts in univariate datasets (Makri\\x02dakis et al., 2020), we adopt wide-recognized benchmarks from Liu et al. (2023). Although these\\r\\ndatasets are originally multivariate, they aim to be predicted in a univariate approach with the imple\\x02mentation of channel independence. Different from the previous long-term forecasting setting, we\\r\\nfocus on reliable prediction based on a long context. Therefore, we fix the prediction horizon and\\r\\nincrease the lookback length to monthly and yearly levels. We also establish a long-context univariate\\r\\nbenchmark based on the challenging 40-year ECMWF Reanalysis v5 dataset (Hersbach et al., 2020),\\r\\nwhere yearly contexts are adopted to predict the land-surface temperature of a single site (ERA5-S).\\r\\nResults As shown in Figure 3, the accuracy of univariate prediction can generally be improved\\r\\nby extending the daily context to monthly. We draw a similar conclusion on ERA5 (Table 14),\\r\\n5\\n1 Day\\r\\n4 Day\\r\\n1 Week\\r\\n1 Month\\r\\n4 Month\\r\\n4 Day\\r\\n1 Week\\r\\n1 Month 4 Month 1 Year\\r\\n4 Day\\r\\n1 Week\\r\\n1 Month 4 Month\\r\\n1 Year\\r\\n8 Hour\\r\\n1 Day 4 Day 1 Week 1 Month\\r\\nFigure 3: Univariate forecasting (pred-96) of well-acknowledged benchmarks under channel indepen\\x02dence (Nie et al., 2022). We increase the lookback length to encompass monthly and yearly contexts.\\r\\nwhere extending the context consistently helps in the specific model architecture. Empirically, Timer\\x02XL, the generative Transformer, outperforms state-of-the-art encoder-only Transformer and linear\\r\\nforecaster in excessively long contexts. Further, we conduct representation analysis in Appendix E.4,\\r\\nrevealing that Timer-XL is proficient at adaptively selecting information in vast observations, and\\r\\nthus achieves breakthrough performance. It is also noteworthy that the performance of monthly and\\r\\nyearly contexts improves slowly and deteriorates, which may stem from increased noise and training\\r\\ndifficulty inherent in data, which leaves a future direction to improve the context efficiency. Table 1\\r\\nprovides results on ERA5-S. Timer-XL consistently outperforms PatchTST on all sites, which can be\\r\\ncredited to the maintenance of causality and token-wise supervision in the decoder-only architecture.\\r\\nAnalysis Furthermore, we analyze the widespread non-stationary challenge in univariate tasks. It is\\r\\ncommonly tackled by normalization (Kim et al., 2021) that greatly improves Transformer performance\\r\\nin previous benchmarks. However, we find it may be caused by the insufficient time span and training\\r\\nsamples in these datasets. Thus, normalization enriches training samples by aligning time series with\\r\\ndifferent means and variances to the same distribution. Instead, it makes Transformer constrained on\\r\\nthe temporal variation within windows, preventing them from learning variations among windows\\r\\nand resulting in oversmooth predictions and failures in long contexts. In Table 1 and Table 15, we\\r\\nevaluate the performance on ERA5 and widely-acknowledged datasets, which validates the claim\\r\\nthat generative Transformers can achieve better results even without instance normalization.\\r\\nTable 1: Univariate forecasting (input-3072-pred-96) of ERA5-S (40 years), encompassing 117k time\\r\\npoints in each station. We evaluate PatchTST and Timer-XL with and without normalization (Kim\\r\\net al., 2021). + Norm. indicates using the normalization. We train one model for each site separately.\\r\\nStation Beijing Hongkong London New York Paris Seoul Shanghai Average\\r\\nModel MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nPatchTST 0.0791 0.221 0.189 0.327 0.277 0.415 0.186 0.334 0.266 0.407 0.0940 0.238 0.137 0.289 0.175 0.319\\r\\n+ Norm. 0.0797 0.220 0.191 0.323 0.281 0.419 0.184 0.334 0.272 0.411 0.0914 0.233 0.136 0.287 0.176 0.319\\r\\nTimer-XL 0.0739 0.210 0.179 0.316 0.262 0.404 0.182 0.327 0.254 0.399 0.0901 0.229 0.134 0.282 0.168 0.310\\r\\n+ Norm. 0.0742 0.210 0.183 0.317 0.278 0.418 0.181 0.330 0.264 0.407 0.0896 0.227 0.133 0.281 0.172 0.313\\r\\n4.2 MULTIVARIATE TIME SERIES FORECASTING\\r\\nSetups We follow iTransformer (Liu et al., 2023) to evaluate multivariate forecasting performance.\\r\\nToward a one-for-all forecaster, we also evaluate rolling forecast performance, that is, we trained one\\r\\nmodel for all prediction horizons by integrating the previous prediction into the lookback window\\r\\nin the next iteration. We further establish long-context multivariate forecasting benchmarks: ERA5\\r\\nmulti-station land-surface temperature prediction (ERA5-MS), and the global temperature and wind\\r\\nspeed forecasting challenge (GTWSF) (Wu et al., 2023), to learn complex temporal dynamics and\\r\\nvariable correlations with sufficient training samples.\\r\\nResults As shown in Tables 2-4 and Figure 4, Timer-XL achieves the best results on both previous\\r\\nand new benchmarks. Essentially, Transformers that explicitly capture inter-series dependencies, such\\r\\nas UniTST (Liu et al., 2024a) and iTransformer, reasonably achieve decent performance in Table 2.\\r\\n6\\n13.241\\r\\n11.626\\r\\n9.706\\r\\n14.93313.448\\r\\n32.249\\r\\n9.203\\r\\n13.926\\r\\n23.326\\r\\n7.709 7.602 7.172\\r\\nMSE (Global Temp.)\\r\\nHolt-Winers\\r\\nProphet\\r\\nGDBT\\r\\nGFS\\r\\nERA5\\r\\nDeep-AR\\r\\nN-BEATS\\r\\nStemGNN\\r\\nPyraformer\\r\\nCorrformer\\r\\nUniRepLKNet\\r\\nUniTST\\r\\nStatistics-based Numerical Simulation Deep Models Ours MSE (Global Wind)\\r\\n5.912\\r\\n9.691\\r\\n4.101\\r\\n9.993\\r\\n4.999 5.248\\r\\n4.124 4.066\\r\\n4.614\\r\\n7.356 3.889 3.865 3.807 3.786\\r\\nTimer-XL\\r\\nHolt-Winers\\r\\nProphet\\r\\nGDBT\\r\\nGFS\\r\\nERA5\\r\\nDeep-AR\\r\\nN-BEATS\\r\\nStemGNN\\r\\nPyraformer\\r\\nCorrformer\\r\\nUniRepLKNet\\r\\nUniTST\\r\\nTimer-XL\\r\\nFigure 4: Multivariate forecasting of GTWSF (2-day-pred-1-day), involving 3850 worldwide stations\\r\\nspanning two years. Results of the baseline models are officially reported by Ding et al. (2024).\\r\\nBeyond iTransformer, Timer-XL can model fine-grained patch-wise temporal dependencies. With\\r\\nTimeAttention, Timer-XL outperforms Timer especially on high-dimensional time series (13.2% in\\r\\nECL and 6.3% in Traffic, with thousands of tokens in the context). Compared with the encoder-only\\r\\nUniTST, decoder-only Transformers excel at generalizing across varying prediction lengths in Table 3.\\r\\nTable 2: Multivariate forecasting (96-pred-96) of well-acknowledged benchmarks. All models are\\r\\ntrained from scratch. Results of baseline models are officially reported by Liu et al. (2023).\\r\\nModels Timer-XL Timer UniTST iTransformer DLinear PatchTST TimesNet Stationary Autoformer\\r\\n(Ours) (2024c) (2024a) (2023) (2023) (2022) (2022) (2022b) (2021)\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nECL 0.138 0.233 0.159 0.244 0.139 0.235 0.148 0.240 0.197 0.282 0.181 0.270 0.168 0.272 0.169 0.273 0.201 0.317\\r\\nETTh1 0.381 0.399 0.386 0.401 0.385 0.402 0.386 0.405 0.386 0.400 0.414 0.419 0.384 0.402 0.513 0.491 0.449 0.459\\r\\nTraffic 0.387 0.260 0.413 0.265 0.389 0.265 0.395 0.268 0.650 0.396 0.462 0.295 0.593 0.321 0.612 0.338 0.613 0.388\\r\\nWeather 0.165 0.209 0.176 0.215 0.165 0.210 0.174 0.214 0.196 0.255 0.177 0.218 0.172 0.220 0.173 0.223 0.266 0.336\\r\\nSolar-Energy 0.200 0.229 0.204 0.234 0.203 0.232 0.203 0.237 0.290 0.378 0.234 0.286 0.250 0.292 0.215 0.249 0.884 0.711\\r\\nTable 3: Multivariate forecasting (672-pred-{96, 192, 336, 720}) of well-acknowledged benchmarks.\\r\\nWe evaluate one-for-all forecasters following Liu et al. (2024b): rolling forecasting for four forecast\\r\\nlengths with one model. Averaged results are reported here and full results are provided in Table 11.\\r\\nModels Timer-XL Timer UniTST iTransformer DLinear PatchTST TimesNet Stationary Autoformer\\r\\n(Ours) (2024c) (2024a) (2023) (2023) (2022) (2022) (2022b) (2021)\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nECL 0.155 0.246 0.161 0.251 0.163 0.257 0.164 0.258 0.165 0.265 0.169 0.268 0.201 0.303 0.265 0.358 0.289 0.379\\r\\nETTh1 0.409 0.430 0.418 0.436 0.429 0.447 0.421 0.445 0.426 0.444 0.412 0.435 0.495 0.491 0.505 0.513 0.517 0.528\\r\\nTraffic 0.374 0.255 0.384 0.259 0.385 0.265 0.384 0.274 0.423 0.298 0.391 0.275 0.602 0.322 0.630 0.347 0.684 0.433\\r\\nWeather 0.240 0.273 0.232 0.270 0.231 0.272 0.266 0.291 0.239 0.291 0.226 0.268 0.264 0.293 0.308 0.329 0.435 0.455\\r\\nSolar-Energy 0.198 0.249 0.233 0.249 0.241 0.275 0.213 0.291 0.222 0.283 0.202 0.269 0.213 0.295 0.254 0.315 0.265 0.325\\r\\nAnalysis Patching (Nie et al., 2022) has been demonstrated as an effective tokenization approach\\r\\nfor the time series modality, leading to the boom of Transformers in supervised deep forecasters and\\r\\nlarge time series models. To better cope with multivariate time series forecasting, we compared these\\r\\nTransformers on ERA5-MS to answer the following questions: (1) whether to conduct explicit inter\\x02series modeling or not (channel independence) and (2) whether to use decoder-only or encoder-only\\r\\nTransformers. The combination presents four typical Transformers in Table 4, which shows that\\r\\nTimer-XL combines the advantages of explicit inter-series modeling and the decoder-only architecture,\\r\\nwhich is suitable for multivariate time series forecasting with arbitrary prediction horizons.\\r\\n4.3 COVARIATE-INFORMED TIME SERIES FORECASTING\\r\\nSetups For the covariate-informed forecasting, we adopt the well-acknowledged electricity price\\r\\nforecasting (EPF) task (Lago et al., 2021). Each subset contains electricity price as the endogenous\\r\\n7\\nTable 4: Multivariate forecasting (input-3072-pred-96) of ERA5-MS (40 years and 7 stations). We\\r\\nfairly evaluate Transformers that adopt patched time series. CI. indicates whether the Transformer\\r\\nuses channel independence (Nie et al., 2022). Arch. categorizes them into the encoder-only (E) and\\r\\ndecoder-only (D) architectures. Different from ERA5-S in Table 1, we train one model for all sites.\\r\\nStation Beijing Hongkong London New York Paris Seoul Shanghai Average\\r\\nModel CI. Arch. MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nPatchTST Yes E 0.0815 0.222 0.190 0.326 0.275 0.414 0.185 0.333 0.265 0.407 0.0977 0.240 0.139 0.290 0.176 0.319\\r\\nUniTST No E 0.0753 0.213 0.179 0.318 0.269 0.410 0.185 0.330 0.256 0.401 0.0901 0.230 0.135 0.284 0.170 0.312\\r\\nTimer Yes D 0.0734 0.210 0.182 0.319 0.268 0.407 0.183 0.329 0.255 0.399 0.0877 0.226 0.132 0.281 0.169 0.310\\r\\nTimer-XL No D 0.0736 0.209 0.174 0.309 0.263 0.404 0.182 0.327 0.252 0.396 0.0872 0.225 0.130 0.278 0.166 0.307\\r\\nvariable and two exogenous variables. Therefore, the variable dependency for Timer-XL is formulated\\r\\nas C = [[1, 1, 1], [0, 1, 0], [0, 0, 1]]. To investigate whether to learn causal or noncausal patch-wise\\r\\ndependencies in covariates, we implement two versions of Timer-XL: the original one with temporal\\r\\ncausal mask T , and the noncausal one with T replaced by an all-one matrix.\\r\\nResults As shown in Table 5, Timer-XL outperforms state-of-the-art models in covariate-informed\\r\\ntasks. Compared with TimeXer (Wang et al., 2024b), which treats an entire covariate as a token,\\r\\nTimer-XL learns fine-grained patch-wise dependencies. By the noncausal version of Timer-XL, we\\r\\nsurprisingly find consistent conclusions with endogenous variables: results will be better if Timer-XL\\r\\nlearns causal dependencies within exogenous variables. It again validates that next token prediction\\r\\nthat maintains causality has a higher upper limit of performance.\\r\\nTable 5: Covariate-informed forecasting (168-pred-24) of EPF. We implement two versions of Timer\\x02XL: Noncausal indicates that we do not maintain the causality within covariates by replacing temporal\\r\\ncausal mask with all-one matrix. Results of baselines are officially reported by Wang et al. (2024b).\\r\\nModels Timer-XL Timer-XL TimeXer iTransformer DLinear PatchTST Crossformer TimesNet Autoformer\\r\\n(Ours) (Noncausal) (2024b) (2023) (2023) (2022) (2022) (2022) (2021)\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nNP 0.234 0.262 0.237 0.265 0.238 0.268 0.265 0.300 0.309 0.321 0.267 0.284 0.245 0.289 0.250 0.289 0.402 0.398\\r\\nPJM 0.089 0.187 0.092 0.188 0.088 0.188 0.097 0.197 0.108 0.215 0.106 0.209 0.149 0.198 0.097 0.195 0.168 0.267\\r\\nBE 0.371 0.243 0.410 0.279 0.379 0.243 0.394 0.270 0.463 0.313 0.403 0.264 0.436 0.294 0.419 0.288 0.500 0.333\\r\\nFR 0.381 0.204 0.406 0.220 0.384 0.208 0.439 0.233 0.429 0.260 0.411 0.220 0.440 0.216 0.431 0.234 0.519 0.295\\r\\nDE 0.434 0.415 0.435 0.415 0.440 0.418 0.479 0.443 0.520 0.463 0.461 0.432 0.540 0.423 0.502 0.446 0.674 0.544\\r\\nAverage 0.302 0.262 0.316 0.273 0.306 0.265 0.335 0.289 0.366 0.314 0.330 0.282 0.362 0.284 0.340 0.290 0.453 0.368\\r\\n4.4 PRE-TRAINED TIME SERIES TRANSFORMERS\\r\\nSetups Pre-training enriches time-series Transformers with generalizable forecasting capabilities.\\r\\nThe outcome large time series model can better cope with widespread challenges of few-shot and\\r\\nzero-shot forecasting. To scale Timer-XL as large models, we conduct pre-training on UTSD (Liu\\r\\net al., 2024c) and LOTSA (Woo et al., 2024) for zero-shot forecasters. We also curate a large-scale\\r\\nmultivariate dataset (ERA5-Large: 40 years and 4920 stations) for a domain-specific model. In this\\r\\ntask, we evaluate in-dataset generalization performance of PatchTST and Timer-XL: pre-training on\\r\\n80% stations and 80% time span and then forecast on the remaining stations (variable generalization),\\r\\ntime span (temporal generalization), and the cross-time and cross-station split. To evaluate the benefit\\r\\nof pre-training with longer context, we compare the zero-shot performance of Timer (2024c) and\\r\\nTimer-XL by pre-training on UTSD (1B time points), where the context length of Timer-XL is\\r\\nincreased from 672 to 2880. To establish a fair zero-shot benchmark, we pre-train Timer-XL on\\r\\nLOTSA (27B observations) such that baseline models are built upon a comparable pre-training scale.\\r\\nResults We provide in-dataset generalization results of ERA5-Large in the middle of Figure 5\\r\\n(a). Timer-XL achieves better results than PatchTST in all cases, supporting that decoder-only\\r\\n8\\narchitecture has stronger generalization performance. Figure 5 (b) compares zero-shot performance\\r\\nof two generative Transformers pre-trained on the same UTSD, where Timer-XL outperforms previous\\r\\nTimer on all benchmark datasets, validating that long-context pre-training empowers large time series\\r\\nmodels. In Table 6, we provide a comprehensive zero-shot evaluation under a comparable pre-training\\r\\nscale and model size, where Timer-XL achieves notable performance with better sample efficiency.\\r\\nThe versatility and scalability make it a promising backbone of foundation models.\\r\\nVariables\\r\\nTime\\r\\nDatasets\\r\\nOne-for-All \\r\\nGeneralization\\r\\nUnified \\r\\nContext\\r\\n(a) In-Dataset Generalization (ERA5) (b) Out-of-Dataset Generalization (UTSD) \\r\\n0.438\\r\\n0.394\\r\\n0.314\\r\\n0.280\\r\\n0.690\\r\\n0.621\\r\\n0.213\\r\\n0.204\\r\\n0.192\\r\\n0.141\\r\\n0.458\\r\\n0.405\\r\\n0.181\\r\\n0.167\\r\\nFigure 5: Illustration of one-for-all generalization (left). Based on the contextual flexibility, Timer-XL\\r\\ncan predict heterogeneous time series, indicating three directions of generalization shown on the left.\\r\\nWe compare performance when generalizing across the time and variables (middle), and zero-shot\\r\\nresults across datasets (right), emphasizing the benefit of long-context pre-training.\\r\\nTable 6: Zero-shot forecasting (pred-96) benchmark. Full results of all prediction lengths are provided\\r\\nin Table 12. The configuration of Timer-XLBase shown in Table 10 is comparable with MoiraiBase,\\r\\nwhich is pre-trained on UTSD (Liu et al., 2024c) and LOTSA (Woo et al., 2024) respectively. Dataset\\r\\nfor pre-training is not evaluated on corresponding models, which is denoted by a dash (−).\\r\\nModels Timer-XLBase Timer-XLBase MoiraiSmall MoiraiBase MoiraiLarge TimesFM MOMENT ChronosBase ChronosLarge\\r\\n(UTSD) (LOTSA) (2024) (2024) (2024) (2023) (2024) (2024) (2024)\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nETTh1 0.394 0.413 0.363 0.390 0.401 0.402 0.376 0.392 0.381 0.388 0.414 0.404 0.688 0.557 0.440 0.393 0.441 0.390\\r\\nETTh2 0.280 0.343 0.287 0.344 0.297 0.336 0.294 0.330 0.296 0.330 0.315 0.349 0.342 0.396 0.308 0.343 0.320 0.345\\r\\nETTm1 0.621 0.512 0.308 0.355 0.418 0.392 0.363 0.356 0.380 0.361 0.361 0.370 0.654 0.527 0.454 0.408 0.457 0.403\\r\\nETTm2 0.204 0.292 0.192 0.281 0.214 0.288 0.205 0.273 0.211 0.274 0.202 0.270 0.260 0.335 0.199 0.274 0.197 0.271\\r\\nECL 0.141 0.236 0.135 0.229 0.212 0.300 0.162 0.249 0.155 0.237 - - 0.291 0.355 0.154 0.231 0.152 0.229\\r\\nWeather 0.167 0.224 0.169 0.223 0.198 0.222 0.220 0.217 0.199 0.211 - - 0.243 0.255 0.203 0.238 0.194 0.235\\r\\n1\\r\\nst Count 2 6 0 1 3 1 0 0 0\\r\\n∗ Traffic is not evaluated here because it is generally included in the pre-training datasets of large models.\\r\\n4.5 MODEL ANALYSIS\\r\\nModel Efficiency To evaluate the model efficiency of Timer-XL with respect to the context length,\\r\\nit is essential to recognize the distinct characteristics of time series data compared to 1D sequences.\\r\\nUnlike natural language, the time series modality is characterized by the variable number N and the\\r\\ninput length. We adopt two representative multivariate datasets with different N, and provide the\\r\\nmemory footprint and training speed under gradually prolonged input. We evaluate typical approaches\\r\\nto handle multivariate series: (1) Timer-XL and Moiria that adopt channel dependence; (2) Timer\\r\\nthat adopts channel independence. Intuitively, the complexity of the first type is O(N2T\\r\\n2\\r\\n) while the\\r\\ncomplexity of self-attention under channel independence is O(NT2). However, results shown in\\r\\nFigure 6 reveal that the measured cost of Timer-XL is much less than N times of Timer.\\r\\nSince the previous analysis of model efficiency on time-series Transformer predominantly focuses on\\r\\nthe self-attention on 1D time series, we initially present a theoretical derivation of the computational\\r\\ncomplexity of Transformers on 2D time series, including the parameter counts, memory footprint,\\r\\nand FLOPs in Table 7. We find that other parts of Transformers, such as feed-forward network, have\\r\\na complexity of O(NT) no matter which approach is adopted to handle multivariate time series.\\r\\nThey also cause the dominant overhead in existing benchmarks, since their context length is not large\\r\\nenough, confirming our empirical results. Further, we introduce FlashAttention (Dao et al., 2022) to\\r\\n9\\nreduce the memory footprint and training speed, which is computationally equivalent and reduces the\\r\\noverall memory footprint of Timer-XL to O(NT) without affecting performance.\\r\\nWeather (21 Variables) Weather (21 Variables)\\r\\nECL (321 Variables) ECL (321 Variables)\\r\\nFigure 6: Efficiency analysis. We compare representative time-series Transformers on multivariate\\r\\ndatasets with variable numbers ranging from ten to hundred and increase the lookback length.\\r\\nAveraged Block Raw Data\\r\\nSub-Block(3, 3) ACF Plot\\r\\nLearned Attention\\r\\nAttn(6, 2) Attn(6, 7) \\r\\nToken-6\\r\\nCorr(6, 2) = 0.64\\r\\nConcentrated Attention\\r\\nStrong Autocorrelation\\r\\nToken-5 Token-4 Token-3 Token-2 Token-1 Token-0\\r\\nCorr(6, 7) = 0.78\\r\\nFigure 7: Visualization of TimeAttention. It is from the first sample of a length 672 in the test split of\\r\\nTraffic. We visualize the last 10 variables and each contains 7 tokens. We present auto-correlation\\r\\nfunction plot. Auto-correlation can be reflected by the distribution of the attention (bottom right). We\\r\\naverage TimeAttention across sub-blocks, which can reveal Pearson correlations (upper right).\\r\\nRepresentation Analysis In addition to the enhanced performance, fine-grained token dependencies\\r\\noffer improved interpretability. We present a showcase visualization from Traffic in Figure 7. It is\\r\\nobserved that sub-matrices along the diagonal generally receive greater attention, which reasonably\\r\\nreveals predominant dependencies within the endogenous variable. By zooming in a sub-block that\\r\\ncorresponds to Variable-3, we observe that the attention distribution of the last row can indicate certain\\r\\nstrong dependencies among patch tokens. This observation is also supported by the auto-correlation\\r\\nfunction plot (ACF), which reveals auto-correlations with certain lags and thus the model pays special\\r\\nattention to these tokens. Furthermore, we average each sub-matrix into one scalar. The outcome\\r\\nmatrix can also illustrate Pearson correlations presented in the raw data.\\r\\n5 CONCLUSION AND FUTURE WORK\\r\\nGrounded in the principles of forecasting, we highlight the urgency to extend the context length in\\r\\nthe time series field. To facilitate long-context forecasters on diverse tasks, we propose multivariate\\r\\nnext token prediction, a novel paradigm to predict 1D and 2D time series with covariates. We present\\r\\nTimer-XL enhanced by TimeAttention as an extra-long version of generative time-series Transformers.\\r\\nIt simultaneously captures temporal dynamics and variable correlations by causal self-attention. In\\r\\naddition to achieving state-of-the-art performance on extensive datasets, we establish challenging\\r\\nbenchmarks for long-context forecasting. Further, by pre-training on large-scale heterogeneous time\\r\\nseries, Timer-XL demonstrates significant generalization capabilities as a one-for-all large model. In\\r\\nthe future, we will improve the context utilization and computational efficiency.\\r\\n10\\nREFERENCES\\r\\nAbdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen,\\r\\nOleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, et al.\\r\\nChronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024.\\r\\nYoshua Bengio, Rejean Ducharme, and Pascal Vincent. A neural probabilistic language model. ´\\r\\nAdvances in neural information processing systems, 13, 2000.\\r\\nGeorge Box. Box and jenkins: time series analysis, forecasting and control. In A Very British Affair:\\r\\nSix Britons and the Development of Time Series Analysis During the 20th Century, pp. 161–215.\\r\\nSpringer, 2013.\\r\\nDefu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong\\r\\nXu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time-series\\r\\nforecasting. Advances in neural information processing systems, 33:17766–17778, 2020.\\r\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi\\x02nov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\\r\\narXiv:1901.02860, 2019.\\r\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory- ´\\r\\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems,\\r\\n35:16344–16359, 2022.\\r\\nAbhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for\\r\\ntime-series forecasting. arXiv preprint arXiv:2310.10688, 2023.\\r\\nXiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, and Ying Shan.\\r\\nUnireplknet: A universal perception large-kernel convnet for audio video point cloud time-series\\r\\nand image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\r\\nPattern Recognition, pp. 5513–5524, 2024.\\r\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\r\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\\r\\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\r\\narXiv:2010.11929, 2020.\\r\\nMononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.\\r\\nMoment: A family of open time-series foundation models. arXiv preprint arXiv:2402.03885,\\r\\n2024.\\r\\nHans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andras Hor ´ anyi, Joaqu ´ ´ın Munoz-Sabater, ˜\\r\\nJulien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis.\\r\\nQuarterly Journal of the Royal Meteorological Society, 146(730):1999–2049, 2020.\\r\\nRJ Hyndman. Forecasting: principles and practice. OTexts, 2018.\\r\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\\r\\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\\r\\narXiv preprint arXiv:2001.08361, 2020.\\r\\nTaesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Re\\x02versible instance normalization for accurate time-series forecasting against distribution shift. In\\r\\nInternational Conference on Learning Representations, 2021.\\r\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\r\\narXiv:1412.6980, 2014.\\r\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\\r\\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings\\r\\nof the IEEE/CVF International Conference on Computer Vision, pp. 4015–4026, 2023.\\r\\n11\\nJesus Lago, Grzegorz Marcjasz, Bart De Schutter, and Rafał Weron. Forecasting day-ahead electricity\\r\\nprices: A review of state-of-the-art algorithms, best practices and an open-access benchmark.\\r\\nApplied Energy, 293:116983, 2021.\\r\\nGuokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term\\r\\ntemporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on\\r\\nresearch & development in information retrieval, pp. 95–104, 2018.\\r\\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng\\r\\nYan. Enhancing the locality and breaking the memory bottleneck of transformer on time series\\r\\nforecasting. Advances in neural information processing systems, 32, 2019.\\r\\nBryan Lim, Sercan O Arık, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for ¨\\r\\ninterpretable multi-horizon time series forecasting. International Journal of Forecasting, 37(4):\\r\\n1748–1764, 2021.\\r\\nJuncheng Liu, Chenghao Liu, Gerald Woo, Yiwei Wang, Bryan Hooi, Caiming Xiong, and Doyen\\r\\nSahoo. Unitst: Effectively modeling inter-series and intra-series dependencies for multivariate\\r\\ntime series forecasting. arXiv preprint arXiv:2406.04975, 2024a.\\r\\nMinhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet:\\r\\nTime series modeling and forecasting with sample convolution and interaction. Advances in Neural\\r\\nInformation Processing Systems, 35:5816–5828, 2022a.\\r\\nShizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dust\\x02dar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and\\r\\nforecasting. In International conference on learning representations, 2021.\\r\\nYong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring\\r\\nthe stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:\\r\\n9881–9893, 2022b.\\r\\nYong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.\\r\\nitransformer: Inverted transformers are effective for time series forecasting. arXiv preprint\\r\\narXiv:2310.06625, 2023.\\r\\nYong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Autotimes: Autore\\x02gressive time series forecasters via large language models. arXiv preprint arXiv:2402.02370,\\r\\n2024b.\\r\\nYong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long.\\r\\nTimer: Generative pre-trained transformers are large time series models. In Forty-first International\\r\\nConference on Machine Learning, 2024c.\\r\\nSpyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: 100,000\\r\\ntime series and 61 forecasting methods. International Journal of Forecasting, 36(1):54–74, 2020.\\r\\nYuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64\\r\\nwords: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.\\r\\nR OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023.\\r\\nBoris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis\\r\\nexpansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437,\\r\\n2019.\\r\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\\r\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\\r\\nhigh-performance deep learning library. Advances in neural information processing systems, 32,\\r\\n2019.\\r\\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases\\r\\nenables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\\r\\n12\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\\r\\nunderstanding by generative pre-training. OpenAI, 2018.\\r\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\r\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\\r\\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\\r\\nKashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos,\\r\\nRishika Bhagwatkar, Marin Bilos, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schnei- ˇ\\r\\nder, et al. Lag-llama: Towards foundation models for time series forecasting. arXiv preprint\\r\\narXiv:2310.08278, 2023.\\r\\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic\\r\\nforecasting with autoregressive recurrent networks. International journal of forecasting, 36(3):\\r\\n1181–1191, 2020.\\r\\nXiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, and Ming Jin. Time\\x02moe: Billion-scale time series foundation models with mixture of experts. arXiv preprint\\r\\narXiv:2409.16040, 2024.\\r\\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\\r\\ntransformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\\r\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee´\\r\\nLacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and `\\r\\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\r\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\r\\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\\r\\nsystems, 30, 2017.\\r\\nXindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan\\r\\nEshaghi. Beyond the limits: A survey of techniques to extend the context length in large language\\r\\nmodels. arXiv preprint arXiv:2402.02244, 2024a.\\r\\nYuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, Haoran Zhang, Jianmin Wang,\\r\\nand Mingsheng Long. Timexer: Empowering transformers for time series forecasting with\\r\\nexogenous variables. arXiv preprint arXiv:2402.19072, 2024b.\\r\\nGerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo.\\r\\nUnified training of universal time series forecasting transformers. arXiv preprint arXiv:2402.02592,\\r\\n2024.\\r\\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers\\r\\nwith auto-correlation for long-term series forecasting. Advances in Neural Information Processing\\r\\nSystems, 34:22419–22430, 2021.\\r\\nHaixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:\\r\\nTemporal 2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186,\\r\\n2022.\\r\\nHaixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting for\\r\\nworldwide stations with a unified deep model. Nature Machine Intelligence, 5(6):602–611, 2023.\\r\\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on\\r\\nmultimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\\r\\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and\\r\\nAlexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017.\\r\\nAiling Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\\r\\nforecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp.\\r\\n11121–11128, 2023.\\r\\n13\\nYunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency\\r\\nfor multivariate time series forecasting. In The Eleventh International Conference on Learning\\r\\nRepresentations, 2022.\\r\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\\r\\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv\\r\\npreprint arXiv:2303.18223, 2023.\\r\\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\r\\nInformer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings\\r\\nof the AAAI conference on artificial intelligence, volume 35, pp. 11106–11115, 2021.\\r\\nA PROOF OF MODEL EFFICIENCY\\r\\nA.1 SETUPS\\r\\nGiven an input univariate time series divided into T tokens according to the patch size P, which is\\r\\nfed into the vanilla Transformer. The training objective is to predict the next token of P time points.\\r\\nWe will generalize the derivation from 1D sequences to 2D time series based on different approaches\\r\\nto handle multivariate data with the variable number N. We adopt the same denotations as before:\\r\\nTransformer consists of L blocks with model dimension D. The multi-head attention mechanism\\r\\nhas H heads, each with a dimension of dk for query, key, and value, and dk =\\r\\nD\\r\\nH\\r\\n. The intermediate\\r\\ndimension of feed-forward network is set as Dff = αD. The results are summarized in Table 7, we\\r\\nprovide the detailed proof in the following sections.\\r\\nTable 7: Parameters count and computational complexity of Transformers for multivariate time series.\\r\\nMetric Type Count / Complexity\\r\\nFLOPs Channel Independence (Nie et al., 2022) O(P DNT + L(D + H)NT 2 + (2 + α)LD2NT)\\r\\n(Training Speed) Channel Dependence (Ours) O(P DNT + L(D + H)N\\r\\n2T2 + (2 + α)LD2NT)\\r\\nParameters Flatten head (Nie et al., 2022) (4 + 2α)LD2 + 4LD + (1 + T)P D\\r\\nToken-wise projector (Liu et al., 2024c) (4 + 2α)LD2 + 4LD + 2P D\\r\\nMemory Self-Attention (Vaswani et al., 2017) 4(D + P)NT + (32 + 8α)LDNT + 4LHN2T\\r\\n2\\r\\nFootprint FlashAttention (Dao et al., 2022) 4(D + P)NT + (32 + 8α)LDNT\\r\\n∗ L is the block number of Transformers. D is the dimension of embeddings (the hidden dimension of FFN Dff is set\\r\\nas αD). H is the head number and the dimension of query, key, and value dk = D/H. The overhead is to train on a\\r\\nmultivariate time series (N-variables and T P time points) and predict the next patch with the length P.\\r\\nA.2 FLOPS\\r\\nAs a preliminary, the multiplication between matrix A ∈ R\\r\\nn×m and matrix C ∈ Rm×p\\r\\nrequires\\r\\nmnp multiplications and mnp additions, resulting in 2mnp floating-point operations. Given batched\\r\\nmatrices A ∈ R\\r\\nB×n×m and C ∈ RB×m×p\\r\\n, B times matrix multiplications will be performed. It is\\r\\nevident that the batch size is a linear multiplier. Thus, we first omit B to calculate the operations of\\r\\ndealing with one univariate series, and then we will reintroduce it to analyze channel independence.\\r\\nThe computational cost of Transformers can be primarily categorized into two types: (1) multi-head\\r\\nattention calculation and (2) linear transformations. In contrast, the operations of layer normalization,\\r\\nresidual connection, activation functions, and position embedding with the complexity of O(T D) are\\r\\nless significant. Therefore, we derive the computational complexity mainly with respect to the above\\r\\ntwo types by delving into the forwarding process of one univariate series.\\r\\nPatch Embedding The tokenized time series {xi} ∈ R\\r\\nT ×P is mapped into the embedding space\\r\\nthrough the patch-wise embedding We ∈ R\\r\\nD×P , resulting in 2P DT operations.\\r\\nSelf-Attention The calculation of self-attention begins with the computation of query, key and\\r\\nvalue by multiplying the patch embeddings with matrices Wq,Wk,Wv ∈ R\\r\\nD×dk respectively in H\\r\\nheads, which incurs a computational cost of 6HDdkT = 6D2T and yields Q, K, V ∈ R\\r\\nH×T ×dk .\\r\\n14\\nNext, the dot product QK⊤ ∈ R\\r\\nH×T ×T\\r\\nis conducted in each head, leading to 2HdkT\\r\\n2 = 2DT2\\r\\noperations. Following this, the Pre-Softmax map is divided by √dk and processed through Softmax,\\r\\nwhich includes exponentiation, summation, and normalization of each element, resulting in 4HT2\\r\\noperations. The subsequent multiplication with V incurs 2HdkT\\r\\n2 = 2DT2 operations. Finally,\\r\\nmultiple heads are concatenated and multiplied by Wo ∈ R\\r\\nD×D, contributing 2D2T operations.\\r\\nFeed-Forward Network It first projects the token representations into the dimension of Df f and\\r\\nsubsequently projects it back to the dimension D, resulting in a total operations of 4αD2T.\\r\\nPatch Projection For encoder-only models, all token representations are flattened and mapped\\r\\ndirectly to P time points by Wd ∈ R\\r\\nTD×P . In contrast, token-wise projector Wd ∈ RD×P in\\r\\ndecoder-only models independently map each token to the predicted next token. In both cases, the\\r\\nnumber of operations is 2P DT, but the token-wise projector will result in a smaller parameter count.\\r\\nThe forwarding operations in L-layers Transformer is 4P DT + 4L(D + H)T\\r\\n2 + (8 + 4α)LD2T in\\r\\nsum. Considering that the majority of operations in Transformers are binary operations (e.g., matrix\\r\\nmultiplications), the gradients for both matrices are computed separately. As a result, the number of\\r\\noperations in backpropagation is the twice of forwarding. Therefore, the total operations of training a\\r\\nTransformer on a univariate series consisting of T patches, each of length P, is derived as:\\r\\nf(T) = 12P DT + 12L(D + H)T\\r\\n2 + (24 + 12α)LD2T.\\r\\nWe plug typical hyperparameters in the current time-series Transformers and forecasting benchmarks:\\r\\nD = 512, H = 8, L = 4, α = 4, T = 7, and P = 96, we obtain that:\\r\\nf(T) = 24960T\\r\\n2 + 76087296T ∝ 3.28 ∗ 10−4T2 + T.\\r\\nDue to the prevalence of short contexts in the time series field, where T ≪ D leads to a significant\\r\\ncoefficient in O(T), we find the primary computational burden of time-series Transformer lies in\\r\\nlinear transformations with O(T), rather than in multi-head self-attention with the O(T\\r\\n2\\r\\n) complexity.\\r\\nFor multivariate series with N variables, FLOPs is influenced by the handling of multivariate data.\\r\\nWhen adopting channel independence (Timer and PatchTST), N can be regarded as the batch size B:\\r\\nNf(T) = 12P DNT + 12L(D + H)NT2 + (24 + 12α)LD2NT. (9)\\r\\nFor models that capture fine-grained intra- and inter-series dependencies (Timer-XL and UniTST) in\\r\\nmultivariate series, N is reflected as the enlarged number of tokens:\\r\\nf(NT) = 12P DNT + 12L(D + H)N\\r\\n2T2 + (24 + 12α)LD2NT. (10)\\r\\nNotably, FLOPs is not entirely equivalent to actual runtime. While FlashAttention increases the\\r\\noverall FLOPs due to its recomputation process, it reduces the number of memory reads and writes.\\r\\nGiven that on GPUs, computation is significantly faster than memory access, using FlashAttention\\r\\ncan actually lead to further improvements in runtime performance.\\r\\nA.3 PARAMETER COUNT\\r\\nFrom the above analysis, we observe that the parameter count of Transformers includes the following:\\r\\nPatch Embedding We ∈ R\\r\\nD×P to obtain patch embeddings.\\r\\nSelf-Attention Wq,Wk,Wv ∈ R\\r\\nD×dk of H heads and Wo ∈ RD×D for all heads.\\r\\nFeed-Forward Network Wffn1,Wffn2 ∈ R\\r\\nD×Dff in feed-forward network.\\r\\nLayer Normalization It contains the weight W ∈ R\\r\\nD and the bias b ∈ RD. Every Transformer\\r\\nblock includes two normalizations after multi-head attention and feed-forward network respectively.\\r\\nPatch Projection Wd ∈ R\\r\\nTD×P in flatten head and Wd ∈ RD×P in token-wise projection.\\r\\nIn sum, the total count of parameters in time-series Transformers can be expressed as:\\r\\nParameter Count =\\r\\n\\x1a\\r\\n(4 + 2α)LD2 + 4LD + (1 + T)P D, using flatten head,\\r\\n(4 + 2α)LD2 + 4LD + 2P D, using token-wise projection. (11)\\r\\n15\\nA.4 MEMORY FOOTPRINT\\r\\nThe memory footprint during training can be primarily categorized into three parts: activation values\\r\\nstored for backpropagation, model parameters, and optimizer parameters.\\r\\nRegardless of other precision types (e.g., FP16), model parameters and gradients are typically stored\\r\\nas 32-bit floating-point numbers, with each parameter occupying 4 bytes of memory. For time-series\\r\\nTransformers, memory footprint of activation values is given as follows:\\r\\nPatch Embedding Gradient computation for We preserves its input {xi} ∈ R\\r\\nT ×P of 4P T bytes.\\r\\nSelf-Attention Gradient calculation for Wq,Wk,Wv ∈ R\\r\\nD×dk requires their inputs H ∈ RT ×D,\\r\\namounting to a total of 4DT bytes. The dot product for attention map also needs to store Q, K, V ∈\\r\\nR\\r\\nH×T ×dk , which collectively require a total of 12DT bytes of memory. Gradient computation of\\r\\nWo ∈ R\\r\\nD×D necessitates the concatenated multi-head attention representations H ∈ RT ×D, which\\r\\noccupies 4DT bytes. If memory-efficient attention mechanisms like FlashAttention (Dao et al., 2022)\\r\\nis not applied, the outcome QK⊤ will be stored and occupy 4HT2 bytes. Instead, if FlashAttention\\r\\nis adopted, the storage overhead can be avoided.\\r\\nFeed-Forward Network ReLU activation function is typically employed in this module. The input\\r\\nH ∈ R\\r\\nT ×D must be retained, requiring a total of 4DT bytes. Additionally, the product Wffn1H also\\r\\nneeds to be stored, amounting to 4DffT bytes. Similarly, the output activations of ReLU, which serve\\r\\nas the input for subsequent linear transformations, necessitate another 4DffT bytes.\\r\\nLayer Normalization Each block of Transformer encompasses two layer normalizations, with\\r\\neach normalization retaining its input, resulting in the memory requirement of 8DT bytes.\\r\\nPatch Projection To perform backpropagation for Wd ∈ R\\r\\nD×P , it is necessary to retain its input\\r\\nH ∈ R\\r\\nT ×D, resulting in a total memory requirement of 4DT bytes.\\r\\nThe formula for the total activation values of the entire model occupying GPU memory is as follows:\\r\\nMemory Footprint =\\r\\n\\x1a\\r\\n4(D + P)T + (32 + 8α)LDT + 4LHT2, w/o FlashAttention,\\r\\n4(D + P)T + (32 + 8α)LDT, with FlashAttention. (12)\\r\\nThe derived occupancy of activation values increases proportionally with the batch size B. For\\r\\nmultivariate series, N can be used as a multiplier in channel independence. For channel independence\\r\\nmodels, we can substitute T with NT as before. The total memory footprint is the sum of activation\\r\\nvalues and parameters of model and optimizer, which are proportional to the parameter count derived\\r\\nin Equation 11. Due to the limited model size in the time series field, the memory consumption of\\r\\nparameters is minimal and can be considered negligible in practice. Therefore, the overall memory\\r\\nfootprint can be predominantly determined by the occupied memory of activation values.\\r\\nB EXPERIMENTAL DETAILS\\r\\nB.1 DATASETS\\r\\nWe conduct experiments on well-acknowledged benchmarks to evaluate performance of the proposed\\r\\nTimer-XL, which includes (1) ETT (Zhou et al., 2021) contains 7 factors of electricity transformers\\r\\nfrom July 2016 to July 2018, which is recorded every hour or 15 minutes. (2) Weather (Wu et al., 2021)\\r\\nincludes 21 meteorological factors collected every 10 minutes from the Max Planck Biogeochemistry\\r\\nInstitute Weather Station in 2020. (3) ECL (Wu et al., 2021) records the hourly electricity consumption\\r\\ndata of 321 clients. (4) Traffic (Wu et al., 2021) collects hourly road occupancy rates measured by\\r\\n862 sensors on the San Francisco Bay area highways from January 2015 to December 2016. (5)\\r\\nSolar-Energy (Lai et al., 2018) records the solar power production of 137 PV plants in 2006, which\\r\\nare sampled every 10 minutes. (7) PEMS (Liu et al., 2022a) contains records from the public traffic\\r\\nnetwork in California collected in 5-minute time windows. (8) EPF (Lago et al., 2021) includes\\r\\nfive subsets that span six years. Each contains the electricity price as the endogenous variable to be\\r\\npredicted and two exogenous variables of the day-ahead electricity markets. (9) GTWSF (Wu et al.,\\r\\n16\\n2023) is a dataset collected from the National Centers for Environmental Information (NCEI). This\\r\\nlarge-scale collection contains hourly averaged wind speed and temperature data from 3850 stations\\r\\nwith different geographical scales and densities each, spanning from 2019 to 2021. (10) UTSD (Liu\\r\\net al., 2024c) is a multi-domain time series dataset, which includes seven domains with a hierarchy of\\r\\nfour volumes. We adopt the largest volume that encompasses 1 billion time points for pre-training.\\r\\nWe further establish challenging forecasting benchmarks based on the ECMWF Reanalysis v5 (ERA5)\\r\\ndataset (Hersbach et al., 2020) to prevent potential overfitting and performance saturation of deep\\r\\nforecasters in existing benchmarks. Concretely, ERA5 is the fifth generation ECMWF atmospheric\\r\\nreanalysis of the global climate covering the period from January 1940 to the present, which provides\\r\\nhourly estimates of a large number of atmospheric, land, and oceanic climate variables, and includes\\r\\ninformation about uncertainties for all variables at reduced spatial and temporal resolutions. Due to\\r\\nits pattern sufficiency of temporal dynamics and variable correlations, we could establish practical\\r\\nbenchmarks to thoroughly evaluate the performance for univariate and multivariate forecasting, as\\r\\nwell as adopt it for large-scale pre-training to develop domain-specific large time series models.\\r\\nOur datasets are constructed as follows:\\r\\n• ERA5-S: To establish a realistic univariate forecasting benchmark, we start from the basic\\r\\nprinciple of forecastability and make the prediction on sufficient lookback lengths. Instead\\r\\nof the short time span of training in previous benchmarks (generally no more than 2 years),\\r\\nwe curated a three-hour frequency dataset spanning 40 years (January 1979 to December\\r\\n2018) from ERA5, encompassing 116880 time points. In order to prevent overfitting on a\\r\\nsingle time series, we selected worldwide stations to form seven subsets.\\r\\n• ERA5-MS: Each univariate series of ERA5-S provides partial observations governed by\\r\\nthe spatio–temporal global weather system. Since discovering the global spatio-temporal\\r\\ncorrelations presents a fundamental challenge in meteorology, we convert ERA5-S into\\r\\nERA5-MS by using seven subsets as a challenging multivariate forecasting benchmark.\\r\\nBased on the average results in Tables 1 and 4, we can validate the existence of multi-station\\r\\ncorrelations among selected stations, which have enhanced the average prediction accuracy.\\r\\n• ERA5-Large: To explore the pure data-driven approach to build domain-specific large time\\r\\nseries models, we further expanded the number of stations as ERA5-Large, a dataset that\\r\\nevenly covers meteorological 4920 worldwide stations and spans 40 years. We establish the\\r\\ndataset for pre-training, which is expected to generalize across the time (train on the past\\r\\nobservations and generalize to the future) and across stations (train on partial stations and\\r\\ngeneralize to other unseen stations). The total number of time points is around half a billion.\\r\\nWe follow the same data processing and train-validation-test split protocol used in TimesNet (Wu\\r\\net al., 2022), where the train, validation, and test datasets are divided according to chronological order\\r\\nto prevent data leakage. Detailed dataset descriptions and prediction settings are provided in Table 8.\\r\\nB.2 BASELINE MODELS\\r\\nWe aim to present Timer-XL as a foundation model for unified time series forecasting. We thoroughly\\r\\ninclude well-acknowledged and advanced models in each forecasting task. For univariate time series\\r\\nforecasting, we compare Timer-XL with PatchTST (Nie et al., 2022) under channel independence. For\\r\\nmultivariate time series prediction, we report official results from Liu et al. (2023; 2024b); Ding et al.\\r\\n(2024), including UniRepLKNet (2024), iTransformer (2023), Corrformer (2023), DLinear (2023),\\r\\nTimesNet (2022), Non-stationary Transformer (2022b), Pyraformer (2021), Autoformer (2021) ,\\r\\nStemGNN (2020), DeepAR (2020), and N-BEATS (2019). We further reproduce the performance\\r\\nof related Transformers: Timer (2024c) and UniTST (2024a) based on their official repositories.\\r\\nFor covariate-informed time series forecasting, we report the official results of TimeXer (2024b).\\r\\nFor zero-shot forecasting, we follow Liu et al. (2024c) that predicts future length-96 windows in\\r\\nwell-acknowledged datasets. Totally, more than 20 baselines are included for a complete comparison.\\r\\nB.3 IMPLEMENTATION DETAILS\\r\\nAll the experiments are implemented by PyTorch (Paszke et al., 2019) on NVIDIA A100 Tensor Core\\r\\nGPUs. We employ the Adam optimizer (Kingma & Ba, 2014) and MSE loss for model optimization.\\r\\n17\\nTable 8: Dataset descriptions. Dim. denotes the number of variables (For univariate forecasting, we\\r\\nadopt channel independence (Nie et al., 2022) or train separate models on each variable). Dataset\\r\\nLength denotes the number of time points in the (train, validation, test) splits.\\r\\nTasks Dataset Dim. Prediction Setting Dataset Length Information (Frequency)\\r\\nETTh1 7 {24, 96, 168, 672, 2880}→96 (8545, 2881, 2881) Electricity (Hourly)\\r\\nUnivariate ECL 321 {24, 96, 168, 672, 2880, 8832}→96 (18317, 2633, 5261) Electricity (Hourly)\\r\\nForecasting Traffic 862 {24, 96, 168, 672, 2880, 8832}→96 (12185, 1757, 3509) Transportation (Hourly)\\r\\nPEMS03 358 {96, 288, 1152, 2016, 8064}→96 (15617, 5135, 5135) Transportation (5 mins)\\r\\nERA5-S 7 3072→96 (81816, 11688, 23376) Climate (3 Hours)\\r\\nETTh1, ETTh2 7 {96, 672}→{96, 192, 336, 720} (8545, 2881, 2881) Electricity (Hourly)\\r\\nETTm1, ETTm2 7 {96, 672}→{96, 192, 336, 720} (34465, 11521, 11521) Electricity (15 mins)\\r\\nECL 321 {96, 672}→{96, 192, 336, 720} (18317, 2633, 5261) Electricity (Hourly)\\r\\nMultivariate Traffic 862 {96, 672}→{96, 192, 336, 720} (12185, 1757, 3509) Transportation (Hourly)\\r\\nForecasting Weather 21 {96, 672}→{96, 192, 336, 720} (36792, 5271, 10540) Climate (10 mins)\\r\\nSolar-Energy 137 {96, 672}→{96, 192, 336, 720} (36601, 5161, 10417) Energy (10 mins)\\r\\nERA5-MS 7 3072→96 (81816, 11688, 23376) Climate (3 Hours)\\r\\nGTWSF 3850 48→24 (12280, 1755, 3509) Wu et al. (2023)\\r\\nNP 1+2 168→24 (36500, 5219, 10460) Electricity (Hourly)\\r\\nForecasting PJM 1+2 168→24 (36500, 5219, 10460) Electricity (Hourly)\\r\\nwith Covariates BE 1+2 168→24 (36500, 5219, 10460) Electricity (Hourly)\\r\\nFR 1+2 168→24 (36500, 5219, 10460) Electricity (Hourly)\\r\\nDE 1+2 168→24 (36500, 5219, 10460) Electricity (Hourly)\\r\\nPre-training ERA5-Large 4920 3072→96 (81816, 11688, 23376) Climate (3 Hours)\\r\\nUTSD - 2880→96 (868778970, 96530996, -) Liu et al. (2024c)\\r\\nTable 9: Performance robustness of Timer-XL. The prediction settings and results keep the same with\\r\\nTable 11. The standard deviation is obtained from three random seeds.\\r\\nDataset ECL ETTh1 Traffic\\r\\nHorizon MSE MAE MSE MAE MSE MAE\\r\\n96 0.127±0.001 0.219±0.001 0.364±0.002 0.397±0.001 0.340±0.002 0.238±0.001\\r\\n192 0.145±0.001 0.236±0.001 0.405±0.002 0.424±0.001 0.360±0.001 0.247±0.001\\r\\n336 0.159±0.001 0.252±0.001 0.427±0.003 0.439±0.002 0.377±0.002 0.256±0.002\\r\\n720 0.187±0.003 0.277±0.003 0.439±0.002 0.459±0.004 0.418±0.003 0.279±0.002\\r\\nDataset Solar-Energy Weather ERA5-MS\\r\\nHorizon MSE MAE MSE MAE MSE MAE\\r\\n96 0.162±0.003 0.221±0.002 0.157±0.002 0.205±0.001 0.164±0.001 0.307±0.000\\r\\n192 0.187±0.003 0.239±0.002 0.206±0.003 0.250±0.002\\r\\n336 0.205±0.003 0.255±0.002 0.259±0.003 0.291±0.003\\r\\n720 0.238±0.003 0.279±0.003 0.337±0.002 0.344±0.002\\r\\n18\\nWe adopt channel independence from Nie et al. (2022) in univariate time series forecasting. Based on\\r\\nthe prevalence of patch-level tokenization in the time series field, we reproduce typical Transformers:\\r\\nPatchTST (2022), Timer (2024c), and UniTST (2024a) based on their official repositories, and keep\\r\\ntheir model hyperparameters and training configurations the same to evaluate the inherent capability\\r\\nof base models. The results of other baselines are based on the benchmark provided by Liu et al.\\r\\n(2023; 2024b); Ding et al. (2024); Wang et al. (2024b), which is fairly built on the configurations\\r\\nprovided by their original paper. Detailed experimental configurations are provided in Table 10. We\\r\\nalso report the standard deviations under three runs with different random seeds in Table 9, which\\r\\nexhibits that the performance of Timer-XL is stable.\\r\\nFor the metrics, we adopt the symmetric mean absolute percentage error (SMAPE), a metric that is\\r\\nindependent of the numerical range, to evaluate one-for-all generalization performance on ERA5-\\r\\nLarge. For other experiments, we adopt the root mean square error (MSE) and mean absolute error\\r\\n(MAE) that follows previous work. These metrics can be calculated as follows:\\r\\nSMAPE =\\r\\n200\\r\\nT\\r\\nX\\r\\nT\\r\\ni=1\\r\\n|Xi − Xb i|\\r\\n|Xi| + |Xb i|\\r\\n, MSE =\\r\\nX\\r\\nT\\r\\ni=1\\r\\n|Xi − Xb i|\\r\\n2\\r\\n, MAE =\\r\\nX\\r\\nT\\r\\ni=1\\r\\n|Xi − Xb i|.\\r\\nHere X ∈ R\\r\\nT\\r\\nis a univariate time series and Xb is the corresponding prediction. For multivariate time\\r\\nseries, we further calculate the mean metric in the variable dimension.\\r\\nTable 10: Experimental configurations of Timer-XL and other baseline Transformers. All the experi\\x02ments adopt the ADAM (2014) optimizer with the default hyperparameter (β1, β2) = (0.9, 0.999).\\r\\nExperiment Model Dataset Configuration Training Process\\r\\nL D dk H P LR Loss Batch Size Epochs\\r\\nUnivariate Forecasting\\r\\nECL 3 512 64 8 96 0.0005 MSE 2048 10\\r\\nTimer-XL Traffic 3 512 64 8 96 0.001 MSE 2048 10\\r\\nPatchTST ETTh1 1 512 64 8 96 0.0005 MSE 256 10\\r\\nPEMS03 3 512 64 8 96 0.0005 MSE 2048 10\\r\\nERA5-S 1 512 64 8 96 0.0005 MSE 2048 10\\r\\nMultivariate Forecasting\\r\\nGlobal Temp. 3 1024 128 8 24 0.0001 MSE 8 10\\r\\nGlobal Wind 3 1024 128 8 24 0.0001 MSE 8 10\\r\\nTimer-XL ECL 5 512 64 8 96 0.0005 MSE 4 10\\r\\nUniTST Traffic 4 512 64 8 96 0.0005 MSE 4 10\\r\\nTimer ETTh1 1 1024 128 8 96 0.0001 MSE 32 10\\r\\nPatchTST Weather 4 512 64 8 96 0.0005 MSE 32 10\\r\\nSolar. 6 512 64 8 96 0.0001 MSE 16 10\\r\\nERA5-MS 3 512 64 8 96 0.0001 MSE 256 10\\r\\nForecasting with Covariates\\r\\nTimer-XL NP 3 512 64 8 24 0.0001 MSE 4 10\\r\\nTimeXer PJM 2 512 64 8 24 0.0001 MSE 16 10\\r\\nTimer BE 2 512 64 8 24 0.0001 MSE 16 10\\r\\nPatchTST FR 2 512 64 8 24 0.0001 MSE 16 10\\r\\nDE 2 512 64 8 24 0.0001 MSE 16 10\\r\\nPre-training\\r\\nTimer-XL\\r\\nERA5-Large\\r\\n4 512 64 8 96 0.0001 MSE 40960 10\\r\\nPatchTST 4 512 64 8 96 0.0001 MSE 40960 10\\r\\nTimer-XL UTSD 8 1024 128 8 96 0.00005 MSE 16384 10\\r\\nTimer (Liu et al., 2024c) 8 1024 128 8 96 0.00005 MSE 16384 10\\r\\nTimer-XL 8 1024 128 8 96 0.001 MSE 32768 -\\r\\nMoiraiSmall LOTSA 6 384 64 6 -\\r\\nMoiraiBase (Woo et al., 2024) 12 768 64 12 - Woo et al. (2024)\\r\\nMoiraiLarge 24 1024 64 16 -\\r\\n∗ L is the layer number of Transformers, D is the dimension of token embedding (the hidden dimension of FFN is set as 4D), dk is\\r\\nthe dimension of query, key, and value, H is the multi-head number, P is the patch size, and LR is the initial learning rate.\\r\\n19\\nC HYPERPARAMETER SENSITIVITY\\r\\nWe evaluate the hyperparameter sensitivity of Timer-XL on the ERA5-MS benchmark, as illustrated in\\r\\nFigure 8, concerning the following factors: the number of layers L, the patch size P, and the lookback\\r\\nlength during inference. Our findings indicate that performance of Timer-XL generally improves with\\r\\nincreases with L, suggesting that Timer-XL is a scalable deep forecaster. Furthermore, our analysis of\\r\\nthe influence of P reveals that the optimal patch size is generally close to the predicted length, since\\r\\nit avoid multi-step error accumulations. Toward better long-term forecasting performance, it leaves a\\r\\nfuture improvement to adopt different patch sizes of input and output tokens. Finally, we investigate\\r\\nthe impact of input length during inference. We discover that the optimal lookback length of during\\r\\nis not necessarily the length during training. Given that generative Transformers can accommodate\\r\\ninference inputs shorter than those used during training, this finding is noteworthy and indicates the\\r\\npotential to improve the performance of generative Transformers.\\r\\n1 2 3\\r\\n0.166\\r\\n0.167\\r\\n0.168\\r\\n0.169\\r\\n0.170\\r\\n0.171\\r\\n0.172\\r\\nMSE\\r\\nLayer Number\\r\\n24 48 96\\r\\n0.166\\r\\n0.167\\r\\n0.168\\r\\n0.169\\r\\n0.170\\r\\n0.171\\r\\nMSE\\r\\nPatch Size\\r\\n96 384 768 1152 1536 1920 2304 2688 3072\\r\\n0.1675\\r\\n0.1700\\r\\n0.1725\\r\\n0.1750\\r\\n0.1775\\r\\nMSE\\r\\nLookback Length\\r\\nFigure 8: Hyperparameter sensitivity of Timer-XL (input-3072-pred-96 on ERA5-MS), including the\\r\\nnumber of Transformer blocks L, the patch size P, and the input lookback length during inference.\\r\\nD SHOWCASES\\r\\nTo facilitate a clear comparison among various models, we present additional prediction visualization\\r\\nfrom diverse datasets in Figure 9 and 10. Showcases are randomly selected from Timer-XL and the\\r\\nfollowing time-series Transformers: PatchTST (2022), Timer (2024c), and UniTST (2024a). Among\\r\\nthem, Timer-XL presents the most accurate predictions.\\r\\nECL ETTh1 PEMS Traffic\\r\\nPatchTST Timer-XL\\r\\nFigure 9: Visualization results on univariate time series dataset. We adopt the forecasting setting of\\r\\n2880-pred-96 on ECL, ETTh1 and Traffic, and 2016-pred-96 on PEMS.\\r\\nE SUPPLEMENTARY RESULTS\\r\\nE.1 FULL RESULT OF MULTIVARIATE FORECASTING\\r\\nTable 11 provides the complete results of the one-for-all multivariate forecasting benchmark across\\r\\nwell-acknowledged datasets. We evaluate Timer-XL and baseline models by rolling forecasting: each\\r\\n20\\nPatchTST UniTST Timer-XL\\r\\nETTh1 Traffic\\r\\nTimer\\r\\nFigure 10: Visualization results on multivariate time series dataset. We adopt the forecasting setting\\r\\nof 672-pred-96 on ETTh1 (7 Variables) and Traffic (862 Variables).\\r\\nmodel is trained with input length 672 and output length 96, and the predicted values are integrated as\\r\\npart of the input in the next iteration until reaching the desired forecast length in {96, 192, 336, 720}.\\r\\nWe highlight that this benchmark evaluates the fundamental model versatility of deep forecasters,\\r\\nwhich aims to break the awkward situation of extensive training and model storage in pursuit of\\r\\nbetter practice for real-world forecasting requirements. On this benchmark, time-series Transformers\\r\\nsignificantly stand out from other baseline models, and our proposed Timer-XL can achieve state-of\\x02the-art performance, making it a nice fundamental backbone of a one-for-all forecaster.\\r\\nE.2 FULL RESULT OF ZERO-SHOT FORECASTING\\r\\nTable 12 provides the full results of zero-shot forecasting on the benchmark from Wu et al. (2022).\\r\\nWe build Timer-XL based on the configuration in Table 10, which is pre-trained on the aggregated\\r\\ndatasets of UTSD (Liu et al., 2024c) and LOTSA (Woo et al., 2024). The patch size of Timer-XL is\\r\\nset as 96 and we conduct rolling forecast to obtain the desired forecast length in {96, 192, 336, 720}.\\r\\nWe evaluate most advanced large models based on their official model checkpoints, including Time\\x02MoE (Shi et al., 2024), Moirai (Woo et al., 2024), TimesFM (Das et al., 2023), MOMENT Goswami\\r\\net al. (2024), and Chronos (Ansari et al., 2024). We conduct zero-shot evaluations on datasets that\\r\\nare not included during the pre-training of corresponding models. For each of the evaluated model,\\r\\nwe use their maximum input length during inference. The metric (MSE/MAE) is averaged from all\\r\\npredicted windows in the test split.\\r\\nE.3 ABLATION STUDY OF TIMEATTENTION\\r\\nWe conduct evaluations on TimeAttention to validate the effectiveness of position embeddings. As\\r\\nfor variable embedding, the distinction between endogenous and exogenous variables can improve\\r\\nperformance. Based on our observation of the learned u > v, we find that the token reasonably pays\\r\\nmore attention to tokens of the endogenous variable. It leaves a prior to mask out minor dependencies\\r\\nthat focuses less on exogenous variables. For the temporal dimension, other position embeddings are\\r\\ninferior to RoPE, since it uses the affine transformation, while others are additive, and thereby less\\r\\nconfused with the same additive embedding for variables.\\r\\nE.4 SUPPLEMENTARY RESULTS OF LONG-CONTEXT FORECASTING\\r\\nLong context is a basic indicator of foundation models, which can support emergence capabilities such\\r\\nas prompting, in-context learning, retrieval-augmented generation, etc. However, the long-context\\r\\nforecasting paradigm receives less attention in the current community, which can be due to the lack\\r\\nof benchmarks. In the meteorological ERA5, it is necessary to support the context of more than\\r\\nyears to contain a specific cycle (such as El Nino). In Table 14, the performance of Timer-XL and\\r\\nDLinear generally improves with the increased context length. By contrast, it reveals the performance\\r\\n21\\nTable 11: Full multivariate forecasting results: we conduct rolling forecast with a single model trained\\r\\non each dataset (lookback length is 672) and accomplish four forecast lengths in {96, 192, 336, 720}.\\r\\nModels Timer-XL Timer UniTST iTransformer DLinear PatchTST TimesNet Stationary Autoformer\\r\\n(Ours) (2024c) (2024a) (2023) (2023) (2022) (2022) (2022b) (2021)\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1\\r\\n96 0.364 0.397 0.371 0.404 0.379 0.415 0.387 0.418 0.369 0.400 0.373 0.403 0.452 0.463 0.452 0.478 0.467 0.499\\r\\n192 0.405 0.424 0.407 0.429 0.415 0.438 0.416 0.437 0.405 0.422 0.405 0.425 0.474 0.477 0.484 0.510 0.492 0.523\\r\\n336 0.427 0.439 0.434 0.445 0.440 0.454 0.434 0.450 0.435 0.445 0.423 0.440 0.493 0.489 0.511 0.522 0.519 0.531\\r\\n720 0.439 0.459 0.461 0.466 0.482 0.482 0.447 0.473 0.493 0.508 0.445 0.471 0.560 0.534 0.571 0.543 0.589 0.560\\r\\nAvg 0.409 0.430 0.418 0.436 0.429 0.447 0.421 0.445 0.426 0.444 0.412 0.435 0.495 0.491 0.505 0.513 0.517 0.528\\r\\nETTh2\\r\\n96 0.277 0.343 0.285 0.344 0.343 0.398 0.304 0.362 0.305 0.371 0.289 0.347 0.340 0.374 0.348 0.403 0.358 0.397\\r\\n192 0.348 0.391 0.365 0.400 0.376 0.420 0.372 0.407 0.412 0.439 0.360 0.393 0.402 0.414 0.408 0.448 0.435 0.451\\r\\n336 0.375 0.418 0.412 0.440 0.399 0.435 0.418 0.440 0.527 0.508 0.389 0.420 0.452 0.452 0.424 0.457 0.454 0.475\\r\\n720 0.409 0.458 0.468 0.487 0.419 0.457 0.463 0.476 0.830 0.653 0.398 0.440 0.462 0.468 0.448 0.476 0.479 0.492\\r\\nAvg 0.352 0.402 0.382 0.418 0.384 0.428 0.389 0.421 0.518 0.493 0.359 0.400 0.414 0.427 0.407 0.446 0.431 0.454\\r\\nETTm1\\r\\n96 0.290 0.341 0.281 0.338 0.289 0.348 0.311 0.365 0.307 0.350 0.285 0.346 0.338 0.375 0.414 0.414 0.466 0.466\\r\\n192 0.337 0.369 0.330 0.368 0.332 0.375 0.353 0.390 0.337 0.368 0.329 0.372 0.371 0.387 0.524 0.482 0.504 0.496\\r\\n336 0.374 0.392 0.367 0.393 0.365 0.397 0.387 0.411 0.366 0.387 0.363 0.394 0.410 0.411 0.541 0.497 0.574 0.530\\r\\n720 0.437 0.428 0.432 0.433 0.421 0.431 0.452 0.445 0.419 0.419 0.421 0.426 0.478 0.450 0.578 0.509 0.596 0.558\\r\\nAvg 0.359 0.382 0.352 0.383 0.352 0.388 0.376 0.403 0.357 0.381 0.349 0.385 0.399 0.406 0.514 0.475 0.535 0.512\\r\\nETTm2\\r\\n96 0.175 0.257 0.175 0.257 0.171 0.260 0.183 0.272 0.167 0.263 0.172 0.259 0.187 0.267 0.237 0.306 0.255 0.339\\r\\n192 0.242 0.301 0.239 0.301 0.228 0.230 0.250 0.315 0.230 0.311 0.233 0.299 0.249 0.309 0.330 0.387 0.279 0.335\\r\\n336 0.293 0.337 0.293 0.342 0.282 0.336 0.311 0.356 0.298 0.361 0.280 0.331 0.321 0.351 0.404 0.424 0.331 0.374\\r\\n720 0.376 0.390 0.392 0.407 0.380 0.398 0.417 0.419 0.432 0.446 0.357 0.382 0.497 0.403 0.525 0.486 0.413 0.450\\r\\nAvg 0.271 0.322 0.275 0.327 0.265 0.306 0.290 0.340 0.282 0.345 0.261 0.318 0.314 0.333 0.374 0.401 0.320 0.374\\r\\nECL\\r\\n96 0.127 0.219 0.129 0.221 0.130 0.225 0.133 0.229 0.138 0.238 0.132 0.232 0.184 0.288 0.185 0.287 0.256 0.357\\r\\n192 0.145 0.236 0.148 0.239 0.150 0.244 0.158 0.258 0.152 0.251 0.151 0.250 0.192 0.295 0.282 0.368 0.291 0.376\\r\\n336 0.159 0.252 0.164 0.256 0.166 0.262 0.168 0.262 0.167 0.268 0.171 0.272 0.200 0.303 0.289 0.377 0.290 0.379\\r\\n720 0.187 0.277 0.201 0.289 0.206 0.297 0.205 0.294 0.203 0.302 0.222 0.318 0.228 0.325 0.305 0.399 0.320 0.403\\r\\nAvg 0.155 0.246 0.161 0.251 0.163 0.257 0.164 0.258 0.165 0.265 0.169 0.268 0.201 0.303 0.265 0.358 0.289 0.379\\r\\nTraffic\\r\\n96 0.340 0.238 0.348 0.240 0.359 0.250 0.353 0.259 0.399 0.285 0.359 0.255 0.593 0.315 0.610 0.322 0.675 0.412\\r\\n192 0.360 0.247 0.369 0.250 0.373 0.257 0.373 0.267 0.409 0.290 0.377 0.265 0.596 0.317 0.626 0.346 0.679 0.423\\r\\n336 0.377 0.256 0.388 0.260 0.386 0.265 0.386 0.275 0.422 0.297 0.393 0.276 0.600 0.319 0.633 0.352 0.688 0.440\\r\\n720 0.418 0.279 0.431 0.285 0.421 0.286 0.425 0.296 0.461 0.319 0.436 0.305 0.619 0.335 0.651 0.366 0.693 0.457\\r\\nAvg 0.374 0.255 0.384 0.259 0.385 0.265 0.384 0.274 0.423 0.298 0.391 0.275 0.602 0.322 0.630 0.347 0.684 0.433\\r\\nWeather\\r\\n96 0.157 0.205 0.151 0.202 0.152 0.206 0.174 0.225 0.169 0.229 0.149 0.202 0.169 0.228 0.185 0.241 0.355 0.409\\r\\n192 0.206 0.250 0.196 0.245 0.198 0.249 0.227 0.268 0.211 0.268 0.194 0.245 0.222 0.269 0.286 0.325 0.421 0.450\\r\\n336 0.259 0.291 0.249 0.288 0.251 0.291 0.290 0.309 0.258 0.306 0.244 0.285 0.290 0.310 0.323 0.347 0.452 0.465\\r\\n720 0.337 0.344 0.330 0.344 0.322 0.340 0.374 0.360 0.320 0.362 0.317 0.338 0.376 0.364 0.436 0.401 0.513 0.496\\r\\nAvg 0.240 0.273 0.232 0.270 0.231 0.272 0.266 0.291 0.239 0.291 0.226 0.268 0.264 0.293 0.308 0.329 0.435 0.455\\r\\nSolar-Energy\\r\\n96 0.162 0.221 0.212 0.230 0.190 0.240 0.183 0.265 0.193 0.258 0.168 0.237 0.180 0.272 0.199 0.290 0.206 0.296\\r\\n192 0.187 0.239 0.232 0.246 0.223 0.264 0.205 0.283 0.214 0.274 0.189 0.257 0.199 0.286 0.243 0.307 0.254 0.328\\r\\n336 0.205 0.255 0.237 0.253 0.250 0.283 0.224 0.299 0.233 0.291 0.212 0.277 0.220 0.301 0.264 0.322 0.272 0.330\\r\\n720 0.238 0.279 0.252 0.266 0.292 0.311 0.239 0.316 0.246 0.307 0.240 0.305 0.251 0.321 0.310 0.339 0.326 0.347\\r\\nAvg 0.198 0.249 0.233 0.249 0.241 0.275 0.213 0.291 0.222 0.283 0.202 0.269 0.213 0.295 0.254 0.315 0.265 0.325\\r\\n1\\r\\nst Count 23 21 1 8 1 2 0 0 3 5 14 9 0 0 0 0 0 0\\r\\n22\\nTable 12: Full results of zero-shot forecasting (pred-{96, 192, 336, 720}). We add new baseline large\\r\\nmodels, such as Time-MoE (Shi et al., 2024). Our model size is comparable with MoiraiBase, which\\r\\nis pre-trained on UTSD (Liu et al., 2024c) and LOTSA (Woo et al., 2024).\\r\\nModels Timer-XLBase Time-MoEBase Time-MoELarge Time-MoEUltra MoiraiSmall MoiraiBase MoiraiLarge TimesFM MOMENT ChronosBase ChronosLarge\\r\\n(Ours) (2024) (2024) (2024) (2024) (2024) (2024) (2023) (2024) (2024) (2024)\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTm1\\r\\n96 0.317 0.356 0.338 0.368 0.309 0.357 0.281 0.341 0.418 0.392 0.363 0.356 0.380 0.361 0.361 0.370 0.654 0.527 0.454 0.408 0.457 0.403\\r\\n192 0.358 0.381 0.353 0.388 0.346 0.381 0.305 0.358 0.431 0.405 0.388 0.375 0.412 0.383 0.414 0.405 0.662 0.532 0.567 0.477 0.530 0.450\\r\\n336 0.386 0.401 0.381 0.413 0.373 0.408 0.369 0.395 0.433 0.412 0.416 0.392 0.436 0.400 0.445 0.429 0.672 0.537 0.662 0.525 0.577 0.481\\r\\n720 0.430 0.431 0.504 0.493 0.475 0.477 0.469 0.472 0.462 0.432 0.460 0.418 0.462 0.420 0.512 0.471 0.692 0.551 0.900 0.591 0.660 0.526\\r\\nAvg 0.373 0.392 0.394 0.415 0.376 0.405 0.356 0.391 0.436 0.410 0.406 0.385 0.422 0.391 0.433 0.418 0.670 0.536 0.645 0.500 0.555 0.465\\r\\nETTm2\\r\\n96 0.189 0.277 0.201 0.291 0.197 0.286 0.198 0.288 0.214 0.288 0.205 0.273 0.211 0.274 0.202 0.270 0.260 0.335 0.199 0.274 0.197 0.271\\r\\n192 0.241 0.315 0.258 0.334 0.250 0.322 0.235 0.312 0.284 0.332 0.275 0.316 0.281 0.318 0.289 0.321 0.289 0.350 0.261 0.322 0.254 0.314\\r\\n336 0.286 0.348 0.324 0.373 0.337 0.375 0.293 0.348 0.331 0.362 0.329 0.350 0.341 0.355 0.360 0.366 0.324 0.369 0.326 0.366 0.313 0.353\\r\\n720 0.375 0.402 0.488 0.464 0.480 0.461 0.427 0.428 0.402 0.408 0.437 0.411 0.485 0.428 0.462 0.430 0.394 0.409 0.455 0.439 0.416 0.415\\r\\nAvg 0.273 0.336 0.317 0.365 0.316 0.361 0.288 0.344 0.307 0.347 0.311 0.337 0.329 0.343 0.328 0.346 0.316 0.365 0.310 0.350 0.295 0.338\\r\\nETTh1\\r\\n96 0.369 0.391 0.357 0.381 0.350 0.382 0.349 0.379 0.401 0.402 0.376 0.392 0.381 0.388 0.414 0.404 0.688 0.557 0.440 0.393 0.441 0.390\\r\\n192 0.405 0.413 0.384 0.404 0.388 0.412 0.395 0.413 0.435 0.421 0.412 0.413 0.434 0.415 0.465 0.434 0.688 0.560 0.492 0.426 0.502 0.524\\r\\n336 0.418 0.423 0.411 0.434 0.411 0.430 0.447 0.453 0.438 0.434 0.433 0.428 0.485 0.445 0.503 0.456 0.675 0.563 0.550 0.462 0.576 0.467\\r\\n720 0.423 0.441 0.449 0.477 0.427 0.455 0.457 0.462 0.439 0.454 0.447 0.444 0.611 0.510 0.511 0.481 0.683 0.585 0.882 0.591 0.835 0.583\\r\\nAvg 0.404 0.417 0.400 0.424 0.394 0.419 0.412 0.426 0.428 0.427 0.417 0.419 0.480 0.439 0.473 0.443 0.683 0.566 0.591 0.468 0.588 0.466\\r\\nETTh2\\r\\n96 0.283 0.342 0.305 0.359 0.302 0.354 0.292 0.352 0.297 0.336 0.294 0.330 0.296 0.330 0.315 0.349 0.342 0.396 0.308 0.343 0.320 0.345\\r\\n192 0.340 0.379 0.351 0.386 0.364 0.385 0.347 0.379 0.368 0.381 0.365 0.375 0.361 0.371 0.388 0.395 0.354 0.402 0.384 0.392 0.406 0.399\\r\\n336 0.366 0.400 0.391 0.418 0.417 0.425 0.406 0.419 0.370 0.393 0.376 0.390 0.390 0.390 0.422 0.427 0.356 0.407 0.429 0.430 0.492 0.453\\r\\n720 0.397 0.431 0.419 0.454 0.537 0.496 0.439 0.447 0.411 0.426 0.416 0.433 0.423 0.418 0.443 0.454 0.395 0.434 0.501 0.477 0.603 0.511\\r\\nAvg 0.347 0.388 0.366 0.404 0.405 0.415 0.371 0.399 0.361 0.384 0.362 0.382 0.367 0.377 0.392 0.406 0.361 0.409 0.405 0.410 0.455 0.427\\r\\nECL\\r\\n96 0.141 0.237 - - - - - - 0.189 0.280 0.160 0.250 0.153 0.241 - - 0.745 0.680 0.154 0.231 0.152 0.229\\r\\n192 0.159 0.254 - - - - - - 0.205 0.292 0.175 0.263 0.169 0.255 - - 0.755 0.683 0.179 0.254 0.172 0.250\\r\\n336 0.177 0.272 - - - - - - 0.221 0.307 0.187 0.277 0.187 0.273 - - 0.766 0.687 0.214 0.284 0.203 0.276\\r\\n720 0.219 0.308 - - - - - - 0.258 0.335 0.228 0.309 0.237 0.313 - - 0.794 0.696 0.311 0.346 0.289 0.337\\r\\nAvg 0.174 0.278 - - - - - - 0.218 0.303 0.187 0.274 0.186 0.270 - - 0.765 0.686 0.214 0.278 0.204 0.273\\r\\nWeather\\r\\n96 0.171 0.225 0.160 0.214 0.159 0.213 0.157 0.211 0.198 0.222 0.220 0.217 0.199 0.211 - - 0.243 0.255 0.203 0.238 0.194 0.235\\r\\n192 0.221 0.271 0.210 0.260 0.215 0.266 0.208 0.256 0.247 0.265 0.271 0.259 0.246 0.251 - - 0.278 0.329 0.256 0.290 0.249 0.285\\r\\n336 0.274 0.311 0.274 0.309 0.291 0.322 0.255 0.290 0.283 0.303 0.286 0.297 0.274 0.291 - - 0.306 0.346 0.314 0.336 0.302 0.327\\r\\n720 0.356 0.370 0.418 0.405 0.415 0.400 0.405 0.397 0.373 0.354 0.373 0.354 0.337 0.340 - - 0.350 0.374 0.397 0.396 0.372 0.378\\r\\nAvg 0.256 0.294 0.265 0.297 0.270 0.300 0.256 0.288 0.275 0.286 0.287 0.281 0.264 0.273 - - 0.294 0.326 0.292 0.315 0.279 0.306\\r\\n1\\r\\nst Count 15 10 2 1 3 0 10 7 0 0 0 5 1 10 0 1 2 0 0 0 0 2\\r\\n∗ Dataset for pre-training is not evaluated on corresponding models, which is denoted by a dash (−).\\r\\n∗ Traffic from Wu et al. (2022) is generally used during the pre-training of large models and thus not evaluated here.\\r\\n∗ Our model checkpoint is available at https://huggingface.co/thuml/timer-base-84m.\\r\\nTable 13: Embedding ablation in TimeAttention. For the temporal dimension, we compare prevalent\\r\\nrelative and absolute position embeddings. As for the variable dimension, we explore the effectiveness\\r\\nof the variable embedding that distinguishes endogenous and exogenous variables.\\r\\nDesign Temporal Variable Traffic Weather Solar-Energy ERA5-MS\\r\\nMSE MAE MSE MAE MSE MAE MSE MAE\\r\\nTimer-XL RoPE (2024) with 0.340 0.238 0.157 0.205 0.162 0.221 0.164 0.307\\r\\nReplace\\r\\nALiBi (2021) with 0.351 0.246 0.162 0.212 0.188 0.210 0.167 0.308\\r\\nRelative (2020) with 0.361 0.250 0.163 0.214 0.197 0.215 0.168 0.309\\r\\nAbsolute (2017) with 0.381 0.270 0.159 0.207 0.171 0.204 0.165 0.306\\r\\nw/o RoPE (2024) w/o 0.361 0.254 0.171 0.217 0.181 0.221 0.235 0.373\\r\\nw/o w/o 0.363 0.253 0.164 0.215 0.194 0.215 0.167 0.309\\r\\n23\\nEncoder-only\\r\\nDecoder-only\\r\\nEncoder-only Decoder-only\\r\\nFocused part of each token\\r\\nFocused part of the last token\\r\\nEncoder-only\\r\\nDecoder-only\\r\\nFocused part of each token\\r\\nFocused part of the last token\\r\\nEncoder-only Decoder-only\\r\\nFigure 11: Case studies of learned attention in encoder-/decoder-only Transformers.\\r\\ndegradation of PatchTST. Similar to the observations in Figure 3, the encoder-only architecture\\r\\nproduces inferior predictions after thousands of time points, which can be concealed due to the short\\r\\ncontext adopted in previous benchmarks. Although PatchTST has conducted an initial exploration in\\r\\nthe context of hundreds of time points, it inappropriately works in ever-long contexts. Therefore, we\\r\\nbelieve that context bottlenecks deserve further exploration in this community.\\r\\nTable 14: Performance on ERA5 (pred-1day). Lookback lengths vary from daily to yearly contexts.\\r\\nModels Timer-XL PatchTST DLinear\\r\\nMetric MSE | MAE MSE | MAE MSE | MAE\\r\\nLookback-8 (1 Day) 0.0847 | 0.2100 0.0897 | 0.2196 0.0970 | 0.2276\\r\\nLookback-32 (4 Day) 0.0713 | 0.1928 0.0778 | 0.2080 0.0841 | 0.2113\\r\\nLookback-56 (1 Week) 0.0688 | 0.1891 0.0785 | 0.2082 0.0814 | 0.2081\\r\\nLookback-224 (1 Month) 0.0675 | 0.1868 0.0745 | 0.2042 0.0788 | 0.2048\\r\\nLookback-960 (4 Month) 0.0667 | 0.1863 0.1194 | 0.2696 0.0773 | 0.2031\\r\\nLookback-2944 (1 Year) 0.0663 | 0.1857 0.1109 | 0.2638 0.0763 | 0.2024\\r\\nRepresentation Analysis We further delve into long-context modeling from the perspective of\\r\\nlearned representations. As shown in Figure 11, the decoder-only model can selectively focus on\\r\\nthe previous context while PatchTST wrongly focuses on noisy parts. Since causality is the basis of\\r\\nforecasting, using causal masks leads to coherent token embeddings, while the unmasked attention\\r\\nmechanism may break the causality and prevent the model from telling each tokens.\\r\\nNormalization Section 4.1 has discussed instance normalization (Kim et al., 2021). It generally\\r\\nimproves the performance of the previous encoder-only Transformers but leads to special problems\\r\\nin generative Transformers (e.g., unmatched statistics in multi-step autoregression). However, it is\\r\\nindicative that Timer-XL without ReVIN can achieve competitive performance on well-acknowledged\\r\\nbenchmarks in Table 15, while the performance of PatchTST may heavily rely on this normalization.\\r\\nE.5 ILLUSTRATION OF TIMEATTENTION\\r\\nAlthough the formulation to generalize from 1D sequences to multivariate time series is straightfor\\x02ward, Timer-XL is built on a generative Transformer, an underexploited backbone among current\\r\\ntime series models. As shown in Figure 12, challenges lie in capturing fine-grained dependencies\\r\\n24\\nTable 15: Evaluations (672-pred-96) on the effect of ReVIN (Kim et al., 2021) on Transformers.\\r\\nModels Timer-XL with ReVIN Timer-XL w/o ReVIN PatchTST with ReVIN PatchTST w/o ReVIN\\r\\nMetric MSE | MAE MSE | MAE MSE | MAE MSE | MAE\\r\\nETTh1 0.364 | 0.397 0.370 | 0.401 0.370 | 0.399 0.421 | 0.448\\r\\nWeather 0.157 | 0.205 0.151 | 0.205 0.149 | 0.198 0.173 | 0.242\\r\\nECL 0.127 | 0.219 0.130 | 0.225 0.129 | 0.222 0.138 | 0.244\\r\\nTime Series A Time Series B\\r\\n2’ 3’ 4’ 5’ 6’ 7’\\r\\nTimer-XL\\r\\nToken Depedencies\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\nA-Tokens B-Tokens\\r\\nDependencies\\r\\n1 2 3 4 5 6\\r\\n1 2 3 4 5 6\\r\\nTimeAttention\\r\\n1 2 3\\r\\nDependencies\\r\\nTokens\\r\\n1\\r\\n2\\r\\n3\\r\\nLookback\\r\\n2’ 3’ 4’\\r\\nTimer-XL\\r\\n1 2 3\\r\\nTimeAttention\\r\\nDepedencies\\r\\nForecast Lookback Forecast\\r\\nPatch Tokens Patch Tokens\\r\\n(a) Univariate Data (b) Multivariate Data\\r\\nO\\r\\n\\uf8ff 1 1\\r\\n1 1\\r\\n\\r\\nT\\r\\n=\\r\\nC\\r\\nUnmasked\\r\\nMasked\\r\\nFigure 12: Illustration of TimeAttention for modeling univariate and multivariate time series.\\r\\nbetween all variables in the patch level, while maintaining temporal causality in multiple sequences.\\r\\nTechnically, we introduce the masking formulation, whose key lies in the grouped causality of\\r\\nflattened 2D sequences. We derive it based on the Kronecker Product, which disentangles the large\\r\\nattention map into formalizable temporal and variable dependencies. It can be naturally extended to\\r\\ncovariates or pre-defined variable dependencies, which may inspire a lot of future explorations.\\r\\nF LIMITATIONS\\r\\nAs a generative Transformer, Timer-XL necessitates iterative generation for long-term forecasting,\\r\\nwhich may lead to error accumulation and inflexibility in the output length. In the future, we plan to\\r\\nincorporate multi-resolution patches for input and output series. Furthermore, given that Timer-XL\\r\\nexplicitly captures fine-grained token dependencies, there remains significant potential to reduce the\\r\\ncomplexity of TimeAttention, particularly in high-dimensional and lengthy time series. Finally, we\\r\\nwill investigate the factors contributing to the stagnation of Transformer performance in extremely\\r\\nlong contexts, and seek insights in the time series modality to improve context efficiency.\\r\\n25'},\n",
       " {'name': '1809.09441v2.pdf',\n",
       "  'content': 'Temporal Relational Ranking for Stock Prediction\\r\\nFULI FENG, National University of Singapore, Singapore\\r\\nXIANGNAN HE, University of Science and Technology of China, China\\r\\nXIANG WANG, National University of Singapore, Singapore\\r\\nCHENG LUO, Tsinghua University, China\\r\\nYIQUN LIU, Tsinghua University, China\\r\\nTAT-SENG CHUA, National University of Singapore, Singapore\\r\\nStock prediction aims to predict the future trends of a stock in order to help investors to make good investment\\r\\ndecisions. Traditional solutions for stock prediction are based on time-series models. With the recent success\\r\\nof deep neural networks in modeling sequential data, deep learning has become a promising choice for stock\\r\\nprediction.\\r\\nHowever, most existing deep learning solutions are not optimized towards the target of investment, i.e.,\\r\\nselecting the best stock with highest expected revenue. Specifically, they typically formulate stock prediction\\r\\nas a classification (to predict stock trend) or a regression problem (to predict stock price). More importantly,\\r\\nthey largely treat the stocks as independent of each other. The valuable signal in the rich relations between\\r\\nstocks (or companies), such as two stocks are in the same sector and two companies have a supplier-customer\\r\\nrelation, is not considered.\\r\\nIn this work, we contribute a new deep learning solution, named Relational Stock Ranking (RSR), for stock\\r\\nprediction. Our RSR method advances existing solutions in two major aspects: 1) tailoring the deep learning\\r\\nmodels for stock ranking, and 2) capturing the stock relations in a time-sensitive manner. The key novelty of\\r\\nour work is the proposal of a new component in neural network modeling, named Temporal Graph Convolution,\\r\\nwhich jointly models the temporal evolution and relation network of stocks. To validate our method, we\\r\\nperform back-testing on the historical data of two stock markets, NYSE and NASDAQ. Extensive experiments\\r\\ndemonstrate the superiority of our RSR method. It outperforms state-of-the-art stock prediction solutions\\r\\nachieving an average return ratio of 98% and 71% on NYSE and NASDAQ, respectively.\\r\\nCCS Concepts: • Information systems → Data mining; • Computing methodologies → Neural net\\x02works; Machine learning; Logical and relational learning; • Applied computing → Computers in other\\r\\ndomains;\\r\\nAdditional Key Words and Phrases: Stock Prediction, Learning to Rank, Graph-based Learning\\r\\n1 INTRODUCTION\\r\\nAccording to the statistics reported by the World Bank in 2017, the overall capitalization of stock\\r\\nmarkets worldwide has exceeded 64 trillion U.S. dollars1. With the continual increase in stock\\r\\n1https://data.worldbank.org/indicator/CM.MKT.LCAP.CD/.\\r\\nAuthors’ addresses: Fuli Feng, National University of Singapore, 13 Computing Drive, 117417, Singapore, fulifeng93@gmail.\\r\\ncom; Xiangnan He, University of Science and Technology of China, 443 Huangshan Road, Hefei, 230031, China, xiangnanhe@\\r\\ngmail.com; Xiang Wang, National University of Singapore, 13 Computing Drive, 117417, Singapore, xiangwang@u.nus.edu;\\r\\nCheng Luo, Tsinghua University, 30 Shuangqing Rd, Haidian, Beijing, China, chengluo@tsinghua.edu.cn; Yiqun Liu,\\r\\nTsinghua University, 30 Shuangqing Rd, Haidian, Beijing, China, yiqunliu@tsinghua.edu.cn; Tat-Seng Chua, National\\r\\nUniversity of Singapore, 13 Computing Drive, 117417, Singapore, dcscts@nus.edu.sg.\\r\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\r\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\r\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\r\\ncontact the owner/author(s).\\r\\n© 2019 Copyright held by the owner/author(s).\\r\\n1046-8188/2019/12-ART\\r\\nhttps://doi.org/10.475/123_4\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\r\\narXiv:1809.09441v2 [cs.CE] 19 Jan 2019\\n:2 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\nTable 1. An intuitive example that one method predicting the price change of stocks more accurately (i.e.,\\r\\nsmaller MSE) leads to a less profitable stock selection (i.e., smaller profit). Method 1 selects stock A (30) while\\r\\nMethod 2 selects stock B (10).\\r\\nGround Truth Method 1 Method 2\\r\\nPrediction Performance Prediction Performance\\r\\nA B C A B C MSE Profit A B C MSE Profit\\r\\n+30 +10 -50 +50 -10 -50 266 30 +20 +30 -40 200 10\\r\\nA, B, C denote three stocks; numbers (+20) are the true/predicted price change of stocks; values in bold correspond to suggested selections.\\r\\nmarket capitalization, trading stocks has become an attractive investment instrument for many\\r\\ninvestors. However, whether an investor could earn or lose money depends heavily on whether\\r\\nhe/she can make the right stock selection. Stock prediction, which aims to predict the future trend\\r\\nand price of stocks, is one of the most popular techniques to make profitable stock investment [32],\\r\\nalthough there are still debates about whether the stock market is predictable (aka. the Efficient\\r\\nMarkets Hypothesis) among financial economists [24, 27]. Some recent evidences indicate the\\r\\npredictability of stock markets, which motivates further exploration of stock prediction techniques\\r\\n[17, 23, 36, 42? ].\\r\\nTraditional solutions for stock prediction are based on time-series analysis models, such as\\r\\nKalman Filters [39], Autoregressive Models and their extensions [1]. Given an indicator of a\\r\\nstock (e.g., stock price), this kind of models represents it as a stochastic process and takes the\\r\\nhistorical data of the indicator to fit the process. We argue that such mainstream solutions for stock\\r\\nprediction have three main drawbacks: 1) The models heavily rely on the selection of indicators,\\r\\nwhich is usually done manually and is hard to optimize without special knowledge of finance. 2)\\r\\nThe hypothesized stochastic processes are not always compatible with the volatile stock in the\\r\\nreal world. 3) These models can only consider a few indicators since their inference complexity\\r\\ntypically increases exponentially with the number of indicators. As such, they lack the capability\\r\\nto comprehensively describe a stock that could be influenced by a plethora of factors. Towards\\r\\nthese drawbacks, advanced techniques like deep neural networks, especially the recurrent neural\\r\\nnetworks (RNNs), have become a promising solution to substitute the traditional time-series models\\r\\nto predict the future trend or exact price of a stock [4, 42–44].\\r\\nA state-of-the-art neural network-based solution is the State Frequency Memory (SFM) network\\r\\n[42], which models the historical data in a recurrent fashion and captures temporal patterns in\\r\\ndifferent frequencies. This method achieves promising performance of predicting the daily opening\\r\\nprice of fifty U.S. stocks one day ahead with a mean square error (MSE) of less than six dollars.\\r\\nHowever, we argue that such prediction methods are suboptimal to guide stock selection, since\\r\\ntheir optimization target is not at selecting the top stocks with the highest expected revenue. To\\r\\nbe specific, they typically address stock prediction as either a classification (on price movement\\r\\ndirection) or a regression (on price value) task, which would cause a large discrepancy on the\\r\\ninvestment revenue. Table 1 gives an intuitive example, where a method with better prediction\\r\\nperformance (measured by regression MSE) suggests a less profitable stock. This implies the possible\\r\\ndiscrepancy between the actual target of stock selection and the optimized target of regression\\r\\n(classification), such that an optimal method of regression (classification) does not necessarily select\\r\\nthe optimal stock to trade.\\r\\nAnother limitation of existing neural network-based solutions is that they typically treat stocks\\r\\nas independent of each other and ignore the relations between stocks. However, the rich relations\\r\\nbetween stocks and the corresponding companies may contain valuable clues for stock prediction.\\r\\nFor example, stocks under the same sector or industry like GOOGL (Alphabet Inc.) and FB (Facebook\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :3\\r\\n…\\r\\nAAPL\\r\\n… … …\\r\\nGOOGL FB\\r\\nSequential Inputs\\r\\nLSTM LSTM LSTM\\r\\nSequential \\r\\nEmbedding Layer …\\r\\nTGC\\r\\nRelational \\r\\nEmbedding Layer\\r\\n…\\r\\n…\\r\\n…\\r\\nFC FC FC Prediction Layer\\r\\nRanking Scores\\r\\nFig. 1. Relational stock ranking framework. It should be noted that the LSTM cells and FC units (Fully\\r\\nConnected layer) depicted in the same layer share the same parameters.\\r\\nInc.) might have similar long-term trends. Besides, the stock of a supplier company might impact\\r\\nthe stock of its consumer companies especially when a scandal of the supplier company is reported,\\r\\nsuch as the falsification of product quality data. To integrate stock relations into prediction, an\\r\\nintuitive solution is to represent the stock relations as a graph and then regularize the prediction\\r\\nof stocks based on the graph (i.e., graph-based learning) [11, 18, 20, 30]. However, conventional\\r\\ngraph learning techniques cannot capture the temporal evolution property of stock markets (e.g.,\\r\\nthe strength of influence between two given stocks may vary quickly), since the graph is fixed at a\\r\\nparticular time.\\r\\nTo address the aforementioned limitations of existing solutions, we formulate stock prediction\\r\\nas a ranking task, for which the target is to directly predict a stock list ranked by a desired criteria\\r\\nlike return ratio. We then propose an end-to-end framework, named Relational Stock Ranking\\r\\n(RSR), to solve the stock ranking problem. An illustration of our framework can be found in\\r\\nFigure 1. Specifically, we first feed the historical time series data of each stock to a Long Short\\x02Term Memory (LSTM) network to capture the sequential dependencies and learn a stock-wise\\r\\nsequential embedding. By devising a new Temporal Graph Convolution (TGC), we next revise the\\r\\nsequential embeddings by accounting for stock relations in a time-sensitive way. Finally, we feed\\r\\nthe concatenation of sequential embeddings and relational embeddings to a fully connected layer to\\r\\nobtain the ranking score of stocks. To justify our proposed method, we employ it on two real-world\\r\\nmarkets, New York Stock Exchange (NYSE) and NASDAQ Stock Market (NASDAQ). Extensive\\r\\nback-testing results demonstrate that our RSR significantly outperforms SFM [42] with more than\\r\\n115% improvements in return ratio.\\r\\nThe key contributions of the paper are summarized as follows.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:4 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n• We propose a novel neural network-based framework, named Relational Stock Ranking, to\\r\\nsolve the stock prediction problem in a learning-to-rank fashion.\\r\\n• We devise a new component in neural network modeling, named Temporal Graph Convolution,\\r\\nto explicitly capture the domain knowledge of stock relations in a time-sensitive manner.\\r\\n• We empirically demonstrate the effectiveness of our proposals on two real-world stock\\r\\nmarkets, NYSE and NASDAQ.\\r\\nThe remainder of this paper is organized as follows. Section 2 introduces the preliminary knowl\\x02edge about LSTM and graph-based learning, which forms the building blocks of our method.\\r\\nSection 3 presents our proposed RSR. Section 4 and 5 describe the datasets and experiment, respec\\x02tively. In Section 6, we review related work, followed by conclusion in Section 7.\\r\\n2 PRELIMINARIES\\r\\nIn this paper, we use bold capital letters (e.g., X), bold lowercase letters (e.g., x), and capital script\\r\\nletters (e.g., X) to denote matrices, vectors, and tensors, respectively. Scalars and hyperparameters\\r\\nare respectively represented as normal lowercase letters (e.g., x) and Greek letters (e.g., λ). If not\\r\\notherwise specified, all vectors are in a column form, and Xij denotes the entry at the i-th row\\r\\nand the j-th column of X. The symbols σ, tanh, and ⊙ stand for the sigmoid function, hyperbolic\\r\\ntangent function, and element-wise production operation, respectively.\\r\\n2.1 Long Short-Term Memory\\r\\nLSTM [16] networks have been widely used to process sequential data, such as natural language\\r\\n[40], voice [13], and video [35]. LSTM is a special kind of Recurrent Neural Networks (RNNs) [12]\\r\\nthat evolve hidden states through time to capture the sequential pattern of input data, e.g., the\\r\\ndependency between words in a sentence. Compared to the vanilla RNN, which is known to suffer\\r\\nfrom vanishing gradients while trained with Back-Propagation Through Time (BPTT), LSTM adds\\r\\ncell states to store the long-term memory and capture the long-term dependency in a sequence2.\\r\\nBefore providing the specific formulation of LSTM, we first describe the terms associated with\\r\\nLSTM. At each time-step t, x\\r\\nt ∈ RD denotes an input vector (e.g., embedding vector of the t-th\\r\\nword in a given sentence), where D is the input dimension. Vectors c\\r\\nt\\r\\nand h\\r\\nt ∈ RU denote the cell\\r\\n(memory) state vector and the hidden state vector, respectively, where U is the number of hidden\\r\\nunits. Vector z\\r\\nt ∈ RU is an information transformation module. Vectors it\\r\\n, o\\r\\nt\\r\\n, and f\\r\\nt ∈ RU denote\\r\\nthe input, output, and forget gate, respectively. Formally, the transformation module, state vectors,\\r\\nand controlling gates are defined via the following equations:\\r\\nz\\r\\nt = tanh(Wzxt + Qzht−1 + bz)\\r\\ni\\r\\nt = σ(Wixt + Qiht−1 + bi)\\r\\nf\\r\\nt = σ(Wfxt + Qfht−1 + bf)\\r\\nc\\r\\nt = ft ⊙ ct−1 + it ⊙ zt\\r\\no\\r\\nt = σ(Woxt + Whht−1 + bo)\\r\\nh\\r\\nt = ot ⊙ tanh(ct\\r\\n),\\r\\n(1)\\r\\nwhere Wz, Wi, Wf, Wo ∈ R\\r\\nU ×D , and Qz, Qi\\r\\n, Qf ∈ R\\r\\nU ×U are mapping matrices; bz, bi\\r\\n, bf, and\\r\\nbo ∈ R\\r\\nU are bias vectors. The updating formulation can be understood as performing the following\\r\\nprocedures: (1) calculate the information to be transformed from the input x\\r\\nt\\r\\nto the memory states\\r\\n2Detailed illustration of LSTM and its comparison against vanilla RNN are referred to: http://colah.github.io/posts/\\r\\n2015-08-Understanding-LSTMs/.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :5\\r\\nc\\r\\nt by updating zt\\r\\n; (2) update the input gate i\\r\\nt\\r\\nto control the information from z\\r\\nt\\r\\nto c\\r\\nt\\r\\n; (3) update\\r\\nthe forget gate f\\r\\nt\\r\\nto decide how much information should be kept in the memory state; (4) refresh\\r\\nthe memory state c\\r\\nt by fusing the information flows from the input gate and memory gate; (5)\\r\\nupdate the output gate o\\r\\nt\\r\\nto regulate the amount of information that can be outputted; (6) update\\r\\nthe hidden state h\\r\\nt\\r\\n. As can be seen, the memory state h\\r\\nt only has linear adding interactions, which\\r\\nallows the information to be unchanged during the BPTT. Benefited by the memory state, LSTM is\\r\\ncapable of capturing the long-term dependency in the sequential data.\\r\\n2.2 Graph-based Learning\\r\\nGraph-based learning has been applied to various machine learning tasks to utilize entity relations [2,\\r\\n11, 26, 41? ]. The general problem setting is to learn a prediction function ˆy = f (x), which maps\\r\\nan entity from the feature space to the target label space. It is usually achieved by minimizing an\\r\\nobjective function abstracted as:\\r\\nΓ = Ω + λΦ, (2)\\r\\nwhere Ω is a task-specific loss that measures the error between prediction ˆy and ground-truth y, Φ\\r\\nis a graph regularization term that smooths the prediction over the graph, and λ is a hyperparameter\\r\\nto balance the two terms. The regularization term typically implements the smoothness assumption\\r\\nthat similar vertices tend to have similar predictions. A widely used Φ is defined as:\\r\\nΦ =\\r\\nÕ\\r\\nN\\r\\ni=1\\r\\nÕ\\r\\nN\\r\\nj=1\\r\\nд(xi, xj)\\r\\n| {z }\\r\\nstrength of smoothness\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nf (xi)\\r\\n√\\r\\nDi i\\r\\n−\\r\\nf (xj)\\r\\np\\r\\nDjj\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n2\\r\\n| {z }\\r\\nsmoothness\\r\\n, (3)\\r\\nwhere д(xi, xj) is the similarity between the feature vectors of an entity pair (e.g., the edge weight\\r\\nbetween the corresponding vertices); Di i =\\r\\nÍN\\r\\nj=1\\r\\nд(xi, xj) is the degree of vertex i. The regularization\\r\\nterm operates smoothness on each pair of entities, enforcing their predictions (after normalized by\\r\\ntheir degrees) to be close to each other. The strength of smoothness is determined by the similarity\\r\\nover their feature vectors д(xi, xj). It can be equivalently written in a more concise matrix form:\\r\\nG = trace(YLˆ Yˆ\\r\\nT\\r\\n), (4)\\r\\nwhere Yˆ = [ ˆy1, ˆy2, · · · , ˆyN], L is defined as L = D\\r\\n−1/2\\r\\n(D − A)D\\r\\n−1/2\\r\\n, also known as the graph\\r\\nLaplacian matrix, and each element of A is Aij = д(xi, xj).\\r\\n2.2.1 Graph Convolutional Networks. Graph Convolutional Network (GCN) is a special kind\\r\\nof graph-based learning methods, which integrates the core idea of graph-based learning (i.e.,\\r\\nthe smoothness assumption over graphs) with advanced convolutional neural networks (CNNs)\\r\\n[8, 10, 14, 20]. The core idea of standard CNNs [21] is using convolutions (e.g., 3 × 3 filter matrices)\\r\\nto capture the local patterns in input data (e.g., oblique lines in an image). Following the idea of\\r\\nCNNs, the aim of GCN is to capture the local connection patterns on graphs. However, intuitive\\r\\nsolutions like directly applying convolution operations on the adjacency matrix of a graph are not\\r\\nfeasible. Because the filtering output of convolutions might change when we switch two rows of the\\r\\nadjacency matrix, while the switched adjacency matrix still represent the same graph structure. An\\r\\nalternative solution is to use spectral convolutions to capture the local connections in the Fourier\\r\\ndomain, such as:\\r\\nf (F, X) = UFUT X, (5)\\r\\nwhere f denotes the filtering operation of a convolution parameterized by a diagonal matrix F, and\\r\\nU is the eigenvector matrix of the graph Laplacian matrix, i.e., L = UΛUT.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:6 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\nTable 2. Terms and notations.\\r\\nSymbol Definition\\r\\nXt ∈ R\\r\\nN ×S×D = [Xt\\r\\n1\\r\\n, · · · , X\\r\\nt\\r\\nN\\r\\n]\\r\\nT historical prices of N stocks on trading day t.\\r\\nA ∈ R\\r\\nN ×N ×K binary encoding of stock relations.\\r\\nE\\r\\nt = [et\\r\\n1\\r\\n, · · · , e\\r\\nt\\r\\nN\\r\\n]\\r\\nT ∈ RN ×U sequential embedding of N stocks learned from historical prices.\\r\\nE\\r\\nt = [e\\r\\nt\\r\\n1\\r\\n, · · · , e\\r\\nt\\r\\nN\\r\\n]\\r\\nT ∈ RN ×U relational embedding of all stocks learned from Et\\r\\nand A.\\r\\nr\\r\\nt+1\\r\\n, ˆr\\r\\nt+1 ∈ RN ground-truth and predicted ranking scores of N stocks.\\r\\nw, b weights and bias to be learned.\\r\\nSuffering from the overhead of computing the eigendecomposition of L, it is suggested to treat F\\r\\nas a function of Λ. Then it can be approximated by the Chebyshev polynomials Tk (x) of up to the\\r\\nK-th order,\\r\\nF ≈\\r\\nÕ\\r\\nK\\r\\nk=0\\r\\nθkTk (Λˆ), (6)\\r\\nwhere Λˆ =\\r\\n2\\r\\nλmax\\r\\nΛ − I with λmax denotes the largest eigenvalue of L; θk represents the Chebyshev\\r\\ncoefficient; Tk (x) = 2xTk−1(x) − Tk−2(x) with T1x = x and T0x = 0. In [20], the authors proved\\r\\nthat the GCN performed well enough while setting K to 1. As such, they reduced Equation (5) to\\r\\nf (F, X) = AX and injected the convolution into a fully connected layer as A(XW + b), which is the\\r\\nstate-of-the-art formulation of GCN3.\\r\\n3 RELATIONAL STOCK RANKING\\r\\nThe typical problem setting of stock prediction (i.e., price movement classification and price\\r\\nregression) is to learn a prediction function yˆ\\r\\nt+1 = f (Xt\\r\\n) which maps a stock from the feature\\r\\nspace to the target label space at time-step t. Matrix X\\r\\nt = [xt−S+1\\r\\n, · · · , x\\r\\nt\\r\\n]\\r\\nT ∈ RS×D represents\\r\\nthe sequential input features, where D is the dimension of features at each time-step and S is the\\r\\nlength of the sequence. Distinct from the typical problem setting of stock prediction, which treats\\r\\ndifferent stocks as independent sequences, our target is to learn a ranking function ˆr\\r\\nt+1 = f (Xt\\r\\n),\\r\\nwhich simultaneously maps a bunch of stocks to a ranking list. In the learned ranking list, stocks\\r\\nwith higher ranking scores are expected to achieve higher investment revenue at time-step t + 1.\\r\\nAssuming we have N stocks, then Xt ∈ R\\r\\nN ×S×D = [Xt\\r\\n1\\r\\n, · · · , X\\r\\nt\\r\\nN\\r\\n]\\r\\nT\\r\\nis the collected features. In\\r\\naddition, we further associate the problem with a set of explicit stock relations (e.g., supplier\\x02consumer relations), which reflect the potential influence between different stocks. Given K types\\r\\nof relations, we encode the pairwise relation between two stocks as a multi-hot binary vector\\r\\naij ∈ R\\r\\nK and represent the relation of all stocks as a tensor A ∈ RN ×N ×K , of which the entry at\\r\\nthe i-th row and j-th column is aij.\\r\\nIn what follows, we first present the overall solution. We then elaborate our proposed Temporal\\r\\nGraph Convolution for handling stock relations, followed by discussing its connections to existing\\r\\ngraph-based learning methods. In Table 2, we summarize some of the terms and notations.\\r\\n3.1 Framework\\r\\nAs illustrated in Figure 1, RSR contains three layers, named a sequential embedding layer, a relational\\r\\nembedding layer, and a prediction layer, which are elaborated as follows.\\r\\nSequential Embedding Layer. Considering the strong temporal dynamics of stock markets, it is\\r\\nintuitive to regard the historical status of a stock as the most influential factor to predict its future\\r\\ntrend. As such, we first apply a sequential embedding layer to capture the sequential dependencies\\r\\n3Note that in the reduced form of GCN, the input diagonal matrix F is omitted due to the Chebyshev approximation.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :7\\r\\nin the historical data. Since RNN has achieved significant performance to process sequential data\\r\\n[13, 35, 40] and demonstrated to be effective in recent stock prediction research [4, 42], we opt for\\r\\nRNN to learn the sequential embeddings. Among various RNN models, such as vanilla RNN, LSTM,\\r\\nand Gated Recurrent Unit (GRU) [7], we choose LSTM owing to its ability to capture long-term\\r\\ndependency, which is of great importance to stock prediction. This is because that many factors\\r\\nhave long-term effects on a stock, such as the rise of interest rates, the release of annual reports, a\\r\\nrapid drop in its price, among others. For example, if a stock has experienced a very rapid drop\\r\\nin its price, after that, the stock’s price tends to exhibit an upward trend in the following days or\\r\\nweeks (aka. the mean reversion phenomenon). As such, we feed the historical time series data of\\r\\nstock i at time-step t (X\\r\\nt\\r\\ni\\r\\n) to a LSTM network and take the last hidden state (h\\r\\nt\\r\\ni\\r\\n) as the sequential\\r\\nembedding (e\\r\\nt\\r\\ni\\r\\n) of a stock (note that e\\r\\nt\\r\\ni\\r\\n= h\\r\\nt\\r\\ni\\r\\n), i.e., we have,\\r\\nE\\r\\nt = LSTM(Xt\\r\\n), (7)\\r\\nwhere E\\r\\nt = [et\\r\\n1\\r\\n, · · · , e\\r\\nt\\r\\nN\\r\\n]\\r\\nT ∈ RN ×U denotes the sequential embeddings of all stocks, and U denotes\\r\\nthe embedding size (i.e., U is the number of hidden units in LSTM).\\r\\nRelational Embedding Layer. We now consider how to model the influence between different\\r\\nstocks, especially the ones with explicit relations. Note that it can be seen as an injection of explicit\\r\\ndomain knowledge (i.e., stock relations) into the data-driven approach for sequential embedding\\r\\nlearning. Here we give two cases for illustration:\\r\\n• If two companies are in the same sector or industry, they may exhibit similar trends in their\\r\\nstock prices, since they tend to be influenced by similar external events. Figure 2(a) shows two\\r\\nexample stocks, MSFT (Microsoft Inc.) and GOOGL (Alphabet Inc.), both of which are in the\\r\\nsame sector (Technology) and industry (Computer Software)4. As can be seen in Figure 2(a), the\\r\\ntwo stocks exhibit quite similar trends in terms of the change on price in 2017. Note that we\\r\\nnormalize the prices of each stock separately by calculating the increase ratio at each trading\\r\\nday according to the price on the first day to reflect the price changes.\\r\\n• If two companies are partners in a supply chain, then the events of the upstream company may\\r\\naffect the stock price of the downstream company. Figure 2(b) shows an example to demonstrate\\r\\nthe impact of such supplier-consumer relation, which shows the stock price change of Lens\\r\\nTechnology Co Ltd after the release of iPhone 8 (09/22/2017)5. Since the Lens Technology Co Ltd\\r\\nis the supplier of the screen of iPhone, which was expected to be selling well, its stock price kept\\r\\nincreasing in the following several weeks of 09/22/2017.\\r\\nTo capture such patterns in stock historical data, we devise a new component of neural network\\r\\nmodeling, named Temporal Graph Convolution to revise the sequential embeddings according to\\r\\nstock relations. It generates the relational embeddings E\\r\\nt ∈ R\\r\\nN ×U in a time-sensitive (dynamic)\\r\\nway, which is a key technical contribution of this work and will be elaborated later in Section 3.2.\\r\\nPrediction Layer. Lastly, we feed the sequential embeddings and revised relational embeddings\\r\\nto a fully connected layer to predict the ranking score of each stock; the ranked list of stocks\\r\\nrecommended to buy is then generated based on the prediction scores.\\r\\nTo optimize the model, we propose an objective function that combines both pointwise regression\\r\\nloss and pairwise ranking-aware loss:\\r\\nl(ˆr\\r\\nt+1\\r\\n, r\\r\\nt+1\\r\\n) =\\r\\n\\r\\r\\n\\rˆr\\r\\nt+1 − rt+1\\r\\n\\r\\r\\n\\r\\r\\n2\\r\\n+ α\\r\\nÕ\\r\\nN\\r\\ni=0\\r\\nÕ\\r\\nN\\r\\nj=0\\r\\nmax(0, −(rˆi\\r\\nt+1 − rˆjt+1\\r\\n)(r\\r\\nt+1\\r\\ni − r\\r\\nt+1\\r\\nj\\r\\n)), (8)\\r\\n4http://www.nasdaq.com/screening/companies-by-industry.aspx\\r\\n5https://www.techradar.com/reviews/iphone-8-review\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:8 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n(a) Sector-industry relation (b) Supplier-consumer relation\\r\\nFig. 2. Two examples of stock price history (normalized as increase ratio as compared to the first depicted\\r\\ntrading day) to illustrate the impact of company relations on the stock price.\\r\\nwhere r\\r\\nt+1 = [rt+1\\r\\n1\\r\\n, · · ·r\\r\\nt+1\\r\\nN\\r\\n] and ˆr\\r\\nt+1 = [rˆt+1\\r\\n1\\r\\n, · · · ,rˆ\\r\\nt+1\\r\\nN\\r\\n] ∈ R\\r\\nN are ground-truth and predicted\\r\\nranking scores, respectively, and α is a hyperparameter to balance the two loss terms. Since we\\r\\nfocus on identifying the most profitable stock to trade, we use the 1-day return ratio of a stock as\\r\\nthe ground-truth rather than the normalized price used in previous work [42]. We will provide\\r\\nmore details on computing the ground-truth in Section 4.1 in our data collection.\\r\\nThe first regression term punishes the difference between the scores of ground-truth and predic\\x02tion. The second term is pair-wise max-margin loss [45], which encourages the predicted ranking\\r\\nscores of a stock pair to have the same relative order as the ground-truth. The similar max-margin\\r\\nloss has been used in several applications such as recommendation [38] and knowledge based\\r\\ncompletion [34] and demonstrated good performance in ranking tasks. Minimizing our proposed\\r\\ncombined loss will force the prediction ranking scores to be close to both 1) the return ratios of\\r\\nstocks in terms of absolute values, and 2) the relative orders of return ratios among stocks, so as\\r\\nto facilitate investors making better investment decisions. On one hand, correct relative order of\\r\\nstocks could help to select the investment targets (i.e., the top ranked stocks). On the other hand,\\r\\nthe accurate prediction of return ratio would facilitate deciding the timing of investment since the\\r\\ntop ranked stocks are valuable targets only when the return ratios would largely increase.\\r\\n3.2 Temporal Graph Convolution\\r\\nGiven N stocks with their sequential embeddings E\\r\\nt ∈ RN ×U (i.e., the output of sequential em\\x02bedding layer) and their multi-hot binary relation encodings A ∈ R\\r\\nN ×N ×K , the aim of Temporal\\r\\nGraph Convolution is to learn revised embeddings E\\r\\nt ∈ R\\r\\nN ×U that encode the relation information.\\r\\nInstead of directly presenting the formulation of TGC, we detail how we design the component\\r\\nto shed some lights on its rationale. Lastly, we discuss its connection with existing graph-based\\r\\nlearning methods.\\r\\na) Uniform Embedding Propagation. Our first inspiration comes from the link analysis research,\\r\\nwhere in a graph, the impact of a vertex on another one can be captured by propagating information\\r\\non the graph. A well-known example is the PageRank [31] method that propagates the importance\\r\\nscore of a vertex to its connected vertices. Since a stock relation encodes certain similarity informa\\x02tion between two connected stocks, we consider relating their embeddings through the similar\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :9\\r\\npropagation process as in link analysis:\\r\\ne\\r\\nt\\r\\ni\\r\\n=\\r\\nÕ\\r\\n{j |sum(aji)>0}\\r\\n1\\r\\ndj\\r\\ne\\r\\nt\\r\\nj\\r\\n, (9)\\r\\nwhere sum(aji) is the sum of all elements in the relation vector aji (recall that aji is a multi-hot binary\\r\\nvector where each element denotes whether the corresponding type of relation exists between j and\\r\\ni). The condition sum(aji) > 0 ensures that only stocks have at least one relation will be considered.\\r\\ndjis the number of stocks satisfying the condition sum(aji) > 0. After such a propagation in the\\r\\nembedding space, the relational embedding e\\r\\nt\\r\\ni\\r\\nencodes the impacts coming from other stocks that\\r\\nhave relations with stock i at time t.\\r\\nb) Weighted Embedding Propagation. Consider that different relations between two stocks may\\r\\nhave varying impacts on their prices, we apply a non-uniform coefficient when propagating the\\r\\nembeddings:\\r\\ne\\r\\nt\\r\\ni\\r\\n=\\r\\nÕ\\r\\n{j |sum(aji)>0}\\r\\nд(aji)\\r\\ndj\\r\\ne\\r\\nt\\r\\nj\\r\\n, (10)\\r\\nwhere д(aji) is a mapping function that aims to learn the impact strength of the relations in aji, and\\r\\nwe term it as the relation-strength function. As an example, suppose we have two relations named\\r\\nsupplier_customer and same_industry, and three stocks j, i, and k. Given that stock j is a supplier of\\r\\nstock i while stock k is in the same industry as stock i, we can encode their relations as two different\\r\\nvectors: aji = [1, 0] and aki = [0, 1]. We can see that by feeding different relation vectors into a\\r\\nlearnable relation-strength function for different stock pairs, we allow the embedding propagation\\r\\nprocess to account for both the topology of relation graph and the semantics of relations.\\r\\nc) Time-aware Embedding Propagation. A limitation of the above weighted propagation process\\r\\nis that the relation-strength function returns a fixed weight for a given relation vector aji regardless\\r\\nthe evolution across different time-steps. As stock market is highly dynamic such that the status of\\r\\na stock and the strength of a relation are continuously evolving, assuming a relation vector to have\\r\\na static weight limits the modeling fidelity. For instance, in the previous example of Figure 2(b), the\\r\\nsupplier_customer relation between Apple Inc. and Lens Technology Co Ltd has a larger impact\\r\\non the Lens’s stock price in the period of releasing new version of iPhone than usual. To address\\r\\nthis limitation, we propose to encode the temporal information into the relation-strength function\\r\\nand define the Time-aware Embedding Propagation process as follows:\\r\\ne\\r\\nt\\r\\ni\\r\\n=\\r\\nÕ\\r\\n{j |sum(aji)>0}\\r\\nд(aji, e\\r\\nt\\r\\ni\\r\\n, e\\r\\nt\\r\\nj\\r\\n)\\r\\ndj\\r\\ne\\r\\nt\\r\\nj\\r\\n, (11)\\r\\nwhich takes the sequential embeddings (note that they are time-sensitive) into account to estimate\\r\\nthe strength of a relation. Besides encoding the temporal information, another benefit of such a\\r\\ndesign is that sequential embedding also encodes the stock information. This allows the relation\\x02strength function to estimate the impact of a relation vector based on the stocks of concern, which\\r\\nis very desirable.\\r\\nNext we describe two designs of the time-sensitive relation-strength function, which differ in\\r\\nwhether to model the interaction between two stocks in an explicit or implicit manner.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:10 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n• Explicit Modeling. For the explicit way, we define the relation strength function as:\\r\\nд(aji, e\\r\\nt\\r\\ni\\r\\n, e\\r\\nt\\r\\nj\\r\\n) = e\\r\\nt\\r\\ni\\r\\nT\\r\\ne\\r\\nt\\r\\nj\\r\\n|{z}\\r\\nsimilarity\\r\\n× ϕ(w\\r\\nT\\r\\naji + b)\\r\\n| {z }\\r\\nrelation importance\\r\\n, (12)\\r\\nwhere w ∈ R\\r\\nK and b are model parameters to be learned; ϕ is an activation function6\\r\\n. The relation\\r\\nstrength of aji is determined by two terms – similarity and relation importance. Specifically, the\\r\\nfirst term measures the similarity between the two stocks at the current time-step. The intuition\\r\\nis that the more similar the two stocks are at the current time, it is more likely that their relations\\r\\nwill impact their prices in the near future. We use inner product to estimate the similarity, inspired\\r\\nby its effectiveness in modeling the similarity (interaction) between two entities (embeddings) in\\r\\nCollaborative Filtering [15]. The second term is a nonlinear regression model on the relations,\\r\\nwhere each element in w denotes the weight of a relation in general and b is a bias term. Since\\r\\nboth terms of this function are directly interpretable, we call it as Explicit Modeling.\\r\\n• Implicit Modeling. In this design, we feed the sequential embeddings and the relation vector\\r\\ninto a fully connected layer to estimate the relation strength:\\r\\nд(aji, e\\r\\nt\\r\\ni\\r\\n, e\\r\\nt\\r\\nj\\r\\n) = ϕ(w\\r\\nT\\r\\n[e\\r\\nt\\r\\ni\\r\\nT\\r\\n, e\\r\\nt\\r\\nj\\r\\nT\\r\\n, aji\\r\\nT\\r\\n]\\r\\nT + b), (13)\\r\\nwhere w ∈ R\\r\\n2U +K and b are model parameters to be learned; ϕ is an activation function same as\\r\\nthe one in Equation 12. Then we normalize the outputs using a softmax function, which also\\r\\nendows it with more non-linearities. Since this way of interaction is implicitly captured by the\\r\\nparameters, we call it as Implicit Modeling.\\r\\n3.2.1 Connection with Graph-based Learning. The embedding propagation is equivalent to the\\r\\ngraph convolutional network (GCN). To show the relation, let us first construct a graph based\\r\\non the stock relation encodings At, where vertices represent stocks and edges connect vertices\\r\\nwith at least one relation, i.e., we connect vertex i and j if they satisfy the condition sum(aij) > 0.\\r\\nIf we represent the graph with an adjacency matrix A and normalize it by column, the Uniform\\r\\nEmbedding Propagation (i.e., Equation 10) has exactly the same effect as the state-of-the-art graph\\r\\nconvolutional operation (i.e., f (F, X) = AX; details see Section 2.2.1). However, GCN cannot capture\\r\\nthe temporal evolution properties as designed in our TGC, since the adjacency matrix A has to be\\r\\nfixed in GCN. As such, our proposed operation can be seen as generalizing the GCN by specifically\\r\\nmodeling the temporal patterns, thus we term it as the Temporal Graph Convolution.\\r\\n4 DATA COLLECTION\\r\\nMost existing works evaluate stock prediction on dozens of stocks, and there lacks a large stock\\r\\ndataset for an extensive evaluation. As such, we consider constructing data by ourselves, which is\\r\\naccessible through: https://github.com/hennande/Temporal_Relational_Stock_Ranking. Specifically,\\r\\nwe collect the stocks from the NASDAQ and NYSE markets that have transaction records between\\r\\n01/02/2013 and 12/08/2017, obtaining 3, 274 and 3, 163 stocks respectively. Note that we select these\\r\\ntwo markets for their representative properties that NASDAQ is more volatile whereas NYSE is\\r\\nmore stable [33]. Furthermore, we perform a filtering on the stocks by retaining the stocks satisfying\\r\\nthe two conditions: 1) have been traded on more than 98% of trading days since 01/02/2013; 2)\\r\\nhave never been traded at less than five dollars per share during the collection period. It should be\\r\\nnoted that the first condition is based on concerns that intermittent sequences may bring abnormal\\r\\npatterns; the second condition ensures that the selected stocks are not penny stocks7, which are\\r\\n6Note that we employ the leaky rectifier [25] with a slope of 0.2 as the activation function in our implementation.\\r\\n7https://www.sec.gov/fast-answers/answerspennyhtm.html\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :11\\r\\nTable 3. Statistics of the sequential data.\\r\\nMarket Stocks#\\r\\nTraining Days#\\r\\n01/02/2013\\r\\n12/31/2015\\r\\nValidation Days#\\r\\n01/04/2016\\r\\n12/30/2016\\r\\nTesting Days#\\r\\n01/03/2017\\r\\n12/08/2017\\r\\nNASDAQ 1,026 756 252 237\\r\\nNYSE 1,737 756 252 237\\r\\ntoo risky for general investors as suggested by the U.S. Securities and Exchange Commission.\\r\\nThis results in 1, 026 NASDAQ and 1, 737 NYSE stocks for our experiments. For these stocks, we\\r\\ncollect three kinds of data: 1) historical price data, 2) sector-industry relations, and 3) Wiki relations\\r\\nbetween their companies such as supplier-consumer relation and ownership relation. Next, we\\r\\npresent the details of these data.\\r\\n4.1 Sequential Data\\r\\nFollowing [42], we set the prediction frequency as daily-level. Under our problem formulation, we\\r\\naim to predict a ranking list of stocks for the following trading day, based on the daily historical\\r\\ndata in the last S trading days. As the return ratio of a stock indicates the expected revenue of the\\r\\nstock, we set the ground-truth ranking score of stock i as its 1-day return ratio r\\r\\nt+1\\r\\ni = (p\\r\\nt+1\\r\\ni −p\\r\\nt\\r\\ni\\r\\n)/p\\r\\nt\\r\\ni\\r\\nwhere p\\r\\nt\\r\\ni\\r\\nis the closing price at day t. To calculate the ground-truth, we first collect the daily closing\\r\\nprice of each stock ranging from 01/02/2013 and 12/08/2017. After the collection, we normalize the\\r\\nprice of each stock via dividing it by its maximum value throughout the entire 2013-2017 dataset. In\\r\\naddition to the normalized closing price, we calculate four more sequential features: 5, 10, 20, and 30\\r\\ndays moving averages which represent the weekly and monthly trends. Following the existing work\\r\\nof stock prediction [42], we chronologically separate the sequential data into three time periods for\\r\\ntraining (2013-2015), validation (2016), and evaluation (2017), respectively, and summarize the basic\\r\\nstatistics in Table 3. As can be seen, there are 756, 252, and 237 trading days in training, validation,\\r\\nand evaluation, respectively.\\r\\n4.2 Stock Relation Data\\r\\n4.2.1 Sector-Industry relations. Observing the trends that stocks under the same industry are\\r\\nsimilarly influenced by the prospect of the industry, we collect the sector-industry relation between\\r\\nstocks. In NASDAQ and NYSE, each stock is classified into a sector and an industry as illustrated\\r\\nin Figure 3 in which stocks in the same industry are organized under the corresponding industry\\r\\nnode. We collect the hierarchy structure of NASDAQ and NYSE stocks from the official company\\r\\nlist maintained by NASDAQ Inc.8and extract industry relations for each stock pair under the same\\r\\nindustry node, such as (GOOGL; Computer Software: Programming, Data Processing; FB). The specific\\r\\nrelations extracted are detailed in the Appendix A.1 at the end of this paper. After the extraction,\\r\\nwe count the number of industry (i.e., relation) types occurred in each market, and the ratio of\\r\\nstock pairs having industry relations and summarize them in Table 4. As can be seen, there are\\r\\n112 and 130 types of industry relations between stock pairs in NASDAQ and NYSE, respectively.\\r\\nMoreover, the industry relation data is sparse since less than 10% of stock pairs have at least one\\r\\ntype of industry relation in both stock markets.\\r\\n4.2.2 Wiki Company-based Relations. As rich sources of entity relations, knowledge bases\\r\\ncontain company entities and company relations, which might reflect the impact across stocks. As\\r\\nsuch, we extract the first-order and second-order company relations from Wikidata [37], one of\\r\\nthe biggest and most active open domain knowledge bases with more than 42 million items (e.g.,\\r\\n8https://www.nasdaq.com/screening/industries.aspx\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:12 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\nFig. 3. Illustration of sector-industry hierarchy of companies in NASDAQ and NYSE.\\r\\nTable 4. Statistics of sector-industry relation and Wiki relation data in the NASDAQ and NYSE datasets.\\r\\nSector-Industry Relation Wiki Relation\\r\\nRelation Types# Relation Ratio (Pairwise) Relation Types# Relation Ratio (Pairwise)\\r\\nNASDAQ 112 5.00% 42 0.21%\\r\\nNYSE 130 9.37% 32 0.30%\\r\\nGoogle \\r\\nLLC\\r\\nAlphabet \\r\\nInc.\\r\\nParent \\r\\ncompany\\r\\nCitigroup\\r\\nInc.\\r\\nBlackRock\\r\\nInc.\\r\\nOwned \\r\\nby\\r\\nProduce\\r\\nBoeing 747\\r\\nBoeing \\r\\nInc.\\r\\nUnited \\r\\nAirlines, Inc.\\r\\nItem \\r\\noperated\\r\\n(a) First-order relation (b) Second-order relation\\r\\nFig. 4. Examples of the first-order and second-order company relations extracted from Wikidata.\\r\\nAlphabet Inc.) and 367 million statements (e.g., Alphabet Inc.; founded by; Larry Page) in the format\\r\\nof (subject; predicate; object)\\r\\n9\\r\\n. As shown in Figure 4, company i has a first-order relation with j if\\r\\nthere is a statement that has i and j as the subject and object, respectively. Companies i and j have\\r\\na second-order relation if they have statements sharing the same object, such as Boeing Inc. and\\r\\nUnited Airlines, Inc. have different statements towards Boeing 747. After an exhausted exploration\\r\\nof a recent dump of Wikidata (01/05/2018), we obtain 5 and 53 types of first-order and second-order\\r\\nrelations, respectively10. The detailed description of these relations is elaborated in the Appendix\\r\\nA.2 at the end of this paper. We then summarize the count of relation types and the ratio of stock\\r\\npairs with at least one Wiki company-based relation in Table 4. As can be seen, there are 42 and 32\\r\\ntypes of company relations occurring between stock pairs in NASDAQ and NYSE, respectively.\\r\\n5 EXPERIMENT\\r\\nTo the best of our knowledge, our work is the first one to incorporate stock relations into the\\r\\nmodels for stock prediction, especially neural network-based ones. As such, in this section, we\\r\\nconduct experiments with the aim of answering the following research questions:\\r\\n(1) RQ1: How is the utility of formulating the stock prediction as a ranking task? Can our\\r\\nproposed RSR solution outperform state-of-the-art stock prediction solutions?\\r\\n(2) RQ2: Do stock relations enhance the neural network-based solution for stock prediction? How\\r\\nis the effectiveness of our proposed TGC component compared to conventional graph-based\\r\\nlearning?\\r\\n(3) RQ3: How does our proposed RSR solution perform under different back-testing strategies?\\r\\n9https://www.mediawiki.org/wiki/Wikibase/DataModel/JSON\\r\\n10We manually filter out less informative relations such as located at the same timezone.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :13\\r\\nIn what follows, we first present the experimental settings, followed by answering the above\\r\\nthree research questions.\\r\\n5.1 Experimental Setting\\r\\n5.1.1 Evaluation Protocols. Following [9], we adopt a daily buy-hold-sell trading strategy to\\r\\nevaluate the performance of stock prediction methods regarding the revenue. On each trading day\\r\\nt + 1 during the testing period (from 01/03/2017 to 12/08/2017), we simulate a trader using a stock\\r\\nprediction method to trade in the following way:\\r\\n(1) When the market closes at trading day t: The trader uses the method to get the prediction,\\r\\na ranking list with predicted return ratio of each stock. The trader buys the stock with the\\r\\nhighest expected revenue (i.e., ranked at the top).\\r\\n(2) When the market closes at trading day t + 1: The trader sells the stock purchased at day\\r\\nt.\\r\\nIn calculating the cumulative investment return ratio, we follow several simple assumptions:\\r\\n(1) The trader spends the same amount of money (e.g., 50 thousand dollars) on every trading day.\\r\\nWe make this assumption to eliminate the temporal dependency of the testing procedure for a fair\\r\\ncomparison. (2) The market is always sufficiently liquid such that the buying order gets filled at\\r\\nthe closing price of day t and the selling price is the closing price of day t + 1. (3) The transaction\\r\\ncosts are ignored since the costs for trading US stocks through brokers are quite cheap no matter\\r\\ncharging by trades or shares. For instance, Fidelity Investments and Interactive Brokers charge\\r\\nonly 4.95 dollars per trade and 0.005 dollar per share, respectively11.\\r\\nSince the target is to accurately predict the return ratio of stocks and appropriately rank the\\r\\nrelative order of stocks, we employ three metrics, Mean Square Error (MSE), Mean Reciprocal Rank\\r\\n(MRR), and the cumulative investment return ratio (IRR), to report model performance. MSE has\\r\\nbeen widely used for evaluating regression tasks such as stock price prediction [22, 28, 42]. We thus\\r\\ncalculate the MSE over all stocks on every trading day within the testing period. MRR [? ] is a widely\\r\\nused metric for ranking performance evaluation. Here, we calculate the average reciprocal rank of\\r\\nthe selected stock over the testing days. Since directly reflecting the effect of stock investment, IRR\\r\\nis our main metric, which is calculated by summing over the return ratios of the selected stock\\r\\non each testing day. Smaller value of MSE (≥ 0) and larger value of MRR ([0, 1]) and IRR indicate\\r\\nbetter performance. For each method, we repeat the testing procedure five times and report the\\r\\naverage performance to eliminate the fluctuations caused by different initializations.\\r\\n5.1.2 Methods. We compare with the following stock price prediction baselines with regression\\r\\nformulation:\\r\\n• SFM [42]: This method is the state-of-the-art stock price prediction method. It takes the historical\\r\\nclosing prices as input and decomposes the prices into signals of different frequencies with a\\r\\nDiscrete Fourier Transform (DFT). It then feeds the DFT coefficients into an extended LSTM\\r\\nwith separate memory states for different frequencies to learn the frequency-aware sequential\\r\\nembeddings, which are fed into a FC layer to make the prediction.\\r\\n• LSTM [4]: This method is the vanilla LSTM, which operates on the sequential data including\\r\\nclosing prices and moving averages of 5, 10, 20, and 30 days, to obtain a sequential embedding;\\r\\nand then a FC layer is used to make prediction of the return ratio.\\r\\nIt should be noted that we ignore the potential baselines based on time-series models and shallow\\r\\nmachine learning models, since they have been reported to be less effective than SFM and LSTM in\\r\\n11https://www.stockbrokers.com/guides/commissions-fees\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:14 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\nTable 5. Performance comparison between the solutions with regression formulation (SFM and LSTM) and\\r\\nranking formulation (Rank_LSTM).\\r\\nNASDAQ NYSE\\r\\nMSE MRR IRR MSE MRR IRR\\r\\nSFM 5.20e-4±5.77e-5 2.33e-2±1.07e-2 -0.25±0.52 3.81e-4±9.30e-5 4.82e-2±4.95e-3 0.49±0.47\\r\\nLSTM 3.81e-4±2.20e-6 3.64e-2±1.04e-2 0.13±0.62 2.31e-4±1.43e-6 2.75e-2±1.09e-2 -0.90±0.73\\r\\nRank_LSTM 3.79e-4±1.11e-6 4.17e-2±7.50e-3 0.68±0.60 2.28e-4±1.16e-6 3.79e-2±8.82e-3 0.56±0.68\\r\\nseveral previous works [4, 17, 42]. Moreover, we also compare with several methods with ranking\\r\\nformulation:\\r\\n• Rank_LSTM: We remove the relational embedding layer of the proposed RSR to obtain this\\r\\nmethod, i.e., this method ignores stock relations.\\r\\n• Graph-based ranking (GBR): According to Equation 2, we add the graph regularization term\\r\\nto the loss function of Rank_LSTM, which smooths predicted return ratios over the graph of\\r\\nstock relations. In the graph, we connect a pair of vertices (i.e., stocks) having at least one type of\\r\\nrelations.\\r\\n• GCN [20]: GCN is the state-of-the-art graph-based learning method. We obtain this method by\\r\\nreplacing the TGC layer of our proposed RSR with a GCN layer. The graph of stock relations in\\r\\nGBR is fed into the GCN layer.\\r\\n• RSR_E: Our proposed RSR with explicit modeling in the TGC.\\r\\n• RSR_I: Our proposed RSR with implicit modeling in the TGC.\\r\\n5.1.3 Parameter Settings. We implement the models with TensorFlow12 except SFM of which\\r\\nwe use the original implementation13. It is worth mentioning that the implementations can be\\r\\naccessed through: https://github.com/hennande/Temporal_Relational_Stock_Ranking. We employ\\r\\ngrid search to select the optimal hyperparameters regarding IRR for all methods. For SFM, we\\r\\nfollow the original setting in [42], optimizing it by RMSProp with a learning rate of 0.5, and tuning\\r\\nthe number of frequencies and hidden units within {5, 10, 15} and {10, 20, 30}, respectively. For\\r\\nall other methods, we apply the Adam [19] optimizer with a learning rate of 0.001. We tune two\\r\\nhyperparameters for LSTM, the length of sequential input S and the number of hidden units U ,\\r\\nwithin {2, 4, 8, 16} and {16, 32, 64, 128}, respectively. Besides S andU , we further tune α in Equation\\r\\n8, which balances the point-wise and pair-wise terms; specifically, we tune α within {0.1, 1, 10}\\r\\nfor Rank_LSTM, GCN, RSR_E, and RSR_I. We further tune the λ of the regularization term in\\r\\nGBR within {0.1, 1, 10}.\\r\\n5.2 Study of Stock Ranking Formulation (RQ1)\\r\\nTable 5 summarizes the performance of baselines in regression fashion and Rank_LSTM our basic\\r\\nsolution of stock ranking w.r.t. MSE, MRR, and IRR, from which we have the following observations:\\r\\n• Rank_LSTM outperforms both SFM and LSTM on the two markets with great improvement\\r\\nw.r.t. IRR (>14%). It verifies the advantage of the stock ranking solutions and answers RQ1 that\\r\\nstock ranking is a promising formulation of stock prediction. Moreover, it indicates the potential\\r\\nof advanced learning-to-rank techniques in solving the stock prediction task.\\r\\n• However, Rank_LSTM fails to consistently beat SFM and LSTM regarding all evaluation mea\\x02sures, its performance on NYSE w.r.t. MRR is worse than SFM. The reason could be attributed to\\r\\nminimizing the combination of point-wise and pair-wise losses, which would lead to a tradeoff\\r\\nbetween accurately predicting absolute value of return ratios and their relative order.\\r\\n12https://www.tensorflow.org/\\r\\n13https://github.com/z331565360/State-Frequency-Memory-stock-prediction\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :15\\r\\n(a) NASDAQ (b) NYSE\\r\\nFig. 5. Performance comparison of Rank_LSTM, SFM, and LSTM regarding IRR.\\r\\nTable 6. Performance comparison among relational ranking methods with industry relations.\\r\\nNASDAQ NYSE\\r\\nMSE MRR IRR MSE MRR IRR\\r\\nRank_LSTM 3.79e-4±1.11e-6 4.17e-2±7.50e-3 0.68±0.60 2.28e-4±1.16e-6 3.79e-2±8.82e-3 0.56±0.68\\r\\nGBR 5.80e-3±1.20e-3 4.46e-2±5.20e-3 0.57±0.29 2.29e-4±2.02e-6 3.43e-2±6.26e-3 0.68±0.31\\r\\nGCN 3.80e-4±2.24e-6 3.45e-2±8.36e-3 0.24±0.32 2.27e-4±1.30e-7 5.01e-2±5.56e-3 0.97±0.56\\r\\nRSR_E 3.82e-4±2.69e-6 3.16e-2±3.45e-3 0.20±0.22 2.29e-4±2.77e-6 4.28e-2±6.18e-3 1.00±0.58\\r\\nRSR_I 3.80e-4±7.90e-7 3.17e-2±5.09e-3 0.23±0.27 2.26e-4±5.30e-7 4.51e-2±2.41e-3 1.06±0.27\\r\\n• The performance w.r.t. IRR varies a lot under different runs of a method. It is reasonable since\\r\\nthe absolute value of daily return ratio varies from 0 to 0.98 in our dataset, which means that a\\r\\ntiny switch of the top 2 ranked stocks may lead to a huge change of the IRR. Such results also\\r\\nindicate that learning to rank techniques emphasizing the top-ranked stocks is worthwhile to be\\r\\nexplored in the future.\\r\\n• The performance of LSTM on the NYSE market w.r.t. IRR is unexpectedly bad. We repeat the\\r\\nparameter tuning and testing procedure several times and find that LSTM could achieve better\\r\\nperformance (with IRR value between 0.1 and 0.2) with other settings of hyperparameters.\\r\\nHowever, the selected setting always beats the others on the validation. This result indicates the\\r\\npotential difference between the validation and testing.\\r\\nFigure 5 illustrates the procedure of back-testing regarding the cumulative return ratios. As can\\r\\nbe seen, in all cases, the curves are volatile, which indicates that selecting only one stock from\\r\\nmore than 1,000 is a highly risk operation. Consequently, it also suggests the worth of introducing\\r\\nrisk-oriented criteria into stock ranking tasks in the future.\\r\\n5.3 Impact of Stock Relations (RQ2)\\r\\nEffect of Industry Relations: Table 6 shows the performance of methods considering the industry\\r\\nrelation of stocks. We can see that:\\r\\n• Considering industry relations is more beneficial to stock ranking on NYSE as compared to NAS\\x02DAQ. It could be attributed to that the industry relations reflect more of long-term correlations\\r\\nbetween stocks, since NASDAQ is considered as a much more volatile market as compared to\\r\\nNYSE and dominated by short-term factors [33].\\r\\n• On NYSE, all methods considering stock relations, i.e., GBR, GCN, RSR_E, and RSR_I, out\\x02perform Rank_LSTM w.r.t. IRR. Considering that all these methods take Rank_LSTM as the\\r\\nbuilding block, this result verifies the effectiveness of encoding stock relations in stock prediction.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:16 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n(a) NASDAQ (b) NYSE\\r\\nFig. 6. Back-testing procedure of relational ranking methods with industry relations regarding IRR.\\r\\n• Moreover, RSR_E and RSR_I achieve improvement over GCN and GBR. This result verifies\\r\\nthe effectiveness of the proposed Temporal Graph Convolution as compared to the traditional\\r\\nmodeling of relational data. Considering that GCN and GBR utilize a static graph to represent\\r\\nstock relations, the result also indicates the rationale of considering temporal property in stock\\r\\nrelation modeling.\\r\\n• Again, the performance regarding different evaluation measures is inconsistent. We speculate\\r\\nthe reason is that we tune the hyperparameters regarding IRR, which focuses more on correct\\r\\nranking on testing days with high return ratios. For instance, correct prediction on a trading\\r\\nday with ground truth return ratio of 0.5 would lead to higher IRR than correct predictions in\\r\\nten trading days with return ratio of 0.01. As such, a model achieves better IRR could achieve\\r\\nsuboptimal MSE and MRR.\\r\\nFigure 6 illustrates the IRR curve of the compared methods in the back-testing. Again, the curves\\r\\nare volatile of which the reason has been discussed in Section 5.2. On NYSE, the IRR of the methods\\r\\npresents huge increases on the 206-th and 209-th trading days when the best-performed stock\\r\\nexhibits return ratios larger than 0.6. This result further highlights the importance of accurately\\r\\npredicting both the return ratio of single stock and the relative order of stocks. Note that capturing\\r\\nrare opportunities by precisely ranking the stocks on trading days with huge change of return\\r\\nratios would lead to satisfied IRR.\\r\\nEffect of Wiki Relations: Similarly, Table 7 shows the results of considering the Wiki relation\\r\\nof stocks. We observe that:\\r\\n• In all cases, our proposed RSR_E and RSR_I achieve the best performance w.r.t. IRR. It further\\r\\ndemonstrates the effectiveness of the approach we model stock relations, that is, the TGC\\r\\ncomponent.\\r\\n• All methods considering Wiki relations outperform Rank_LSTM with a significant improvement\\r\\n(>0.09) w.r.t. IRR on NYSE. It again verifies the merit of encoding stock relations in stock prediction\\r\\nand the effectiveness of the RSR framework.\\r\\nFigure 7 shows the associated back-testing procedure, which presents similar trends as the results\\r\\nof considering industry relations (Figure 6).\\r\\nSector-wise Performance: Taking RSR_I as an example, we then investigate whether the\\r\\nperformance is sensitive to sectors via evaluating its performance over the stocks in each sector,\\r\\ni.e., separately conducting back-testing for each sector. Recall that, on NASDAQ, the performance\\r\\nof RSR_I considering industry relations is not promising (with an IRR of 0.23). We take this case\\r\\nto investigate the sector-wise performance, which is presented in Table 8. Note that we only show\\r\\nthe performance on sectors with the top-5 most stocks. We can see that, the method only achieves\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :17\\r\\nTable 7. Performance comparison among relational ranking methods with Wiki relations.\\r\\nNASDAQ NYSE\\r\\nMSE MRR IRR MSE MRR IRR\\r\\nRank_LSTM 3.79e-4±1.11e-6 4.17e-2±7.50e-3 0.68±0.60 2.28e-4±1.16e-6 3.79e-2±8.82e-3 0.56±0.68\\r\\nGBR 3.80e-4±2.40e-7 3.32e-2±4.50e-3 0.33±0.34 2.26e-4±4.20e-7 3.64e-2±5.35e-3 0.65±0.27\\r\\nGCN 3.79e-4±9.70e-7 3.24e-2±3.21e-3 0.11±0.06 2.26e-4±6.60e-7 3.99e-2±1.03e-2 0.74±0.30\\r\\nRSR_E 3.80e-4±7.20e-7 3.94e-2±8.15e-3 0.81±0.85 2.29e-4±2.77e-6 4.28e-2±6.18e-3 0.96±0.47\\r\\nRSR_I 3.79e-4±6.60e-7 4.09e-2±5.18e-3 1.19±0.55 2.26e-4±1.37e-6 4.58e-2±5.55e-3 0.79±0.34\\r\\n(a) NASDAQ (b) NYSE\\r\\nFig. 7. Performance comparison of relational ranking methods with Wiki relations regarding IRR.\\r\\nTable 8. Performance of RSR_I on ranking stocks in different sectors of NASDAQ w.r.t. IRR.\\r\\nSector Finance Technology N/A Consumer Services Health Care\\r\\n#Stocks 222 182 156 117 91\\r\\nIRR 0.33 1.12 -0.70 0.57 -0.85\\r\\nTable 9. Impacts of different types of Wiki relation regarding the Relative Performance Decrease (RPD) of\\r\\nRSR_I on NASDAQ as removing the selected relation.\\r\\nRelation P1056_P1056 P463_P463 P452_P452 P361_P361 P1056_P452\\r\\nID in Table 14 46 R38 35 31 45\\r\\n#Occurrences 130 58 506 1,194 10\\r\\nRPD -144.00% -70.13% -21.66% -17.52% -15.71%\\r\\nacceptable performance with an IRR of 1.12 on the Technology sector. This result further indicates\\r\\nthe less effectiveness of considering industry relation on the NASDAQ market, which is coherent\\r\\nwith the results in Table 6. In addition, it also suggests the separate consideration of stocks in each\\r\\nsingle sector.\\r\\nImportance of Each Type of Wiki relation: By comparing the performance of RSR_I when\\r\\na type of relation is removed, we investigate the importance of different types of Wiki relations.\\r\\nTable 9 shows the relative performance decrease w.r.t. IRR as compared to RSR_I with all Wiki\\r\\nrelations as input (NASDAQ). Note that we only present the relations with the top-5 largest\\r\\nperformance decreases. We can see that the most importance relation is the P1056_P1056 of which\\r\\nP1056 denotes the predicate of product or material produced. Mainly, two stocks have the relation of\\r\\nP1056_P1056 means the associated companies collaborate on producing the same product. The high\\r\\nimpact is reasonable, considering that collaborated companies have closely connected revenues\\r\\nand would be affected by similar factors.\\r\\nBrief Conclusion: a) Considering stock relations is helpful for stock ranking, especially on\\r\\nthe stable markets (e.g., NYSE). 2) The proposed TGC is a promising solution for encoding stock\\r\\nrelations. 3) It is important to consider appropriate relations suitable for the target market, for\\r\\nexample, encoding industry relations on NASDAQ is a suboptimal choice.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:18 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n(a) NASDAQ-Industry (b) NASDAQ-Wiki (c) NYSE-Industry (d) NYSE-Wiki\\r\\nFig. 8. Comparison on back-testing strategies (Top1, Top5, and Top10) w.r.t. IRR based on prediction of\\r\\nRSR_I.\\r\\n5.4 Study on Back-testing Strategies (RQ3)\\r\\nWe then investigate the performance of our proposed methods under three different back-testing\\r\\nstrategies, named Top1, Top5, and Top10, buying stocks with top-1, 5, 10 highest expected revenue,\\r\\nrespectively. For instance, with the back-testing strategy of Top10, we equally split our budget to\\r\\ntrade the top-10 ranked stocks on each testing day. Note that we accordingly calculate the IRR by\\r\\nsumming the mean return ratio of the 10 selected stocks on each testing day. Figure 8 illustrates\\r\\nthe performance comparison of these strategies with the predictions of RSR_I. Similar trends are\\r\\nobserved on the predictions of RSR_E, which are omitted for the consideration of saving space.\\r\\nFrom the figure, we have the following observations:\\r\\n• RSR_I (Figure 8(a)) fails to achieve expected performance with different back-testing strategies\\r\\nunder the NASDAQ-Industry setting (i.e., ranking stocks in NASDAQ and modeling their industry\\r\\nrelations). It further indicates the less effectiveness of industry relations on NASDAQ.\\r\\n• In the other cases, the performance of Top1, Top5, and Top10 on most testing days follows the\\r\\norder of Top1 > Top5 > Top10, i.e., the Top1 and Top10 achieve the highest and lowest IRR,\\r\\nrespectively. The reason could be that the ranking algorithm could accurately rank the relative\\r\\norder of stocks regarding future return ratios. Once the order is accurate, buying and selling the\\r\\nstock with higher expected profit (e.g., , the top-1 ranked one) would achieve higher cumulative\\r\\nreturn ratio.\\r\\nConsidering that it would easier to achieve better performance regarding IRR in a bullish market,\\r\\nwe further compare the performance of our method with two market indices, S&P 500 Index\\r\\nand Dow Jones Industrial Average Index (DJI). Moreover, in order to better judge the achieved\\r\\nperformance, we compare two more ideal investment strategies: a) selecting the stocks with highest\\r\\nreturn ratio (e.g., Top10) in the testing period from the whole market; and b) among the stocks\\r\\ntraded by the proposed method, selecting the stocks with highest return ratio in the testing period.\\r\\nTable 10 shows the performance of the compared investment strategies w.r.t. IRR. From which, we\\r\\nhave the following observations:\\r\\n• In the testing period, the stock market is bullish, which suggests future exploration of the proposed\\r\\nmethod in bearish market. In addition, noting that market indices are competitive portfolios\\r\\n(investment strategies)14, achieving IRR higher than market indices justifies the effectiveness of\\r\\nthe proposed method.\\r\\n• When trading the same number of stocks (e.g., Top5), the performance of the proposed method\\r\\npresents a significant gap towards the ideal investment strategies. This result is acceptable since\\r\\naccurately selecting the stock performing best in the range of almost one year is non-trivial, but\\r\\nreflects the huge improvement space for stock prediction methods.\\r\\n14From 2008 to 2017, the S&P 500 achieved a return ratio of 125.8%, beating most of the portfolios of funds of hedge funds.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :19\\r\\nTable 10. Performance of RSR_I as compared to market indices and ideal portfolios.\\r\\nNASDAQ NYSE\\r\\nTop1 Top5 Top10 Top1 Top5 Top10\\r\\nMarket 3.40 2.36 1.99 2.42 1.90 1.47\\r\\nSelected 1.63 0.81 1.10 2.24 1.78 1.39\\r\\nRSR_I 1.19 0.40 0.27 1.06 0.18 0.26\\r\\nS&P 500 0.17\\r\\nDJI 0.22\\r\\n• The Top1 version of our method, i.e., trading the top-1 ranked stock on each trading day, achieves\\r\\nan IRR comparable to the investment strategy Selected under Top10. This further justifies the\\r\\ncompetitivity of our proposed method.\\r\\n6 RELATED WORK\\r\\nOur work is directly related to the recent work on stock prediction, graph-based learning, and\\r\\nknowledge graph embedding.\\r\\n6.1 Stock Prediction\\r\\nRecent work on stock prediction can be separated into two main categories: stock price regression\\r\\nand stock trend classification. On one hand, Bao et al. predicted the 1-day ahead closing price of\\r\\nstocks with historical prices as input. The authors viewed the historical price as a signal and\\r\\ndecomposed the historical price into multiple frequencies with a Wavelet Transform. They then\\r\\nfiltered out noises in the frequency domain with a Stacked Autoencoder (SAE) and fed the output\\r\\nof SAE to an LSTM to make prediction [4]. Zhang et al. devised an extension of LSTM, which\\r\\ndecomposes the historical prices into frequency domain with a Discrete Fourier Transform and\\r\\nequips each frequency with a memory state to capture the patterns in different frequencies [42].\\r\\nInstead of directly modeling the stock prices, Alberg and Lipton used a combination of an LSTM\\r\\nand Multi-Layer Perception to predict the future trend of fundamental indicators of a company and\\r\\ntrade the corresponding stock based on the predicted indicators [3].\\r\\nOn the other hand, Nguyen and Shirai proposed a stock trend classification solution, which learns\\r\\na topic distribution representation of each stock from posts mentioning it on stock message boards,\\r\\nand fed the topic representation into a Support Vector Machine to make the trend classification\\r\\n[29]. Under a similar classification setting, another line of research is classifying the trend of a\\r\\nstock from relevant financial news reports [17, 44? ? ? ]. For instance, Zhao et al. achieve it via\\r\\nconstructing a event causality network of news reports and learning news embeddings from the\\r\\ncausality networks, which is fed into a classification layer [44]. Taking financial news as input as\\r\\nwell, Hu et al. devised a neural network-based solution, named Hybrid Attention Networks, which\\r\\nleverages a hybrid attention mechanism to attentively fuse multiple news reports mentioning a\\r\\nstock into a joint representation [17]. In addition, textual contents mentioning stocks in social\\r\\nmedial are also used to forecast the movement of stocks [? ].\\r\\nHowever, none of the existing work is able to incorporate the rank/relative order among stocks\\r\\nregarding the expected revenue, tending to lead to suboptimal stock selections. Moreover, the\\r\\nexisting work either totally ignores stock relations or heuristically models such relations. For\\r\\ninstance, an intuitive consideration of sector-industry relation is to separately train a predictor for\\r\\nstocks under each sector [? ]. To the best of our knowledge, our work is to first one to leverage\\r\\ntechniques of learning-to-rank to solve the stock prediction task and inject the stock relations into\\r\\nthe learning framework with a new neural network component.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:20 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n6.2 Graph-based Learning\\r\\nIn the literature of graph-based learning, it has been intensively studied that incorporating the\\r\\nrelationship among entities into the learning procedure of the target task to achieve better per\\x02formance. Work on graph-based learning are mainly in two fashions: graph regularization and\\r\\ngraph convolution. On one hand, Zhu et al. proposed a regularization term based on directed and\\r\\nundirected simple graphs with pair-wise entity relations to smooth the predictions across the\\r\\ntopology of the graph [47]. Zhou et al. regularized the learning procedure of the target task with a\\r\\nhypergraph that captures the higher-order relations among entities [46]. On the other hand, Bruna\\r\\net al. proposed spectral graph convolutions to capture the local connection patterns in graphs\\r\\nand propagate information of locally connected vertices for better representations [6]. Upon the\\r\\nspectral graph convolution, several fast approximations have been proposed for accelerating [8, 20].\\r\\nHowever, most of the works of graph-based learning fail to handle the temporal evolution property\\r\\nof stock market. Consequently, they would suffer from severe information loss and achieve limited\\r\\nimprovement when directly applied to model stock relations.\\r\\n6.3 Knowledge Graph Embedding\\r\\nIn a similar line, modeling the relations of two entities (a.k.a. knowledge graph embedding) has\\r\\nbeen intensively studied in the literature of knowledge graphs. Bordes, et al. represented entities\\r\\nand relations with embedding vectors and transferred entity embeddings through adding relation\\r\\nembedding [5]. Similarly, Socher et al. represented relations as matrices and transferred entity\\r\\nembeddings via matrix multiplication [34]. Such techniques mainly focus on solving knowledge\\r\\ngraph-oriented problems such as knowledge graph completion, while we target on a different\\r\\nproblem setting of stock ranking. The idea of embedding propagation that revises stock sequential\\r\\nembeddings through stock relations is partially inspired by TransE, our TGC is more generic in\\r\\nterms of jointly capturing temporal properties and topologies of relations.\\r\\n7 CONCLUSIONS\\r\\nIn this paper, we formulated stock prediction as a ranking task and demonstrated the potential of\\r\\nlearning-to-rank methods for predicting stocks. To solve the problem, we proposed a Relational\\r\\nStock Ranking framework. The core of the framework is a neural network modeling component,\\r\\nnamed Temporal Graph Convolution, which can handle the impact between different stocks by\\r\\nencoding stock relations in a time-sensitive way. Experimental results on NASDAQ and NYSE\\r\\ndemonstrate the effectiveness of our solution — with three different back-testing strategies, the\\r\\nRSR framework outperforms the S&P 500 Index with significantly higher return ratio.\\r\\nAs mentioned in Section 5, we will explore the potential of emphasizing top-ranked entities\\r\\nwith more advanced learning-to-rank techniques. In addition, we will integrate risk management\\r\\ntechniques in finance into the RSR framework to force the predictions to be risk sensitive. Fur\\x02thermore, we will investigate the performance of RSR under multiple investment operations such\\r\\nas buy-hold-sell (aka. long position) and borrow-sell-buy (aka. short position). Moreover, we will\\r\\nintegrate alternative data such as financial news and social media contents into the predictive\\r\\nmodel. Lastly, considering that the proposed TGC is a general component to model relational\\r\\ndata, especially structured domain knowledge, we would like to explore the potential of TGC in\\r\\nenhancing the neural network solutions for tasks with such relational data, such as recommender\\r\\nsystem and product search.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :21\\r\\nA STOCK RELATION\\r\\nIn this appendix, we describe the details of stock relations (i.e., sector-industry relations and Wiki\\r\\ncompany-based relations) in our collected data (Section 4).\\r\\nA.1 Sector-Industry Relation\\r\\nWe extract 112 and 130 types of industry relations from the company classification hierarchy\\r\\nstructure of NASDAQ and NYSE stocks, respectively. Table 11 and 12 illustrates the specific industry\\r\\nrelations in NYSDAQ and NYSE markets, respectively.\\r\\nTable 11. Industry relations among 1,026 selected stocks from the NASDAQ market.\\r\\nSectors Industries Count of\\r\\nIndustries\\r\\nConsumer\\r\\nDurables\\r\\nOffice Equipment/Supplies/Services, Consumer Specialties,\\r\\nSpecialty Chemicals, Metal Fabrications, Consumer\\r\\nElectronics/Appliances, Building Products, Containers/\\r\\nPackaging, Miscellaneous manufacturing industries,\\r\\nAutomotive Aftermarket\\r\\n9\\r\\nTransportation Transportation Services, Air Freight/Delivery Services,\\r\\nTrucking Freight/Courier Services, Oil Refining/Marketing 4\\r\\nFinance\\r\\nSpecialty Insurers, Commercial Banks, Savings Institutions,\\r\\nReal Estate, Major Banks, Investment Managers, Investment\\r\\nBankers/Brokers/Service, Life Insurance, Finance: Consumer\\r\\nServices, Banks, Property-Casualty Insurers, Finance\\r\\nCompanies\\r\\n12\\r\\nPublic\\r\\nUtilities\\r\\nTelecommunications Equipment, Environmental Services,\\r\\nNatural Gas Distribution 3\\r\\nEnergy Electric Utilities: Central, Coal Mining, Oil & Gas Production 3\\r\\nMiscellaneous Business Services, Publishing, Multi-Sector Companies 3\\r\\nConsumer\\r\\nNon-Durables\\r\\nPlastic Products, Meat/Poultry/Fish, Beverages (Production/\\r\\nDistribution), Shoe Manufacturing, Packaged Foods, Package\\r\\nGoods/Cosmetics, Apparel, Farming/Seeds/Milling, Food\\r\\nDistributors, Specialty Foods, Recreational Products/Toys\\r\\n11\\r\\nHealth Care\\r\\nOphthalmic Goods, Medical/Nursing Services, Hospital/\\r\\nNursing Management, Biotechnology: In Vitro & In Vivo\\r\\nDiagnostic Substances, Biotechnology: Commercial Physical\\r\\n& Biological Resarch, Other Pharmaceuticals, Major\\r\\nPharmaceuticals, Medical Specialities, Medical Electronics,\\r\\nBiotechnology: Electromedical & Electrotherapeutic\\r\\nApparatus, Biotechnology: Biological Products, Medical/\\r\\nDental Instruments, Industrial Specialties\\r\\n13\\r\\nConsumer\\r\\nServices\\r\\nOther Consumer Services, Restaurants, Clothing/Shoe/\\r\\nAccessory Stores, Marine Transportation, Television\\r\\nServices, Consumer Electronics/Video Chains, Other\\r\\nSpecialty Stores, Home Furnishings, Diversified\\r\\nCommercial Services, Paper, Professional Services, Hotels/\\r\\nResorts, Rental/Leasing Companies, Real Estate Investment\\r\\nTrusts, Food Chains, Broadcasting, Books, Motor Vehicles,\\r\\nMovies/Entertainment, RETAIL: Building Materials,\\r\\nAdvertising, Catalog/Specialty Distribution, Services-Misc.\\r\\nAmusement & Recreation, Department/Specialty Retail Stores\\r\\n24\\r\\nBasic\\r\\nIndustries\\r\\nWater Supply, Miscellaneous, Forest Products, Precious\\r\\nMetals, Mining & Quarrying of Nonmetallic Minerals,\\r\\nEngineering & Construction, Major Chemicals\\r\\n7\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:22 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\nCapital\\r\\nGoods\\r\\nMilitary/Government/Technical, Biotechnology: Laboratory\\r\\nAnalytical Instruments, Electrical Products, Building Materials,\\r\\nRailroads, Ordnance And Accessories, Homebuilding,\\r\\nElectronic Components, Aerospace, Industrial Machinery/\\r\\nComponents, Construction/Ag Equipment/Trucks, Auto Parts:\\r\\nO.E.M., Steel/Iron Ore, Auto Manufacturing\\r\\n14\\r\\nTechnology\\r\\nComputer Manufacturing, Radio And Television Broadcasting\\r\\nAnd Communications Equipment, Computer Communications\\r\\nEquipment, Computer peripheral equipment, EDP Services,\\r\\nComputer Software: Programming, Data Processing, Computer\\r\\nSoftware: Prepackaged Software, Semiconductors, Retail:\\r\\nComputer Software & Peripheral Equipment\\r\\n9\\r\\nN/A N/A 1\\r\\nTable 12. Industry relations among 1,737 selected stocks from the NYSE market.\\r\\nSectors Industries Count of\\r\\nIndustries\\r\\nConsumer\\r\\nDurables\\r\\nElectrical Products, Home Furnishings, Specialty Chemicals, Metal\\r\\nFabrications, Consumer Electronics/Appliances, Building Products,\\r\\nMiscellaneous manufacturing industries, Containers/Packaging,\\r\\nPublishing, Automotive Aftermarket, Industrial Specialties\\r\\n11\\r\\nTransportation Transportation Services, Air Freight/Delivery Services, Trucking\\r\\nFreight/Courier Services, Railroads, Oil Refining/Marketing 5\\r\\nFinance\\r\\nFinance/Investors Services, Specialty Insurers, Commercial Banks,\\r\\nSavings Institutions, Real Estate, Major Banks, Investment Managers,\\r\\nInvestment Bankers/Brokers/Service, Life Insurance, Diversified\\r\\nFinancial Services, Accident &Health Insurance, Finance: Consumer\\r\\nServices, Banks, Property-Casualty Insurers, Finance Companies\\r\\n15\\r\\nPublic\\r\\nUtilities\\r\\nElectric Utilities: Central, Telecommunications Equipment, Oil/Gas\\r\\nTransmission, Water Supply, Power Generation 5\\r\\nEnergy Coal Mining, Oil & Gas Production, Integrated oil Companies,\\r\\nOilfield Services/Equipment, Natural Gas Distribution 5\\r\\nMiscellaneous Business Services, Office Equipment/Supplies/Services, Multi\\x02Sector Companies 3\\r\\nConsumer\\r\\nNon-Durables\\r\\nElectronic Components, Plastic Products, Meat/Poultry/Fish, Shoe\\r\\nManufacturing, Beverages, Packaged Foods, Consumer Specialties,\\r\\nApparel, Farming/Seeds/Milling, Food Distributors, Specialty Foods,\\r\\nMotor Vehicles, Recreational Products/Toys\\r\\n13\\r\\nHealth Care\\r\\nOphthalmic Goods, Medical/Nursing Services, Hospital/Nursing\\r\\nManagement, Major Pharmaceuticals, Biotechnology: Commercial\\r\\nPhysical Resarch, Biotechnology: Electromedical Apparatus, Other\\r\\nPharmaceuticals, Medical/Dental Instruments, Medical Specialities\\r\\n9\\r\\nConsumer\\r\\nServices\\r\\nOther Consumer Services, Restaurants, Clothing/Shoe/Accessory\\r\\nStores, Electronics/Video Chains, Other Specialty Stores, Home\\r\\nFurnishings, Diversified Commercial Services, Paper, Professional\\r\\nServices, Hotels/Resorts, Rental/Leasing Companies, Real Estate\\r\\nInvestment Trusts, Food Chains, Broadcasting, Books, Motor\\r\\nVehicles, Movies/Entertainment, RETAIL: Building Materials,\\r\\nAdvertising, Catalog/Specialty Distribution, Services-Misc.\\r\\nAmusement & Recreation, Department/Specialty Retail Stores\\r\\n24\\r\\nBasic\\r\\nIndustries\\r\\nWater Supply, Miscellaneous, Forest Products, Precious\\r\\nMetals, Mining & Quarrying of Nonmetallic Minerals,\\r\\nEngineering & Construction, Major Chemicals\\r\\n7\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :23\\r\\nCapital\\r\\nGoods\\r\\nPackage Goods/Cosmetics, Forest Products, Precious Metals,\\r\\nEnvironmental Services, Paper, Agricultural Chemicals, Mining &\\r\\nQuarrying of Nonmetallic Minerals, Engineering & Construction,\\r\\nGeneral Bldg Contractors - Nonresidential Bldgs, Aluminum,\\r\\nMajor Chemicals, Paints/Coatings, Steel/Iron Ore, Textiles\\r\\n13\\r\\nTechnology\\r\\nComputer Manufacturing, Computer peripheral equipment, Computer\\r\\nSoftware: Programming, Semiconductors, Data Processing, Computer\\r\\nSoftware: Prepackaged Software, Diversified Commercial Services,\\r\\nProfessional Services, Computer Communications Equipment, EDP\\r\\nServices, Retail: Computer Software & Peripheral Equipment, Radio\\r\\nAnd Television Broadcasting Equipment, Advertising\\r\\n12\\r\\nN/A N/A 1\\r\\nA.2 Wiki Company-based Relations\\r\\nFrom Wikidata, one of the biggest and most active open domain knowledge bases, we obtain 5\\r\\nand 53 types of first-order (in the format of \\rA\\r\\nR\\r\\n−→ \\rB ) and second-order relations (in the format\\r\\nof \\rA\\r\\nR1\\r\\n−−→ \\rC\\r\\nR2 ←−− \\rB ) between companies corresponding to the selected stocks in NASDAQ and\\r\\nNYSE markets, respectively. Note that A and B denote entities in Wikidata corresponding to two\\r\\ncompanies; C denotes another entity bridging two company-entities in a second-order relation; R,\\r\\nR1, and R2 denotes different types of entity relation defined in Wikidata15. In Table 13 and 14, we\\r\\nsummarize the extracted first-order and second-order relations, respectively.\\r\\nTable 13. First-order Wiki company-based relations in the format of \\rA\\r\\nR\\r\\n−→ \\rB .\\r\\nWikidata\\r\\nRelation (R)\\r\\nRelation Description\\r\\n1 P127 Owned by: owner of the subject.\\r\\n2 P155 Follows: immediately prior item in a series of which the subject is a part.\\r\\n3 P156 Followed by: immediately following item in a series of which the subject is a part.\\r\\n4 P355 Subsidiary: subsidiary of a company or organization.\\r\\n5 P749 Parent organization: parent organization of an organisation, opposite of subsidiaries.\\r\\nTable 14. Second-order Wiki company-based relations in the format of \\rA\\r\\nR1\\r\\n−−→ \\rC\\r\\nR2 ←−− \\rB .\\r\\nWikidata\\r\\nRelations Relation Descriptions\\r\\n1\\r\\nR1 = P31 Instance of : that class of which this subject is a particular example and member.\\r\\nR2 = P366 Use: main use of the subject.\\r\\n2\\r\\nR1 = P31 Instance of : that class of which this subject is a particular example and member.\\r\\nR2 = P452 Industry: industry of company or organization.\\r\\n3\\r\\nR1 = P31 Instance of : that class of which this subject is a particular example and member.\\r\\nR2 = P1056 Product or material produced: material or product produced by an agency.\\r\\n4\\r\\nR1 = P112 Founded by: founder or co-founder of this organization.\\r\\nR2 = P112 Founded by: founder or co-founder of this organization.\\r\\n5\\r\\nR1 = P112 Founded by: founder or co-founder of this organization.\\r\\nR2 = P127 Owned by: owner of the subject.\\r\\n6\\r\\nR1 = P112 Founded by: founder or co-founder of this organization.\\r\\nR2 = P169 Chief executive officer: the CEO within an organization.\\r\\n7\\r\\nR1 = P113 Airline hub: airport that serves as a hub for an airline.\\r\\nR2 = P113 Airline hub: airport that serves as a hub for an airline.\\r\\n15https://www.wikidata.org/wiki/Wikidata:List_of_properties/all\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:24 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n8\\r\\nR1 = P114 Airline alliance: alliance the airline belongs to.\\r\\nR2 = P114 Airline alliance: alliance the airline belongs to.\\r\\n9\\r\\nR1 = P121 Item operated: equipment, installation or service operated by the subject.\\r\\nR2 = P1056 Product or material produced: material or product produced by an agency.\\r\\n10\\r\\nR1 = P121 Item operated: equipment, installation or service operated by the subject.\\r\\nR2 = P121 Item operated: equipment, installation or service operated by the subject.\\r\\n11\\r\\nR1 = P127 Owned by: owner of the subject.\\r\\nR2 = P112 Founded by: founder or co-founder of this organization.\\r\\n12\\r\\nR1 = P127 Owned by: owner of the subject.\\r\\nR2 = P127 Owned by: owner of the subject.\\r\\n13\\r\\nR1 = P127 Owned by: owner of the subject.\\r\\nR2 = P169 Chief executive officer: the CEO within an organization.\\r\\n14\\r\\nR1 = P127 Owned by: owner of the subject.\\r\\nR2 = P355 Subsidiary: subsidiary of a company or organization.\\r\\n15\\r\\nR1 = P127 Owned by: owner of the subject.\\r\\nR2 = P749 Parent organization: parent organization of an organisation.\\r\\n16\\r\\nR1 = P127 Owned by: owner of the subject.\\r\\nR2 = P1830 Owner of : entities owned by the subject.\\r\\n17\\r\\nR1 = P127 Owned by: owner of the subject.\\r\\nR2 = P3320 Board member: member(s) of the board for the organization.\\r\\n18\\r\\nR1 = P155 Follows: immediately prior item in a series of which the subject is a part.\\r\\nR2 = P155 Follows: immediately prior item in a series of which the subject is a part.\\r\\n19\\r\\nR1 = P155 Follows: immediately prior item in a series of which the subject is a part.\\r\\nR2 = P355 Subsidiary: subsidiary of a company or organization.\\r\\n20\\r\\nR1 = P166 Award received: award or recognition received by a person, organisation.\\r\\nR2 = P166 Award received: award or recognition received by a person, organisation.\\r\\n21\\r\\nR1 = P169 Chief executive officer: the CEO within an organization.\\r\\nR2 = P112 Founded by: founder or co-founder of this organization.\\r\\n22\\r\\nR1 = P169 Chief executive officer: the CEO within an organization.\\r\\nR2 = P127 Owned by: owner of the subject.\\r\\n23\\r\\nR1 = P169 Chief executive officer: the CEO within an organization.\\r\\nR2 = P169 Chief executive officer: the CEO within an organization.\\r\\n24\\r\\nR1 = P169 Chief executive officer: the CEO within an organization.\\r\\nR2 = P3320 Board member: member(s) of the board for the organization.\\r\\n25\\r\\nR1 = P199 Business division: divisions of this organization.\\r\\nR2 = P355 Subsidiary: subsidiary of a company or organization.\\r\\n26\\r\\nR1 = P306 Operating system: operating system (OS) on which a software works.\\r\\nR2 = P1056 Product or material produced: material or product produced by an agency.\\r\\n27\\r\\nR1 = P355 Subsidiary: subsidiary of a company or organization.\\r\\nR2 = P127 Owned by: owner of the subject.\\r\\n28\\r\\nR1 = P355 Subsidiary: subsidiary of a company or organization.\\r\\nR2 = P155 Follows: immediately prior item in a series of which the subject is a part.\\r\\n29\\r\\nR1 = P355 Subsidiary: subsidiary of a company or organization.\\r\\nR2 = P199 Business division: divisions of this organization.\\r\\n30\\r\\nR1 = P355 Subsidiary: subsidiary of a company or organization.\\r\\nR2 = P355 Subsidiary: subsidiary of a company or organization.\\r\\n31\\r\\nR1 = P361 Part of : object of which the subject is a part.\\r\\nR2 = P361 Part of : object of which the subject is a part.\\r\\n32\\r\\nR1 = P366 Use: main use of the subject.\\r\\nR2 = P31 Instance of : that class of which this subject is a particular example and member.\\r\\n33\\r\\nR1 = P400 Platform: platform for which a work was developed or released.\\r\\nR2 = P1056 Product or material produced: material or product produced by an agency.\\r\\n34\\r\\nR1 = P452 Industry: industry of company or organization.\\r\\nR2 = P31 Instance of : that class of which this subject is a particular example and member.\\r\\n35\\r\\nR1 = P452 Industry: industry of company or organization.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :25\\r\\nR2 = P452 Industry: industry of company or organization.\\r\\n36\\r\\nR1 = P452 Industry: industry of company or organization.\\r\\nR2 = P1056 Product or material produced: material or product produced by an agency.\\r\\n37\\r\\nR1 = P452 Industry: industry of company or organization.\\r\\nR2 = P2770 Source of income: source of income of an organization or person.\\r\\n38\\r\\nR1 = P463 Member of : organization or club to which the subject belongs.\\r\\nR2 = P463 Member of : organization or club to which the subject belongs.\\r\\n39\\r\\nR1 = P749 Parent organization: parent organization of an organisation.\\r\\nR2 = P127 Owned by: owner of the subject.\\r\\n40\\r\\nR1 = P749 Parent organization: parent organization of an organisation.\\r\\nR2 = P1830 Owner of : entities owned by the subject.\\r\\n41\\r\\nR1 = P1056 Product or material produced: material or product produced by an agency.\\r\\nR2 = P31 Instance of : that class of which this subject is a particular example and member.\\r\\n42\\r\\nR1 = P1056 Product or material produced: material or product produced by an agency.\\r\\nR2 = P121 Item operated: equipment, installation or service operated by the subject.\\r\\n43\\r\\nR1 = P1056 Product or material produced: material or product produced by an agency.\\r\\nR2 = P306 Operating system: operating system (OS) on which a software works.\\r\\n44\\r\\nR1 = P1056 Product or material produced: material or product produced by an agency.\\r\\nR2 = P400 Platform: platform for which a work was developed or released.\\r\\n45\\r\\nR1 = P1056 Product or material produced: material or product produced by an agency.\\r\\nR2 = P452 Industry: industry of company or organization.\\r\\n46\\r\\nR1 = P1056 Product or material produced: material or product produced by an agency.\\r\\nR2 = P1056 Product or material produced: material or product produced by an agency.\\r\\n47\\r\\nR1 = P1344 Participant of : event a person or an organization was a participant in.\\r\\nR2 = P1344 Participant of : event a person or an organization was a participant in.\\r\\n48\\r\\nR1 = P1830 Owner of : entities owned by the subject.\\r\\nR2 = P127 Owned by: owner of the subject.\\r\\n49\\r\\nR1 = P1830 Owner of : entities owned by the subject.\\r\\nR2 = P749 Parent organization: parent organization of an organisation.\\r\\n50\\r\\nR1 = P2770 Source of income: source of income of an organization or person.\\r\\nR2 = P452 Industry: industry of company or organization.\\r\\n51\\r\\nR1 = P3320 Board member: member(s) of the board for the organization.\\r\\nR2 = P127 Owned by: owner of the subject.\\r\\n52\\r\\nR1 = P3320 Board member: member(s) of the board for the organization.\\r\\nR2 = P169 Chief executive officer: the CEO within an organization.\\r\\nACKNOWLEDGMENTS\\r\\nThis research is part of NExT++ project, supported by the National Research Foundation, Prime\\r\\nMinisterś Office, Singapore under its IRC@Singapore Funding Initiative.\\r\\nREFERENCES\\r\\n[1] Ayodele Ariyo et al. Adebiyi. 2014. Comparison of ARIMA and artificial neural networks models for stock price\\r\\nprediction. Journal of Applied Mathematics 2014 (2014).\\r\\n[2] Charu C Aggarwal and Chandan K Reddy. 2013. Data clustering: algorithms and applications. CRC press.\\r\\n[3] John Alberg and Zachary C Lipton. 2017. Improving Factor-Based Quantitative Investing by Forecasting Company\\r\\nFundamentals. arXiv preprint arXiv:1711.04837 (2017).\\r\\n[4] Wei et al. Bao. 2017. A deep learning framework for financial time series using stacked autoencoders and long-short\\r\\nterm memory. PloS one 12, 7 (2017), e0180944.\\r\\n[5] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating\\r\\nembeddings for modeling multi-relational data. In NIPS. 2787–2795.\\r\\n[6] Joan et al. Bruna. 2014. Spectral networks and locally connected networks on graphs. In ICLR.\\r\\n[7] Kyunghyun et al. Cho. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine\\r\\ntranslation. arXiv preprint arXiv:1406.1078 (2014).\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\n:26 Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua\\r\\n[8] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with\\r\\nfast localized spectral filtering. In NIPS. 3844–3852.\\r\\n[9] Matthew et al. Dixon. 2016. Classification-based financial markets prediction using deep neural networks. Algorithmic\\r\\nFinance Preprint (2016), 1–11.\\r\\n[10] Claire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. 2017. Spectral Graph Wavelets for Structural Role\\r\\nSimilarity in Networks. arXiv preprint arXiv:1710.10321 (2017).\\r\\n[11] Fuli et al. Feng. 2017. Computational social indicators: a case study of chinese university ranking. In SIGIR. ACM,\\r\\n455–464.\\r\\n[12] Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation\\r\\nthrough structure. In International Conference on Neural Networks, Vol. 1. IEEE, 347–352.\\r\\n[13] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural\\r\\nnetworks. In ICASSP. IEEE, 6645–6649.\\r\\n[14] David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. 2011. Wavelets on graphs via spectral graph theory.\\r\\nApplied and Computational Harmonic Analysis 30, 2 (2011), 129–150.\\r\\n[15] Xiangnan et al. He. 2017. Neural collaborative filtering. In WWW. International World Wide Web Conferences Steering\\r\\nCommittee, 173–182.\\r\\n[16] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.\\r\\n[17] Ziniu et al. Hu. 2018. Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend\\r\\nPrediction. In WSDM. ACM, 403–412.\\r\\n[18] Shan et al. Jiang. 2016. Learning query and document relevance from a web-scale click graph. In SIGIR. ACM, 185–194.\\r\\n[19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980\\r\\n(2014).\\r\\n[20] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. ICLR\\r\\n(2017).\\r\\n[21] Alex et al. Krizhevsky. 2012. Imagenet classification with deep convolutional neural networks. In NIPS. 1097–1105.\\r\\n[22] B Shravan Kumar and Vadlamani Ravi. 2016. A survey of the applications of text mining in financial domain.\\r\\nKnowledge-Based Systems 114 (2016), 128–147.\\r\\n[23] Qing et al. Li. 2016. A tensor-based information framework for predicting the stock market. TOIS 34, 2 (2016), 11.\\r\\n[24] Andrew W Lo and A Craig MacKinlay. 2002. A non-random walk down Wall Street. Princeton University Press.\\r\\n[25] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic\\r\\nmodels. In Proc. icml, Vol. 30. 3.\\r\\n[26] Tao Mei, Yong Rui, Shipeng Li, and Qi Tian. 2014. Multimedia search reranking: A literature survey. ACM Computing\\r\\nSurveys (CSUR) 46, 3 (2014), 38.\\r\\n[27] Gerald L Musgrave. 1997. A Random Walk Down Wall Street. Business Economics 32, 2 (1997), 74–76.\\r\\n[28] Arman Khadjeh et al. Nassirtoussi. 2014. Text mining for market prediction: A systematic review. Expert Systems with\\r\\nApplications 41, 16 (2014), 7653–7670.\\r\\n[29] Thien Hai Nguyen and Kiyoaki Shirai. 2015. Topic modeling based sentiment analysis on social media for stock market\\r\\nprediction. In ACL, Vol. 1. 1354–1364.\\r\\n[30] Adi et al. Omari. 2016. Novelty Based Ranking of Human Answers for Community Questions. In SIGIR. ACM, 215–224.\\r\\n[31] Lawrence et al. Page. 1999. The PageRank citation ranking: Bringing order to the web. Technical Report. Stanford\\r\\nInfoLab.\\r\\n[32] G Preethi and B Santhi. 2012. STOCK MARKET FORECASTING TECHNIQUES: A SURVEY. Journal of Theoretical &\\r\\nApplied Information Technology 46, 1 (2012).\\r\\n[33] G William Schwert. 2002. Stock volatility in the new millennium: how wacky is Nasdaq? Journal of Monetary Economics\\r\\n49, 1 (2002), 3–26.\\r\\n[34] Richard et al. Socher. 2013. Reasoning with neural tensor networks for knowledge base completion. In NIPS. 926–934.\\r\\n[35] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. 2015. Unsupervised learning of video representations\\r\\nusing lstms. In ICML. 843–852.\\r\\n[36] Wenting et al. Tu. 2016. Investment recommendation using investor opinions in social media. In SIGIR. ACM, 881–884.\\r\\n[37] Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM 57, 10\\r\\n(2014), 78–85.\\r\\n[38] Markus et al. Weimer. 2007. COFIRANK Maximum Margin Matrix Factorization for Collaborative Ranking. In NIPS.\\r\\n1593–1600.\\r\\n[39] Yan Xu and Guosheng Zhang. 2015. Application of Kalman Filter in the Prediction of Stock Price. In KAM.\\r\\n[40] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to respond with deep neural networks for retrieval-based human\\x02computer conversation system. In Proceedings of the 39th ACM SIGIR. ACM, 55–64.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.\\nTemporal Relational Ranking for Stock Prediction :27\\r\\n[41] Rose Yu, Huida Qiu, Zhen Wen, ChingYung Lin, and Yan Liu. 2016. A survey on social media anomaly detection.\\r\\nSIGKDD 18, 1 (2016), 1–14.\\r\\n[42] Liheng et al. Zhang. 2017. Stock Price Prediction via Discovering Multi-Frequency Trading Patterns. In SIGKDD. ACM,\\r\\n2141–2149.\\r\\n[43] Yao et al. Zhang. 2017. Learning Node Embeddings in Interaction Graphs. In CIKM. ACM, 397–406.\\r\\n[44] Sendong et al. Zhao. 2017. Constructing and embedding abstract event causality networks from text snippets. In\\r\\nWSDM. ACM, 335–344.\\r\\n[45] Zhaohui et al. Zheng. 2007. A regression framework for learning ranking functions using relative relevance judgments.\\r\\nIn SIGIR. ACM, 287–294.\\r\\n[46] Denny et al. Zhou. 2007. Learning with hypergraphs: Clustering, classification, and embedding. In NIPS. 1601–1608.\\r\\n[47] Xiaojin et al. Zhu. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In ICML. 912–919.\\r\\nACM Transactions on Information Systems, Vol. 1, No. 1, Article . Publication date: December 2019.'},\n",
       " {'name': '2408.03872v1.pdf',\n",
       "  'content': 'Inter-Series Transformer: Attending to Products in Time Series\\r\\nForecasting\\r\\nRares Cristianb, Pavithra Harshaa, Clemente Ocejob, Georgia Perakisb, Brian Quanza, Ioannis\\r\\nSpantidakisb, Hamza Zerhounib\\r\\na\\r\\nIBM Research, IBM TJ Watson Research Center, Yorktown Heights, 10598, NY, USA\\r\\nbMIT, 77 Massachusetts Ave, Cambridge, 02139, MA, USA\\r\\nAbstract\\r\\nTime series forecasting is an important task in many fields ranging from supply chain management\\r\\nto weather forecasting. Recently, Transformer neural network architectures have shown promising\\r\\nresults in forecasting on common time series benchmark datasets. However, application to supply\\r\\nchain demand forecasting, which can have challenging characteristics such as sparsity and cross-series\\r\\neffects, has been limited.\\r\\nIn this work, we explore the application of Transformer-based models to supply chain demand\\r\\nforecasting. In particular, we develop a new Transformer-based forecasting approach using a shared,\\r\\nmulti-task per-time series network with an initial component applying attention across time series,\\r\\nto capture interactions and help address sparsity. We provide a case study applying our approach\\r\\nto successfully improve demand prediction for a medical device manufacturing company. To further\\r\\nvalidate our approach, we also apply it to public demand forecasting datasets as well and demonstrate\\r\\ncompetitive to superior performance compared to a variety of baseline and state-of-the-art forecast\\r\\nmethods across the private and public datasets.\\r\\n1. Introduction\\r\\nTime series forecasting is a fundamental problem in machine learning with applications across many\\r\\ndomains. Common applications of time series forecasting include supply chain management [1], finan\\x02cial modeling [2], weather forecasting [3], and many more. Since many of these problems have been\\r\\naround for far longer than more modern problems in machine learning such as robotic manipulation,\\r\\noften the models used in production are more theoretical in foundation and rely less heavily on data.\\r\\nThat is, they often encode strong priors, or inductive biases [4, 5], from the assumptions made and\\r\\nthe corresponding simple parametric form the models take. Furthermore, they may not be designed\\r\\nto fully utilize the broader set of related data often available these days for forecasting particular time\\r\\nPreprint submitted to International Journal of Forecasting August 8, 2024\\r\\narXiv:2408.03872v1 [cs.LG] 7 Aug 2024\\nseries. Traditional time series forecasting methods like exponential smoothing [6], state space models\\r\\n[7], and auto-regressive ARIMA models [8, 9] are still widely used in industry.\\r\\nAs our world becomes increasingly connected and availability of data rises, so does the pursuit of\\r\\ndeep learning models (e.g., deep neural networks) to tackle complex tasks such as image identification\\r\\nor voice recognition systems, as well as modeling sequential data such as natural language and time\\r\\nseries. Recurrent neural networks (RNNs) [10, 11] were one of the first types of neural network model\\r\\narchitectures developed for sequential data, and variants such as long-short term memory (LSTM) [12]\\r\\nhave also been popularly used in forecasting [13–19]. However, the recurrent and state-based process of\\r\\nRNNs limits the amount of historical information retained and used in predictions and can also make\\r\\ntraining more challenging. One family of models that takes inspiration from RNNs and also aims to\\r\\nsolve many of its limitations via leveraging attention instead of recurrence to model sequential data\\r\\nis the Transformer model [20] family. Transformer models have shown promising results for modeling\\r\\nsequential data across many domains [21], including recently in time series forecasting [22–36]. These\\r\\nTransformer forecast models are all general-purpose by design and results are reported on a variety of\\r\\ncommon benchmark datasets, often with clear temporal patterns and signals. However, their applica\\x02tion to supply chain demand forecasting has been limited, with only the Temporal Fusion Transformer\\r\\n(TFT) [25] paper including a retail dataset in results, and without these models being specialized for\\r\\ndemand forecasting. Supply chain demand time series can exhibit challenging properties, such as spar\\x02sity in terms of sales observations at the granular product-location level and skewed value distributions\\r\\n(as it is count data), cross-product effects (demand / sales change for one product can affect demand\\r\\nfor others) [37–39], and misalignment / changing sets of time series as new products are added and\\r\\nremoved over time, which may not be fully addressed via existing Transformer models. Furthermore,\\r\\nsome recent work called into question the benefits of Transformers for forecasting by showing much\\r\\nsimpler models were often able to outperform a large set of recently proposed Transformer forecast\\r\\nmodels on the same common benchmark datasets [40]. This highlights the need for more study of\\r\\nTransformer model application on targeted real world datasets for particular tasks. To enhance the\\r\\npractical application of Transformers in supply chain demand forecasting and potentially supplant the\\r\\nuse of classical models, it is crucial to conduct more case studies that report results and analyses of\\r\\napplying Transformers on real-world data, which is currently lacking.\\r\\nIn this work we aim to explore the application of Transformer models to supply chain demand\\r\\nforecasting. We make the following contributions:\\r\\n• We develop new Transformer model variations targeting supply chain demand forecasting to\\r\\naddress the aforementioned challenging characteristics. This includes developing a new architec\\x02ture consisting of an initial component applying self-attention transformation across time series\\r\\n2\\nfollowed by a shared per-time series Transformer network, applied independently per time se\\x02ries with shared training across time series in a multi-task manner. This can enable capturing\\r\\ndescriptive interactions between time series via the initial cross-series attention transformation\\r\\nwhile also helping to address sparsity in the form of limited observations both in terms of lim\\x02ited individual time series length and limited non-zero demand. The multi-task aspect (shared\\r\\nnetwork applied to each time series) can also help overcome overfitting associated with a direct\\r\\nmultivariate approach and along with the initial cross-series attention allows for application when\\r\\nthe set of time series changes over time. In short our approach is aimed at achieving the best\\r\\nof both worlds (multivariate / cross-series modeling and multi-task per-time series modeling)\\r\\ncompared to prior approaches.\\r\\n• We provide a case study applying Transformers and our approach to forecast demand for a med\\x02ical device manufacturing company, analyzing results for multiple horizons and target metrics to\\r\\nsupport their different business needs. We successfully demonstrate that our method significantly\\r\\nreduces forecast errors compared to their currently used classical forecasting method as well as\\r\\nmany other other time series models including a variety of neural network and Transformer\\r\\nforecasters.\\r\\n• We compare our proposed approach on two additional publicly available retail demand forecasting\\r\\ndatasets to give a better sense of performance and how it compares to other methods. We\\r\\nshow competitive to superior performance on two publicly available retail datasets compared to\\r\\nTransformer and state-of-the-art baselines.\\r\\n• Finally, we further present additional analyses and an ablation study, examining which aspects\\r\\nof our model were advantageous and which did not enhance Transformer forecasting in this case\\r\\nstudy.\\r\\nIn summary our new approach is specifically designed and evaluated to better tackle the challenges\\r\\nof supply chain demand forecasting such as sparsity, overfitting and cross-series effects. We focus\\r\\nparticularly on demonstrating the effectiveness of our methodology through a novel case study aimed\\r\\nat improving demand prediction for a medical device manufacturing company, as well as additional\\r\\ndemand forecasting data sets.\\r\\n2. Related Work\\r\\nTime series forecasting has emerged in recent years as a crucial topic in machine learning. There\\r\\nhas been a growing interest in using time series models for forecasting in various domains such as\\r\\n3\\ntransportation [14–17, 19, 23–26, 29, 31, 35, 36], energy [16, 17, 19, 23, 24, 26–29, 31–33, 35, 36],\\r\\nweather [28, 29, 32, 33, 35, 36] or retail sales [14]. As evidenced by the recent M5 competition [41],\\r\\nthere is also growing interest in developing new machine learning and deep learning time series methods\\r\\nfor supply chain demand forecasting specifically. However, the literature on this kind of time series\\r\\nmodels in the context of retail demand forecasting is limited. We provide a brief overview of existing\\r\\nwork to establish the motivation and context for our proposed approach.\\r\\n2.1. Traditional time series models\\r\\nMany traditional time series forecasting methods are still used in industry [6–9]. Exponential\\r\\nsmoothing [6, 7] produces a prediction that is a weighted sum of past observations with exponentially\\r\\ndecreasing weights for past data points. Holt-Winters [7], that falls into the exponential smoothing\\r\\nfamily, adds model components to enable capturing trend and seasonality. Autoregressive Integrated\\r\\nMoving Average (ARIMA) [8, 9] is a large class of models combining an auto-regressive model, a\\r\\nmoving average model and a differencing step to forecast stationary and non-stationary time series.\\r\\nAs we will see later, these traditional methods often have limitations compared to machine learning\\r\\nand deep learning time series models, especially when it comes to large and complex datasets. In\\r\\nthe framework of the M5 competition [41], the results achieved show that the traditional time series\\r\\nmethods were outperformed by state-of-the-art machine learning models.\\r\\nThe book [9] provides a comprehensive reference to a range of traditional time series forecasting\\r\\nmethods including exponential smoothing, ARIMA and state space models.\\r\\n2.2. RNN and CNN-based time series models\\r\\nA Recurrent Neural Network (RNN) is a type of neural network used for sequential data with the\\r\\nability of capturing past information stored by having a recurrent hidden state whose activation at each\\r\\ntime is dependent on the previous time state and current time input. RNN-based models can be used\\r\\nfor language translation, natural language processing or image identification as well as for time series\\r\\nforecasting showing promising results in this area. RNN-based models have been extensively applied\\r\\nto time series forecasting [10–19]. One variant of Recurrent Neural Network is the Long-Short Term\\r\\nMemory model (LSTM) [11, 12]. The purpose of this method is to overcome the challenge of long\\x02term dependencies in time series forecasting by introducing a sophisticated hidden layer that controls\\r\\nthe flow of information required to predict the output in the network. By maintaining a memory\\r\\nof past information, the hidden layer regulates the flow of data and stores the relevant information\\r\\nto use for the next steps. This method can capture long-term patterns in the time series and make\\r\\nmore accurate predictions for future time points. DeepAR [14] is a forecasting method based on\\r\\n4\\nautoregressive RNNs which learns seasonal behaviors and dependencies on given covariates across time\\r\\nseries to make probabilistic forecast. By using a shared model across multiple time series (i.e., a multi\\x02task forecast approach), DeepAR can generate forecasts for time series with limited historical data.\\r\\nTemporal Latent Auto-Encoder [19] is a recent model designed to tackle high-dimensional multivariate\\r\\ntime series forecasting and model cross-series correlations by combining nonlinear factorization of time\\r\\nseries and a temporal latent space LSTM forecast model.\\r\\nConvolutional neural networks (CNNs) [42] are a variant of deep neural networks (DNNs) that use\\r\\na sequence of convolutional layers, pooling layers, and fully-connected layers. CNN-based approaches\\r\\nhave been extensively applied and evaluated across a wide spectrum of sequence modeling tasks such\\r\\nas Natural Language Processing (NLP) or speech recognition, and have been shown to outperform\\r\\nRNN-based methods in some cases [43–45]. CNN models have also demonstrated effectiveness in\\r\\nthe domain of time series modeling and forecasting [16, 46–48]. The temporal convolutional network\\r\\n(TCN) [49] is a CNN-based model using causal convolutions, that is, convolution applied across time\\r\\nsuch that the convolution output for a given time point depends only on the current and previous\\r\\ntime points. The TCN architecture can take a sequence of any length and map it to an output\\r\\nsequence of the same length via multiple layers of convolution (and padding sequences as necessary).\\r\\nNotably, TCNs have demonstrated superior empirical performance compared to RNNs and LSTMs in\\r\\nvarious sequence modeling tasks. For instance, DeepGLO [48], a TCN-based model, showed superior\\r\\nresults compared to LSTM and DeepAR in time series prediction tasks such as electricity and traffic\\r\\nforecasting. The DeepGLO method is a hybrid model that combines a global matrix factorization model\\r\\nregularized by a temporal convolution network, along with another temporal network that can capture\\r\\nlocal properties of each time-series and associated covariates. Moreover, the WaveNet architecture\\r\\nhas been adapted for conditional time series forecasting [47] by using stacked dilated convolutions\\r\\nproviding access to a wide range of historical data, and multiple convolutional filters in parallel to\\r\\nseparate time series facilitating rapid data processing and exploiting the correlation structure between\\r\\nmultivariate time series. LSTNet [16] combines a traditional linear autoregressive model with RNN\\r\\nand CNN to extract short-term local dependency patterns among variables and to discover long-term\\r\\npatterns for time series trends. It leverages the strengths of both the convolutional layer to discover the\\r\\nlocal dependency patterns among multi-dimensional input variables and the recurrent layer to capture\\r\\ncomplex long-term dependencies.\\r\\nThe recurrence mechanism of RNNs restricts the amount of historical information that can be used\\r\\nfor predictions and can complicate training as well, which can limit the capability of such models to\\r\\ncapture long-term and complex temporal patterns effectively. Similarly, CNN models typically use\\r\\ndilation to increase their receptive fields, necessitating deeper networks to capture longer-term depen\\x025\\ndencies. Moreover, these longer-term dependencies are modeled only after several levels of padding\\r\\nand processing in higher layers, which may limit their effectiveness in modeling temporal patterns.\\r\\nTo overcome these limitations, Transformer-based models have emerged as a family of models that\\r\\nleverage attention mechanisms instead of recurrence or convolution to model sequential data. Such an\\r\\napproach enables directly modeling the impact of time points within the history window on predicting\\r\\nthe future values. This approach has been proven effective in addressing the flaws of RNNs and CNNs\\r\\nin time series forecasting, especially when large amounts of data are available.\\r\\n2.3. Transformer-based time series models\\r\\nTransformer-based methods have been proposed to overcome the limitations of previous DNN ap\\x02proaches for time series forecasting and tackle the challenges posed by having modern-day datasets,\\r\\nwhich are often larger and more complex. The Transformer model [20] avoids using a recurrence mech\\x02anism and relies mainly on a self-attention mechanism to capture cross-sequence (time) interactions\\r\\nand generate an output. This model achieves very good results in natural language processing, com\\x02puter vision and time series forecasting [22–36, 50] which may in part be due to its ability to directly\\r\\ncapture long-range dependencies and interactions in sequential data. Temporal Fusion Transformer\\r\\n(TFT) [25] is a Transformer-based model incorporating variable selection networks to choose pertinent\\r\\ninput variables at each time step and static covariate encoders to integrate static features into the\\r\\nmodel. It processes known and observed inputs using a sequence-to-sequence layer and implements\\r\\na novel self-attention mechanism with interpretable multi-head attention. This enables TFT to learn\\r\\nlong-term relationships across different time steps, making it a powerful tool for capturing complex\\r\\ntemporal dependencies in the data. The self-attention mechanism also facilitates the interpretation\\r\\nof feature importance, allowing for the identification of critical factors affecting the forecasting task.\\r\\nPyraformer [30] is a recent Transformer-based model that introduces the pyramidal attention mod\\x02ule to describe the temporal dependencies of different ranges leveraging a pyramidal graph and its\\r\\nattention mechanism. Autoformer [29] replaces the self-attention mechanism in the Transformer by\\r\\nan Auto-Correlation mechanism with dependency discovery and information aggregation at the series\\r\\nlevel to tackle long-term time series forecasting with intricate temporal patterns. FEDformer [35] is\\r\\na frequency enhanced Transformer, decomposing the input into a trend component with a moving\\r\\naverage kernel and a seasonal component. These models are specifically designed for long sequence\\r\\nforecasting. Using the seasonal-trend decomposition structure of FEDformer, DLinear combines it\\r\\nwith simple linear layers. Two one-layer linear networks are applied to each component and the two\\r\\nfeatures are summed to get the final prediction. This simple linear approach has outperformed the\\r\\nstate-of-the-art (SOTA) FEDformer for multivariate and univariate forecasting as well as other recent\\r\\n6\\nSOTA Transformer-based solutions on different common benchmark datasets.\\r\\nNote for the multivariate (multiple time series) case these transformer approaches typically treat\\r\\nmultiple time series values as part of the input vector for one time point, and an initial embedding\\r\\nlinearly maps the combination to a new embedded vector which is subsequently transformed via at\\x02tention across time points. Recently PatchTST [51] instead applied a shared transformer backbone\\r\\nindependently per channel (time series), i.e., in a multi-task manner, and on patches of time points,\\r\\nto transform each channel independently, with a simple linear transformation at the end to get a final\\r\\nprediction for each series, and found this approach to often out-perform the previous channel-mixing\\r\\ntransformer forecasting approaches.\\r\\n2.3.1. Key model differences with our approach\\r\\nIn summary past transformer-based forecast models were either applied per-series (univariate fore\\x02casting) or typically by jointly embedding all variables per time point (multivariate forecasting), with\\r\\nPatchTST using per-series transformation for the Transformer part with a shared network (multi-task\\r\\napproach) but also using a static linear mapping on the transformed variables at the end. On the\\r\\none hand, per-series approaches like PatchTST and univariate transformer models often achieve better\\r\\nresults than multivariate modeling for many datasets (especially when trained in a multi-task manner\\r\\nacross time series) as they can leverage more data (from the multiple time series independently) to\\r\\ntrain the shared-parameter network and avoid overfitting that can occur with multivariate modeling,\\r\\nespecially when there are many time series, and can also be more computationally efficient. However,\\r\\non the other hand, per-series models are unable to leverage cross-series information and capture joint\\r\\ntime series distribution relationships in forecast outputs. I.e., while multivariate time series forecast\\r\\nmodeling can capture complex relationships between time series it is also more prone to overfitting and\\r\\nmore expensive to train. Furthermore, the static, typically linear, encoding per time point as typically\\r\\nused in multivariate transformer and other neural net forecasting approaches, and as used in PatchTST\\r\\nafter independent per-series transformation, limits the relationships that can be captured and modeled\\r\\nbetween time series - for example, a different kind of transformation, or different information among\\r\\ntime series should be shared, depending on the context given by the particular time window being\\r\\nconsidered.\\r\\nInstead with our novel transformer forecasting approach, we aim to capture the best of both worlds,\\r\\nof cross-time-series multivariate modeling along with per-series, shared network, multi-task modeling.\\r\\nInstead of using a static linear transformation of multiple time series values, we apply self-attention\\r\\nacross time-series, for the current segment of each time series - to capture the relationships between\\r\\ntime series in a dynamic and more flexible and powerful way. Afterward, a shared transformer network\\r\\n7\\nis applied per-series to finish transforming each individual time series and derive the final forecast. This\\r\\nwhole network can then be trained in a multi-task fashion - i.e., applied to each time series individually\\r\\n(each time also feeding in the other time series initially for the cross-series attention transformation\\r\\npart), so it can leverage the benefit of having effectively larger data for training by sharing parameters\\r\\nacross time series as in multi-task approaches like PatchTST. Note that while some past work has\\r\\nexplored inter-series attention as well, e.g., for epidemiological analysis [52], unlike such prior work, we\\r\\napply this in a more constrained way as just an initial transformation of directly aligned time series\\r\\nsegments only, and critically and uniquely still use a shared multi-task forecast network applied per\\r\\nseries after.\\r\\n2.3.2. Experiment differences with our approach\\r\\nAdditionally, most transformer forecasting papers provide results of using these novel transformer\\r\\nmethods on widely used, publicly available time series forecasting benchmark datasets, many of which\\r\\nare derived from real-world practical applications, or else on specific domains like epidemiology or\\r\\nlogistics. However, the application of these methods to supply chain demand forecasting is very limited.\\r\\nIndeed, most of the papers do not include any retail dataset in the evaluation of their method. The\\r\\nTFT paper [25] is one of the only papers introducing a deep learning method for time series forecasting\\r\\napplied on various domains including a sales forecast dataset in the set of results, but this is not the\\r\\nfocus of the study. These models have yet to be explored and demonstrate broad success in supply\\r\\nchain demand forecasting, which can have challenging characteristics such as sparsity and cross-series\\r\\neffects. As a key distinguishing part of this work we benchmarked many transformer forecast methods,\\r\\nincluding TFT, with the private dataset provided by the medical device manufacturing company as\\r\\nwell as the best of the methods on other retail datasets.\\r\\nOur method and a diverse set of the models described will be applied on retail datasets to help\\r\\nprovide an analysis of the application of these time series methods for supply chain demand forecasting.\\r\\nThis includes an in-depth case study conducted on a supply chain demand forecasting dataset for a\\r\\nmedical device manufacturing company. The study provides results for multiple horizons and target\\r\\nmetrics, with the objective of catering to the business needs of the company using real-world data.\\r\\nFinally, we evaluated the effectiveness of our proposed approach on two publicly available real-world\\r\\nsales datasets, to provide a better understanding of its performance and its comparison with other\\r\\nmethods.\\r\\n8\\n3. Methodology\\r\\n3.1. Problem Definition\\r\\nLet a generic matrix be denoted by a bold capital letter, a vector by a bold lower case letter,\\r\\nand any scalars including individual entries of a vector or matrix by an non-bolded letter. Then,\\r\\nmultivariate time-series can be represented by a matrix (with each row being an individual time series\\r\\n/ variate) and univariate time series by a vector (with each entry being a time point). For a matrix\\r\\nX, we denote the i-th column of the matrix with a subscript and lowercase letter xi. Let us consider a\\r\\nsituation where we have several products for which we want to forecast demand. Each product can be\\r\\nrepresented as a multivariate time-series denoted by Yj, where j represents a specific product. The\\r\\ncolumns of Yj, denoted as (y\\r\\nj\\r\\n1\\r\\n, ..., y\\r\\nj\\r\\nT\\r\\n), represent the T time points of our time series, while the rows\\r\\ncorrespond to the different variates. We consider having n variates. Specifically, at each time point\\r\\ni, the vector y\\r\\nj\\r\\ni\\r\\nconsists of the n observed values for the different variables considered. For example,\\r\\nsuch values consist of useful information such as the sales of the product at a particular location or the\\r\\nprice of the product. We consider the problem of forecasting h future values (y\\r\\nj\\r\\nT +1,l, ..., y\\r\\nj\\r\\nT +h,l) where\\r\\neach y\\r\\nj\\r\\ni,l represents the value for the dimension l we are interested in, at time point i for product j.\\r\\nIn this case, we are interested in the same dimension across all future time periods and we call this\\r\\nthe label or target variable. In the demand forecasting case, this may correspond to the sales of the\\r\\nparticular product.\\r\\n3.2. Preliminaries\\r\\nThe Transformer model is based on attention, replacing the recurrent layers most commonly used in\\r\\nencoder-decoder architectures with multi-headed self-attention [20]. Through the attention mechanism\\r\\nand the positional encoding, the model is able to appropriately attribute the impact of each element\\r\\nin our input on each other element and compute a representation of a sequence by relating different\\r\\npositions in the sequence. The Transformer architecture consists of stacked self-attention layers and\\r\\npoint-wise fully connected layers (i.e., the latter applied per time point, in the case of time series),\\r\\nthat sequentially transforms the (vector) representations (also known as encodings), for elements of a\\r\\nsequence with each layer.\\r\\nAttention Mechanism The main component of the Transformer responsible for its success\\r\\nis the attention mechanism which can model the relationships between elements in a sequence. The\\r\\noriginal Transformer paper [20] uses two different attention mechanisms which are the same at the\\r\\ncore: the self-attention mechanism and the encoder-decoder attention. An attention function can be\\r\\ndescribed as mapping a query and a set of key-value pairs to an output, where each query, key and\\r\\nvalue corresponds to an element in a sequence, e.g., a time point in the case of time series. The input\\r\\n9\\nconsists of queries and keys of dimension dk, and values of dimension dv for all elements in a sequence.\\r\\nQ, K and V are respectively a packed set of queries, a packed set of keys and a packed set of values\\r\\n(with each item in the sequence corresponding to a row). dmodel is the model dimension to which\\r\\nwe project our input. The output for each sequence element is computed as a weighted sum of the\\r\\nvalues, where the weight assigned to each value is computed by a compatibility function of the query\\r\\nfor that element with the corresponding key. In matrix form the set of outputs is given by the following\\r\\nfunction:\\r\\nAttention(Q, K, V) = softmax(QKT\\r\\n√\\r\\ndk\\r\\n)V, (1)\\r\\nwith softmax applied per row. In Transformers, these queries, keys, and values are typically determined\\r\\nfor each sequence element by linearly projecting the current representation of each element using a\\r\\nlearned weight matrix (i.e., which are parameters of the model) for each of the query, key, and value\\r\\nrepresentations, i.e., Q = XWQ, K = XWK, and V = XWV, where X is the current representation\\r\\nof the sequence. Thus the output of the self-attention operation results in a new, transformed encoding\\r\\n/ representation for each element in the sequence.\\r\\nInstead of performing a single attention function, an extension to this mechanism is to linearly\\r\\nproject the queries, keys and values h times with different learned linear projections to dk, dk and dv\\r\\ndimensions respectively. The attention function is then performed on each of these projected versions of\\r\\nqueries, keys and values, which are concatenated and once again projected. This multi-head attention\\r\\ncan allow learning different relationships for different parts of a sequence. We label this the MultiHead\\r\\nattention function, described with the following:\\r\\nMultiHead(Q, K, V) = concat(head1, ..., headh)WO (2)\\r\\nwhere headi = Attention(QWQ\\r\\ni\\r\\n, KWK\\r\\ni\\r\\n, VWV\\r\\ni\\r\\n)\\r\\nwhere WQ\\r\\ni ∈ R\\r\\ndmodel×dk ,WK\\r\\ni ∈ R\\r\\ndmodel×dk ,WV\\r\\ni ∈ R\\r\\ndmodel×dv and WO ∈ Rhdv×dmodel . For self\\r\\nattention as used in Transformers, Q = K = V = X is the input to MultiHead.\\r\\nPositional Encoding Positional encoding [53–56] assigns a unique encoding vector to each time\\r\\nstep, which is added to the input embedding vector (initial encoding) at that time step. The positional\\r\\nencoding has the same dimension dmodel as the embeddings, so that the two can be summed. The\\r\\nencoding vector captures information about the position of the time step in the sequence, such as its\\r\\nrelative position to other time steps. This enables the model to distinguish between different time\\r\\nsteps and understand the temporal ordering of the sequence.\\r\\nEncoder The encoder is composed of a stack of blocks, each composed of self-attention layers\\r\\nand position-wise feed forward network layers (i.e., the same network applied independently to each\\r\\nsequence element) followed by the residual connection and layer normalization to assist the training\\r\\n10\\nstability. The self-attention component of each block of the encoder is in charge of computing the\\r\\nattention weights between all of the elements in the block’s input sequence and transforming the\\r\\nelements based on these attentions, as described above. The encoder thus performs a sequence of\\r\\ntransformations to the input sequence representations, and then passes this information onto the\\r\\ndecoder for it to roll-out the final predictions.\\r\\nDecoder The decoder architecture is similar to the encoder architecture. It is also composed of\\r\\na stack of identical blocks with the same components. In addition to the two sub-layers in each encoder\\r\\nblock, the decoder adds a third sub-layer, which performs multi-head attention over the output of the\\r\\nencoder stack as well. In this way sequential decoder outputs are generated based on both the encoded\\r\\nrepresentations of the input sequence, and the output sequence of the decoder generated so far.\\r\\nPlease refer to [20] for more details about the Transformer architecture.\\r\\n3.3. Model Architecture\\r\\nWe design a new Transformer-based model, referred to as Inter-Series Transformer, to overcome\\r\\nthe different challenging characteristics described in Section 2. As mentioned in Section 2.3.1, a key\\r\\ndifferentiating aspect of our approach is combining controlled, cross-series attention based transforma\\x02tion of different time series, along with per-time-series multi-task modeling via a shared network (for\\r\\ntemporal transformation), to capture the best of both worlds.\\r\\nNote, in the context of supply chain demand forecasting, each time series typically corresponds to a\\r\\nproduct or product group / category (or product and location combination) and each could also itself\\r\\nbe a multivariate time series, incorporating other features like price or promotions.\\r\\nIn our proposed architecture, there are four main differentiating / new components compared to\\r\\nthe vanilla Transformer architecture and typical / past Transformer forecasting approaches:\\r\\n• Inter-Series Attention Layer: we introduce a new custom attention layer to get a better informed\\r\\nrepresentation of the target time series by learning dynamics between the different time series\\r\\n/ products and incorporating the other time series into the prediction. As shown in Figure 1,\\r\\nthis new component is the first layer (or layers) of our custom Transformer and takes all the\\r\\ntime series as inputs as detailed in 3.3.1. Note that unlike past Transformer approaches that\\r\\napply attention just across time, our can leverage attention to capture cross-product / time series\\r\\neffects. Additionally, unlike applying attention across both all time series and time points at all\\r\\nlayers, which could more easily lead to overfitting, ours enables capturing cross time-series effects\\r\\nin a more controlled manner (transforming just the target time series themselves up-front).\\r\\n• Multi-task, shared per-series transformer: by limiting cross-product / series attention to initial\\r\\nlayers and select time series, we control complexity, and enable using a shared Transformer\\r\\n11\\nnetwork afterwards to separately transform each individual multivariate time series (e.g., per\\r\\nproduct), capturing the temporal effects and effectively expanding the amount of data used to\\r\\ntrain this shared network.\\r\\n• Projection to High Dimensional Representations for various features: we address mixed feature\\r\\nand feature type inputs to create a more comprehensive and informative input for the Transformer\\r\\nby learning separate mappings to high dimensional representations for different feature types.\\r\\n• Abandonment of Positional Encoding: we abandon the positional encoding and we capture rel\\x02ative positioning by defining and incorporating specific features that change with time.\\r\\nFigure 1: Inter-Series Transformer Diagram with Inter-Series Attention, illustrated here with a single encoder and a\\r\\nsingle decoder block. Inputs include P the matrix containing all target time series, Pq the target time series of product\\r\\nq, Xq the feature matrix of product q, and XIS the output from the Inter-Series Attention layer. The circled plus symbol\\r\\nindicates concatenation in the last dimension\\r\\n3.3.1. Attention layers\\r\\nAs mentioned in the previous section, the traditional Transformer architecture involves two different\\r\\nattention layers: self-attention and encoder-decoder attention. These two layers allow for the model\\r\\nto learn complex relationships between the different elements in the input/target sequences. However,\\r\\ngiven that the traditional application was on natural language, this architecture treats each different\\r\\nsequence as somewhat independent to each other. In the context of time-series forecasting within\\r\\na singular retailer, there are multiple sequences/time series occurring in overlapping time periods,\\r\\ne.g., corresponding to different products or product groups (or product-location combinations). It is\\r\\nimportant to recognize cross-series effects such as the cannibalization of one product by another. For\\r\\n12\\nthis reason, we introduce a new attention layer that incorporates other time-series into the prediction\\r\\nfor the current time-series.\\r\\nInter-Series Attention Layer. We introduce our own custom attention layer which utilizes\\r\\nthe same form described above in order to help learn dynamics between different products. We refer to\\r\\nthis as Inter-Series attention Layer. As we will see next, this layer takes the Attention function in 3.2\\r\\nand changes the traditional form of the inputs to learn complex dynamics through attention weights\\r\\nbetween separate time series in addition to different time periods.\\r\\nThe main goal of this layer is to learn attention weights between the context window of the product\\r\\nfor which we are providing a forecast and the context windows of all other time series (products) to\\r\\nproduce a better informed representation of our desired time series context window. This can also help\\r\\nwith sparsity as sparser time series can pay attention to larger-volume time series to improve their\\r\\npredictions, as we will show in our experiment results. To yield a single time series representation from\\r\\nthe attention between all products, we must input our target time series context window as the query\\r\\nvector and the context windows of all other time series as the key and value vectors. Therefore, this\\r\\ncreates a difference in the input shape between the Inter-Series attention layer and the other attention\\r\\nmechanisms described in 3.2. In the self-attention layer, the query vector and the key/value vectors are\\r\\nall of shape context-window×dmodel. In the encoder-decoder attention layer, the query vector and the\\r\\nkey/value vectors are respectively of shape forecast-window × dmodel and context-window × dmodel.\\r\\nThis is intuitive since we are matching like-sequences with each other and relating positions in the\\r\\nsequences to each other based on a high-dimensional representation of our original sequence. In our\\r\\nInter-Series attention layer, the query vector is instead of shape 1 × context-window, correspond\\x02ing to the context window of the target time series, and the key/value vectors stacked are of shape\\r\\ntotal-series × context-window, corresponding to the context windows of all the other time series. We\\r\\nchose to remove the projection to higher dimension since we are only concerned with the target of\\r\\nour other products and want to preserve the notion of a context window in the output. This layer\\r\\nresults in an output of dimension 1 ×context-window, meant to represent a better-informed version of\\r\\nour target time series within the context window. It is worth noting that the overall architecture can\\r\\nalso capture past information through other means such as using other features or incorporating past\\r\\nwindows of values. We augment the original target series in the context window with the informed\\r\\nfeature output by the Inter-Series attention Layer and proceed through the model as normal, more\\r\\n13\\nformally:\\r\\nP = all products target time series (3)\\r\\nPq = target time series of product q\\r\\nXq = features of product q\\r\\nXIS = MultiHead(Pq, P, P)\\r\\nY = T ransformer([Pq, Xq, XIS])\\r\\nHere, XIS is the informed feature output of our Inter-Series attention. As detailed in the equations\\r\\nbelow, the query Pq is the context window of the target time series, and the key/value P are the\\r\\ncontext windows of all the time series. As shown in Figure 1, the first layer is our custom Inter-Series\\r\\nattention layer that outputs XIS. This new informed feature XIS is then concatenated with Xq, the\\r\\nmatrix feature of product q, and Pq, its target time series. This concatenation is then fed as input\\r\\ninto a Transformer model. If our Inter-Series attention layer is not used, the initial input would simply\\r\\nconsist of the concatenation of Pq and Xq, resulting in Y = T ransformer([Pq, Xq]).\\r\\nNote, to keep our architecture simple / control complexity, in our experiments we limited our\\r\\nnetworks to a single layer of this inter-series attention, but it’s also possible to apply this for multiple\\r\\nlayers to enable a more complex transformation of the target time series based on the other series.\\r\\nMulti-task per-series transformation. Note, as depicted in Figure 1, after the inter-series\\r\\nattention layer application, a single transformer network is applied to the combined, transformed\\r\\nrepresentation of the target series, q. This applies time series transformation as is commonly done\\r\\nwith Transformers, across time, to capture temporal affects and derive a forecast for future time points.\\r\\nNotably, this network is shared for all time series (e.g., all products q). That is, the same network\\r\\nparameters are used for all time series, and thus all data can be used to learn the parameters of\\r\\nthis shared network - which as mentioned in the Related Work, Section 2, can often enable better\\r\\nperformance / avoid overfitting of multivariate / multi-time-series output approaches. We call this\\r\\nmulti-task application because one shared model is used for each task (product / time series) and\\r\\napplied separately for each, and the model network can also learn to adapt its behavior based on the\\r\\nidentifying features and representation elements for each time series, as needed (i.e., as driven by the\\r\\ndata).\\r\\n3.3.2. Projection to High Dimensional Representation\\r\\nIn NLP tasks, inputs are typically sequences of single discrete (categorical) values that need to be\\r\\nquantized through an embedding process to create fixed vector-value representations for each word.\\r\\nYet, in the context of time series forecasting, features may contain both discrete and real-valued\\r\\n14\\nquantities, and of different types (for example multiple different categorical variables), presenting a\\r\\nmore complex challenge, not addressed by the original Transformer. Projecting the features to a\\r\\nmeaningful representation is a critical aspect for Transformer models that use real-valued features as\\r\\nwell.\\r\\nTo address this issue, we map categorical and real-valued features independently from each other\\r\\nto increased dimensions. For real-valued features, we use a linear layer of weights to learn the optimal\\r\\nmapping. Specifically, we address the issue of low-dimensional real-valued features by replacing the\\r\\noriginal embedding layer with a linear layer of weights, which allows us to learn the optimal mapping\\r\\nfor projecting the inputs to the desired dimensionality. This process creates a set of high-dimensional\\r\\ncontinuous input features. For categorical features, we employ the original embedding layers to create\\r\\nfeature vectors for different features. Finally, the feature vectors and the projected continuous inputs\\r\\nare concatenated before being passed through the Transformer.\\r\\nOur approach allows us to take advantage of the full range of feature types and dimensions available\\r\\nin the data, creating a more comprehensive and informative input for the Transformer model. By\\r\\nmapping the different types of features independently, we can ensure that each feature is represented\\r\\noptimally in the model, enhancing its ability to capture the complex patterns present in time series\\r\\nforecasting data.\\r\\n3.3.3. Abandonment of Positional Encoding\\r\\nThe positional encoding layer is responsible for assigning a relative position to each element in\\r\\na given sequence. In the context of time series forecasting, information such as the date and time\\r\\nof day can be critical for producing accurate forecasts. While one approach to incorporating date\\r\\ninformation into the positional encoding involves adding it as input features and enforcing the positional\\r\\nencoding, this can potentially corrupt the original input and negatively impact the model’s stability\\r\\nand ability to learn relationships, as observed in 6.2. To address this issue, we propose removing\\r\\nthe positional encoding altogether and relying solely on the date-time features to capture relative\\r\\npositioning. Specifically, we map the date into two separate features, one capturing age (year) and the\\r\\nother capturing the month of the year. Note, for more granular time series, a similar process could be\\r\\nperformed for other more granular time segments. To ensure stable training in the deep network, we\\r\\nscale the age feature using the natural log and the month feature to be between -0.5 and 0.5. This\\r\\napproach allows the model to learn the relative positioning of elements in the sequence based on the\\r\\nknown fact that each element follows the prior by one month.\\r\\n15\\n4. Experimental Setup\\r\\n4.1. Datasets\\r\\nOur study involves evaluating our proposed model and other time series methods on a private\\r\\ndataset provided by a medical device manufacturing company, which is split into two parts, type 1 and\\r\\ntype 2, as explained below. This dataset serves as a guiding case study to improve the limitations of\\r\\ntime series forecasting in retail settings. To further validate our approach, we apply our method to two\\r\\npublicly available retail datasets, namely Walmart Stores Sales and Walmart M5 [57]. This allows us\\r\\nto compare the effectiveness of our Inter-Series Transformer model with other time series forecasting\\r\\ntechniques, which have been discussed in section 2, on large-scale sales datasets. By evaluating our\\r\\napproach on both private and public datasets, we can gain a more comprehensive understanding of its\\r\\npotential impact and applicability in real-world retail forecasting scenarios.\\r\\nPrivate Dataset Type 1 Private Dataset Type 2 Walmart Store Sales Walmart M5\\r\\nNumber of time series 65 50 4,410 44,280\\r\\nFrequency Monthly Monthly Weekly Daily\\r\\n1-3 Months 1-3 Months\\r\\nForecast window 4-12 Months 4-12 Months 39 weeks 28 days\\r\\n13-24 Months 13-24 Months\\r\\nMetric wMAPE wMAPE RMSE RMSSE\\r\\nTable 1: Information on datasets.\\r\\nPrivate Small Retail Dataset. The primary dataset used to evaluate our models is a small\\r\\nretail sales dataset of a medical device manufacturer. Products can be identified with three unique\\r\\nidentifiers in a hierarchical structure. Our dataset consists of products across different distribution\\r\\ncenters in the world, and each time series corresponds to a specific product at a specific distribution\\r\\ncenter. We use this approach to avoid having multiple data points at a single time point for a product,\\r\\nwhich could potentially result in the loss of information through aggregation.\\r\\nMost of the literature on time series forecasting methods focuses on extremely large datasets, which\\r\\nhave been shown to be effective with deep learning techniques that can learn a general understanding of\\r\\ntime series mechanisms. However, these approaches may not be applicable in smaller private settings\\r\\nsuch as ours. Our data consists of two types of products identified at level 1 and resulting in two\\r\\nseparate datasets. The first dataset comprises 65 time series corresponding to type 1 products, which\\r\\nexhibit a general increasing trend. The second dataset comprises 50 time series corresponding to type\\r\\n2 products with a general decreasing trend. Retail datasets often exhibit unpredictable patterns due\\r\\nto external forces that we may not be able to model. Additionally, time series can drop in and out\\r\\n16\\nat different time points due to new product introductions, adding to the complexity of forecasting.\\r\\nOur Transformer method employs learned embedding vectors to ensure that new time series can still\\r\\ngenerate predictions through their identifier features, along with self-attention which is applicable for\\r\\nvariable length sequences and multi-task learning so the shared network is trained across the variety\\r\\nof series and this can work effectively on new series.\\r\\nRetail Datasets. In addition to conducting experiments on the private dataset provided by a\\r\\nthird-party company, we also evaluated the effectiveness of our custom Transformer and other bench\\x02mark models on two publicly available Walmart datasets. This allowed us to measure the performance\\r\\nof our model in comparison to other models on much larger datasets, providing insights from our\\r\\ncustom attention mechanisms on a massive number of products.\\r\\nThe first dataset we used is the Store Sales forecasting dataset provided by the Walmart Recruiting\\r\\nteam for a Kaggle competition. This dataset contains time series of sales for 98 departments at 45\\r\\ndifferent Walmart stores. While there is no information on specific products, we can consider each\\r\\ndepartment as a separate product, giving us a total of 4,410 time series for potential input during\\r\\ntraining. One key difference between this dataset and the private one is that we do not have access to\\r\\nany department-specific features, only store-specific ones.\\r\\nThe second dataset we experimented with is the M5 dataset obtained from Walmart [57], which\\r\\nis even larger in size compared to the previous Walmart dataset. The data is separated into 3,049\\r\\ndifferent products sold by Walmart in the US at different stores, and includes aggregated series as well\\r\\nbased on the category and department of the product and state location of the stores. This results in\\r\\na total of 44,280 time series to model on. The significant increase in data makes it difficult to obtain\\r\\ngood performance with sufficient epochs, but we present results on a small number of epochs across\\r\\nseveral algorithms.\\r\\n4.2. Training Procedure & Metrics\\r\\nTo maximize the performance of the Inter-Series Transformer model, we conducted hyperparameter\\r\\ntuning on several key parameters such as the number of encoder / decoder layers, model dimension,\\r\\nembedding dimension, batch size, and the number of training epochs on validation data. We also\\r\\nexperimented with different learning rate scheduling approaches in preliminary study on a subset of\\r\\nthe data. Since we did not see much improvement with more exotic approaches, we used a common\\r\\napproach for our experiments. We fixed the learning rate schedule to reduce the learning rate on\\r\\nplateau (by 5%), with a starting learning rate of 0.0015 and using the Adam optimizer. In the\\r\\nfinal training setup, we determined the best hyperparameters to use were 2 encoder/decoder layers,\\r\\n128 model dimension, an embedding dimension of 6 (this is the dimensionality of each time-step\\r\\n17\\nin our series), batch size of 64 and 1000 epochs. This resulted in the best validation results for our\\r\\nmodel. In addition, we similarly fine-tuned the hyperparameters of all neural network methods used for\\r\\ncomparison to optimize their performance, ensuring consistency and soundness of the results presented\\r\\nin Section 5. The inter-series transformer required its own tuning-framework as prototype code was\\r\\ndeveloped, but since the others are popular models we were able to use AutoML directly to train\\r\\nseveral versions in a parallelized fashion and compare. The shown results are the best results after\\r\\nhyperparameter tuning.\\r\\nAs shown in Table 1, the evaluation window depends on the dataset. For the private dataset, we\\r\\nevaluated our model on three distinct time ranges: short-range (1-3 months), mid-range (4-12 months),\\r\\nand long-range (13-24 months). For the Walmart Store Sales, the forecast window is 39 weeks, while\\r\\nfor Walmart M5, it is 28 days. To obtain the final score for each range, we calculated the average of\\r\\nscores / metrics across all evaluation frequencies within the respective period.\\r\\nThe choice of metric also depends on our datasets as well as the recommendations given to us by the\\r\\nprivate retailer providing the medical dataset - i.e., the metrics the retailer wanted to use for evaluation\\r\\nand uses internally. Precise definitions of the metrics used are given below. Regarding the metrics used\\r\\nin the Kaggle competitions, while the Walmart Store Sales competition employed weighted-MAE, we\\r\\nuse RMSE instead as that competition was an older one, and future modern competitions have preferred\\r\\nsquared-error metrics, such as the M5 competition. Furthermore, weighting the absolute errors ends\\r\\nup doing something similar to MSE anyway which naturally weights larger errors more and on average\\r\\nlarger magnitude series. Additionally as the data is at an aggregate level scaling is likely not necessary,\\r\\nand thus RMSE should work comparably to the original competition metric. For the Walmart M5\\r\\ncompetition, we employ the Root Mean Squared Scaled Error (RMSSE) as used in the competition -\\r\\nwhere scaling is applied to account for the greater variation and sporadic nature and sparsity in the\\r\\nseries, enabling directly comparing scores across series. However, unlike the competition we do not\\r\\nfurther weight each series by recent training sales volumes. The main reason is this weighting biases\\r\\nthe scoring to more heavily weight the fewer, high level aggregate series, which become smoother and\\r\\neasier to predict as the aggregation increases. This can be seen from the final weighted average scores\\r\\nin the competition, as well as the average high level aggregate scores, having significantly lower RMSSE\\r\\nthan the granular, low-level series average scores [57]. This evaluation would defeat the purpose of\\r\\napplying and evaluating our method for which we want to see if it can improve the performance across\\r\\ndiverse retail series and in particular on the sparse, sporadic series in the lower levels of the hierarchy\\r\\nmaking up the majority of the time series. Therefore we use the unweighted RMSSE to obtain a fair\\r\\ncomparison of overall predictive performance across the diverse set of series.\\r\\n18\\nwMAPE =\\r\\nP\\r\\nall series and timesteps|Actual − Forecast|\\r\\nP\\r\\nall series and timesteps|Actual|\\r\\n(4)\\r\\nRMSE = sP\\r\\nall series and timesteps(Actual − Forecast)2\\r\\nmT\\r\\n(5)\\r\\nHere, m is the number of series and T the number of timesteps.\\r\\nRMSSE =\\r\\nvuut\\r\\n1\\r\\nh\\r\\nPn+h\\r\\nt=n+1(yt − yˆt)\\r\\n2\\r\\n1\\r\\nn−1\\r\\nPn\\r\\nt=2(yt − yt−1)\\r\\n2\\r\\n(6)\\r\\nHere RMMSE is shown for one series, yt is the actual future value at time t, ˆyt is the predicted value,\\r\\nn is the number of historical observations and h is the horizon.\\r\\n5. Results\\r\\nWe applied our custom Inter-Series Transformer model, along with traditional time series methods\\r\\nand neural network models, to the datasets described in 4.1. In this section, we present and compare\\r\\nthe results of our approach with those of benchmark models.\\r\\nMethod Type 1 Type 2 Training Time\\r\\n1-3 Months 4-12 Months 13-24 Months 1-3 Months 4-12 Months 13-24 Months in seconds\\r\\nBaseline 21.7% 49.3% 71.3% 16.8% 27.8% 58.2% N/A\\r\\nSES 65.3% 73.2% 121.1% 55.8% 63.8% 109.5% 10s\\r\\nTraditional time series HW 60.2% 65.8% 96.6% 53.5% 61.8% 93.0% 27s\\r\\nARIMA 71.9% 77.0% 109.3% 56.9% 68.5% 132.7% 478s\\r\\nFeed-Forward 68.2% 69.0% 109.1% 54.8% 68.3% 105.2% 448s\\r\\nDeepAR 37.6% 39.3% 37.8% 21.9% 23.5% 35.6% 897s\\r\\nNeural Networks GluonTS Transformer 35.0% 36.6% 38.4% 17.0% 19.0% 19.1% 3537s\\r\\nModels TFT 30.0% 28.6% 31.4% 17.0% 20.0% 21.2% 3698s\\r\\nFEDFormer 68.2% 72.2% 85.7% 61.5% 70.9% 80.2% 2843s\\r\\nDLinear 31.0% 48.3% 64.8% 26.6% 37.1% 50.0% 1981s\\r\\nPatchTST 16.5% 26.2% 29.8% 14.1% 18.1% 35.8% 3291s\\r\\nInter-Series Transformer 14.7% 24.9% 59.2% 12.9% 18.8% 40.3% 4137s\\r\\nTable 2: wMAPE Results of Methods discussed on Type 1 and Type 2 Medical Device Datasets Separated by Forecast\\r\\nRange.\\r\\nRegarding our private medical device manufacturing dataset, we also include the results of a baseline\\r\\nmethod, which is the forecast provided to us by a third-party. It was obtained using a business\\x02oriented approach, which was derived from several stakeholder meetings, as opposed to our data\\x02driven approach. As shown in Table 2, the Inter-Series Transformer outperforms the baseline and\\r\\ntraditional time series methods across all forecast ranges, as well as neural networks models for short\\x02term forecasting (1-3 Months) and mid-term forecasting (4-12 Months).\\r\\n19\\nWe first analyze the results from some of the more traditional time series forecasting models. The\\r\\nmethods used include Holt-Winters, Autoregressive Integrated Moving Average (ARIMA), and Simple\\r\\nExponential Smoothing (SES) models. For all of these examples, we fit individual models to each time\\r\\nseries in our private dataset, where a time series pertains to the combination of a specific product at\\r\\na specific distribution center. Comparing the results of these three models to the baseline, it is clear\\r\\nthat the traditional methods alone are not sufficient to reach the desired levels of accuracy on either\\r\\ntype of device.\\r\\nWe then experiment with more complex methods that rely on neural networks. The feed-forward\\r\\nmethod performs worse than some of the traditional methods since there is no form of sequential\\r\\nmodeling using this technique. We also explore models discussed in Section 2, including DeepAR,\\r\\nGluonTS Transformer, and TFT. GluonTS Transformer is representative of the base Transformer\\r\\napproach to forecasting and serves as a starting Transformer model for comparison. It has the same\\r\\nbase architecture as our Inter-Series Transformer, of Transformer encoder-decoder, without all the\\r\\nenhancements / modifications we introduced, and is fed the same features and inputs. While these\\r\\nmethods do not outperform the Inter-Series Transformer for short-range and mid-range forecasting,\\r\\nthey perform better for long-term forecasting (13-24 months), with TFT achieving the best overall\\r\\nresults in this range. In our approach, we prioritize capturing cross-series effects and addressing\\r\\nsparsity in the retail setup. The longer-term targets are set for longer aggregate periods and further\\r\\ninto the future. Consequently, there may be less advantage in adapting long-term predictions solely\\r\\nbased on the recent behavior of other series. That is, recent cross time series history may be more\\r\\nbeneficial for nearer-term forecasting, at less-aggregated horizons, and different time series may be\\r\\nmore likely to diverge further into the future, or at least not follow short-term patterns. As such\\r\\nthe Inter-Series modeling we introduce may add additional complexity that hurts overall accuracy in\\r\\nthese long-term aggregate predictions. On the other hand, one of TFT’s notable contributions was the\\r\\nintroduction of a multi-head attention mechanism with an additive aggregation of the different heads,\\r\\nspecifically designed to capture long-term dependencies, which could explain its superior performance\\r\\nin long-term predictions.\\r\\nAdditionally, we initially applied two of the most recent, state-of-the-art, time series forecasting\\r\\nmodels to our datasets, FEDformer, and DLinear, and compared their performances with the other\\r\\nmethods. Although these models are designed to improve performance for long-term time series fore\\x02casting, the FEDformer model does not outperform the baseline for any range level, and the DLinear\\r\\nmodel outperforms it by a small margin for long-term forecasting. However, the proposed Inter-Series\\r\\nTransformer achieves better results across all ranges. This may be in part due to designing and testing\\r\\nthese approaches on non-retail data that has different and often more predictable temporal patterns\\r\\n20\\nas opposed to retail / count data type of time series. This further illustrates the importance of test\\x02ing algorithms on diverse use cases and evaluating them with targeted case studies for these different\\r\\ndomains such as we provide here for retail data.\\r\\nSubsequent to our initial experiments and write up, PatchTST was released as a new state-of\\x02the-art transformer forecasting method, particularly for longer time ranges, so we compared results\\r\\nwith this method as well. For Type 1, PatchTST does perform better at the long time range but is\\r\\nout-performed by our method in the shorter 2 ranges, and for Type 2 our method out-performs it at\\r\\nthe shortest range, has comparable performance at the medium range, and is out-performed by it at\\r\\nthe longer range. This demonstrates PatchTST can be more effective at longer ranges, and similar\\r\\nexplanation and conclusion as given with TFT above can be made.\\r\\nMethod Walmart Store Sales Walmart M5\\r\\nRMSE RMSSE\\r\\nDeepAR 0.651 1.010\\r\\nTFT 0.528 1.083\\r\\nDLinear 0.556 0.956\\r\\nHW 1.190 1.581\\r\\nInter-Series Transformer 0.511 0.809\\r\\nPatchTST 0.492 0.976\\r\\nTable 3: Results of Methods discussed on Walmart Retail Datasets.\\r\\nFurthermore, we evaluated the performance of our Inter-Series Transformer on the two Walmart\\r\\ndatasets described in Section 4.1. The Walmart M5 dataset is very large and at a more granular level,\\r\\nwhile the Walmart Store Sales dataset is smaller and at a more aggregate level, though both are larger\\r\\nthan our private retail dataset, so collectively these set of datasets make up a suitable collection to\\r\\nevaluate and analyze our method across a range of different retail forecasting use cases. I.e., these\\r\\ntwo datasets allow us to measure the effectiveness of our approach and compare it to other time series\\r\\nmodels on a more diverse set of retail data.\\r\\nAs seen in Table 3, we compare the performance of our model with that of one traditional time\\r\\nseries method, Holt-Winters, as well as deep learning models: DeepAR, TFT, and DLinear, i.e., the\\r\\nbest-performing model of each category previously, as well as PatchTST which performed well in some\\r\\ncases on the previous, target dataset, and is the most recent state-of-the-art comparison method.\\r\\nBased on these experiments, the Inter-Series Transformer outperforms all of these methods on the\\r\\nmore granular M5 Walmart dataset, and is second place and close to the best (PatchTST) on the\\r\\n21\\nmore aggregate and simpler Walmart Store Sales dataset (which also had less features available to\\r\\nmake use of). Therefore, the Inter-Series Transformer is a promising approach that can achieve high\\r\\nperformance on datasets of various sizes, as compared to the benchmarks. For the case of the Walmart\\r\\nStore Sales dataset, the lack of features and similarity and regularity of the different series due to\\r\\naggregate (department) level forecasting, along with the relatively longer forecast window, can explain\\r\\nwhy the unique components of our approach were not able to lead to improved performance over the\\r\\nlong-term, basic per-series forecast approach of PatchTST. Nevertheless our method was still very\\r\\ncompetitive even on this dataset and still obtained the second best score.\\r\\nFinally, Figure 2 highlights how the time series of the different products of type 1 are learning to\\r\\npay attention among themselves - illustrating cross-series attention weights for one prediction / time\\r\\npoint, and provides an example of interpretability achievable with this approach.\\r\\nFigure 2: Attention weights learned between products / time series of type 1 - for one prediction. Each row shows the\\r\\nattention weights for each other series across the columns, for that target series. Lighter color indicates a higher value.\\r\\nBy analyzing the corresponding products and time series, we observe that time series with higher\\r\\n22\\nattention weights generally have greater volume and stability. In contrast, sparser time series, charac\\x02terized by lower volume and fewer observations, rely more on these high-volume time series. Figure 2\\r\\nillustrates how a few series, specifically products 1, 4, 10 and 41, attract most of the attention weight\\r\\nfrom other series. These series are associated with high volumes, as seen in Figure 3. Conversely, the\\r\\nother products that focus their attention on these high-volume products, like products 6 and 45, exhibit\\r\\nsparser series, as illustrated in Figure 4, and demonstrate a skewed-value distribution, as emphasized\\r\\nby Figure 5. These observations support our hypothesis that introducing the Inter-Series Attention\\r\\nlayer can help address issues of sparsity and skewed-value distribution by allowing products to learn\\r\\nfrom larger volume time series, thereby improving predictions.\\r\\nFigure 3: Example of high-volume products. Products 1, 4, 10 and 41 have most of the attention weight from sparser\\r\\ntime series as shown in Figure 2.\\r\\nFigure 4: Example of low-volume and sparse products. Many time series, such as products 6 and 45, depend highly on\\r\\nproducts 1, 4, 10 and 41 as shown in Figure 2.\\r\\n23\\n(a) Distribution of products 6 and 45. (b) Distribution of products 4 and 10.\\r\\nFigure 5: Comparison of value distribution for sparse (products 6 and 45) and high-volume (products 4 and 10) time\\r\\nseries.\\r\\n6. Analysis & Ablation Study\\r\\nWe are able to improve on the existing benchmark for the private dataset provided to us by\\r\\nincorporating the new components described in 3.3. Yet, the effectiveness of our model could be\\r\\nlimited to the specific evaluation period used, so we apply a time series style cross-validation technique\\r\\n[58] to evaluate its robustness. Specifically, for each evaluation period, we trained individual models\\r\\nprior to that period and evaluated them on the evaluation period, and calculated the average of the\\r\\nresults across the periods. The training data included all historical data prior to the evaluation period,\\r\\nand we treated the separate evaluation periods as test sets. Although our training datasets were not\\r\\nall of the same sizes and some contained older data than others, we attempted to balance them by\\r\\nlimiting how far back in time the model could begin training.\\r\\nTo check the robustness of our method, we measure performance across time-periods without tuning\\r\\nany parameters and shift our evaluation set. We provide results across several evaluation periods. Table\\r\\n4 shows that our custom approach can successfully perform across different time periods and is not\\r\\nsimply overfitting or performing well for a particular period. The results were calculated using the\\r\\nsliding window method illustrated in Figure 6. These results indicate that the model can significantly\\r\\noutperform the baseline forecasts across different time periods.\\r\\nFurthermore, we conducted an ablation study focusing on two key aspects of the Transformer\\r\\nmodel:\\r\\n• We explored the different options of projecting our features to a higher dimension to optimize\\r\\nlearning.\\r\\n24\\nForecast Start Date Forecast Range Metric Baseline Result Inter-Series Transformer % Improvement\\r\\n06/2017 1-3 Months wMAPE 20.35% 12.03% 8.32%\\r\\n03/2018 1-3 Months wMAPE 46.54% 21.11% 25.43%\\r\\n09/2018 1-3 Months wMAPE 55.21% 35.90% 19.31%\\r\\nTable 4: Inter-Series Transformer Results Compared to Baseline Results over several evaluation periods for short-range\\r\\nforecast.\\r\\nFigure 6: Sliding evaluation framework for the case of 1-Month forecast.\\r\\n• We investigated the effectiveness of not using positional encoding, as well as alternative ap\\x02proaches to capture temporal ordering in a sequence.\\r\\nThe results presented in this section pertain to the private dataset. In addition, we introduce a new\\r\\nmetric, wBias, which was added to meet specific needs of the medical device manufacturing company\\r\\nin the longer horizons.\\r\\nwBias =\\r\\nP\\r\\nall series(series volume × |mean(Actual)-mean(Forecast)|)\\r\\nP\\r\\nall series\\r\\n(7)\\r\\n6.1. Projection to High Dimensional Representation\\r\\nIn the context of time series forecasting, we are not dealing entirely with discretized quantities\\r\\nanymore and have features that contain a real quantifiable value before any form of embedding. We\\r\\nare also not exclusively dealing with one type of feature (e.g., discrete words) but have access to\\r\\na variety of features of different types, including categorical features such as identifier features and\\r\\ncontinuous features as historical price. These differences leave us with many options for how we choose\\r\\nto handle projecting our features to a higher dimension for the deep network.\\r\\nFor continuous real-valued features, we addressed the issue of our low-dimension by replacing the\\r\\ninput embedding layer with a linear layer. This allows us to learn the optimal way of mapping our\\r\\n25\\ninput to our desired dimensionality. While we treat the date of our input as two separate continuous\\r\\nfeatures, we still have an important categorical feature that we must address: the identifier. Since we\\r\\nhave a hybrid identifier, we experiment with two different approaches to embedding this information.\\r\\nThe first approach was to use the combined identifier as a single feature and employ a single embedding\\r\\nlayer to map a time series to a feature vector. The second approach was to separate the two identifier\\r\\nfeatures and map them with the help of two separate embedding layers to create two feature vectors.\\r\\nThe second option offers the possibility to learn similarities between the same product at different\\r\\ncenters or vice-versa. These feature vectors are created alongside but independently from the projected\\r\\ninput, which we replaced the original embedding layer with. The projected continuous inputs and the\\r\\ncategorical feature vectors are then concatenated before being passed through the Transformer.\\r\\nWe also experimented with different ways of incorporating the categorical ID features into the\\r\\ninput. This required modifying the way we projected our input to a higher dimension in order to\\r\\noptimize learning. Ultimately, the results in Table 5 showed that embedding the two separate ID\\r\\nfeatures (location, product ID) independently yielded the best results since the model can optimize\\r\\nboth embeddings separately.\\r\\nMetric Joint Embedding Separate Embedding with one ID Feature Separate Embedding with two ID Feature\\r\\n1-3 Months wAvgMAPE 25.94% 24.64% 24.58%\\r\\n1-3 Months wMAPE 25.68% 23.86% 23.72%\\r\\n4-12 Months wBias 1632.75 1627.16 1377.52\\r\\n4-12 Months wMAPE 50.78% 53.56% 49.57%\\r\\n13-24 Months wBias 1038.61 813.806 788.290\\r\\n13-24 Months wMAPE 58.12% 66.45% 60.38%\\r\\nTable 5: High-Dimensional Projection Experiments.\\r\\n6.2. Positional Encoding\\r\\nThe positional encoding aspect of Transformers is critical in NLP applications where there is no\\r\\nother notion of a time point, but in time series applications, we have the benefit of having actual\\r\\ntime features. Because of this, we experiment with using the positional encoding alongside our time\\r\\ninformation and simply excluding the positional encoding.\\r\\nRegarding the positional encoding, one attempt to strengthen it was to simply provide any im\\x02portant date information as features of our input as well as enforce the positional encoding. While\\r\\nthis resulted in more accurate forecasts compared to benchmark models, the results were still not\\r\\nsatisfactory. The positional encoding vector which is generated is added to our original sequence to\\r\\nget the input which will continue through our model. The fact that we alter our initial input with\\r\\n26\\nthis positional encoding could impact the stability of our training and ability to learn relationships\\r\\neffectively. The second approach is to remove the positional encoding altogether to combat the fact\\r\\nthat it is corrupting our original input. The model should be able to learn the relative positioning of\\r\\nelements just as easily through the date feature provided, particularly since the elements that follow\\r\\neach other in a sequence will always be one month after the prior. In order to facilitate the learning of\\r\\nthis fact, we decided to map the date into two separate features: one which captures the age (or year)\\r\\nand one which captures the month of the year.\\r\\nWe conducted experiments to compare two possibilities: first, adding positional encoding to the\\r\\ncategorical time features, and second, mapping the date into two continuous time features while re\\x02moving positional encoding. As shown in Table 6, we found that utilizing continuous time features\\r\\nwithout positional encoding yielded the best results. It appeared that positional encoding mostly did\\r\\nnot provide any additional information and only added complexity to the training process for the\\r\\nmodel. Therefore, we made the decision to exclude positional encoding and instead rely on continuous\\r\\nfeatures.\\r\\nMetric Categorical Time Feature with Positional Encoding Continuous Time Features without Positional Encoding\\r\\n1-3 Months wAvgMAPE 30.1% 25.94%\\r\\n1-3 Months wMAPE 31.0% 25.68%\\r\\n4-12 Months wBias 1401.75 1632.75\\r\\n4-12 Months wMAPE 53.7% 50.78%\\r\\n13-24 Months wBias 596.910 1038.61\\r\\n13-24 Months wMAPE 59.3% 58.12%\\r\\nTable 6: Positional Encoding Experiments.\\r\\n7. Conclusion\\r\\nAlthough Transformer-based models have shown remarkable performance in time series forecast\\x02ing, their application to supply chain demand forecasting is still limited. Therefore, we introduce the\\r\\nInter-Series Transformer, a novel architecture that incorporates a new attention layer, the Inter-Series\\r\\nattention layer, to capture cross-series effects in a controlled manner while still enabling multi-task\\r\\nTransformer network application to generate forecasts per-series, and address the sparsity and over\\x02fitting challenges in supply chain-related time series data. We evaluate the proposed model on a\\r\\nprivate dataset from a medical device manufacturer as well as larger retail datasets, and the results\\r\\ndemonstrate that the Inter-Series Transformer outperforms traditional time series models and neural\\r\\nnetwork models, and is competitive with and often out-performs a number of prior state-of-the-art\\r\\nneural network and Transformer forecast models in most cases. Furthermore, the model’s robustness\\r\\n27\\nis evidenced by its ability to outperform the baseline for various start evaluation times. We also found\\r\\nthat positional encoding was not necessary for higher performance and instead recommend adding date\\r\\nfeatures. Our proposed model presents a promising approach for supply chain demand forecasting,\\r\\nwith implications for enhancing supply chain management and operations.\\r\\nSeveral avenues for future work can build on the promising results of the Inter-Series Transformer\\r\\nin supply chain demand forecasting. One potential direction could be to explore extending the Inter\\x02Series Transformer’s attention mechanism to features of the other time series data in addition to the\\r\\ntarget time series. This would allow for a more comprehensive analysis of the relationships between\\r\\ndifferent features and potentially lead to more accurate forecasting results. Additionally, an interesting\\r\\ndirection for further research would be to investigate the potential benefits of increasing the depth of\\r\\nthe Inter-Series Transformer by adding multiple Inter-Series attention layers. This could allow to cap\\x02ture more complex nonlinear transformations of inputs, potentially leading to even better forecasting\\r\\nperformance. These directions present exciting opportunities to further improve and expand upon the\\r\\nproposed Inter-Series Transformer architecture for time series forecasting.\\r\\nReferences\\r\\n[1] R. Fildes, K. Nikolopoulos, S. F. Crone, A. A. Syntetos, Forecasting and operational research: A\\r\\nreview, The Journal of the Operational Research Society 59 (9) (2008) 1150–1172.\\r\\nURL http://www.jstor.org/stable/20202189\\r\\n[2] K. jae Kim, Financial time series forecasting using support vector machines, Neurocom\\x02puting 55 (1) (2003) 307–319, support Vector Machines. doi:https://doi.org/10.1016/\\r\\nS0925-2312(03)00372-2.\\r\\nURL https://www.sciencedirect.com/science/article/pii/S0925231203003722\\r\\n[3] C. Chatfield, Time-Series Forecasting, CRC Press, 2000.\\r\\nURL https://books.google.com/books?id=PFHMBQAAQBAJ\\r\\n[4] T. M. Mitchell, The need for biases in learning generalizations, Tech. Rep. CBM-TR-117, Rutgers\\r\\nUniversity (1980).\\r\\n[5] D. F. Gordon, M. Desjardins, Evaluation and selection of biases in machine learning, Machine\\r\\nlearning 20 (1) (1995) 5–22.\\r\\n[6] E. McKenzie, General exponential smoothing and the equivalent arma process, Journal of Fore\\x02casting 3 (3) (1984) 333–344.\\r\\n28\\n[7] R. Hyndman, A. B. Koehler, J. K. Ord, R. D. Snyder, Forecasting with exponential smoothing:\\r\\nthe state space approach, Springer Science & Business Media, 2008.\\r\\n[8] G. E. Box, G. M. Jenkins, Some recent advances in forecasting and control, Journal of the Royal\\r\\nStatistical Society. Series C (Applied Statistics) 17 (2) (1968) 91–109.\\r\\n[9] R. J. Hyndman, G. Athanasopoulos, Forecasting: principles and practice, OTexts, 2018.\\r\\n[10] D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning representations by back-propagating\\r\\nerrors, nature 323 (6088) (1986) 533–536.\\r\\n[11] Z. C. Lipton, J. Berkowitz, C. Elkan, A critical review of recurrent neural networks for sequence\\r\\nlearning, arXiv preprint arXiv:1506.00019.\\r\\n[12] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9 (8) (1997) 1735–\\r\\n1780.\\r\\n[13] F. Gers, D. Eck, J. Schmidhuber, Applying lstm to time series predictable through time-window\\r\\napproaches, in: Proceedings of the International Conference on Artificial Neural Networks, 2001,\\r\\npp. 669–676.\\r\\n[14] D. Salinas, V. Flunkert, J. Gasthaus, T. Januschowski, Deepar: Probabilistic forecasting with\\r\\nautoregressive recurrent networks, International Journal of Forecasting 36 (3) (2020) 1181–1191.\\r\\n[15] K. Bandara, C. Bergmeir, H. Hewamalage, Lstm-msnet: Leveraging forecasts on sets of related\\r\\ntime series with multiple seasonal patterns, IEEE Transactions on Neural Networks and Learning\\r\\nSystems (2020) 1–14.\\r\\n[16] G. Lai, W.-C. Chang, Y. Yang, H. Liu, Modeling long-and short-term temporal patterns with deep\\r\\nneural networks, in: The 41st international ACM SIGIR conference on research & development\\r\\nin information retrieval, 2018, pp. 95–104.\\r\\n[17] D. Salinas, M. Bohlke-Schneider, L. Callot, R. Medico, J. Gasthaus, High-dimensional multivariate\\r\\nforecasting with low-rank gaussian copula processes, in: Proceedings of the 33rd International\\r\\nConference on Neural Information Processing Systems, 2019, pp. 6827–6837.\\r\\n[18] S. Smyl, A hybrid method of exponential smoothing and recurrent neural networks for time series\\r\\nforecasting, International Journal of Forecasting 36 (1) (2020) 75–85.\\r\\n[19] N. Nguyen, B. Quanz, Temporal latent auto-encoder: A method for probabilistic multivariate\\r\\ntime series forecasting, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35,\\r\\n2021, pp. 9117–9125.\\r\\n29\\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polo\\x02sukhin, Attention is all you need, in: I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\\r\\nS. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Systems,\\r\\nVol. 30, Curran Associates, Inc., 2017.\\r\\nURL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.\\r\\npdf\\r\\n[21] T. Lin, Y. Wang, X. Liu, X. Qiu, A survey of transformers, AI Opendoi:https://doi.org/10.\\r\\n1016/j.aiopen.2022.10.001.\\r\\nURL https://www.sciencedirect.com/science/article/pii/S2666651022000146\\r\\n[22] Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, L. Sun, Transformers in time series: A\\r\\nsurvey, arXiv preprint arXiv:2202.07125.\\r\\n[23] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, X. Yan, Enhancing the locality and\\r\\nbreaking the memory bottleneck of transformer on time series forecasting, Advances in neural\\r\\ninformation processing systems 32.\\r\\n[24] S. Wu, X. Xiao, Q. Ding, P. Zhao, Y. Wei, J. Huang, Adversarial sparse transformer for\\r\\ntime series forecasting, in: H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.),\\r\\nAdvances in Neural Information Processing Systems, Vol. 33, Curran Associates, Inc., 2020, pp.\\r\\n17105–17115.\\r\\nURL https://proceedings.neurips.cc/paper/2020/file/c6b8c8d762da15fa8dbbdfb6baf9e260-Paper.\\r\\npdf\\r\\n[25] B. Lim, S. O. Arık, N. Loeff, T. Pfister, Temporal fusion transformers for interpretable multi- ¨\\r\\nhorizon time series forecasting, International Journal of Forecasting 37 (4) (2021) 1748–1764.\\r\\n[26] B. Tang, D. S. Matteson, Probabilistic transformer for time series analysis, Advances in Neural\\r\\nInformation Processing Systems 34 (2021) 23592–23608.\\r\\n[27] Y. Lin, I. Koprinska, M. Rana, Ssdnet: State space decomposition neural network for time series\\r\\nforecasting, in: 2021 IEEE International Conference on Data Mining (ICDM), IEEE, 2021, pp.\\r\\n370–378.\\r\\n[28] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, W. Zhang, Informer: Beyond efficient\\r\\ntransformer for long sequence time-series forecasting, in: Proceedings of the AAAI Conference on\\r\\nArtificial Intelligence, Vol. 35, 2021, pp. 11106–11115.\\r\\n30\\n[29] H. Wu, J. Xu, J. Wang, M. Long, Autoformer: Decomposition transformers with auto-correlation\\r\\nfor long-term series forecasting, Advances in Neural Information Processing Systems 34 (2021)\\r\\n22419–22430.\\r\\n[30] S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, S. Dustdar, Pyraformer: Low-complexity pyra\\x02midal attention for long-range time series modeling and forecasting, in: International Conference\\r\\non Learning Representations, 2021.\\r\\n[31] L. Shen, Y. Wang, Tcct: Tightly-coupled convolutional transformer on time series forecasting,\\r\\nNeurocomputing 480 (2022) 131–145. doi:https://doi.org/10.1016/j.neucom.2022.01.039.\\r\\nURL https://www.sciencedirect.com/science/article/pii/S0925231222000571\\r\\n[32] R.-G. Cirstea, C. Guo, B. Yang, T. Kieu, X. Dong, S. Pan, Triformer: Triangular, variable\\x02specific attentions for long sequence multivariate time series forecasting, in: L. D. Raedt (Ed.),\\r\\nProceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI\\x0222, International Joint Conferences on Artificial Intelligence Organization, 2022, pp. 1994–2001,\\r\\nmain Track. doi:10.24963/ijcai.2022/277.\\r\\nURL https://doi.org/10.24963/ijcai.2022/277\\r\\n[33] W. Chen, W. Wang, B. Peng, Q. Wen, T. Zhou, L. Sun, Learning to rotate: Quaternion trans\\x02former for complicated periodical time series forecasting, in: Proceedings of the 28th ACM\\r\\nSIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’22, Association for Com\\x02puting Machinery, New York, NY, USA, 2022, p. 146–156. doi:10.1145/3534678.3539234.\\r\\nURL https://doi.org/10.1145/3534678.3539234\\r\\n[34] A. Drouin, E. Marcotte, N. Chapados, TACTiS: Transformer-attentional copulas for time series,\\r\\nin: K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, S. Sabato (Eds.), Proceedings of the\\r\\n39th International Conference on Machine Learning, Vol. 162 of Proceedings of Machine Learning\\r\\nResearch, PMLR, 2022, pp. 5447–5493.\\r\\nURL https://proceedings.mlr.press/v162/drouin22a.html\\r\\n[35] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, R. Jin, FEDformer: Frequency enhanced decomposed\\r\\ntransformer for long-term series forecasting, in: K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\\r\\nG. Niu, S. Sabato (Eds.), Proceedings of the 39th International Conference on Machine Learning,\\r\\nVol. 162 of Proceedings of Machine Learning Research, PMLR, 2022, pp. 27268–27286.\\r\\nURL https://proceedings.mlr.press/v162/zhou22g.html\\r\\n[36] Y. Liu, H. Wu, J. Wang, M. Long, Non-stationary transformers: Rethinking the stationarity in\\r\\ntime series forecasting, arXiv preprint arXiv:2205.14415.\\r\\n31\\n[37] S. Gelper, I. Wilms, C. Croux, Identifying demand effects in a large network of product categories,\\r\\nJournal of Retailing 92 (1) (2016) 25 – 39. doi:https://doi.org/10.1016/j.jretai.2015.05.\\r\\n005.\\r\\nURL http://www.sciencedirect.com/science/article/pii/S0022435915000536\\r\\n[38] P. S. Leeflang, J. P. Selva], A. V. Dijk], D. R. Wittink, Decomposing the sales promotion bump\\r\\naccounting for cross-category effects, International Journal of Research in Marketing 25 (3) (2008)\\r\\n201 – 214. doi:https://doi.org/10.1016/j.ijresmar.2008.03.003.\\r\\nURL http://www.sciencedirect.com/science/article/pii/S0167811608000347\\r\\n[39] S. R. Srinivasan, S. Ramakrishnan, S. E. Grasman, Identifying the effects of cannibalization on\\r\\nthe product portfolio, Marketing intelligence & planning.\\r\\n[40] A. Zeng, M. Chen, L. Zhang, Q. Xu, Are transformers effective for time series forecasting?, arXiv\\r\\npreprint arXiv:2205.13504.\\r\\n[41] S. Makridakis, E. Spiliotis, V. Assimakopoulos, The m5 competition: Background, organization,\\r\\nand implementation, International Journal of Forecasting 38 (4) (2022) 1325–1336, special Issue:\\r\\nM5 competition. doi:https://doi.org/10.1016/j.ijforecast.2021.07.007.\\r\\nURL https://www.sciencedirect.com/science/article/pii/S0169207021001187\\r\\n[42] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recog\\x02nition, Proceedings of the IEEE 86 (11) (1998) 2278–2324.\\r\\n[43] R. Johnson, T. Zhang, Semi-supervised convolutional neural networks for text categorization via\\r\\nregion embedding, in: C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances\\r\\nin Neural Information Processing Systems, Vol. 28, Curran Associates, Inc., 2015.\\r\\n[44] N. Kalchbrenner, L. Espeholt, K. Simonyan, A. van den Oord, A. Graves, K. Kavukcuoglu, Neural\\r\\nmachine translation in linear time, 2016.\\r\\nURL https://arxiv.org/abs/1610.10099\\r\\n[45] W. Yin, K. Kann, M. Yu, H. Sch¨utze, Comparative study of cnn and rnn for natural language\\r\\nprocessing, arXiv preprint arXiv:1702.01923.\\r\\n[46] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner,\\r\\nA. Senior, K. Kavukcuoglu, Wavenet: A generative model for raw audio, in: Arxiv, 2016.\\r\\nURL https://arxiv.org/abs/1609.03499\\r\\n32\\n[47] A. Borovykh, S. Bohte, C. W. Oosterlee, Conditional time series forecasting with convolutional\\r\\nneural networks, arXiv preprint arXiv:1703.04691.\\r\\n[48] R. Sen, H.-F. Yu, I. S. Dhillon, Think globally, act locally: A deep neural network approach to\\r\\nhigh-dimensional time series forecasting, Advances in neural information processing systems 32.\\r\\n[49] S. Bai, J. Z. Kolter, V. Koltun, An empirical evaluation of generic convolutional and recurrent\\r\\nnetworks for sequence modeling, arXiv preprint arXiv:1803.01271.\\r\\n[50] Amazon, Gluonts transformer estimator.\\r\\nURL https://ts.gluon.ai/api/gluonts/gluonts.model.transformer.html\\r\\n[51] Y. Nie, N. H. Nguyen, P. Sinthong, J. Kalagnanam, A time series is worth 64 words: Long-term\\r\\nforecasting with transformers, in: International Conference on Learning Representations, 2023.\\r\\n[52] X. Y. Xiaoyong Jin, Yu-Xiang Wang, Inter-series attention model for covid-19 forecasting\\r\\n(2021) 495–503arXiv:https://epubs.siam.org/doi/pdf/10.1137/1.9781611976700.56, doi:\\r\\n10.1137/1.9781611976700.56.\\r\\nURL https://epubs.siam.org/doi/abs/10.1137/1.9781611976700.56\\r\\n[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\\r\\nAttention is all you need, CoRR abs/1706.03762. arXiv:1706.03762.\\r\\nURL http://arxiv.org/abs/1706.03762\\r\\n[54] J. Gehring, M. Auli, D. Grangier, D. Yarats, Y. N. Dauphin, Convolutional sequence to sequence\\r\\nlearning (2017). arXiv:1705.03122.\\r\\n[55] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, C. Eickhoff, A transformer-based framework\\r\\nfor multivariate time series representation learning (2020). arXiv:2010.02803.\\r\\n[56] Anonymous, MQTransformer: Multi-horizon forecasts with context dependent and feedback\\x02aware attention, in: Submitted to The Tenth International Conference on Learning Represen\\x02tations, 2022, under review.\\r\\nURL https://openreview.net/forum?id=rxF4IN3R2ml\\r\\n[57] S. Makridakis, E. Spiliotis, V. Assimakopoulos, M5 accuracy competition: Results, findings, and\\r\\nconclusions, International Journal of Forecasting 38 (4) (2022) 1346–1364, special Issue: M5\\r\\ncompetition. doi:https://doi.org/10.1016/j.ijforecast.2021.11.013.\\r\\nURL https://www.sciencedirect.com/science/article/pii/S0169207021001874\\r\\n33\\n[58] V. Cerqueira, L. Torgo, I. Mozetiˇc, Evaluating time series forecasting models: an empirical study\\r\\non performance estimation methods, Machine Learning 109 (11) (2020) 1997–2028. doi:10.1007/\\r\\ns10994-020-05910-7.\\r\\nURL https://doi.org/10.1007%2Fs10994-020-05910-7\\r\\n34'},\n",
       " {'name': '1909.12227v1.pdf',\n",
       "  'content': '1\\r\\n\\rc 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any\\r\\ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new\\r\\ncollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other\\r\\nworks.\\r\\narXiv:1909.12227v1 [cs.LG] 25 Sep 2019\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\\r\\nStock Prices Prediction using Deep Learning\\r\\nModels\\r\\nJialin Liu, Fei Chao, Member, IEEE, Yu-Chen Lin, and Chih-Min Lin, Fellow, IEEE,\\r\\nAbstract—Financial markets have a vital role in the develop\\x02ment of modern society. They allow the deployment of economic\\r\\nresources. Changes in stock prices reflect changes in the market.\\r\\nIn this study, we focus on predicting stock prices by deep learning\\r\\nmodel. This is a challenge task, because there is much noise and\\r\\nuncertainty in information that is related to stock prices. So this\\r\\nwork uses sparse autoencoders with one-dimension (1-D) residual\\r\\nconvolutional networks which is a deep learning model, to de\\x02noise the data. Long-short term memory (LSTM) is then used to\\r\\npredict the stock price. The prices, indices and macroeconomic\\r\\nvariables in past are the features used to predict the next day’s\\r\\nprice. Experiment results show that 1-D residual convolutional\\r\\nnetworks can de-noise data and extract deep features better than\\r\\na model that combines wavelet transforms (WT) and stacked\\r\\nautoencoders (SAEs). In addition, we compare the performances\\r\\nof model with two different forecast targets of stock price:\\r\\nabsolute stock price and price rate of change. The results show\\r\\nthat predicting stock price through price rate of change is better\\r\\nthan predicting absolute prices directly.\\r\\nIndex Terms—stock, deep learning, LSTM, SAEs\\r\\nI. INTRODUCTION\\r\\nS\\r\\nTOCK time series forecast is one of the main challenges\\r\\nfor machine learning technology because the time series\\r\\nanalysis is required [1]. Two methods are usually used to\\r\\npredict financial time series: machine learning models and\\r\\nstatistical methods [2].\\r\\nStatistical methods can be used to predict a financial time\\r\\nseries. The common methods are autoregressive conditional\\r\\nheteroscedastic (ARCH) methods [3], and autoregressive mov\\x02ing average (ARMA) [4] or an autoregressive integrated mov\\x02ing average (ARIMA) methods. However, traditional statistical\\r\\nmethods generally assume that the stock time series pertains\\r\\nto a linear process, and model the generation process for a\\r\\nlatent time series to forecast future stock prices [5]. However,\\r\\na stock time series is generally a dynamic nonlinear process\\r\\n[6].\\r\\nMany machine learning models can capture nonlinear char\\x02acters in data without prior knowledge [7]. These models\\r\\nare always used to model a financial time series. The most\\r\\ncommonly used models for stock forecasts are artificial neural\\r\\nnetworks (ANN), support vector machines (SVM), and hybrid\\r\\nand ensemble methods. Artificial neural networks have found\\r\\nJ. Liu and F. Chao are with the Cognitive Science Department, School\\r\\nof Information Science and Engineering, Xiamen University, China e-mail:\\r\\n(fchao@xmu.edu.cn). Y.-C. Lin is with Department of Accounting, National\\r\\nChung Hsing University, Taiwan, R.O.C e-mail: (yuchenlin08@gmail.com).\\r\\nC.-M. Lin is with the Department of Electrical Engineering and Innovation\\r\\nCenter for Biomedical and Healthcare Technology, Yuan Ze University,\\r\\nChung-Li, Tao-Yuan 320, Taiwan, R.O.C e-mail: (cml@saturn.yzu.edu.tw).\\r\\nCorresponding Author: Chih-Min Lin\\r\\nManuscript received April 19, 2005; revised August 26, 2015.\\r\\nmany applications in business because they can deal with data\\r\\nthat is non-linear, non-parametric, discontinuous or chaotic for\\r\\na stock time series [8]. Support vector machine is a statistical\\r\\nmachine learning model that is widely applied for pattern\\r\\nrecognition. A SVM model, which learns by minimizing the\\r\\nrisk function and the empirical error and regularization terms\\r\\nhas been derived to minimize the structural risk [9]. Box et\\r\\nel. presented a revised least squares (LS)-SVM model and\\r\\npredicted movements in the Nasdaq Index after training with\\r\\nsatisfactory results [4].\\r\\nDeep learning models, which are an extension of ANNs,\\r\\nhave seen recent rapid development. Many studies use deep\\r\\nlearning to predict financial time series. For example, Ting et\\r\\nal. used a deep convolutional neural network to forecast the\\r\\neffect of events on stock price movements [10]. Bengio et al.\\r\\nused long-short term memory (LSTM) to predict stock prices\\r\\n[11].\\r\\nThis study addresses the problem of noise in a stock time\\r\\nseries. Noise and volatile features in a stock price forecast\\r\\nare major challenges because they hinder the extraction of\\r\\nuseful information [12]. A stock time series can be considered\\r\\nas waveform data, so the technology from communication\\r\\nelectronics such as wavelet transform is pertinent. Bao et al.\\r\\nused a model that combines wavelet transform and stacked\\r\\nautoencoder (SAE) to de-noise a financial time series [13].\\r\\nThis study de-noises data using an autoencoder [14], [15]\\r\\nwith a convolutional resident neural network (Resnet) [16].\\r\\nThis is an adaptive method to reduce noise and dimension\\r\\nfor time sequences. It is different from wavelet transforms\\r\\nin that the kernel of the convolutional neural network adapts\\r\\nto dataset automatically, so it can more effectively eliminate\\r\\nnoise and retain useful information. The experiments use the\\r\\nCSI 300 index, the Nifty 50 index, the Hang Seng index, the\\r\\nNikkei 225 index, the S&P 500 index and the DJIA index\\r\\nare performed and the results are compared with those for\\r\\n[13]. The proposed model gives more accurate predictions, as\\r\\nmeasured by mean absolute percent error (MAPE), Theil U\\r\\nand the linear correlation between the predicted prices and\\r\\nthe real prices. We do both the experiments on predicting\\r\\nstock price directly and on predicting price rate of change\\r\\nand calculating the price indirectly. We found that the latter\\r\\ncan achieve better accuracy. Predicting future price indirectly\\r\\ncan be seen as adding prior knowledge to improve model\\r\\nperformance.\\r\\nThe remainder of this paper has five sections. The next\\r\\nsection draws the background knowledge of market analysis.\\r\\nSection III details a little experiment about the property of\\r\\nde-noising CNN. Section IV details the structure for the\\r\\nproposed model with sparse autoencoders and LSTM. Section\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\\r\\nV describes the features and data resources for the experiment\\r\\nand details the experiment, and analyzes the results of the\\r\\nexperiment. The last section draws conclusions.\\r\\nII. BACKGROUND\\r\\nUnderstanding the behaviors of the market in order to\\r\\nimprove the decisions of investors is the main purpose of\\r\\nmarket analysis. Several market attributes and features that\\r\\nare related to stock prices time series have been studied.\\r\\nDepending on the market factors that are used, market analysis\\r\\ncan be divided into two categories: fundamental and technical\\r\\nanalysis [17].\\r\\nTechnical analysis often only uses historical prices as market\\r\\ncharacters to identity the pattern of price movement. Stud\\x02ies assume that the relative factors are incorporated in the\\r\\nmovement of the market price and that history will repeat\\r\\nitself. Some investors used technical approaches to predict\\r\\nstock prices with great success [18]. However, the Efficient\\r\\nMarket Hypothesis [19] assumes that all available factors are\\r\\nalready incorporated in the prices so only new information\\r\\naffects the movement of market prices, but new information\\r\\nis unpredictable.\\r\\nFundamental analysis assumes that the related factors are\\r\\nthe internal and external attributes of a company. These\\r\\nattributes include the interest rate, product innovation, the\\r\\nnumber of employees, the management policy and etc [20].\\r\\nIn order to improve the prediction, other information such as\\r\\nthe exchange rate, public policy, the Web and financial news\\r\\nare used as features. Nassirtoussi et al. used news headlines\\r\\nas features to predict the market [21]. Twitter sentiment was\\r\\nused in [22] to improve predictions.\\r\\nIn 1995, one study showed that 85% of responders depend\\r\\non fundamental analysis and technical analysis [23]. Technical\\r\\nanalysis is more useful for short-term forecasting so it is\\r\\npertinent to high frequency trading. Lui et al. showed that\\r\\ntechnical analysis better forecasts turning points than trends,\\r\\nbut fundamental analysis gives a better prediction of trends\\r\\n[23].\\r\\nDepending on the prediction target, tasks can be classified\\r\\nas regression task or classification tasks. For a regression task\\r\\nthe prediction target for the model is the future price, and a\\r\\nclassification task model predicts the rise or fall of the stock\\r\\nprices. If the predicted price is higher than the current price,\\r\\nthe recommended strategy is to buy, and vice versa. This is the\\r\\nbuy-and-sell trading strategy, which is widely used in studies\\r\\n[24]. If the task is to identify the rise or fall in the price,\\r\\nthen the resultant strategy is obvious. Market analysis is also\\r\\nused for recommendation systems. Huang et al. used SVR to\\r\\npredict the return of each stock and to select stocks with the\\r\\nhighest profit margins (top 10, 20 and 30) to calculate the\\r\\nprofit margin [25].\\r\\nThis study uses technical analysis to predict the stock price\\r\\nfor the next day. Sparse autoencoders with 1-D convolution\\r\\nnetworks and prior knowledge are used to give a more accurate\\r\\nprediction than other techniques.\\r\\n0 25 50 75 100 125 150 175 200\\r\\nEpoch\\r\\n0.20\\r\\n0.25\\r\\n0.30\\r\\n0.35\\r\\n0.40\\r\\nLoss on training\\r\\nSingle\\r\\nDenoise\\r\\n(a) Training and test loss\\r\\n0 25 50 75 100 125 150 175 200\\r\\nEpoch\\r\\n0.00\\r\\n0.02\\r\\n0.04\\r\\n0.06\\r\\n0.08\\r\\n0.10\\r\\n0.12\\r\\n0.14\\r\\nGap between train and test\\r\\nSingle\\r\\nDenoise\\r\\n(b) Loss gap\\r\\nFig. 1. Training curve.\\r\\nIII. DE-NOISING CNN\\r\\nTo create a 1-D convolutional neural network for sequence\\r\\nanalysis, a single neural network can be combined with a\\r\\nconvolutional neural network with LSTM. When the features\\r\\nfor the input are extracted at a high-level by the convolution\\r\\nlayer, the price is directly predicted by the LSTM layer. During\\r\\ntraining, the gradient propagates back to convolution through\\r\\nthe LSTM layer. However, if there is too much noise in the\\r\\ndata, this model tends to over-fit.\\r\\nA notional problem is used to compare the model with a\\r\\nsingle neural network. The model uses the features after de\\x02noising. The task is a bias prediction task, in which each data\\r\\npoint corresponds to a function, y = sin(x + 2π ∗ b). The\\r\\ntarget is to predict the value of b in this function, which\\r\\nis sampled from a uniform distribution, U(−1, 1). Here y\\r\\nis the feature vector for the data, where x = [−2π, −2π +\\r\\n4\\r\\nm π, · · · , −2π +\\r\\n4(m−1)\\r\\nm π]\\r\\nT, m is the size of sequence. Two\\r\\ntypes of noises are then added to the features. The first type is\\r\\nGaussian noise, nG ∼ N (µ, δ2). The form of another noise is\\r\\nwritten as λ\\r\\nPn\\r\\ni\\r\\nci exp(x − bri)\\r\\n2\\r\\n, where bri is sampled from\\r\\nthe uniform distribution, U(−1, 1), ciis sampled from the\\r\\n0-1 distribution B(1, 0.5) with possibility p = 0.5 and λ is\\r\\nthe weight of this noise. This noise has multiple peaks that\\r\\ninterfere with prediction. Figure 1a shows the training curves\\r\\nfor both models. The red and green lines are the respective\\r\\ntraining curves for the model that combines CNN with LSTM\\r\\nand uses the features after de-noising. The solid and dashed\\r\\nlines respectively represent the training loss and the test loss.\\r\\nIn Figure 1b the dotted curves indicate the loss gap. When\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\\r\\n6 4 2 0 2 4 6\\r\\nSequence\\r\\n4\\r\\n2\\r\\n0\\r\\n2\\r\\n4\\r\\nFeature value (sin)\\r\\nreal curve\\r\\nfeature point\\r\\nrebuild point\\r\\nFig. 2. Sine curve rebuilding.\\r\\nthe training loss decreases, the loss gap for the model grows\\r\\nslower than that for the single neural network. The minimum\\r\\ntest loss for the proposed model is less than that for the single\\r\\nneural network. It is obvious that de-noising features prevents\\r\\nover-fitting for the model.\\r\\nThe noise for stock forecasting is much more complex than\\r\\nthe noise for this notional task, so in this study the noise in\\r\\nthe stock forecast data is reduced first using 1-D convolution\\r\\nautoencoders. The details of the features of the 1-D CNN\\r\\nautoencoders processes are given. In Figure 2, the yellow dots\\r\\ndenote the rebuilt curve for the sine function. The red curve\\r\\nis the global true, which is the sine function curve without\\r\\nnoise, and the green dots are feature points with noise. The\\r\\nordinate axis represents the specific feature value. Each point\\r\\nrepresents an element input for the model. It is obvious that\\r\\ncurve for the yellow dots is smoother than that for the green\\r\\ndots and it is close to the real curve.\\r\\nThe values for the weights in the convolutional kernel are\\r\\nshown in Figure 3, which is for the model with minimal test\\r\\nloss. The values for the weights in the convolutional kernel\\r\\nare also smoother than those for a single neural network (see\\r\\nFigure 3). However, the sine function is smoother than the\\r\\nnoise, so the kernel in the single network is more likely to\\r\\nmatch the noise than the 1-D convolution autoencoders. This\\r\\nmodel tries to establish a relationship between the noise and\\r\\nthe label. In fact, the noise and the label are irrelevant, so it\\r\\nis more prone to over-fitting.\\r\\nIV. METHODOLOGY\\r\\nIn order to extract high-level abstract features and predict\\r\\nfuture prices from the stock time series, we apply two models\\r\\nin our system, one deep model is used for de-noising and\\r\\nanother is used for prediction. The prediction process involves\\r\\nthree steps:(1) data preprocessing that involves calculating\\r\\ntechnical indicators, clipping and normalizing features, (2)\\r\\nencoding and decoding features using a 1-D ResNet block to\\r\\nminimize the rebuilt loss and (3) using the LSTM to deal with\\r\\nhigh-level abstract features and give a one-step-ahead output.\\r\\nFigure 4 shows the overall framework. The input feature\\r\\nof data sequence is a c × t matrix, where c is the number\\r\\n(a) Kernel in Single\\r\\n(b) Kernel in Autoencoders\\r\\nFig. 3. Convolution kernel of model.\\r\\nof channels, and t is the length of sequence. Daily trading\\r\\ndata, technical indicators and macroeconomic variables are the\\r\\nmatrices of data sequence with size 5×t, 10×t and 2×t. After\\r\\npreprocessing, we merge them into one matrix with size 17×t,\\r\\nso the inputted data sequence has 17 channels. The prices are\\r\\nthen predicted by LSTM after the noise and dimension have\\r\\nbeen reduced by the encoder model.\\r\\nA. Sparse autoencoders\\r\\nSparse autoencoders are models that can reduce the dimen\\x02sion. An autoencoder neural network is used to rebuild the\\r\\ninput (see Figure 5). The loss function, which is used to train\\r\\nautoencoder neural network, is given by [14], [15]\\r\\nL =\\r\\n1\\r\\nN\\r\\nX\\r\\nN\\r\\nn\\r\\n1\\r\\n2\\r\\nkx\\r\\n(n) − y(n)\\r\\nk\\r\\n2 + βLsp (1)\\r\\nwhere N is the number of data points, x\\r\\n(n) denotes the feature\\r\\nvector for the nth sample and y\\r\\n(n) denotes the reconstructed\\r\\nfeature vector for the nth sample. The last term is the sparse\\r\\npenalty term and βsp is the weight. The sparse penalty, which\\r\\nis a kind of regularization, is used to make most units of\\r\\nnetwork tend to non-activity state in order to reduce over\\x02fitting. This is the difference between sparse autoencoders and\\r\\ntraditional autoencoders. The sparse penalty is given by [26],\\r\\nLsp =\\r\\nX\\r\\nS\\r\\nj\\r\\nKL(ρkρˆj )\\r\\n=\\r\\nX\\r\\nS\\r\\nj\\r\\n\\x14\\r\\nρ log ρ\\r\\nρˆj\\r\\n+ (1 − ρ) log 1 − ρ\\r\\n1 − ρˆj\\r\\n\\x15\\r\\n(2)\\r\\nwhere ρ is the sparse parameter, S is the number of units in the\\r\\nhidden layer and ρˆj = σ(xj ), xj is the jth unit in the hidden\\r\\nlayer, σ(x) = 1\\r\\n1+e−x . Weight decay is also used to reduce\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\\r\\n5dim\\r\\n10dim\\r\\n2dim\\r\\n. . . . . .\\r\\n. . . . . .\\r\\n. . . . . .\\r\\nDaily Trading Data\\r\\nTechnical Indicator\\r\\nM acroeconomic Variable\\r\\nUp/\\r\\nDown\\r\\nUp/\\r\\nDown\\r\\nUp/\\r\\nDown\\r\\n1/-1\\r\\n1/-1\\r\\n1/-1\\r\\nD(1) D(2) . . . . . . D(t-1) D(t)\\r\\nNormalization Data\\r\\nf\\r\\nf(1)\\r\\nLSTM\\r\\nUnit\\r\\nf(2)\\r\\nLSTM\\r\\nUnit\\r\\n...\\r\\nLSTM\\r\\nUnit\\r\\nf(N)\\r\\nLSTM\\r\\nUnit\\r\\nLSTM\\r\\nUnit\\r\\nO(N +2)\\r\\n...\\r\\nLSTM\\r\\nUnit\\r\\n...\\r\\nf(t-1)\\r\\nLSTM\\r\\nUnit\\r\\nO(t)\\r\\nf(t)\\r\\nLSTM\\r\\nUnit\\r\\nO(t+1)\\r\\ng\\r\\nh\\r\\nf\\r\\no\\r\\ni\\r\\nCell\\r\\n1Dresnet Sparse\\r\\nAutoencoder\\r\\nlong-short term memory\\r\\nf(N+1)\\r\\nFig. 4. Three steps process of high-level abstract features extraction and prediction.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\\r\\nf\\r\\ny\\r\\n(n)\\r\\nx\\r\\n(n)\\r\\nP\\r\\nS\\r\\nj\\r\\nKL(ρkρˆj )\\r\\n1\\r\\nN\\r\\nPN\\r\\nn\\r\\n1\\r\\n2\\r\\nkx\\r\\n(n) − y(n)\\r\\nk\\r\\n2 + βLsp\\r\\nFig. 5. Sparse autoencoders with 1 dimension convolution neural network.\\r\\nout-fitting of the model. After training, only the features from\\r\\nthe middle layer of the network are used (see Figure 5).\\r\\nThe model for the sparse autoencoders [14], [15] is a 1-\\r\\nD CNN. This is used to compare the performance of WT\\r\\nand CNN in terms of de-noising stock time series data. A\\r\\nconvolution network is used as the encoding network, and\\r\\na deconvolution network is the decoded network [27], so\\r\\nthe model used in SAEs is a fully convolutional network.\\r\\nThe autoencoder’s function is not only to reduce noise, but\\r\\nalso to reduce the dimensions of the features, in order to\\r\\nallow the latter network structure to use a smaller number\\r\\nof weights. The CNN applied here is the ResNet [16], which\\r\\nis a type of convolutional neural network used to speed up\\r\\nthe training by using a “shortcut connections” [16] to back\\x02propagate gradient.\\r\\nB. Long-short term memory\\r\\nLSTM is a type of recurrent neural network (RNN) [28]\\r\\nthat can be used to transfer information from the past to the\\r\\npresent. However, the structure of a RNN has a defect that\\r\\ncan cause the gradient to vanish or explode when the input\\r\\nseries are too long. The problem of the gradient exploding is\\r\\ngenerally solved by gradient clipping\\r\\ngˆ =\\r\\n\\uf8f1\\r\\n\\uf8f2\\r\\n\\uf8f3\\r\\ngˆ ∗ threshold\\r\\nkgˆk\\r\\n, if kgˆk > threshold;\\r\\ng , other, ˆ\\r\\n(3)\\r\\nwhere gˆ represents the gradient of a parameter. The problem\\r\\nof the gradient vanishing is solved by using the structure of the\\r\\nLSTM. A LSTM differs from a conventional RNN in that the\\r\\nLSTM has another memory that transfers its state to the next\\r\\nstate without matrix multiplication and operation of activation\\r\\nfunction, so the gradient is back-propagated smoothly [29].\\r\\nThe details of the LSTM are shown in Figure 6. The left part\\r\\nof figure is the structure of the LSTM unit. The dotted arrows\\r\\nFig. 6. Long-short term memory unit.\\r\\nin the figure indicate the indirect effects. At each step, all the\\r\\ng, i, f and o gates receive the last state and the new feature,\\r\\nand then the cell state and the hidden state are updated at time\\r\\nt, and the input for the unit is the last state vector for the cell\\r\\n(ct−1), the hidden last state vector (ht−1) and the input feature\\r\\n(xt). The four vectors are\\r\\ngt = tanh(Wg[xt, ht−1] + bg) (4)\\r\\nit = σ(Wi[xt, ht−1] + bi) (5)\\r\\nft = σ(Wf[xt, ht−1] + bf ) (6)\\r\\not = σ(Wo[xt, ht−1] + bo) (7)\\r\\nwhere σ(x) = 1\\r\\n1+e−x , and gt is the new information that is\\r\\nused to update the cell state, and it and ft are respectively\\r\\nused to select information that is to be added to cell state or\\r\\nbe forgotten,\\r\\nct = it ∗ gt + ft ∗ ct−1 (8)\\r\\nwhere ∗ denotes element-wise multiplication. The term ot is\\r\\nused to select the output and the hidden state,\\r\\nht = ot ∗ tanh(ct) (9)\\r\\nthen outputt = ht.\\r\\nV. EXPERIMENT\\r\\nThe experiments compare the accuracy of the proposed\\r\\nmethod with that of a deep learning framework [13] for the\\r\\nCSI 300 index, the DJIA index, the Hang Seng index, the\\r\\nNifty 50 index, the Nikkei 225 index and the S&P500 index.\\r\\nSimilar to a previous study [13], more than one market is\\r\\nused. The predictive accuracy is evaluated by MAPE, Theil U\\r\\nand the linear correlation between the prediction and the real\\r\\nprice [30]–[33]. The data is divided into different groups for\\r\\ntraining and testing, in order to reduce the time span.\\r\\nTwo experiments test the performance of the two methods:\\r\\n(1) a 1-D resnet autoencoder is used to predict prices (called\\r\\nC1D-LSTM) and (2) a 1-D resnet autoencoder is used to\\r\\npredict the rate of change of prices (called C1D-ROC). The\\r\\naccuracy of the models is compared and the prediction curve\\r\\nfor one year is plotted.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\\r\\nTABLE I\\r\\nTHE PREDICTION TIME INTERVAL OF EACH YEAR.\\r\\nYear Time Interval\\r\\n1th 2010.10.01∼2011.09.30\\r\\n2th 2011.10.01∼2012.09.30\\r\\n3th 2012.10.01∼2013.09.30\\r\\n4th 2013.10.01∼2014.09.30\\r\\n5th 2014.10.01∼2015.09.30\\r\\n6th 2015.10.01∼2016.09.30\\r\\nA. Data descriptions\\r\\nData resource. The data resource is following a previous\\r\\nstudy [13] from the Figshare website. The data was sampled\\r\\nfrom the WIND(http://www.wind.com.cn) and CSMAR(http:\\r\\n//www.gtarsc.com) databases of the Shanghai Wind Informa\\x02tion Co., Ltd and the Shenzhen GTA Education Tech. Ltd,\\r\\nrespectively. The stock time series is from 1\\r\\nst Jul. 2008 to\\r\\n30th Sep. 2016 (see Table I).\\r\\nData features. Following a previous study [13], three sets\\r\\nof features are selected as the inputs. The first set is the\\r\\ntrading data for the past, including Opening, Closing, High,\\r\\nand Low prices and trading volume. In Table II, Ct, Lt and\\r\\nHt respectively denote the closing price, the low price and\\r\\nthe high price at time t. The second set includes the technical\\r\\nindicators that are widely used for stock analysis. Their\\r\\ncalculation method is shown in Table II, where DIF Ft =\\r\\nEMA(12)t − EMA(26)t, Ds and Dhl respectively denote\\r\\nthe double exponential moving average for C −\\r\\nHH+LL\\r\\n2\\r\\nand\\r\\nHH −LL, where HH and LL respectively denote the highest\\r\\nhigh price and the lowest low price in the range. The last set\\r\\nof features is the macroeconomic information. Stock prices\\r\\nare affected by many factors, so using the macroeconomic\\r\\ninformation as features can reduce uncertainty in the stock\\r\\nprediction. The US dollar index and the Interbank offered rate\\r\\nfor each market are the third set of features.\\r\\nData divide. The data is divided to train multiple models.\\r\\nEach model is trained using past data, and the training data\\r\\nand test data cannot be randomly sampled from the dataset\\r\\nbecause it is irrational. To predict future stock prices, only\\r\\ndata from the past can be used. The greater the time interval\\r\\nbetween the two stock time series data, the smaller is the\\r\\ncorrelation between them; so using outdated data does not\\r\\nimprove performance. In order to take into account the above\\r\\nreason and to simplify the result, the forecast is divided into 6\\r\\nyears; and each year is from 1\\r\\nst Oct. to 30th Sep. (see Table\\r\\nI).\\r\\nB. Evaluation\\r\\nThe experiments use MAPE,the linear correlation between\\r\\nthe predicted price and the real price and Theil U to evaluate\\r\\nthe model. These are defined as\\r\\nMAPE = 1\\r\\nN\\r\\nX\\r\\nN\\r\\nt=1\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\nyt − y\\r\\n∗\\r\\nt\\r\\nyt\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n(10)\\r\\nR =\\r\\nPN\\r\\nt=1(yt − yt)(y\\r\\n∗\\r\\nt − y\\r\\n∗\\r\\nt\\r\\n)\\r\\nqPN\\r\\nt=1(yt − yt)\\r\\n2 PN\\r\\nt=1(y\\r\\n∗\\r\\nt − y\\r\\n∗\\r\\nt\\r\\n)\\r\\n2\\r\\n(11)\\r\\nTheil U =\\r\\nq\\r\\n1\\r\\nN\\r\\nPN\\r\\nt=1(yt − y\\r\\n∗\\r\\nt\\r\\n)\\r\\n2\\r\\nq\\r\\n1\\r\\nN\\r\\nPN\\r\\nt=1(yt)\\r\\n2 +\\r\\nq\\r\\n1\\r\\nN\\r\\nPN\\r\\nt=1(y\\r\\n∗\\r\\nt\\r\\n)\\r\\n2\\r\\n(12)\\r\\nwhere yt and y\\r\\n∗\\r\\nt\\r\\nrespectively denote the predictive price for\\r\\nthe proposed model and the actual price on day t, and yt\\r\\nand y\\r\\n∗\\r\\nt\\r\\nrespectively denote their average values. MAPE is\\r\\na measure of the relative error in the average values. R is\\r\\nthe correlation coefficient for two variables and describes the\\r\\nlinear correlation between them. A large value for R means\\r\\nthat the forecast is close to the actual value. Theil U is also\\r\\ncalled the uncertainty coefficient and is a type of association\\r\\nmeasure. A smaller value for MAPE and Theil U denotes\\r\\ngreater accuracy.\\r\\nC. Predictive accuracy test\\r\\nTables III-VIII show that a 1-D CNN gives slightly better\\r\\nresults than WSAEs. This shows that the convolutional net\\x02work is effective in processing stock data, which is a model\\r\\nthat can adaptively de-noise the noisy data and can reduce\\r\\nthe dimensionality. Markets with higher predicted errors are\\r\\nalmost the same for both two models. Moreover, the CSI 300\\r\\nindex, the HangSeng Index and the Nifty 50 index are more\\r\\ndifficult to be predicted than the DJIA index and the S&P500\\r\\nIndex.\\r\\nIn some individual cases, more closer between predicted and\\r\\nactual prices does not mean that there is a higher prediction\\r\\naccuracy. However, the average for different years shows that\\r\\nthe prediction accuracy and the linear correlation are positively\\r\\ncorrelated.\\r\\nIf past prices are used to predict future stock prices, pre\\x02dicting the rate of change of the price is also able to get the\\r\\ncurrent prices. For most stock price series, the price scale is\\r\\nmuch larger than the rate of change. If the prediction target\\r\\nfor the model is the absolute price, it is easy to ignore the\\r\\ninformation for price changes because changes in the price\\r\\nhas a smaller effect on the loss than the absolute price. Tables\\r\\nIII-VIII show that the model predicts prices indirectly through\\r\\npredicting the rate of change can get higher accuracy. This\\r\\ndemonstrates that predicting the rate of change is a better way\\r\\nthan to predict prices directly.\\r\\nD. Predictive curve\\r\\nThe predicted results for the first year for each market index\\r\\nare shown in Figure 7. The curve for C1D-ROC is closer to\\r\\nthe actual curve than that for C1D-LSTM. The curve for C1D\\x02LSTM occasionally deviates far from the actual price curve but\\r\\nthat for the C1D-ROC does so only rarely. This demonstrates\\r\\nthat future prices can be derived using the current price and\\r\\nprice changes. The current input characteristics include the\\r\\ncurrent price but it is difficult to fully preserve this feature in\\r\\nthe input features for an autoencoder. If the change in the price\\r\\nis predicted directly and then inferred from the exact current\\r\\nvalue, the model can use the full information for the current\\r\\nprice.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n3000\\r\\n3500\\r\\nPrice\\r\\nCSI300 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 64 127 190 253\\r\\nTrading Day\\r\\n11000\\r\\n12000\\r\\nPrice\\r\\nDJIA index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 124 186 248\\r\\nTrading Day\\r\\n17500\\r\\n20000\\r\\n22500\\r\\n25000\\r\\nPrice\\r\\nHangSeng Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 126 188 251\\r\\nTrading Day\\r\\n5000\\r\\n5500\\r\\n6000\\r\\nPrice\\r\\nNifty 50 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n9000\\r\\n10000\\r\\n11000\\r\\nPrice\\r\\nNikkei 225 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 64 127 190 253\\r\\nTrading Day\\r\\n1200\\r\\n1300\\r\\nPrice\\r\\nS&P500 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\nFig. 7. The actual and predicted curves for six stock index from 2010.10.01 to 2011.09.30.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\\r\\nTABLE II\\r\\nTHE TECHNICAL INDICATOR USED IN EXPERIMENT FOLLOWING [13].\\r\\nName Definition Formulas\\r\\nMACD Moving Average Convergence MACD(n)t−1 + 2\\r\\nn+1 × (DIF Ft − MACD(n)t−1)\\r\\nCCI Commodity channel index Mt−SMt\\r\\n0.015Dt\\r\\nATR Average true range 1\\r\\nn\\r\\nPn\\r\\ni=1 T Ri\\r\\nBOLL Bollinger Band MID MA20\\r\\nEMA20 20 day Exponential Moving Average 2\\r\\n21 × (Ct − EMAt−1) + (1 − 221 ) × EMAt−1\\r\\nMA5/MA10 5/10 day Moving Average Ct+Ct−1+···+Ct−4\\r\\n5\\r\\n/\\r\\nCt+Ct−1+···+Ct−9\\r\\n10\\r\\nMTM6/MTM12 6/12 month Momentum Ct − Ct−6/Ct − Ct−12\\r\\nROC Price rate of change Ct−Ct−N\\r\\nCt−N\\r\\n∗ 100\\r\\nSMI Stochastic Momentum Index Ds\\r\\nDhl ∗ 100\\r\\nWVAD Williams’s Variable Accumulation/Distribution ADt−1 +\\r\\n(Ct−Lt)−(Ht−Ct)\\r\\nHt−Ct\\r\\n∗ volume\\r\\nTABLE III\\r\\nTHE PREDICTION ACCURACY IN CSI 300 INDEX.\\r\\nYear Year1 Year2 Year3 Year4 Year5 Year6 Average\\r\\nPanel A.MAPE\\r\\nWSAEs-LSTM 0.025 0.014 0.016 0.011 0.033 0.016 0.019\\r\\nC1D-LSTM 0.015 0.014 0.017 0.011 0.051 0.015 0.020\\r\\nC1D-ROC 0.015 0.011 0.013 0.009 0.025 0.012 0.014\\r\\nPanel B.Correlation coefficient\\r\\nWSAEs-LSTM 0.861 0.959 0.955 0.957 0.975 0.957 0.944\\r\\nC1D-LSTM 0.961 0.960 0.951 0.961 0.976 0.959 0.961\\r\\nC1D-ROC 0.957 0.969 0.959 0.974 0.987 0.969 0.969\\r\\nPanel C.Theil U\\r\\nWSAEs-LSTM 0.017 0.009 0.011 0.007 0.023 0.011 0.013\\r\\nC1D-LSTM 0.009 0.009 0.011 0.007 0.031 0.011 0.013\\r\\nC1D-ROC 0.010 0.007 0.010 0.006 0.017 0.009 0.010\\r\\nTABLE IV\\r\\nTHE PREDICTION ACCURACY IN DJIA INDEX.\\r\\nYear Year1 Year2 Year3 Year4 Year5 Year6 Average\\r\\nPanel A.MAPE\\r\\nWSAEs-LSTM 0.016 0.013 0.009 0.008 0.008 0.010 0.011\\r\\nC1D-LSTM 0.011 0.010 0.010 0.007 0.010 0.011 0.010\\r\\nC1D-ROC 0.011 0.008 0.007 0.007 0.009 0.008 0.008\\r\\nPanel B.Correlation coefficient\\r\\nWSAEs-LSTM 0.922 0.928 0.984 0.952 0.953 0.952 0.949\\r\\nC1D-LSTM 0.958 0.964 0.982 0.975 0.939 0.953 0.962\\r\\nC1D-ROC 0.953 0.975 0.988 0.969 0.946 0.972 0.967\\r\\nPanel C.Theil U\\r\\nWSAEs-LSTM 0.010 0.009 0.006 0.005 0.005 0.006 0.007\\r\\nC1D-LSTM 0.007 0.006 0.007 0.005 0.006 0.007 0.006\\r\\nC1D-ROC 0.008 0.005 0.005 0.004 0.006 0.005 0.005\\r\\nVI. CONCLUSION\\r\\n1-D ResNet sparse autoencoders are used to de-noise and\\r\\nreduce the dimensionality of data. A notional experiment is\\r\\nused to compare the performance of the model that uses\\r\\nfeatures after de-noising and that of a single network with\\r\\nLSTM. The first method reduces over-fitting when there is\\r\\na lot of noise in the data. The results of experiment show\\r\\nthat the proposed method gives a more accurate prediction\\r\\nthan WSAEs. This is the first contribution of this paper.\\r\\nAnother contribution is that we add prior knowledge about\\r\\nthe relationship between prices and the rate of change to the\\r\\nmodel to try to improve the performance, and the results of\\r\\nexperiment show the conclusion that it is more accurate to use\\r\\nTABLE V\\r\\nTHE PREDICTION ACCURACY IN HANGSENG INDEX.\\r\\nYear Year1 Year2 Year3 Year4 Year5 Year6 Average\\r\\nPanel A.MAPE\\r\\nWSAEs-LSTM 0.016 0.017 0.012 0.011 0.021 0.013 0.015\\r\\nC1D-LSTM 0.017 0.012 0.009 0.010 0.022 0.012 0.014\\r\\nC1D-ROC 0.011 0.011 0.008 0.009 0.010 0.011 0.010\\r\\nPanel B.Correlation coefficient\\r\\nWSAEs-LSTM 0.944 0.924 0.920 0.927 0.904 0.968 0.931\\r\\nC1D-LSTM 0.948 0.956 0.955 0.951 0.962 0.975 0.958\\r\\nC1D-ROC 0.979 0.964 0.955 0.952 0.985 0.979 0.969\\r\\nPanel C.Theil U\\r\\nWSAEs-LSTM 0.011 0.010 0.008 0.007 0.018 0.008 0.011\\r\\nC1D-LSTM 0.012 0.008 0.006 0.007 0.015 0.008 0.009\\r\\nC1D-ROC 0.007 0.007 0.006 0.006 0.007 0.007 0.007\\r\\nTABLE VI\\r\\nTHE PREDICTION ACCURACY IN NIFTY 50 INDEX.\\r\\nYear Year1 Year2 Year3 Year4 Year5 Year6 Average\\r\\nPanel A.MAPE\\r\\nWSAEs-LSTM 0.020 0.016 0.017 0.014 0.016 0.018 0.017\\r\\nC1D-LSTM 0.014 0.014 0.022 0.015 0.019 0.014 0.016\\r\\nC1D-ROC 0.012 0.009 0.010 0.008 0.008 0.007 0.009\\r\\nPanel B.Correlation coefficient\\r\\nWSAEs-LSTM 0.895 0.927 0.992 0.885 0.974 0.951 0.937\\r\\nC1D-LSTM 0.946 0.962 0.992 0.866 0.971 0.969 0.951\\r\\nC1D-ROC 0.973 0.968 0.903 0.996 0.960 0.988 0.964\\r\\nPanel C.Theil U\\r\\nWSAEs-LSTM 0.013 0.010 0.010 0.009 0.010 0.011 0.011\\r\\nC1D-LSTM 0.010 0.009 0.014 0.010 0.012 0.009 0.011\\r\\nC1D-ROC 0.007 0.006 0.007 0.005 0.005 0.005 0.006\\r\\nthe rate of change to indirectly predict the price of stocks than\\r\\nto directly predict the price of stocks.\\r\\nFuture study will use an attention model [34] to improve the\\r\\nperformance. This model assumes that the price for the next\\r\\nday is approximately related to the price for previous days.\\r\\nThe attention model will be applied to express the relationship\\r\\nbetween the price for previous day and next day, which will\\r\\ngive improved performance and result that are more easily\\r\\ninterpreted.\\r\\nREFERENCES\\r\\n[1] F. E. Tay and L. Cao, “Application of support vector machines in\\r\\nfinancial time series forecasting,” Omega, vol. 29, no. 4, pp. 309–317,\\r\\n2001.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\\r\\nTABLE VII\\r\\nTHE PREDICTION ACCURACY IN NIKKEI 225 INDEX.\\r\\nYear Year1 Year2 Year3 Year4 Year5 Year6 Average\\r\\nPanel A.MAPE\\r\\nWSAEs-LSTM 0.024 0.019 0.019 0.019 0.018 0.017 0.019\\r\\nC1D-LSTM 0.016 0.011 0.010 0.019 0.012 0.010 0.013\\r\\nC1D-ROC 0.013 0.010 0.013 0.010 0.013 0.013 0.012\\r\\nPanel B.Correlation coefficient\\r\\nWSAEs-LSTM 0.878 0.834 0.665 0.972 0.774 0.924 0.841\\r\\nC1D-LSTM 0.960 0.949 0.913 0.964 0.905 0.979 0.945\\r\\nC1D-ROC 0.957 0.972 0.994 0.943 0.981 0.969 0.969\\r\\nPanel C.Theil U\\r\\nWSAEs-LSTM 0.016 0.013 0.013 0.013 0.012 0.012 0.013\\r\\nC1D-LSTM 0.010 0.007 0.007 0.017 0.008 0.006 0.009\\r\\nC1D-ROC 0.009 0.006 0.009 0.007 0.008 0.009 0.008\\r\\nTABLE VIII\\r\\nTHE PREDICTION ACCURACY IN S&P500 INDEX.\\r\\nYear Year1 Year2 Year3 Year4 Year5 Year6 Average\\r\\nPanel A.MAPE\\r\\nWSAEs-LSTM 0.012 0.014 0.010 0.008 0.011 0.010 0.011\\r\\nC1D-LSTM 0.011 0.011 0.009 0.008 0.013 0.011 0.011\\r\\nC1D-ROC 0.010 0.009 0.008 0.006 0.008 0.007 0.008\\r\\nPanel B.Correlation coefficient\\r\\nWSAEs-LSTM 0.944 0.944 0.984 0.973 0.880 0.953 0.946\\r\\nC1D-LSTM 0.962 0.973 0.988 0.986 0.860 0.958 0.955\\r\\nC1D-ROC 0.965 0.979 0.988 0.982 0.949 0.976 0.973\\r\\nPanel C.Theil U\\r\\nWSAEs-LSTM 0.009 0.010 0.006 0.005 0.008 0.006 0.007\\r\\nC1D-LSTM 0.007 0.007 0.006 0.005 0.008 0.007 0.007\\r\\nC1D-ROC 0.007 0.006 0.005 0.004 0.005 0.005 0.005\\r\\n[2] J.-Z. Wang, J.-J. Wang, Z.-G. Zhang, and S.-P. Guo, “Forecasting stock\\r\\nindices with back propagation neural network,” Expert Systems with\\r\\nApplications, vol. 38, no. 11, pp. 14 346–14 355, 2011.\\r\\n[3] R. F. Engle, “Autoregressive conditional heteroscedasticity with es\\x02timates of the variance of united kingdom inflation,” Econometrica:\\r\\nJournal of the Econometric Society, pp. 987–1007, 1982.\\r\\n[4] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time Series\\r\\nAnalysis: Forecasting and Control. John Wiley & Sons, 2015.\\r\\n[5] D. A. Kumar and S. Murugan, “Performance analysis of indian stock\\r\\nmarket index using neural network time series model,” in Pattern\\r\\nRecognition, Informatics and Mobile Engineering (PRIME), 2013 In\\x02ternational Conference on. IEEE, 2013, pp. 72–78.\\r\\n[6] Y.-W. Si and J. Yin, “Obst-based segmentation approach to financial\\r\\ntime series,” Engineering Applications of Artificial Intelligence, vol. 26,\\r\\nno. 10, pp. 2581–2596, 2013.\\r\\n[7] G. S. Atsalakis and K. P. Valavanis, “Surveying stock market forecast\\x02ing techniques–part ii: Soft computing methods,” Expert Systems with\\r\\nApplications, vol. 36, no. 3, pp. 5932–5941, 2009.\\r\\n[8] F. Liu and J. Wang, “Fluctuation prediction of stock market index by\\r\\nlegendre neural network with random time strength function,” Neuro\\x02computing, vol. 83, pp. 12–21, 2012.\\r\\n[9] J. Chen, “Svm application of financial time series forecasting using em\\x02pirical technical indicators,” in Information Networking and Automation\\r\\n(ICINA), 2010 International Conference on, vol. 1. IEEE, 2010, pp.\\r\\nV1–77.\\r\\n[10] X. Ding, Y. Zhang, T. Liu, and J. Duan, “Deep learning for event-driven\\r\\nstock prediction.” in Ijcai, 2015, pp. 2327–2333.\\r\\n[11] Y. Baek and H. Y. Kim, “Modaugnet: A new forecasting framework for\\r\\nstock market index value with an overfitting prevention lstm module and\\r\\na prediction lstm module,” Expert Systems with Applications, vol. 113,\\r\\npp. 457–480, 2018.\\r\\n[12] B. Wang, H. Huang, and X. Wang, “A novel text mining approach to\\r\\nfinancial time series forecasting,” Neurocomputing, vol. 83, pp. 136–145,\\r\\n2012.\\r\\n[13] W. Bao, J. Yue, and Y. Rao, “A deep learning framework for financial\\r\\ntime series using stacked autoencoders and long-short term memory,”\\r\\nPLOS ONE, vol. 12, no. 7, p. e0180944, 2017.\\r\\n[14] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\\r\\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\\r\\n2006.\\r\\n[15] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy layer\\x02wise training of deep networks,” in Advances in neural information\\r\\nprocessing systems, 2007, pp. 153–160.\\r\\n[16] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\r\\nrecognition,” in Proceedings of the IEEE Conference on Computer Vision\\r\\nand Pattern Recognition, 2016, pp. 770–778.\\r\\n[17] R. C. Cavalcante, R. C. Brasileiro, V. L. Souza, J. P. Nobrega, and A. L.\\r\\nOliveira, “Computational intelligence and financial markets: A survey\\r\\nand future directions,” Expert Systems with Applications, vol. 55, pp.\\r\\n194–211, 2016.\\r\\n[18] A. Rodr´ıguez-Gonzalez, ´ A. Garc ´ ´ıa-Crespo, R. Colomo-Palacios, F. G.\\r\\nIglesias, and J. M. Gomez-Berb ´ ´ıs, “Cast: Using neural networks to\\r\\nimprove trading systems based on technical analysis by means of the\\r\\nrsi financial indicator,” Expert Systems with Applications, vol. 38, no. 9,\\r\\npp. 11 489–11 500, 2011.\\r\\n[19] E. F. Fama, “The behavior of stock-market prices,” The journal of\\r\\nBusiness, vol. 38, no. 1, pp. 34–105, 1965.\\r\\n[20] M. Lam, “Neural network techniques for financial performance predic\\x02tion: integrating fundamental and technical analysis,” Decision Support\\r\\nSystems, vol. 37, no. 4, pp. 567–581, 2004.\\r\\n[21] A. K. Nassirtoussi, S. Aghabozorgi, T. Y. Wah, and D. C. L. Ngo, “Text\\r\\nmining of news-headlines for forex market prediction: A multi-layer\\r\\ndimension reduction algorithm with semantics and sentiment,” Expert\\r\\nSystems with Applications, vol. 42, no. 1, pp. 306–324, 2015.\\r\\n[22] A. Porshnev, I. Redkin, and A. Shevchenko, “Machine learning in\\r\\nprediction of stock market indicators based on historical data and data\\r\\nfrom twitter sentiment analysis,” in Data Mining Workshops (ICDMW),\\r\\n2013 IEEE 13th International Conference on. IEEE, 2013, pp. 440–\\r\\n444.\\r\\n[23] Y.-H. Lui and D. Mole, “The use of fundamental and technical analyses\\r\\nby foreign exchange dealers: Hong kong evidence,” Journal of Interna\\x02tional Money and Finance, vol. 17, no. 3, pp. 535–545, 1998.\\r\\n[24] J. Yao, C. L. Tan, and H.-L. Poh, “Neural networks for technical\\r\\nanalysis: a study on klci,” International Journal of Theoretical and\\r\\nApplied Finance, vol. 2, no. 02, pp. 221–241, 1999.\\r\\n[25] C.-F. Huang, “A hybrid stock selection model using genetic algorithms\\r\\nand support vector regression,” Applied Soft Computing, vol. 12, no. 2,\\r\\npp. 807–818, 2012.\\r\\n[26] A. Ng, “Sparse autoencoder,” CS294A Lecture notes, https://web.\\r\\nstanford.edu/class/cs294a/sparseAutoencoder.pdf.\\r\\n[27] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, “Deconvolu\\x02tional networks,” in Proceedings of the IEEE Conference on Computer\\r\\nVision and Pattern Recognition, 2010.\\r\\n[28] D. N. T. How, C. K. Loo, and K. S. M. Sahari, “Behavior recognition for\\r\\nhumanoid robots using long short-term memory,” International Journal\\r\\nof Advanced Robotic Systems, vol. 13, no. 6, p. 1729881416663369,\\r\\n2016.\\r\\n[29] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\r\\nComputation, vol. 9, no. 8, pp. 1735–1780, 1997.\\r\\n[30] Z. Guo, H. Wang, Q. Liu, and J. Yang, “A feature fusion based\\r\\nforecasting model for financial time series,” PLOS ONE, vol. 9, no. 6,\\r\\np. e101113, 2014.\\r\\n[31] T.-J. Hsieh, H.-F. Hsiao, and W.-C. Yeh, “Forecasting stock markets\\r\\nusing wavelet transforms and recurrent neural networks: An integrated\\r\\nsystem based on artificial bee colony algorithm,” Applied Soft Comput\\x02ing, vol. 11, no. 2, pp. 2510–2525, 2011.\\r\\n[32] E. Altay and M. H. Satman, “Stock market forecasting: artificial neural\\r\\nnetwork and linear regression comparison in an emerging market,”\\r\\nJournal of Financial Management & Analysis, vol. 18, no. 2, p. 18,\\r\\n2005.\\r\\n[33] K. O. Emenike, “Forecasting nigerian stock exchange returns: Evidence\\r\\nfrom autoregressive integrated moving average (arima) model,” Ssrn\\r\\nElectronic Journal, 2010.\\r\\n[34] Q. Chen, Q. Hu, J. X. Huang, L. He, and W. An, “Enhancing recurrent\\r\\nneural networks with positional attention for question answering,” in\\r\\nProceedings of the 40th International ACM SIGIR Conference on\\r\\nResearch and Development in Information Retrieval. ACM, 2017, pp.\\r\\n993–996.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\\r\\n1 61 121 181 242\\r\\nTrading Day\\r\\n2200\\r\\n2400\\r\\n2600\\r\\n2800\\r\\nPrice\\r\\nCSI300 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 126 188 251\\r\\nTrading Day\\r\\n11000\\r\\n12000\\r\\n13000\\r\\nPrice\\r\\nDJIA index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 124 186 248\\r\\nTrading Day\\r\\n16000\\r\\n18000\\r\\n20000\\r\\nPrice\\r\\nHangSeng Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 125 187 250\\r\\nTrading Day\\r\\n4500\\r\\n5000\\r\\n5500\\r\\nPrice\\r\\nNifty 50 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 124 185 247\\r\\nTrading Day\\r\\n9000\\r\\n10000\\r\\nPrice\\r\\nNikkei 225 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 126 188 251\\r\\nTrading Day\\r\\n1200\\r\\n1400\\r\\nPrice\\r\\nS&P500 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\nFig. 8. The actual and predicted curves for six stock index from 2011.10.01 to 2012.09.30.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\\r\\n1 60 119 178 238\\r\\nTrading Day\\r\\n2200\\r\\n2400\\r\\n2600\\r\\n2800\\r\\nPrice\\r\\nCSI300 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 125 187 250\\r\\nTrading Day\\r\\n13000\\r\\n14000\\r\\n15000\\r\\nPrice\\r\\nDJIA index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 61 122 182 243\\r\\nTrading Day\\r\\n20000\\r\\n22000\\r\\n24000\\r\\nPrice\\r\\nHangSeng Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 125 187 249\\r\\nTrading Day\\r\\n5500\\r\\n6000\\r\\nPrice\\r\\nNifty 50 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n10000\\r\\n12000\\r\\n14000\\r\\n16000\\r\\nPrice\\r\\nNikkei 225 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 125 187 250\\r\\nTrading Day\\r\\n1400\\r\\n1600\\r\\nPrice\\r\\nS&P500 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\nFig. 9. The actual and predicted curves for six stock index from 2012.10.01 to 2013.09.30.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n2200\\r\\n2400\\r\\nPrice\\r\\nCSI300 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 126 189 252\\r\\nTrading Day\\r\\n15000\\r\\n16000\\r\\n17000\\r\\nPrice\\r\\nDJIA index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 124 185 247\\r\\nTrading Day\\r\\n22000\\r\\n24000\\r\\nPrice\\r\\nHangSeng Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 124 186 248\\r\\nTrading Day\\r\\n6000\\r\\n7000\\r\\n8000\\r\\nPrice\\r\\nNifty 50 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n14000\\r\\n15000\\r\\n16000\\r\\nPrice\\r\\nNikkei 225 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 126 189 252\\r\\nTrading Day\\r\\n1800\\r\\n2000\\r\\nPrice\\r\\nS&P500 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\nFig. 10. The actual and predicted curves for six stock index from 2013.10.01 to 2014.09.30.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\\r\\n1 61 122 183 244\\r\\nTrading Day\\r\\n3000\\r\\n4000\\r\\n5000\\r\\nPrice\\r\\nCSI300 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 126 189 252\\r\\nTrading Day\\r\\n16000\\r\\n17000\\r\\n18000\\r\\nPrice\\r\\nDJIA index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 123 184 246\\r\\nTrading Day\\r\\n22500\\r\\n25000\\r\\n27500\\r\\nPrice\\r\\nHangSeng Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n7500\\r\\n8000\\r\\n8500\\r\\n9000\\r\\nPrice\\r\\nNifty 50 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 61 122 183 244\\r\\nTrading Day\\r\\n16000\\r\\n18000\\r\\n20000\\r\\nPrice\\r\\nNikkei 225 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 63 126 189 252\\r\\nTrading Day\\r\\n1900\\r\\n2000\\r\\n2100\\r\\nPrice\\r\\nS&P500 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\nFig. 11. The actual and predicted curves for six stock index from 2014.10.01 to 2015.09.30.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n3000\\r\\n3500\\r\\nPrice\\r\\nCSI300 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 64 127 190 253\\r\\nTrading Day\\r\\n16000\\r\\n17000\\r\\n18000\\r\\nPrice\\r\\nDJIA index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 124 185 247\\r\\nTrading Day\\r\\n18000\\r\\n20000\\r\\n22000\\r\\n24000\\r\\nPrice\\r\\nHangSeng Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 123 184 246\\r\\nTrading Day\\r\\n7000\\r\\n8000\\r\\n9000\\r\\nPrice\\r\\nNifty 50 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 62 123 184 245\\r\\nTrading Day\\r\\n16000\\r\\n18000\\r\\n20000\\r\\nPrice\\r\\nNikkei 225 index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\n1 64 127 190 253\\r\\nTrading Day\\r\\n1900\\r\\n2000\\r\\n2100\\r\\n2200\\r\\nPrice\\r\\nS&P500 Index\\r\\nActual Data\\r\\nC1D-ROC\\r\\nC1D-LSTM\\r\\nFig. 12. The actual and predicted curves for six stock index from 2015.10.01 to 2016.09.30.'},\n",
       " {'name': '1907.10306v1.pdf',\n",
       "  'content': 'TESTING NEW PROPERTY OF ELLIPTICAL\\r\\nMODEL FOR STOCK RETURNS\\r\\nDISTRIBUTION.\\r\\nPetr Koldanov\\r\\nJuly 25, 2019\\r\\nAbstract\\r\\nWide class of elliptically contoured distributions is a popular model of\\r\\nstock returns distribution. However the important question of adequacy of\\r\\nthe model is open. There are some results which reject and approve such\\r\\nmodel. Such results are obtained by testing some properties of elliptical\\r\\nmodel for each pair of stocks from some markets. New property of equal\\x02ity of τ Kendall correlation coefficient and probability of sign coincidence\\r\\nfor any pair of random variables with elliptically contoured distribution\\r\\nis proved in the paper. Distribution free statistical tests for testing this\\r\\nproperty for any pair of stocks are constructed. Holm multiple hypotheses\\r\\ntesting procedure based on the individual tests is constructed and applied\\r\\nfor stock markets data for the concrete year. New procedure of testing\\r\\nthe elliptical model for stock returns distribution for all years of observa\\x02tion for some period is proposed. The procedure is applied for the stock\\r\\nmarkets data of China, USA, Great Britain and Germany for the period\\r\\nfrom 2003 to 2014. It is shown that for USA, Great Britain and Germany\\r\\nstock markets the hypothesis of elliptical model of stock returns distribu\\x02tion could be accepted but for Chinese stock market is rejected for some\\r\\ncases.\\r\\nKeywords: Stock return distribution, elliptical model,τ -Kendall correlation,\\r\\nprobability of sign coincidence, distribution free test, multiple decision statistical\\r\\nprocedure, rejection graph.\\r\\n1 Introduction\\r\\nModels of multivariate stock returns distribution attract growing attention last\\r\\ndecade. In particular such model are needed for portfolio construction and risk\\r\\nmanagement. Popular model of joint stock return distribution is wide class of\\r\\nelliptically contoured distributions (see [Anderson(2003)],[Gupta(2013)]). Con\\x02sistency of the elliptical model with US and Japanese stock market data was\\r\\nstudied in [Chicheportiche & Bouchaud(2012)] where it was shown that joint 1\\r\\narXiv:1907.10306v1 [stat.AP] 24 Jul 2019\\ndistribution of stock return is not elliptical. Such result was obtained by com\\x02parison of pairwise dependence measures between any pair of stocks. At the\\r\\nsame time it was noted by authors that applied methodology is not statistical.\\r\\nIn [Koldanov(2016)] statistical methodology to testing symmetry of stock\\r\\nreturns distribution was proposed. Distribution free tests for testing symmetry\\r\\nof any pair of stock returns distribution was constructed. These individual tests\\r\\nusing well known Holm procedure [Holm(1979)] was combined to testing sym\\x02metry of joint distribution of stock returns for US and UK markets. The concept\\r\\nof rejection graph was introduced and it was shown that deleting hubs (vertices\\r\\nwith high degree) of the graph lead to acceptance of symmetry hypothesis of\\r\\nstock return distribution.\\r\\nThe present work continues the studies begun in [Koldanov(2016)]. New\\r\\nproperty of elliptically contoured distributions namely the property of equality\\r\\nof τ Kendall correlation coefficient and probability of sign coincidence (measure\\r\\nQ([Kruskal(1958)])) for any pair of random variables is proved in the paper.\\r\\nThis property is used for testing elliptical model of stock returns distribution.\\r\\nDistribution free tests for individual hypotheses testing hi,j : τi,j = Qi,j against\\r\\nki,j : τi,j 6= Qi,j for any i, j = 1, . . . , p (where p is the number of stocks in the\\r\\nmarket) are constructed. These individual tests using Holm procedure are com\\x02bined to test consistency of elliptical model with joint stock return distribution.\\r\\nThe concept of rejection graph [Koldanov(2016)] is used to describe results of\\r\\nthe Holm procedure. Obtained results shows that rejection of elliptical model\\r\\nis connected with small number of pairs of stocks from Chinese stock market.\\r\\nRemoving this stocks leads to nonrejection of elliptical model for stock return\\r\\ndistribution.\\r\\nNew procedure of testing the elliptical model for stock returns distribution\\r\\nfor some period of years is constructed. The procedure is based on combination\\r\\nof the results of the Holm procedures for different years. The new procedure is\\r\\napplied for the stock markets data of China, USA, Great Britain and Germany\\r\\nfor the period from 2003 to 2014. It is shown that for USA, Great Britain\\r\\nand Germany stock markets the hypothesis of elliptical model of stock returns\\r\\ndistribution could be accepted. In contrast it is shown that for Chinese stock\\r\\nmarket the hypothesis of elliptical model of stock returns distribution is rejected\\r\\nfor some cases.\\r\\nThe paper is organized as follows: in section 2 main notations and definitions\\r\\nare introduced and theorems of equality of τ -Kendall correlation coefficient and\\r\\nmeasure Q ([Kruskal(1958)]) for elliptically contoured distributions are proved.\\r\\nIn section 3 individual tests are constructed. In section 4 Holm procedure is\\r\\ndescribed. In section 5 the new procedure is presented and obtained results\\r\\nof experiments are described. In section 6 discussion of obtained results is\\r\\npresented.\\r\\n2\\n2 Pairwise measures for elliptically contoured\\r\\ndistributions\\r\\nLet\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ed\\r\\nX1\\r\\nX2\\r\\n. . .\\r\\nXp\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f8\\r\\nbe the continuous random vector. There are several correla\\x02tion coefficient (or pairwise measures of dependence) between random variables\\r\\nXi, Xj such as Pearson correlation, Spearman correlation, Kendall correlation,\\r\\nFechner correlation, Kruscal correlation. Some of them was investigated in\\r\\n[Kruskal(1958)]. Kruscal correlation is based on the following measure.\\r\\nDefinition 1\\r\\nMeasure Q of dependence between random variables Xi and Xj ([Kruskal(1958)])\\r\\nis Qi,j = P [(Xi − med(Xi))(Xj − med(Xj )) > 0] where med(Xi) is the median\\r\\nof the distribution FXi(x) of the random variable Xii.e. P(Xi > med(Xi)) =\\r\\nP(Xi < med(Xi)) = 1\\r\\n2\\r\\nor FXi(med(Xi)) = 1\\r\\n2\\r\\n.\\r\\nAlso it was shown that τ -Kendall correlation coefficient ˆτi,j is based on the\\r\\nfollowing measure:\\r\\nDefinition 2\\r\\nLet \\x12\\r\\nXi\\r\\nXj\\r\\n\\x13\\r\\nbe random vector with cumulative distribution function FXiXj(x, y)\\r\\nand let \\x12\\r\\nXi(t)\\r\\nXj (t)\\r\\n\\x13\\r\\n,\\r\\n\\x12\\r\\nXi(t + 1)\\r\\nXj (t + 1) \\x13\\r\\nbe the independent copies of the vector\\r\\n\\x12\\r\\nXi\\r\\nXj\\r\\n\\x13\\r\\n.\\r\\nKendall measure τi,j of dependence between random variables Xi and Xj is\\r\\ndefined by equality\\r\\nτi,j = P [(Xi(t) − Xi(t − 1))(Xj (t) − Xj (t − 1)) > 0]\\r\\nIt was proved that if random vector X = (X1, X2, . . . , Xp) has multivariate\\r\\nnormal distribution N(µ,Λ) with known µ then one has\\r\\nQi,j = τi,j for all i, j = 1, . . . , p;i 6= j (1)\\r\\nLet us prove that (1) is true for elliptically contoured distributions ECD(µ,Λ, g)\\r\\nwith any function g.\\r\\nRandom vector X = (X1, X2, . . . , Xp) has elliptically contoured distribution\\r\\nECD(µ,Λ, g) if it’s density has the form:\\r\\nf(x; µ,Λ) = |Λ|\\r\\n− 1\\r\\n2 g{(x − µ)\\r\\n0Λ−1\\r\\n(x − µ)} (2)\\r\\nwhere Λ is a positive definite matrix, function g(x) ≥ 0 and\\r\\nZ ∞\\r\\n−∞\\r\\n. . . Z ∞\\r\\n−∞\\r\\ng(y\\r\\n0\\r\\ny)dy1 . . . dyp = 1\\r\\n3\\nThe class of elliptically contoured distributions contain in particular multivariate\\r\\nnormal distribution and multivariate Student distribution. It is known that\\r\\nE(Xi) = µi, i = 1, 2, . . . , p if it exist.\\r\\nTheorem 1\\r\\nIf random vector X has elliptically contoured distribution ECD(µ,Λ, g) with\\r\\nknown µ then for any g one has Qi,j = τi,j , ∀i, j = 1, . . . , p, i 6= j .\\r\\nProof\\r\\nVector X has elliptically contoured distribution ECD(µ,Λ, g) then med(Xi) =\\r\\nµi, i.e. vector U = X−med(X) has elliptically contoured distribution ECD(0,Λ, g).\\r\\nVectors X(t), X(t−1) are independent (by definition 2) and have elliptically con\\x02toured distribution ECD(µ,Λ, g) then by lemma 1 from [Lindskog(2003)] vector\\r\\nV = X(t)−X(t−1) has elliptically contoured distribution ECD(0,Λ, g1) where\\r\\nZ ∞\\r\\n−∞\\r\\n. . . Z ∞\\r\\n−∞\\r\\ng1(y\\r\\n0\\r\\ny)dy1 . . . dyp = 1\\r\\nIn [Kalyagin(2017)], lemma 2 it was shown that if X has elliptically con\\x02toured distribution ECD(0,Λ, g) then P(Xi > 0, Xj > 0) = 1\\r\\n4+\\r\\n1\\r\\n2π\\r\\narcsin \\x10\\r\\n√\\r\\nλij\\r\\nλiiλjj \\x11\\r\\n(where λij are the elements of the matrix Λ) and does not depend from g.\\r\\nTherefore P(UiUj > 0) = P(ViVj > 0) or Qi,j = τi,j for ∀g. The theorem 1\\r\\nis proved.\\r\\nThe theorem 1 has the following\\r\\nCorollary 1\\r\\nIf vector µ is known and exists i, j = 1, . . . , p;i 6= j such that Qi,j 6= τi,j\\r\\nthen the vector X has not elliptically contoured distribution.\\r\\nIn real practice it is unrealistic to assume known vector µ. To deal with\\r\\nunknown µ let us prove the following\\r\\nTheorem 2\\r\\nLet\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ed\\r\\nX1(1)\\r\\nX2(1)\\r\\n. . .\\r\\nXp(1)\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f8 ,\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ed\\r\\nX1(2)\\r\\nX2(2)\\r\\n. . .\\r\\nXp(2)\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f8 , . . . ,\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ed\\r\\nX1(n)\\r\\nX2(n)\\r\\n. . .\\r\\nXp(n)\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f8\\r\\nbe the sample of indepen\\x02dent identically distributed observations from vector X =\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ed\\r\\nX1\\r\\nX2\\r\\n. . .\\r\\nXp\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f8\\r\\nwhich has\\r\\nelliptically contoured distribution ECD(µ,Λ, g). Let\\r\\nXi =\\r\\n1\\r\\nn\\r\\nXn\\r\\nt=1\\r\\nXi(t)\\r\\nThen for n ≥ 2 one has\\r\\nP((Xi(t)−Xi) > 0,(Xj (t)−Xj ) > 0) = 1\\r\\n4\\r\\n+\\r\\n1\\r\\n2π\\r\\narcsin p\\r\\nλij\\r\\nλiiλjj\\r\\n, ∀i, j = 1, . . . , p; ∀t = 1, . . . , n\\r\\nProof\\r\\n4\\nAccording to lemma 1 from [Lindskog(2003)] if vector Z1 has elliptically\\r\\ncontoured distribution ECD(µ1,Λ, g1) and vector Z2 has elliptically contoured\\r\\ndistribution ECD(µ2,Λ, g2), then vector aZ1 + bZ2 has elliptically contoured\\r\\ndistribution ECD(aµ1 + bµ2,Λ, g3). Without loss of generality, consider the\\r\\ncase t = 1. One has\\r\\nX(1) − X =\\r\\nn − 1\\r\\nn\\r\\nX(1) −\\r\\n1\\r\\nn\\r\\nXn\\r\\nt=2\\r\\nX(t)\\r\\nFrom lemma 1 of [Lindskog(2003)] vector\\r\\n1\\r\\nn\\r\\nXn\\r\\nt=2\\r\\nX(t) has elliptically contoured distribution ECD(\\r\\nn − 1\\r\\nn\\r\\nµ,Λ, g?)\\r\\nSince vectors X(1) and 1\\r\\nn\\r\\nPn\\r\\nt=2 X(t) are independent then vector X(1)− X has\\r\\nelliptically contoured distribution ECD(0,Λ, g??)\\r\\nFrom lemma 2 of [Kalyagin(2017)] one has\\r\\nP((Xi(t)−Xi) > 0,(Xj (t)−Xj ) > 0) = 1\\r\\n4\\r\\n+\\r\\n1\\r\\n2π\\r\\narcsin p\\r\\nλij\\r\\nλiiλjj\\r\\n, ∀t = 1, . . . , n; ∀i, j = 1, . . . , p, i 6= j\\r\\nThe theorem 2 is proved.\\r\\nIt follows from theorem 2 that if vector X has elliptically contoured distri\\x02bution ECD(µ,Λ, g) then\\r\\nP((Xi(t)−Xi) > 0,(Xj (t)−Xj ) > 0) = P((Xi(t)−µi) > 0,(Xj (t)−µj ) > 0) = Qi,j\\r\\nFrom theorem 2 and corollary 1 one has the following\\r\\nCorollary 2\\r\\nIf exist i, j = 1, . . . , p;i 6= j such that Qi,j 6= τi,j then the vector X has not\\r\\nelliptically contoured distribution.\\r\\n3 Individual hypotheses testing\\r\\nAs follows from corollary 2 for study consistency with elliptical model it is\\r\\nnecessary to test the equality Qi,j = τi,j for any i, j = 1, . . . , p;i 6= j. This\\r\\nproblem can be formulated as problem of simultaneous testing of the following\\r\\nhypotheses\\r\\nhi,j : Qi,j = τi,j against ki,j : Qi,j 6= τi,j (3)\\r\\nIn the present section distribution free tests are constructed for testing in\\x02dividual hypotheses (3).\\r\\nWithout loss of generality let us consider the case i = 1, j = 2.\\r\\nLet\\r\\n\\x12\\r\\nx1(1)\\r\\nx2(1) \\x13\\r\\n,\\r\\n\\x12\\r\\nx1(2)\\r\\nx2(2) \\x13\\r\\n, . . . , \\x12\\r\\nx1(n + m)\\r\\nx2(n + m)\\r\\n\\x13\\r\\n5\\nbe the sample from random vector \\x12\\r\\nX1\\r\\nX2\\r\\n\\x13\\r\\n.\\r\\nFor testing hypothesis h1,2 : Q1,2 = τ1,2 against k1,2 : Q1,2 6= τ1,2 let us\\r\\nconsider the statistics\\r\\nτˆ1,2 =\\r\\n[\\r\\nnX2 ]\\r\\ni=1\\r\\nS{x1(2i) − x1(2i − 1))(x2(2i) − x2(2i − 1)} (4)\\r\\nand\\r\\nQˆ\\r\\n1,2 =\\r\\nnX+m\\r\\ni=n+1\\r\\nS((x1(i) − x1) × (x2(i) − x2)) (5)\\r\\nwhere\\r\\nS(x) = \\x1a\\r\\n1, x > 0\\r\\n0, x ≤ 0\\r\\nxk =\\r\\n1\\r\\nm\\r\\nnX+m\\r\\ni=n+1\\r\\nxk(i), k = 1, 2\\r\\nStatistic ˆτ1,2 has binomial distribution b(r, τ1,2) where r =\\r\\n\\x02\\r\\nn\\r\\n2\\r\\n\\x03\\r\\n, statistic\\r\\nQˆ\\r\\n1,2 has binomial distribution b(m, Q1,2). Moreover statistics ˆτ1,2, Qˆ1,2 are\\r\\nindependent since they are based on different observations. Therefore\\r\\nP(ˆτ1,2 = k, Qˆ\\r\\n1,2 = l) = C\\r\\nk\\r\\nr\\r\\nτ\\r\\nk\\r\\n1,2\\r\\n(1 − τ1,2)\\r\\nr−kCl\\r\\nmQl\\r\\n1,2\\r\\n(1 − Q1,2)\\r\\nm−l =\\r\\n= C\\r\\nk\\r\\nr C\\r\\nl\\r\\nm exp n\\r\\nk ln( τ1,2\\r\\n1−τ1,2\\r\\n) + l ln( Q1,2\\r\\n1−Q1,2\\r\\n)\\r\\no\\r\\n(1 − τ1,2)\\r\\nr\\r\\n(1 − Q1,2)\\r\\nm\\r\\nThen uniformly most powerful unbiased (UMPU) test ( see [Lehmann(2005)],\\r\\np. 126) for testing h1,2 : Q1,2 = τ1,2 against k1,2 : Q1,2 6= τ1,2 has the form:\\r\\nϕ1,2(x) = \\x1a\\r\\n0, c1(l + k) ≤ k ≤ c2(l + k)\\r\\n1, k < c1(l + k) or k > c2(l + k)\\r\\n(6)\\r\\nwhere c1(l + k), c2(l + k) are defined from:\\r\\nPQ1,2=τ1,2(k < c1(l + k) or k > c2(l + k)/τˆ1,2 + Qˆ\\r\\n1,2 = l + k) = α (7)\\r\\nFor simplicity let us consider the test ϕ1,2(x) where constants c1(l+k), c2(l+k)\\r\\nare defined from equations:\\r\\nPQ1,2=τ1,2(k < c1(l + k)/τˆ1,2 + Qˆ\\r\\n1,2 = l + k) = α\\r\\n2\\r\\nPQ1,2=τ1,2(k > c2(l + k)/τˆ1,2 + Qˆ\\r\\n1,2 = l + k) = α\\r\\n2\\r\\n(8)\\r\\nSince\\r\\nPQ1,2=τ1,2(ˆτ1,2 = k/τˆ1,2 + Qˆ\\r\\n1,2 = l + k) = C\\r\\nk\\r\\nr C\\r\\nl\\r\\nm\\r\\nC\\r\\nk+l\\r\\nr+m\\r\\nthen c1(l + k) is greatest integer number satisfying:\\r\\nc1X(l+k)\\r\\ni=0\\r\\nC\\r\\ni\\r\\nrC\\r\\nl+k−i\\r\\nm\\r\\nC\\r\\nk+l\\r\\nr+m\\r\\n≤\\r\\nα\\r\\n2\\r\\n(9)\\r\\n6\\nc2(l + k) is smallest integer number satisfying:\\r\\nXr\\r\\ni=c2(l+k)\\r\\nC\\r\\ni\\r\\nrC\\r\\nl+k−i\\r\\nm\\r\\nC\\r\\nk+l\\r\\nr+m\\r\\n≤\\r\\nα\\r\\n2\\r\\n(10)\\r\\nThe p-value of the test (6) is defined as\\r\\nq1,2 = 2 min ( X\\r\\nk\\r\\ni=0\\r\\nC\\r\\ni\\r\\nrC\\r\\nl+k−i\\r\\nm\\r\\nC\\r\\nk+l\\r\\nr+m\\r\\n,\\r\\nXr\\r\\ni=k\\r\\nC\\r\\ni\\r\\nrC\\r\\nl+k−i\\r\\nm\\r\\nC\\r\\nk+l\\r\\nr+m\\r\\n)\\r\\n(11)\\r\\n4 Holm procedure\\r\\nIn the case of large stock market the number of pairs of stocks is huge and it is\\r\\nnecessary to take into account so called multiplicity phenomenon ([Bretz(2011)]).\\r\\nTo study consistency of joint stock returns distribution with elliptical model the\\r\\nmultiple hypotheses testing approach is used (see [Lehmann(2005)], ch.9). Mul\\x02tiple comparison procedures adjust statistical inferences from an experiment for\\r\\nmultiplicity and thus enable better decision making. In our approach signifi\\x02cance level of multiple test is given by the probability of at least one Type I\\r\\nerror which is known as Family Wise Error Rate (FWER).\\r\\nThe tests (6) with p-values from (11) for testing the individual hypotheses\\r\\n(3) are combined in simultaneous testing procedure. In order to control FWER\\r\\nwhich is probability of at least one false rejection of true individual hypothesis\\r\\n(3) the well known Holm procedure [Holm(1979)] is used.\\r\\nIn our case Holm procedure contain at most M = C\\r\\n2\\r\\np\\r\\nsteps. At any step\\r\\none individual hypothesis hi,j is rejected or all other hypotheses are accepted.\\r\\nHolm procedure is defined by the following algorithm:\\r\\n• Step 1: If\\r\\nmin\\r\\ni,j=1,...,p\\r\\nqi,j ≥\\r\\nα\\r\\nM\\r\\nthen all hypotheses hi,j , i, j = 1, 2, . . . , p are accepted,\\r\\nelse if mini,j=1,...,p qi,j = qi1,j1then hypothesis hi1,j1is rejected and go to\\r\\nthe step 2.\\r\\n• . . .\\r\\n• Step K: Let I = {(i1, j1),(i2, j2), . . . ,(iK−1, jK−1)} be the set of indexes\\r\\nof previously rejected hypotheses. If\\r\\nmin\\r\\n(i,j)∈/I\\r\\nqi,j ≥\\r\\nα\\r\\nM − K + 1\\r\\nthen accept the hypotheses hi,j , (i, j) ∈/ I,\\r\\nelse if min(i,j)∈/I qi,j = qiK,jK then reject hypothesis hiK,jK and go to the\\r\\nstep (K+1).\\r\\n7\\n• . . .\\r\\n• Step M: Let I = {(i1, j1), . . . ,(iM−1, jM−1)} be the set of indexes of pre\\x02viously rejected hypotheses. Let (iM, jM) ∈/ I. If\\r\\nqiM,jM ≥ α\\r\\nthen accept hypothesis hiM,jM , else reject hypothesis hiM,jM (reject all\\r\\nhypotheses).\\r\\n5 Testing of elliptical model. Experimental re\\x02sults\\r\\nOur experimental results are presented in the section. First we present results\\r\\nfor Chinese stock market which shows that hypotheses of elliptical model for\\r\\nstock returns could be rejected for the years 2006, 2010, 2011 for small number\\r\\nof stocks only. Then we consider stock markets of different countries for some\\r\\nperiod and propose the procedure to test the hypothesis of elliptical model for\\r\\nstock returns distribution for the whole period. In our experimental results\\r\\nwe consider stock markets of China, USA, Great Britain and Germany for the\\r\\nperiod from 2003 to 2014 years.\\r\\n5.1 Testing of elliptical model for Chinese stock market.\\r\\nOne year period\\r\\nTo obtain experimental results for Chinese stock market 100 most traded stocks\\r\\nfrom Chinese stock market for 2006 year was selected. The Holm procedure with\\r\\nFWER=0.05; 0.5 and individual tests (6) with p-values from (11) was applied\\r\\nto selected stocks for every year from the period from 2003 to 2014.\\r\\nTo describe the results of Holm procedure the concept of rejection graph\\r\\nintroduced in [Koldanov(2016)] was used. Edge (i, j) was added in the rejection\\r\\ngraph if and only if the individual hypothesis hi,j : Qi,j = τi,j was rejected\\r\\nby Holm multiple testing procedure, nodes of the rejection graph were vertices\\r\\nadjacent to this edge.\\r\\nIn all presented results the values n = m = 125 was selected. By the\\r\\nobservations from the first half of a year the τ Kendall measure (Definition 2)\\r\\nwas estimated by the observations from the second half of a year the Q Kruscall\\r\\nmeasure (Definition 1) was estimated.\\r\\nThe obtained results are shown at the figures 1 (year 2006) and 2 (year\\r\\n2010). In the figures it is shown only verteces which are incident to some edges\\r\\nin the rejection graph. Note that in contrast to the rejection graph constructed\\r\\nin [Koldanov(2016)] in this case there are no hubs. At the same time the number\\r\\nof rejected hypotheses is very small and much less than in [Koldanov(2016)].\\r\\nThe list of stock tickers from these figures is as follows:\\r\\n8\\nFigure 1: Rejection graph for Chinese market 2006 year. FWER=0.5. n = m =\\r\\n125\\r\\nFigure 2: Rejection graph for Chinese market 2010 year. FWER=0.5. n = m =\\r\\n125\\r\\n9\\n• Figure 1: vertex 0 correspond the ticker ’600037.SS’, vertex 1 correspond\\r\\nthe ticker ’600016.SS’, vertex 2 correspond the ticker ’600031.SS’, ver\\x02tex 3 correspond the ticker ’600015.SS’, vertex 4 correspond the ticker\\r\\n’600028.SS’, vertex 5 correspond the ticker ’600435.SS’, vertex 6 corre\\x02spond the ticker ’600519.SS’, vertex 7 correspond the ticker ’600060.SS’,\\r\\nvertex 8 correspond the ticker ’600252.SS’, vertex 9 correspond the ticker\\r\\n’600649.SS’\\r\\n• Figure 2: vertex 0 correspond the ticker ’601555.SS’, vertex 1 correspond\\r\\nthe ticker ’600029.SS’, vertex 2 correspond the ticker ’600037.SS’, ver\\x02tex 3 correspond the ticker ’600031.SS’, vertex 4 correspond the ticker\\r\\n’600795.SS’, vertex 5 correspond the ticker ’600010.SS’, vertex 6 corre\\x02spond the ticker ’601098.SS’, vertex 7 correspond the ticker ’600005.SS’,\\r\\nvertex 8 correspond the ticker ’601225.SS’, vertex 9 correspond the ticker\\r\\n’600372.SS’, vertex 10 correspond the ticker ’600519.SS’, vertex 11 corre\\x02spond the ticker ’601336.SS’, vertex 12 correspond the ticker ’601186.SS’.\\r\\nIn the following table the number of rejected pairs of stocks from Chinese\\r\\nstock market for different α(first row) and for different years (first column) are\\r\\npresented. For example the value 6 in the second column and second row of\\r\\nthe table mean that in 2006 year only for six pairs of stocks from Chinese stock\\r\\nmarket individual hypotheses (3) were rejected. From the table one can see that\\r\\nfor 2010 year and α = 0.05 hypothesis of elliptical model is not rejected.\\r\\nTable 1.\\r\\nyear /α 0.5 0.25 0.1 0.05\\r\\n2006 6 6 6 3\\r\\n2010 7 7 7 0\\r\\n2011 5 5 5 5\\r\\n5.2 Testing of elliptical model for stock markets of differ\\x02ent countries. Period of several years\\r\\nIn the subsection the following question is considered: is it possible to accept\\r\\nthe hypothesis of elliptical model of stock returns distribution for the period\\r\\nfrom 2003 to 2014 despite on the fact that for some years from the period the\\r\\nelliptical model was rejected by the Holm procedure?\\r\\nTo answer this question, we propose the following procedure: the elliptical\\r\\nmodel is rejected if the number of years for which the elliptical model was\\r\\nrejected by the Holm procedure for one year period is greater than the given\\r\\nthreshold.\\r\\nThe justification of the procedure can be given as follows. Let us formulate\\r\\nthe hypothesis Hi: ’in the year i the distribution of stock returns is elliptical’ .\\r\\nThen the number X of rejected hypotheses Hi, i = 1, . . . , s for the period of\\r\\n10\\nlength s is binomial random variable b(s, α) if all hypotheses Hi\\r\\n, i = 1, . . . , s\\r\\nare true and Holm procedure for each year has FWER=α. Here independence\\r\\nof decisions obtained by the Holm procedure for one year period follows from\\r\\nindependence of observations for different years.\\r\\nLet H =\\r\\nTn\\r\\ni=1 Hi be the hypothesis that elliptical model for stock returns\\r\\ndistribution is correct for all considered years. Then\\r\\nPH(X ≥ c) = Xn\\r\\ni=c\\r\\nC\\r\\ni\\r\\nn\\r\\n(α)\\r\\ni\\r\\n(1 − α)\\r\\nn−i\\r\\nTherefore for given β one can define threshold cβ from the equation\\r\\nPH(X ≥ cβ) = Xn\\r\\ni=cβ\\r\\nC\\r\\ni\\r\\nn\\r\\n(α)\\r\\ni\\r\\n(1 − α)\\r\\nn−i ≤ β (12)\\r\\nand reject the hypothesis H iff X ≥ cβ. This means that the test of the hy\\x02pothesis H has the form:\\r\\nϕH(x) = \\x1a\\r\\n0, x < cβ\\r\\n1, x ≥ cβ\\r\\n(13)\\r\\nwhere x is the observed number of the rejected hypotheses Hi, cβ is defined from\\r\\n(12) and β is the significance level of the test ϕH(x). If the Holm procedure\\r\\nwas applied with FWER=α then the p-value of the test (13) can be calculated\\r\\nfrom:\\r\\npHα(x) = Xn\\r\\ni=x\\r\\nC\\r\\ni\\r\\nn\\r\\n(α)\\r\\ni\\r\\n(1 − α)\\r\\nn−i\\r\\n(14)\\r\\nBelow we apply the procedure for stock markets of China, USA, Great\\r\\nBritain and Germany for the period from 2003 to 2014. Presented experimental\\r\\nresults contain the table with the pairs (i; j) of stocks for which the hypotheses\\r\\nhij : Qi,j = τi,j were rejected by the Holm procedure. In all presented results\\r\\nthe values n = 83, m = 166 were selected. Two different level of FWER for the\\r\\nHolm procedure was considered: α = 0.05, α = 0.5.\\r\\n5.2.1 Chinese stock market\\r\\nFor Chinese stock market (see table 2) the number of rejected hypotheses hij :\\r\\nQi,j = τi,j is 11 for α = 0.5 and 10 for α = 0.05. From the other side the\\r\\nnumber of rejected hypotheses Hiis equal to 5 for α = 0.5 and α = 0.05 (for\\r\\nyears 2005, 2006, 2010, 2011, 2013).\\r\\nThen p-values (14) of the test ϕH(x)(13) of the hypothesis H are\\r\\npH0.5(5) = P(N ≥ 5|α = 0.5) = P12\\r\\ni=5 C\\r\\ni\\r\\n12(0.5)12 = 0.6128\\r\\npH0.05 (5) = P(N ≥ 5|α = 0.05) = P12\\r\\ni=5 C\\r\\ni\\r\\n12(0.05)i\\r\\n(0.95)12−i = 1.110779 ∗ 10−5\\r\\nOne can see that p-value pH0.05 (5) is too small. Then hypothesis H is rejected at\\r\\nany level β > 1.110779 ∗ 10−5if we test individual hypotheses Hi: i = 1, . . . , 12\\r\\n11\\nby Holm procedures with FWER 0.05. In contrast hypothesis H is accepted at\\r\\nany level β ≤ 0.6128 if we test individual hypotheses Hi: i = 1, . . . , 12 by Holm\\r\\nprocedures with FWER 0.5.\\r\\nTable 2.\\r\\nyear α = 0.5 α = 0.05\\r\\n2003 0 0\\r\\n2004 0 0\\r\\n2005 (88;23) (88;23)\\r\\n2006 (4;2) (4;2)\\r\\n2007 0 0\\r\\n2008 0 0\\r\\n2009 0 0\\r\\n2010 (37;17),(85;66),(87;9),(92;56) (37;17),(85;66),(87;9)\\r\\n2011 (22;5),(16;10),(19;16) (22;5),(16;10),(19;16)\\r\\n2012 0 0\\r\\n2013 (14;7),(29;2) (14;7),(29;2)\\r\\n2014 0 0\\r\\n5.2.2 USA stock market\\r\\nFor USA stock market (see table 3) the number of rejected hypotheses hij :\\r\\nQi,j = τi,j is 2 for α = 0.5 and 1 for α = 0.05. The number of rejected\\r\\nhypotheses Hiis equal to 2 for α = 0.5 (for years 2004, 2006) and 1 for α = 0.05\\r\\n(for year 2004). Then p-values (14) of the test ϕH(x)(13) of the hypothesis H\\r\\nare\\r\\npH0.5(2) = P(N ≥ 2|α = 0.5) = P12\\r\\ni=2 C\\r\\ni\\r\\n12(0.5)12 = 0.9807\\r\\npH0.05 (1) = P(N ≥ 1|α = 0.05) = P12\\r\\ni=1 C\\r\\ni\\r\\n12(0.05)i\\r\\n(0.95)12−i = 0.1184\\r\\nThen hypothesis H is rejected at any level β > 0.1184 if we test individual\\r\\nhypotheses Hi: i = 1, . . . , 12 by Holm procedures with FWER 0.05 and hy\\x02pothesis H is accepted at any level β ≤ 0.9807 if we test individual hypotheses\\r\\nHi: i = 1, . . . , 12 by Holm procedures with FWER 0.5.\\r\\nTable 3.\\r\\n5.2.3 Great Britain stock market\\r\\nFor Great Britain stock market (see table 4) the number of rejected hypotheses\\r\\nhij : Qi,j = τi,j is 1 for α = 0.5 and 0 for α = 0.05. The number of rejected\\r\\nhypotheses Hiis equal to 1 for α = 0.5 (for years 2007) and 0 for α = 0.05.\\r\\nThen p-values (14) of the test ϕH(x)(13) of the hypothesis H are\\r\\npH0.5(1) = P(N ≥ 1|α = 0.5) = P12\\r\\ni=1 C\\r\\ni\\r\\n12(0.5)12 = 0.9968\\r\\npH0.05 (0) = P(N ≥ 0|α = 0.05) = 1\\r\\n12\\nyear α = 0.5 α = 0.05\\r\\n2003 0 0\\r\\n2004 (91;9) (91;9)\\r\\n2005 0 0\\r\\n2006 (59;22) 0\\r\\n2007 0 0\\r\\n2008 0 0\\r\\n2009 0 0\\r\\n2010 0 0\\r\\n2011 0 0\\r\\n2012 0 0\\r\\n2013 0 0\\r\\n2014 0 0\\r\\nOne can see that both p-values are not too small. Then hypothesis H is accepted\\r\\nat any level β < 0.9968 if we test individual hypotheses Hi: i = 1, . . . , 12 by\\r\\nHolm procedures with levels 0.05 and hypothesis H is accepted at any level β if\\r\\nwe test individual hypotheses Hi: i = 1, . . . , 12 by Holm procedures with levels\\r\\n0.5.\\r\\nTable 4.\\r\\nyear α = 0.5 α = 0.05\\r\\n2003 0 0\\r\\n2004 0 0\\r\\n2005 0 0\\r\\n2006 0 0\\r\\n2007 (57;29) 0\\r\\n2008 0 0\\r\\n2009 0 0\\r\\n2010 0 0\\r\\n2011 0 0\\r\\n2012 0 0\\r\\n2013 0 0\\r\\n2014 0 0\\r\\n5.2.4 Germany stock market\\r\\nFor Germany stock market (see table 5) the number of rejected hypotheses\\r\\nhij : Qi,j = τi,j is 2 for α = 0.5 and 0 for α = 0.05. The number of rejected\\r\\nhypotheses Hiis equal to 2 for α = 0.5 (for years 2007) and 0 for α = 0.05.\\r\\nThen p-values (14) of the test ϕH(x)(13) of the hypothesis H are\\r\\npH0.5(1) = P(N ≥ 2|α = 0.5) = P12\\r\\ni=2 C\\r\\ni\\r\\n12(0.5)12 = 0.9807\\r\\npH0.05 (0) = P(N ≥ 0|α = 0.05) = 1\\r\\n13\\nOne can see that both p-values are not too small. Then hypothesis H is accepted\\r\\nat any level β < 0.9807 if we test individual hypotheses Hi: i = 1, . . . , 12 by\\r\\nHolm procedures with levels 0.05 and hypothesis H is accepted at any level β if\\r\\nwe test individual hypotheses Hi: i = 1, . . . , 12 by Holm procedures with levels\\r\\n0.5.\\r\\nTable 5.\\r\\nyear α = 0.5 α = 0.05\\r\\n2003 0 0\\r\\n2004 0 0\\r\\n2005 0 0\\r\\n2006 0 0\\r\\n2007 (11;2) 0\\r\\n2008 0 0\\r\\n2009 (12;11) 0\\r\\n2010 0 0\\r\\n2011 0 0\\r\\n2012 0 0\\r\\n2013 0 0\\r\\n2014 0 0\\r\\n6 Concluding remarks\\r\\nNew property of elliptically contoured distributions namely the property of\\r\\nequality of τ Kendall correlation coefficient and measure Q of [Kruskal(1958)]\\r\\nfor any pair of random variables is proved in the paper. Distribution free indi\\x02vidual tests for testing the property are constructed. Using well known Holm\\r\\nprocedure these individual tests are combined to multiple hypotheses testing\\r\\nprocedure.\\r\\nApplication of the procedure to real market data from one year period shows\\r\\nthat only small number of pairs of stocks from Chinese stock market destroy\\r\\nthe tested property. Removing small number of stocks from consideration lead\\r\\nto nonrejection of elliptical model to the remaining stocks of stock market.\\r\\nThis result is consistent with the results obtained in [Koldanov(2016)] where\\r\\nsymmetry hypothesis was tested. However in [Koldanov(2016)] the number of\\r\\npairs of stocks which lead to rejection of symmetry hypotheses is huge but\\r\\nrejection graph has only small number of hubs (verteces with high degree). It\\r\\nis interesting to note than in contrast to [Koldanov(2016)] in the present case\\r\\nthere are no hubs of high degree. For example in the figure 1 there are only two\\r\\nhubs (verteces 3 and 8) of degree two.\\r\\nNew procedure of testing the elliptical model for the period of several years\\r\\nis proposed. This procedure is applied for stock markets of China, USA, Great\\r\\nBritain and Germany for the period from 2003 to 2014. The obtained results\\r\\n14\\nshows that hypothesis of elliptical model of stock returns distribution is accepted\\r\\nfor USA, Great Britain and Germany but is rejected for China.\\r\\nAcknowledgments\\r\\nThis work is partly supported by Laboratory of Algorithms and technologies for\\r\\nnetwork analysis of HSE and RFFI grant N 18-07-00524\\r\\nReferences\\r\\n[Anderson(2003)] Anderson T.W. An introduction to multivariate statistical\\r\\nanalysis. Wiley-Interscience, New-York, 3-d edition, 2003.\\r\\n[Kruskal(1958)] William H. Kruskal (1958) Ordinal Measures of Association,\\r\\nJournal of the American Statistical Association, 53:284, 814-861\\r\\n[Lehmann(2005)] Lehmann E.L., Romano J.P. Testing statistical hypotheses.\\r\\nSpringer, New York, 2005.\\r\\n[Gupta(2013)] Gupta F.K. Varga T. Bodnar T. Elliptically Contoured Models\\r\\nin Statistics and Portfolio Theory, Springer, 2013, ISBN: 978-1-4614-8153-\\r\\n9.\\r\\n[Chicheportiche & Bouchaud(2012)] Chicheportiche R. Bouchaud J-P. The\\r\\njoint distribution of stock returns is not elliptical:International Journal of\\r\\nTheoretical and Applied Finance, 15, 3, 2012.\\r\\n[Koldanov(2016)] P.Koldanov, N. Lozgacheva. MULTIPLE TESTING OF\\r\\nSIGN SYMMETRY FOR STOCK RETURN DISTRIBUTIONS, Interna\\x02tional Journal of Theoretical and Applied Finance Vol. 19, No. 8 (2016)\\r\\n1650049\\r\\n[Holm(1979)] Holm, Sture. 1979. ”A Simple Sequentially Rejective Multiple\\r\\nTest Procedure”. Scandinavian Journal of Statistics 6 (2). [Board of the\\r\\nFoundation of the Scandinavian Journal of Statistics, Wiley]: 65-70.\\r\\nhttp://www.jstor.org/stable/4615733.\\r\\n[Kalyagin(2017)] V.A. Kalyagin, A.P. Koldanov, P.A. Koldanov. Robust iden\\x02tification in random variable networks. Journal of Statistical Planning and\\r\\nInference 181 (2017) 3040\\r\\n[Lindskog(2003)] Lindskog F., McNeil A., Schmock U. (2003) Kendalls Tau for\\r\\nElliptical Distributions. In: Bol G., Nakhaeizadeh G., Rachev S.T., Ridder\\r\\nT., Vollmer KH. (eds) Credit Risk. Contributions to Economics. Physica\\x02Verlag HD\\r\\n[Bretz(2011)] F. Bretz, T. Hothorn & P. Westfall (2011) Multiple Comparisons\\r\\nUsing R. Taylor and Francis Group.\\r\\n15'},\n",
       " {'name': '2305.08740v1.pdf',\n",
       "  'content': 'Temporal and Heterogeneous Graph Neural Network for\\r\\nFinancial Time Series Prediction\\r\\nSheng Xiang\\r\\nAAII\\r\\nUniversity of Technology Sydney\\r\\nSydney, Australia\\r\\nsheng.xiang@student.uts.edu.au\\r\\nDawei Cheng∗\\r\\nDepartment of Computer Science\\r\\nTongji University\\r\\nShanghai, China\\r\\ndcheng@tongji.edu.cn\\r\\nChencheng Shang\\r\\nHSBC Business School\\r\\nPeking University\\r\\nBeijing, China\\r\\npkuscc@stu.pku.edu.cn\\r\\nYing Zhang\\r\\nAAII\\r\\nUniversity of Technology Sydney\\r\\nSydney, Australia\\r\\nying.zhang@uts.edu.au\\r\\nYuqi Liang\\r\\nSeek Data Group\\r\\nEmoney Inc.\\r\\nShanghai, China\\r\\nroly.liang@seek-data.com\\r\\nABSTRACT\\r\\nThe price movement prediction of stock market has been a classical\\r\\nyet challenging problem, with the attention of both economists\\r\\nand computer scientists. In recent years, graph neural network has\\r\\nsignificantly improved the prediction performance by employing\\r\\ndeep learning on company relations. However, existing relation\\r\\ngraphs are usually constructed by handcraft human labeling or na\\x02ture language processing, which are suffering from heavy resource\\r\\nrequirement and low accuracy. Besides, they cannot effectively re\\x02sponse to the dynamic changes in relation graphs. Therefore, in\\r\\nthis paper, we propose a temporal and heterogeneous graph neural\\r\\nnetwork-based (THGNN) approach to learn the dynamic relations\\r\\namong price movements in financial time series. In particular, we\\r\\nfirst generate the company relation graph for each trading day\\r\\naccording to their historic price. Then we leverage a transformer\\r\\nencoder to encode the price movement information into temporal\\r\\nrepresentations. Afterward, we propose a heterogeneous graph at\\x02tention network to jointly optimize the embeddings of the financial\\r\\ntime series data by transformer encoder and infer the probability\\r\\nof target movements. Finally, we conduct extensive experiments on\\r\\nthe stock market in the United States and China. The results demon\\x02strate the effectiveness and superior performance of our proposed\\r\\nmethods compared with state-of-the-art baselines. Moreover, we\\r\\nalso deploy the proposed THGNN in a real-world quantitative algo\\x02rithm trading system, the accumulated portfolio return obtained\\r\\nby our method significantly outperforms other baselines.\\r\\nCCS CONCEPTS\\r\\n• Information systems → Data mining.\\r\\n∗Corresponding Author.\\r\\nPermission to make digital or hard copies of all or part of this work for personal or\\r\\nclassroom use is granted without fee provided that copies are not made or distributed\\r\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\r\\non the first page. Copyrights for components of this work owned by others than ACM\\r\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\r\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\r\\nfee. Request permissions from permissions@acm.org.\\r\\nCIKM ’22, October 17–21, 2022, Atlanta, GA, USA\\r\\n© 2022 Association for Computing Machinery.\\r\\nACM ISBN 978-1-4503-9236-5/22/10. . . $15.00\\r\\nhttps://doi.org/10.1145/3511808.355708910.1145/3511808.3557089\\r\\nKEYWORDS\\r\\nGraph Neural Network, Financial Time Series, Stock Movement\\r\\nPrediction, Heterogeneous Graph.\\r\\nACM Reference Format:\\r\\nSheng Xiang, Dawei Cheng, Chencheng Shang, Ying Zhang, and Yuqi\\r\\nLiang. 2022. Temporal and Heterogeneous Graph Neural Network for Fi\\x02nancial Time Series Prediction. In Proceedings of the 31st ACM International\\r\\nConference on Information and Knowledge Management (CIKM ’22), Octo\\x02ber 17–21, 2022, Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages.\\r\\nhttps://doi.org/10.1145/3511808.355708910.1145/3511808.3557089\\r\\n1 INTRODUCTION\\r\\nStock market is a financial ecosystem that involves transactions\\r\\nbetween companies and investors, with a global market capital\\x02ization of more than $83.5 trillion as of 2020 [33]. The Efficient\\r\\nMarket Hypothesis [31] points out that the stock price represents\\r\\nall the available information, but the stock price is volatile in na\\x02ture, resulting in difficulty on predicting its movement [1]. In recent\\r\\nyears, deep learning has been widely used in predicting the price\\r\\nmovement of stocks [24]. Researchers also explore to improve the\\r\\nprediction performance by incorporating more sources as model in\\x02puts, including more technical indicators [31], factors [14], financial\\r\\nstatus [2], online news [27], social media posts [40], etc.\\r\\nTraditional learning methods treat the time series as independent\\r\\nand identically distributed to each other, which is not coincident\\r\\nto the real situation in the financial market [26]. For example, two\\r\\nstocks in the same sector may have a higher correlation than those\\r\\nin different fields [4]. Therefore, recent studies employ knowledge\\r\\ngraphs to represent the internal relations among entities [7? ] and\\r\\nleverage graph learning for the price movement prediction [28].\\r\\nThese works have shown the effectiveness by integrating stock’s\\r\\nrelations in the prediction models [10]. Thus, graph-based methods\\r\\ncould benefit from learning meaningful representations of inputs,\\r\\nresulting in better prediction accuracy [9, 35].\\r\\nHowever, generating relation graphs of entities is very challeng\\x02ing because it is fluctuate, noisy, amphibolous and dynamically\\r\\nchanged [25]. For example, financial knowledge graphs mainly in\\x02clude the supply chain, primary business and investment relations\\r\\nof entities [15], which is labeled by domain experts or extracted from\\r\\nunstructured texts. But different experts have different knowledge\\r\\narXiv:2305.08740v1 [q-fin.ST] 9 May 2023\\nCIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sheng Xiang et al.\\r\\nbackground which may lead to different relation graphs. Besides,\\r\\nthe current nature language processing (NLP) techniques are still\\r\\nfacing significant shortcomings in high accuracy relation extraction\\r\\n[12]. In other words, the relations may be misled by either unilat\\x02eral text news or inaccurate extracting models. In addition, these\\r\\nrelations may dynamically change in time series. For example, the\\r\\nmain business of a company would change according to the market\\r\\ndemands, and the supply chain graph would be upgraded because of\\r\\nthe technique evolution [19]. Existing graph learning price predic\\x02tion methods are inevitably suboptimal in learning these fluctuate\\r\\nand dynamical situations.\\r\\nTo address the above challenges, we propose a novel tempo\\x02ral and heterogeneous graph neural network-based method for\\r\\nfinancial time series prediction. Specifically, we directly model the\\r\\nrelations of price time series of entities based on historical data\\r\\nand represent them in a temporal and heterogeneous graph, i.e.,\\r\\ncompany relational graph. After obtaining the company relational\\r\\ngraph, we leverage sequential transformers encode the historical\\r\\nprices and graph neural networks to encode internal relations of\\r\\neach company. Specifically, we update each company’s representa\\x02tions by aggregating information from their neighbors in company\\r\\nrelational graphs in two steps. The first step is a time-aware graph\\r\\nattention mechanism. The second is a heterogeneous graph atten\\x02tion mechanism. We thoroughly evaluate our approach on both the\\r\\nS&P 5001and CSI 3002 dataset in the United States and China’s\\r\\nstock markets. The experimental results show that our method sig\\x02nificantly outperforms state-of-the-art baselines. In order to keep\\r\\nthe sententious of our model, we conduct ablation studies to prove\\r\\nthe effectiveness and essential of the each component of our method,\\r\\nincluding transformer encoder, time-aware graph attention, hetero\\x02geneous graph attention. Finally, we deploy our model in real-world\\r\\nquantitative algorithm trading platform, hosted in EMoney Inc.3, a\\r\\nleading financial service provider in China. The cumulative returns\\r\\nof portfolios contributed by our approach is significantly better than\\r\\nexisting models in financial industry. We will release the dataset as\\r\\nwell as the source codes of the proposed techniques along with the\\r\\npaper. In conclusion, our principle contributions are summarized\\r\\nas follows:\\r\\n• We propose a graph learning framework to effectively model\\r\\nthe internal relations among entities for financial time series\\r\\nprediction, which fits the dynamical market status and is\\r\\nconcordant with the ground-truth price movements.\\r\\n• We design a temporal and heterogeneous graph neural net\\x02work model to learn the dynamic relationships by two-stage\\r\\nattention mechanisms. The proposed model is concise and\\r\\neffective in joint and automatically learning from historical\\r\\nprice sequence and internal relations.\\r\\n• Our proposed THGNN is simple and can be easily imple\\x02mented in the industry-level system. Extensive experiments\\r\\non both the Unite States and China stock markets demon\\x02strate the superior performance of our proposed methods.\\r\\nWe also extensively evaluated its effectiveness by real-world\\r\\ntrading platform.\\r\\n1https://www.spglobal.com/spdji/en/indices/equity/sp-500\\r\\n2http://www.cffex.com.cn/en_new/CSI300IndexOptions.html\\r\\n3http://www.emoney.cn/\\r\\n2 RELATED WORKS\\r\\n2.1 Financial Time Series Learning\\r\\nIt is widely known that price movements of stocks are affected\\r\\nby various aspects in financial market [14]. In previous studies, a\\r\\ncommon strategy is to manually construct various factors as feature\\r\\ninputs [8, 30]. For example, Michel et. al, [2] integrate market signals\\r\\nwith stock fundamental and technical indicators to make decisions.\\r\\nLi et. al, [25] establish a link between news articles and the related\\r\\nentities for stock price movement forecasting. A large number of\\r\\nexisting methods employ recurrent neural network and its variants,\\r\\nsuch as LSTM [17] and GRU [11], to learn the sequential latent\\r\\nfeatures of historical information and employ them for downstream\\r\\nprediction task [21].\\r\\nIn these works, the market signals processing of each stock is\\r\\ncarried out independently. However, this inevitably ignores the\\r\\ninternal relationship among stocks and would lead to suboptimal\\r\\nperformance. Some works [16] leverage the correlation informa\\x02tion as model inputs, but cannot automatically capture the dynamic\\r\\nchanges of relations. In this article, we model the relationship be\\x02tween stocks as dynamic company relation graphs and joint learn\\r\\nthe graph relation and historical sequence feature automatically\\r\\nfor future price movement prediction.\\r\\n2.2 Graph Learning for Stock Prediction\\r\\nResearchers have shown that the price movement of stocks is not\\r\\nonly related to its own historical prices, but also connect to its\\r\\nlinked stocks [10]. The link relation includes suppliers and cus\\x02tomers, shareholders and investor, etc. Existing works normally\\r\\nemploy knowledge graphs to store and represent these relations\\r\\n[8, 29]. Recently, graph neural network (GNN) [41] is proposed to\\r\\neffectively learn on graph-structured data, which has shown its\\r\\nsuperior performance in various domains, including fraud detec\\x02tion [5, 6], computer vision [37, 38], etc. Researchers also introduce\\r\\nthe advanced GNN-based approaches in the stock price prediction\\r\\ntask. For example, Chen et. al, [4] models the supply chain relation\\x02ships of entities into knowledge graphs and uses graph convolution\\r\\nnetworks to predict stock movements. Ramit et. al, [36] leverage\\r\\nattentional graph neural network on the connections constructed\\r\\nby social media texts and company correlations. Cheng et.al, [8]\\r\\nleverage multi-modality graph neural network on the connections\\r\\nconstructed by historical price series, media news, and associated\\r\\nevents.\\r\\nHowever, the graph constructed by these methods are limited\\r\\nby constant, predefined corporate relationships, which is powered\\r\\nby handcraft editing or nature language processing techniques,\\r\\nsuffering from heavy resources labeling and low extracting accuracy\\r\\n[20]. But the actual corporate diagram evolves frequently over time.\\r\\nBesides, the company relation graph is also heterogeneous, which\\r\\nmeans there are multiple relation types among entities. Therefore,\\r\\nexisting methods cannot exploit the full information from real-life\\r\\ncompany relation graphs. In this paper, we construct the relation\\r\\ngraph dynamically and automatically based on their ground-truth\\r\\nhistorical price sequences and then propose a novel temporal and\\r\\nheterogeneous graph neural network methods to jointly learn their\\r\\nsequential and relational features for more accurate stock price\\nTemporal and Heterogeneous Graph Neural Network for Financial Time Series Prediction CIKM ’22, October 17–21, 2022, Atlanta, GA, USA\\r\\nTable 1: The summary of symbols\\r\\nSymbol Definition\\r\\nX the historical price of listed companies\\r\\nYˆ the probability of price movements\\r\\n𝑛 the total number of nodes\\r\\n𝑚 the total number of edges\\r\\n𝑟 the number of relationships in graph 𝐺˜\\r\\n𝑇 the number of trading days in 𝐺˜\\r\\n𝐺˜ = (𝑉 , ˜ {𝐸˜\\r\\n𝑟1\\r\\n, 𝐸˜\\r\\n𝑟2\\r\\n, ...}) the temporal and heterogeneous graph\\r\\n𝑉˜ = {𝑣\\r\\n𝑡𝑣1\\r\\n1\\r\\n, ..., 𝑣\\r\\n𝑡𝑣𝑛\\r\\n𝑛 } the set of temporal nodes\\r\\n𝐸˜\\r\\n𝑟 = {𝑒\\r\\n𝑡𝑒1\\r\\n1\\r\\n, ..., 𝑒\\r\\n𝑡𝑒𝑚\\r\\n𝑚 }𝑟 the set of temporal edges of relation 𝑟\\r\\nN (·) the neighborhood function\\r\\n𝑑 the number of dimension\\r\\nprediction. We demonstrate the effectiveness of our methods by\\r\\nextensive experiments and real-world trading platform.\\r\\n3 THE PROPOSED METHOD\\r\\nIn this section, we introduce the framework of our proposed tem\\x02poral and heterogeneous graph neural network and their each\\r\\ncomponent in detail. Our model takes the historical price sequence\\r\\nas inputs and infer the probability of stock movements as outputs.\\r\\nWe represent the relation of stocks in a dynamic heterogeneous\\r\\ngraph with two types of edges. We then jointly encode the histori\\x02cal and relation features by transformers and heterogeneous graph\\r\\nattention network. We report the problem definition first and each\\r\\nmodule of our method in turn.\\r\\n3.1 Problem Definition\\r\\nDifferent from traditional graph-based methods that construct static\\r\\nand homogeneous graphs by handcraft labeling or nature language\\r\\nprocessing techniques to infer stock movements, our model repre\\x02sents the company relation graph as a collection of temporal and\\r\\nheterogeneous graphs, which are automatically generated by his\\x02torical price sequences. In graphs, the node denotes each equity and\\r\\nedge represents their relations in sequential. Temporal graphs are\\r\\ncomposed of timestamped edges and timestamped nodes [34, 45].\\r\\nEach node might be associated with multiple relationships and mul\\x02tiple timestamped edges on different trading days. There are multi\\x02ple types of edge 𝐸 = {𝐸1, ..., 𝐸𝑟 } in company relation graphs. And\\r\\nthe occurences of node 𝑉 = {𝑉\\r\\n𝑡1\\r\\n, ...,𝑉𝑇} and edge 𝐸 = {𝐸\\r\\n𝑡1\\r\\n, ..., 𝐸𝑇}\\r\\nare different on different trading days. Table 1 summarizes the\\r\\nsymbols introduced in this paper.\\r\\nDefinition 3.1. Temporal and Relational Occurrence. In a\\r\\ntemporal company relation graph, an edge 𝑒 is associated with a\\r\\nseries of temporal occurrences 𝑒 = {𝑒\\r\\n𝑡1\\r\\n, 𝑒𝑡2, ...}, which indicate the\\r\\noccurrences of edge 𝑒 at trading days {𝑡1, 𝑡2, ...} in the company re\\x02lation graph. Each type of relational occurrence is associated with a\\r\\nseries of temporal occurrences with 𝐸 = {𝐸𝑟1, 𝐸𝑟2, ...}, which indicate\\r\\nthe occurrences of edge 𝑒 in different relationships {𝑟1, 𝑟2, ...}. Same\\r\\nas temporal occurrences of edge 𝑒, a node 𝑣 is associated with a set of\\r\\ntemporal occurrences with 𝑣 = {𝑣\\r\\n𝑡1\\r\\n, 𝑣𝑡2, ...}.\\r\\nDefinition 3.2. Temporal and Heterogeneous Graph. A tem\\x02poral and heterogeneous company relation graph𝐺˜ = (𝑉 , ˜ 𝐸˜) is formed\\r\\nby a set of temporal nodes 𝑉˜ = {𝑣\\r\\n𝑡𝑣1\\r\\n1\\r\\n, ..., 𝑣\\r\\n𝑡𝑣𝑛\\r\\n𝑛 } and a series of sets of\\r\\ntemporal edges 𝐸˜ = {𝐸˜\\r\\n1, ..., 𝐸˜𝑟 }, where 𝐸˜𝑟 = {𝑒\\r\\n𝑡𝑒1\\r\\n1\\r\\n, ..., 𝑒\\r\\n𝑡𝑒𝑚\\r\\n𝑚 }𝑟 denotes\\r\\nthe edges of relation 𝑟, and 𝑒\\r\\n𝑡𝑒𝑖\\r\\n𝑖\\r\\n= (𝑢𝑒𝑖, 𝑣𝑒𝑖)\\r\\n𝑡𝑒𝑖 denotes the temporal\\r\\nedge.\\r\\nIn existing works [4, 10, 15], the graph neighborhood N (𝑣) of\\r\\nnode 𝑣 is defined as static or homogeneous. Here, we generalize\\r\\nthe definition of graph neighborhood to the temporal and hetero\\x02geneous graph, which is set as follows:\\r\\nDefinition 3.3. Temporal and Heterogeneous Graph Neigh\\x02borhood. Given a temporal node 𝑣, the neighborhood of 𝑣 is defined\\r\\nas N (𝑣) = {𝑣𝑖|𝑓𝑠𝑝 (𝑣𝑖, 𝑣) ≤ 𝑑N, |𝑡𝑣 − 𝑡𝑣𝑖| ≤ 𝑡N }, where 𝑓𝑠𝑝 (·|·)\\r\\ndenotes the shortest path length between two nodes, 𝑑N and 𝑡N de\\x02note the hyper-parameters. As for heterogeneous graph, we define\\r\\nN𝑟 (·) as the neighborhood function of relation 𝑟.\\r\\nFinally, we formally define the stock movement prediction prob\\x02lem as follows:\\r\\nInput: Historical price sequences of listed companies X = {𝑥1, 𝑥2, · · · , 𝑥𝑛},\\r\\nwhere each 𝑥𝑖 = {𝑥𝑖,1, 𝑥𝑖,2, · · · , 𝑥𝑖,𝑡 } denotes the historical price se\\x02quences. We then use the X to generate the temporal and heteroge\\x02neous company relation graph 𝐺˜\\r\\n, with multiple types of temporal\\r\\nedges {𝐸˜\\r\\n𝑟1\\r\\n, 𝐸˜\\r\\n𝑟2\\r\\n, ...}, for downstream tasks .\\r\\nOutput: The probability Yˆ of price movements of each equity.\\r\\n3.2 Stock Correlation Graph Generation\\r\\nIn this subsection, we report the process of the temporal and het\\x02erogeneous graph construction. As mentioned in previous stud\\x02ies [4, 22], there may be multiple relationships between companies\\r\\n(such as suppliers and customers, shareholders and invested compa\\x02nies). Different from conventional knowledge graph-based relations\\r\\nthat construct relation by human labeling or NLP techniques, gener\\x02ating relations directly based on market trend signals has proved to\\r\\nbe effective [25, 42] in practical, which does not require additional\\r\\nambiguity domain knowledge or text news sources, and is easy to\\r\\nbe implemented.\\r\\nTherefore, in this paper, we obtain the correlation matrix by\\r\\ncalculating the correlation coefficients between ground-truth stock\\r\\nhistorical market signals directly. Then, the relationship between\\r\\ncompanies is determined according to the value of each element of\\r\\nthe correlation matrix. The relationship between companies may\\r\\nbe positive (correlation > threshold) or negative (correlation < -\\r\\nthreshold). In order to reduce noise, we connect the edges whose\\r\\nabsolute value is greater than the threshold, and the rest of the\\r\\nedges are considered not to be connected. So far, the edges 𝐸 of\\r\\ncompany relation graph are generated. Therefore, we model the\\r\\ninter-company relation graph as a heterogeneous graph with two\\r\\nrelationships, i.e., 𝐺 = (𝑉 , {𝐸𝑟1, 𝐸𝑟2, ...}), with 𝑟 ∈ {𝑝𝑜𝑠, 𝑛𝑒𝑔}.\\r\\nAs the relationships between companies tend to be dynamic,\\r\\nwe generate the company relation graph in temporal format as\\r\\nour model’s inputs. In particular, within 𝑇 trading days, we gen\\x02erate temporal and heterogeneous company relation graphs with\\r\\n𝑇 timestamps, which is formulated as 𝐺˜ = (𝑉 , ˜ {𝐸˜\\r\\n𝑝𝑜𝑠 , 𝐸˜𝑛𝑒𝑔}). Fi\\x02nally, the generated graphs and original sequence inputs are fed to\\r\\ndownstream learning task simultaneously.\\nCIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sheng Xiang et al.\\r\\nFigure 1: The proposed Temporal and Heterogeneous Graph Neural Networks architecture for stock movement predictions.\\r\\nThe first part is the generation of a stock correlation graph, which builds dynamic relations for stocks in the market every trad\\x02ing day. The second part is the historical price encoding, which selects a temporal node 𝑣\\r\\n𝑡 and its neighbor nodes to encode the\\r\\nhistorical price information. Transformer encoders share their parameters. The third part is the graph attention layer, which\\r\\nadaptively calculates the importance of the neighbors and aggregates the information according to the neighbors’ importance.\\r\\nThe fourth part is the heterogeneous graph attention layer, which adaptively calculates the importance and aggregates infor\\x02mation from different types of neighbors. Then, we leverage a multi-layer perception to give the prediction of each stock’s\\r\\nfuture movement.\\r\\n3.3 Historical Price Encoding\\r\\nThe input stock movement feature of price sequences is defined\\r\\nas X\\r\\n𝑡 ∈ R\\r\\n𝑛×𝑇×𝑑𝑓 𝑒𝑎𝑡 on trading day 𝑡, where 𝑛 denotes the number\\r\\nof stocks, 𝑇 means the number of trading days before 𝑡, and 𝑑𝑓 𝑒𝑎𝑡\\r\\ndenotes the dimension of historical price features. We first leverage\\r\\na linear transformation and positional encoding (PE) [39, 44] on\\r\\ntrading features to obtain the input tensor H\\r\\n𝑡 ∈ R𝑛×𝑇×𝑑𝑖𝑛 , which\\r\\nis formulated as follows:\\r\\nHˆ\\r\\n𝑡 =W𝑖𝑛X𝑡 + b𝑖𝑛\\r\\nH\\r\\n𝑡 =Hˆ𝑡 + PE\\r\\nPE(𝑝, 2𝑖) =𝑠𝑖𝑛(𝑝/100002𝑖/𝑑𝑖𝑛 )\\r\\nPE(𝑝, 2𝑖 + 1) =𝑐𝑜𝑠(𝑝/100002𝑖/𝑑𝑖𝑛 )\\r\\n(1)\\r\\nwhere 𝑝 ∈ {1, 2, ..,𝑇 } is the trading day position, 𝑖 is the dimension,\\r\\n𝑑𝑖𝑛 denotes the dimension of input features, and W𝑖𝑛 ∈ R\\r\\n𝑑𝑓 𝑒𝑎𝑡×𝑑𝑖𝑛\\r\\nand b𝑖𝑛 ∈ R\\r\\n𝑑𝑖𝑛 denote the learnable parameters. After linear trans\\x02formation, we proposed to leverage multi-head attentional trans\\x02former to encode the input feature for each stock in each day. Then,\\r\\nthe proposed encoder outputs H\\r\\n𝑡\\r\\n𝑒𝑛𝑐 ∈ R\\r\\n𝑛×𝑇×𝑑𝑒𝑛𝑐 for downstream\\r\\ntasks, where 𝑑𝑒𝑛𝑐 denotes the output dimension of the encoder.\\r\\nMathematically, we formulate the historical feature encoder’s out\\x02put H\\r\\n𝑡\\r\\n𝑒𝑛𝑐 as follows:\\r\\nH\\r\\n𝑡\\r\\n𝑒𝑛𝑐 = Concat(EncHead𝑡\\r\\n1\\r\\n, . . . , EncHead𝑡\\r\\nℎ𝑒𝑛𝑐\\r\\n)W𝑜 (2)\\r\\nwhere W𝑜 ∈ R\\r\\nℎ𝑒𝑛𝑐𝑑𝑣×𝑑𝑒𝑛𝑐 denotes the output projection matrix,\\r\\nℎ𝑒𝑛𝑐 denotes the number of heads in the encoder, 𝑑𝑣 denotes the\\r\\noutput dimension of each head, Concat denotes a concatenation of\\r\\nthe output of heads, and EncHead𝑡\\r\\n𝑖\\r\\n∈ R\\r\\n𝑛×𝑇×𝑑𝑣 denotes the output\\r\\nof encoder head with EncHead𝑡\\r\\n𝑖\\r\\n= Attention(Q𝑡\\r\\n𝑖\\r\\n, K\\r\\n𝑡\\r\\n𝑖\\r\\n, V\\r\\n𝑡\\r\\n𝑖\\r\\n), which is\\r\\nformulated as follows:\\r\\nQ\\r\\n𝑡\\r\\n𝑖\\r\\n= H\\r\\n𝑡W\\r\\n𝑄\\r\\n𝑖\\r\\n, K\\r\\n𝑡\\r\\n𝑖\\r\\n=H\\r\\n𝑡W𝐾\\r\\n𝑖\\r\\n, V\\r\\n𝑡\\r\\n𝑖\\r\\n= H\\r\\n𝑡W𝑉\\r\\n𝑖\\r\\nAttention(Q, K, V) =softmax(\\r\\nQK𝑇\\r\\n√\\r\\n𝑑𝑖𝑛\\r\\n)V\\r\\n(3)\\r\\nwhere W\\r\\n𝑄\\r\\n𝑖\\r\\n∈ R\\r\\n𝑑𝑖𝑛×𝑑ℎ𝑖𝑑𝑑𝑒𝑛 , W𝐾\\r\\n𝑖\\r\\n∈ R\\r\\n𝑑𝑖𝑛×𝑑ℎ𝑖𝑑𝑑𝑒𝑛 , W𝑉\\r\\n𝑖\\r\\n∈ R\\r\\n𝑑𝑖𝑛×𝑑𝑣\\r\\ndenote the projection matrices, and 𝑑ℎ𝑖𝑑𝑑𝑒𝑛 denotes the dimension\\r\\nof hidden layer.\\r\\n3.4 Temporal Graph Attention Mechanism\\r\\nGiven the historical sequence encoder output H\\r\\n𝑡\\r\\n𝑒𝑛𝑐 and temporal\\r\\nrelation graph𝐺˜, we propose to employ graph attention mechanism\\r\\non the sequential and heterogeneous inputs. In particular, we flatten\\r\\nthe embeddings of all nodes to H\\r\\n𝑡\\r\\n𝑒𝑛𝑐 ∈ R\\r\\n𝑛×𝑇𝑑𝑒𝑛𝑐 and leverage\\r\\ntwo-stage temporal attention mechanism to aggregate messages\\r\\nfrom graph structures and temporal sequences, which is illustrative\\r\\nreported in Figure 1 (c). The two-stage temporal graph attention\\r\\nlayers could aggregate messages from both the postive and negative\\r\\nneighbors simultaneously. For each relationship 𝑟 ∈ {𝑝𝑜𝑠, 𝑛𝑒𝑔}, the\\r\\nmessage aggregating is formulated as follows:\\r\\nH\\r\\n𝑡\\r\\n𝑟 =Concat(TgaHead1\\r\\n, ..., TgaHeadℎ𝑡𝑔𝑎)W𝑜,𝑟 (4)\\r\\nwhere H\\r\\n𝑡\\r\\n𝑟 ∈ R\\r\\n𝑛×𝑑𝑎𝑡𝑡 denotes the output of the temporal graph at\\x02tention layer on trading day 𝑡, and W𝑜,𝑟 ∈ R\\r\\nℎ𝑡𝑔𝑎𝑇𝑑𝑒𝑛𝑐×𝑑𝑎𝑡𝑡 denotes\\r\\nthe output projection matrix, ℎ𝑡𝑔𝑎 denotes the number of heads, and\\r\\neach head of temporal graph attention layer TgaHead𝑖∈ R\\r\\n𝑛×𝑇𝑑𝑒𝑛𝑐\\r\\nis formulated as follows:\\r\\nTgaHead𝑖 =\\r\\n∑︁\\r\\n𝑣\\r\\n𝑡 ∈𝑉˜\\r\\n𝜎(\\r\\n∑︁\\r\\n𝑢\\r\\n𝑡 ∈N𝑟 (𝑣𝑡\\r\\n)\\r\\n𝛼\\r\\n𝑖\\r\\n𝑢\\r\\n𝑡\\r\\n,𝑣𝑡 h𝑢\\r\\n𝑡 )\\r\\n(5)\\r\\nwhere 𝜎 denotes the activation function, h𝑢\\r\\n𝑡 ∈ R\\r\\n𝑇𝑑𝑒𝑛𝑐 denotes the\\r\\n𝑢\\r\\n𝑡\\r\\n-th row of historical price embedding H\\r\\n𝑡\\r\\n𝑒𝑛𝑐 , and 𝛼\\r\\n𝑖\\r\\n𝑢\\r\\n𝑡\\r\\n,𝑣𝑡 denotes\\r\\nthe importance of temporal edge (𝑢\\r\\n𝑡\\r\\n, 𝑣𝑡) in 𝑖-th head, which is\\r\\nformulated as follows:\\r\\n𝛼\\r\\n𝑖\\r\\n𝑢\\r\\n𝑡\\r\\n,𝑣𝑡 =\\r\\nexp(LeakyReLU(a\\r\\n𝑇\\r\\n𝑟,𝑖 [h𝑢\\r\\n𝑡 ||h𝑣𝑡 ]))\\r\\nÍ\\r\\n𝑘\\r\\n𝑡 ∈N𝑟 (𝑣𝑡\\r\\n)\\r\\nexp(LeakyReLU(a\\r\\n𝑇\\r\\n𝑟,𝑖 [h𝑘\\r\\n𝑡 ||h𝑣𝑡 ]))\\r\\n(6)\\r\\nwhere a𝑟,𝑖 ∈ R\\r\\n2𝑇𝑑𝑒𝑛𝑐 denotes the weight vector of relation 𝑟 and\\r\\n𝑖-th head.\\nTemporal and Heterogeneous Graph Neural Network for Financial Time Series Prediction CIKM ’22, October 17–21, 2022, Atlanta, GA, USA\\r\\n3.5 Heterogeneous Graph Attention\\r\\nMechanism\\r\\nAs shown in Figure 1 (d), we already have messages from different\\r\\ntypes of neighbors through the two-stage attention mechanism.\\r\\nThen, we propose the heterogeneous graph attention network to\\r\\nlearn from different relationships in relation graphs.\\r\\nWe define message sources as three types of embeddings, namely,\\r\\nmessages from ourselves H\\r\\n𝑡\\r\\n𝑠𝑒𝑙 𝑓 , positive neighbors H\\r\\n𝑡\\r\\n𝑝𝑜𝑠 , and nega\\x02tive neighbors H\\r\\n𝑡\\r\\n𝑛𝑒𝑔, respectively. H\\r\\n𝑡\\r\\n𝑠𝑒𝑙 𝑓 ∈ R\\r\\n𝑛×𝑑𝑎𝑡𝑡 is derived from\\r\\nH\\r\\n𝑡\\r\\n𝑒𝑛𝑐 through a linear transformation with H\\r\\n𝑡\\r\\n𝑠𝑒𝑙 𝑓 = W𝑠𝑒𝑙 𝑓 H\\r\\n𝑡\\r\\n𝑒𝑛𝑐 +\\r\\nb𝑠𝑒𝑙 𝑓 , where W𝑠𝑒𝑙 𝑓 ∈ R\\r\\n𝑇𝑑𝑒𝑛𝑐×𝑑𝑎𝑡𝑡 and b𝑠𝑒𝑙 𝑓 ∈ R𝑑𝑎𝑡𝑡 denote the\\r\\nlearnable parameters. H\\r\\n𝑡\\r\\n𝑝𝑜𝑠 and H\\r\\n𝑡\\r\\n𝑛𝑒𝑔 are derived from the graph\\r\\nattention mechanism in section 3.4. Taking three groups of node\\r\\nembeddings as input, we can adaptively generate the importance of\\r\\ndifferent relationships through attention mechanism. The weights\\r\\nof three relationships (𝛽𝑠𝑒𝑙 𝑓 , 𝛽𝑝𝑜𝑠 , 𝛽𝑛𝑒𝑔) can be shown as follows:\\r\\n(𝛽𝑠𝑒𝑙 𝑓 , 𝛽𝑝𝑜𝑠 , 𝛽𝑛𝑒𝑔) = HGA(H\\r\\n𝑡\\r\\n𝑠𝑒𝑙 𝑓 , H\\r\\n𝑡\\r\\n𝑝𝑜𝑠 , H\\r\\n𝑡\\r\\n𝑛𝑒𝑔) (7)\\r\\nWe first use three Multi-Layer Perceptrons (MLP) to transform\\r\\nthese three embeddings. Then we measure the importance of each\\r\\nembedding using a heterogeneous attention vector q. Furthermore,\\r\\nwe average the importance of all node embeddings, which can be\\r\\nexplained as the importance of each company relation. The impor\\x02tance of each company relation, denoted as 𝑟 ∈ {𝑠𝑒𝑙 𝑓 , 𝑝𝑜𝑠, 𝑛𝑒𝑔}, is\\r\\nshown as follows:\\r\\n𝑤𝑟 =\\r\\n1\\r\\n|𝑉˜|\\r\\n∑︁\\r\\n𝑣\\r\\n𝑡 ∈𝑉˜\\r\\nq\\r\\n𝑇\\r\\ntanh(Wh𝑣\\r\\n𝑡\\r\\n,𝑟 + b)\\r\\n(8)\\r\\nwhere W ∈ R\\r\\n𝑑𝑎𝑡𝑡×𝑑𝑞 and b ∈ R𝑑𝑞 are the parameters of MLP,\\r\\nq ∈ R\\r\\n𝑑𝑞 denotes the attention vector, and h𝑣\\r\\n𝑡\\r\\n,𝑟 denotes the 𝑣𝑡-\\r\\nth row of H\\r\\n𝑡\\r\\n𝑟\\r\\n. Note that all above parameters are shared for all\\r\\nrelationship of node embeddings. After obtaining the importance of\\r\\neach relationship, we calculate the contribution of each relationship\\r\\nand obtain the final embedding Z\\r\\n𝑡 ∈ R𝑛×𝑑𝑎𝑡𝑡 as follows:\\r\\n𝛽𝑟 =\\r\\nexp(𝑤𝑟)\\r\\nÍ\\r\\n𝑟 ∈ {𝑠𝑒𝑙 𝑓 ,𝑝𝑜𝑠,𝑛𝑒𝑔}\\r\\nexp(𝑤𝑟)\\r\\nZ\\r\\n𝑡 =\\r\\n∑︁\\r\\n𝑟 ∈ {𝑠𝑒𝑙 𝑓 ,𝑝𝑜𝑠,𝑛𝑒𝑔}\\r\\n𝛽𝑟 · H\\r\\n𝑡\\r\\n𝑟\\r\\n(9)\\r\\nFor a better understanding of the aggregating process of heteroge\\x02neous graph attention layer, we give a brief explanation in Figure 1\\r\\n(d). Then we apply the final embedding to a semi-supervised node\\r\\nclassification task.\\r\\n3.6 Optimization Objectives\\r\\nHere we give the implementation of objective functions. We model\\r\\nthe stock movement prediction task as a semi-supervised node\\r\\nclassification problem. Specifically, we selected 200 stocks of which\\r\\nfuture movements are ranked in top-100 or bottom-100, and labeled\\r\\nthe corresponding nodes as 1 and 0, respectively. Then, we use\\r\\none layer of MLP as the classifier to get the classification results\\r\\nof labeled nodes. Furthermore, we use binary cross-entropy to\\r\\ncalculate the objective function 𝐿, which is formulated as follows:\\r\\nYˆ =𝜎(WZ𝑡\\r\\n𝑙\\r\\n+ b)\\r\\nL = −\\r\\n∑︁\\r\\n𝑙 ∈Y𝑡\\r\\n[Y\\r\\n𝑡\\r\\n𝑙\\r\\nlog(Yˆ\\r\\n𝑙\\r\\n) + (1 − Y\\r\\n𝑡\\r\\n𝑙\\r\\n) log(1 − Yˆ\\r\\n𝑙\\r\\n)] (10)\\r\\nwhere Y𝑡 denotes the set of labeled nodes, Y\\r\\n𝑡\\r\\n𝑙\\r\\nand Z\\r\\n𝑡\\r\\n𝑙\\r\\ndenote the\\r\\nlabel and embedding of the labeled node 𝑙, respectively, 𝜎 denotes\\r\\nthe Sigmoid activation function, and W and b are the parameters of\\r\\nMLP. With the guide of labeled data, we use Adam [23] optimizer\\r\\nto update the parameters of our proposed method. Please note that\\r\\nwe use this objective function to jointly optimize the parameters of\\r\\nhistorical price encoder, temporal and heterogeneous graph neural\\r\\nnetwork and node classifier.\\r\\n4 EXPERIMENTS\\r\\nIn this section, we first introduce the datasets and experimental\\r\\nsettings. Then we detail report the experimental results in real\\x02world dataset and applications.\\r\\n4.1 Experimental Setttings\\r\\nDatasets. Extensive experiments are conducted in both the Unite\\r\\nStates and Chinna’s stock markets by choosing the constituted\\r\\nentities in S & P 500 and CSI 300 index. The historical price data\\r\\nfrom 2016 to 2021 are chosen as our datasets. In addition to historical\\r\\nprice data, our input data also includes company relation graphs.\\r\\nThe graphs are generated by the stock price correlation matrix,\\r\\nwhich is introduced in Section 3.2. The stock price correlation\\r\\nmatrix of each day is determined by the historical price movement\\r\\nof past 20 trading days. Specifically, we compare the opening price,\\r\\nclosing price, trading volume, and trading volume of each pair of\\r\\ntwo stocks, calculate the correlation coefficient between them and\\r\\ntake the mean value as the element of the correlation matrix.\\r\\nParameter Settings. Our temporal graph 𝐺˜ contains company\\r\\nrelationships for 20 trading days. The 𝑑N and 𝑡N of neighborhood\\r\\nfunction N (·) are both set as 1. During the graph generation process,\\r\\nthe threshold for generating one edge is set as 0.6. The historical\\r\\nprice data of the previous 20 trading days are used as input features.\\r\\nThe feature dimension 𝑑𝑓 𝑒𝑎𝑡 of the encoding layer is 6, the input\\r\\ndimension 𝑑𝑖𝑛 and encoding dimension 𝑑𝑒𝑛𝑐 of the encoding layer\\r\\nare both 128. The hidden dimension 𝑑ℎ𝑖𝑑𝑑𝑒𝑛 is 512, the dimension\\r\\nof value 𝑑𝑣 is 128, and the number of heads ℎ𝑒𝑛𝑐 is 8. In temporal\\r\\ngraph attention layer, 𝑑𝑎𝑡𝑡 is 256, and the number of heads ℎ𝑡𝑔𝑎 is 4.\\r\\nThe dimension of the attention vector in the heterogeneous graph\\r\\nattention layer, 𝑑𝑞, is 256.\\r\\nTrading Protocols. On the basis of [13], we use the daily buy-hold\\x02sell trading strategy to evaluate the performance of stock movement\\r\\nprediction methods in terms of returns. During each trading day\\r\\nduring the test period (from January 1, 2020 to December 31, 2020),\\r\\nwe use simulated stock traders to predict transactions:\\r\\n(1) When the trading day 𝑡 closes, traders use this method to\\r\\nget the prediction score, that is, the ranking of the predicted\\r\\nrate of return of each stock.\\r\\n(2) When the trading day 𝑡 + 1 opens: the trader sells the stock\\r\\nbought on the trading day 𝑡. Meanwhile, traders buy stocks\\r\\nwith high expected returns, i.e., the stocks with top-𝑘 scores.\\nCIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sheng Xiang et al.\\r\\nTable 2: Performance evaluation of compared models for financial time series prediction in S&P 500 and CSI 300 datasets.\\r\\nACC and ARR measure the prediction performance and portfolio return rate of each prediction model, respectively, where\\r\\nthe higher is better. AV and MDD measure the investment risk of each prediction model where the lower absolute value is\\r\\nbetter. ASR, CR, and IR measure the profit under a unit of risk, where the higher is better.\\r\\nS&P 500 CSI 300\\r\\nACC ARR AV MDD ASR CR IR ACC ARR AV MDD ASR CR IR\\r\\nLSTM 0.532 0.377 0.449 -0.382 0.842 0.989 0.954 0.515 0.291 0.318 -0.240 0.915 1.213 0.877\\r\\nGRU 0.530 0.362 0.445 -0.379 0.813 0.955 0.934 0.517 0.312 0.320 -0.243 0.975 1.284 0.932\\r\\nTransformer 0.533 0.385 0.454 -0.384 0.848 1.005 0.960 0.518 0.327 0.322 -0.245 1.016 1.335 0.969\\r\\neLSTM 0.534 0.434 0.454 -0.373 0.955 1.163 1.041 0.520 0.330 0.323 -0.239 1.022 1.381 0.991\\r\\nLSTM+GCN 0.538 0.470 0.442 -0.354 1.062 1.326 1.103 0.523 0.351 0.320 -0.217 1.097 1.618 1.119\\r\\nLSTM+RGCN 0.565 0.558 0.463 -0.366 1.205 1.522 1.203 0.536 0.509 0.326 -0.235 1.561 2.166 1.537\\r\\nTGC 0.552 0.528 0.455 -0.344 1.163 1.535 1.180 0.531 0.453 0.323 -0.224 1.402 2.022 1.412\\r\\nMAN-SF 0.551 0.527 0.467 -0.357 1.130 1.478 1.157 0.527 0.418 0.334 -0.225 1.251 1.858 1.282\\r\\nHATS 0.541 0.494 0.466 -0.387 1.060 1.277 1.110 0.525 0.385 0.332 -0.249 1.160 1.546 1.116\\r\\nREST 0.549 0.502 0.466 -0.359 1.079 1.398 1.117 0.528 0.425 0.331 -0.228 1.284 1.864 1.298\\r\\nAD-GAT 0.564 0.535 0.457 -0.371 1.170 1.444 1.187 0.539 0.537 0.329 -0.240 1.632 2.238 1.596\\r\\nTHGNN 0.579 0.665 0.468 -0.369 1.421 1.804 1.340 0.551 0.632 0.336 -0.237 1.881 2.667 1.875\\r\\n(3) Please note that if a stock is continuously rated with the\\r\\nhighest expected return, the trader holds the stock.\\r\\nIn calculating the cumulative return on investment, we follow\\r\\nseveral simple assumptions:\\r\\n(1) Traders spend the same amount on each trading day (for\\r\\nexample, $50,000). We made this assumption to eliminate the\\r\\ntime dependence of the testing process in order to make a\\r\\nfair comparison.\\r\\n(2) There is always enough liquidity in the market to satisfy the\\r\\nopening price of the order on the 𝑡 + 1 day, and the selling\\r\\nprice is also the opening price on the 𝑡 + 1 day.\\r\\n(3) Transaction costs are ignored because the cost of trading\\r\\nUS stocks through brokers is relatively cheap, whether on\\r\\na transaction-by-transaction basis or on a stock-by-stock\\r\\nbasis.Fidelity Investments (Fidelity Investments) and Inter\\x02active Brokerage (Interactive Broker), for example, charge\\r\\n$4.95 and $0.005 per transaction, respectively.\\r\\nCompared Baselines. We compared our proposed method with\\r\\nstate-of-the-art sequential-based models as well as the graph-based\\r\\napproaches. They are 1) Non-graph-based Methods, includes LSTM\\r\\n[17], GRU [11], Transformer [39], eLSTM [24]. 2) Graph-based Meth\\x02ods: LSTM-GCN [4], LSTM-RGCN [25], TGC [15], MAN-SF [35],\\r\\nHATS [22], REST [43] and AD-GAT [10].\\r\\nEvaluating Metrics. Since the goal is to accurately select the\\r\\nstocks with highest returns appropriately, we use seven metrics:\\r\\nprediction accuracy (ACC), annual return rate (ARR), annual volatil\\x02ity (AV), maximum draw down (MDD), annual sharpe ratio (ASR),\\r\\ncalmar ratio (CR), and information ratio (IR) to report the perfor\\x02mance of baselines and our proposed model. Prediction accuracy\\r\\nis widely used to evaluate classification tasks, such as stock move\\x02ment prediction [3, 24], so we calculate the prediction accuracy\\r\\nof all stocks for each trading day during the test period. Because\\r\\nARR directly reflects the effect of stock investment strategy, ARR\\r\\nis our main measure, which is calculated by adding up the returns\\r\\nof selected stocks on each test day in a year. AV directly reflects\\r\\nthe average risk of investment strategy per unit time. MDD reflects\\r\\nthe maximal draw down of investment strategy during the whole\\r\\ntest period. ASR reflects the benefits of taking on a unit of volatil\\x02ity with 𝐴𝑆𝑅 =\\r\\n𝐴𝑅𝑅\\r\\n𝐴𝑉 . CR reflects the benefits of taking on a unit\\r\\nof draw down with 𝐶𝑅 =\\r\\n𝐴𝑅𝑅\\r\\nabs(𝑀𝐷𝐷)\\r\\n. IR reflects the excess return\\r\\nunder additional risk. The smaller the absolute value of AV and\\r\\nMDD is, the higher the value of ACC, ARR, ASR, CR, and IR is, the\\r\\nbetter the performance is. For each method, we repeated the test\\r\\nprocess five times and reported average performance to eliminate\\r\\nfluctuations caused by different initialization.\\r\\n4.2 Financial Prediction\\r\\nIn this section, we evaluate the performance of financial time series\\r\\nprediction and portfolio, which is the main task of this paper. Table 2\\r\\nreports the performance through evaluation metrics such as ACC\\r\\nand ARR for each method in two datasets. The first four rows of\\r\\nTable 2 show the performance of models that do not use graph-based\\r\\ntechnology. It it clear that, none of these four methods is satisfactory,\\r\\nand their performance is all lower than that of other baselines. This\\r\\nproves that models without using company relationship data cannot\\r\\nachieve optimal performance.\\r\\nLines 5 to 11 of Table 2 show the performance of the baseline mod\\x02els using graph-based technology. According to line 6, LSTM+RGCN\\r\\nperforms best. This proves the effectiveness of using heterogeneous\\r\\ngraphs of inter-company relationships. Note that according to line 7,\\r\\nTGC’s performance is also competitive and its investment strategy\\r\\nis less volatile. This proves the effectiveness of using the dynamic\\r\\nrelationship between companies.\\r\\nAccording to the previous observation, the financial prediction\\r\\nmodel can be improved by using the heterogeneous or dynamic\\r\\nrelationships of the company relation graph. Therefore, it is nec\\x02essary to design an innovative model to improve the prediction\\r\\nperformance of financial series from both dynamic and heteroge\\x02neous graph structure. According to the last row of Table 2, our\\r\\nproposed THGNN outperforms all baselines and proves the supe\\x02riority of temporal and heterogeneous graph neural network in\\r\\nfinancial time series prediction.\\nTemporal and Heterogeneous Graph Neural Network for Financial Time Series Prediction CIKM ’22, October 17–21, 2022, Atlanta, GA, USA\\r\\nFigure 2: The accumulated returns gained in the test set (2020) by our proposed THGNN and selected baselines. For better\\r\\nillustration, we select one baseline from non-graph-based model and graph-based model, respectively.\\r\\nTable 3: Performance evaluation of ablated models for finan\\x02cial time series prediction in S&P 500 dataset. ACC, ARR,\\r\\nASR, CR, and IR measure the prediction performance and\\r\\nportfolio return rate of each prediction model, where the\\r\\nhigher is better.\\r\\nACC ARR ASR CR IR\\r\\nTHGNN-noenc 0.548 0.571 1.201 1.524 1.082\\r\\nTHGNN-notemp 0.539 0.486 0.964 1.292 0.946\\r\\nTHGNN-nohete 0.553 0.600 1.279 1.618 1.198\\r\\nTHGNN 0.579 0.665 1.421 1.804 1.340\\r\\n4.3 Ablation Study\\r\\nIn this section, we conduct the ablation experiments, that is, evalu\\x02ating the performance of our methods that one part is dropped.\\r\\nAccording to the first row of Table 3, THGNN-noenc can not\\r\\nachieve the best performance after dropping the historical price\\r\\nencoding layer. This is because the encoder is responsible for ex\\x02tracting the time correlation in the historical price information.\\r\\nAccording to the second row, THGNN-notemp achieves unsatis\\x02factory performance after dropping the temporal graph attention\\r\\nlayer. This is because the temporal graph attention layer is respon\\x02sible for dynamically adjusting the relationship between companies.\\r\\nMoreover, the relationship between companies changes dynami\\x02cally over time, especially over a long period of time. According to\\r\\nthe third row, THGNN-nohete cannot achieve the best performance\\r\\nafter dropping the heterogeneous attention mechanism. This is\\r\\nbecause the heterogeneous graph attention layer is responsible for\\r\\nweighing the importance of messages from different relationships.\\r\\n4.4 Performance of the Portfolio\\r\\nIn the performance evaluation of the portfolio strategy, we reported\\r\\n6 widely-used evaluating metrics for portfolio, e.g., the annualized\\r\\nrate of return (ARR), annual sharp ratio (ASR), calmar ratio (CR),\\r\\nand information ratio (IR). Then, we show the accumulative re\\x02turn curve to compare the investment portfolio of our model and\\r\\nbaselines during the test period. According to the trading protocols\\r\\nmentioned by section 4.1, we use the output of the prediction model\\r\\nto adjust our position day by day.\\r\\nTable 2 reports the performance of our model’s and baselines’\\r\\nportfolio strategies. It is clear that our method has achieved the\\r\\nbest performance in four of the six investment evaluating metrics.\\r\\nSpecifically, our method performs best in terms of ARR, ASR, CR\\r\\nand IR. TGC and LSTM+GCN perform better than our model in\\r\\nterms of AV and MDD. This shows that our proposed THGNN takes\\r\\nthe initiative to take more risks to pursue higher risk-return ratio\\r\\nthan other baselines’. According to Table 3, THGNN outperforms its\\r\\nsub-models in terms of portfolio strategy return evaluating metrics\\r\\n(e.g., ARR, ASR, CR, and IR). Therefore, our model shows strong\\r\\neffectiveness in building profitable portfolio strategies.\\r\\nIn order to further establish and evaluate the returns of our\\r\\nmodel during the test time, we calculate the cumulative returns for\\r\\neach trading day during the test period. We report the cumulative\\r\\nreturn curve of our model and other models on each trading day in\\r\\nFigure 2. Due to space constraints, we select some representative\\r\\nbaselines to compare with our model. We can observe that all base\\x02lines outperform the S&P500 index. None of the models beat the\\r\\nothers in the first four months. After starting in May, our model be\\x02gan to stay ahead of other models. In the following time, our model\\r\\ngradually widened the gap with other models. THGNN remained in\\r\\nthe lead in the last month of 2020 and eventually achieved a profit\\r\\non investment of more than 60 per cent. Experimental results on\\r\\nother baselines can draw similar conclusion.\\r\\n4.5 Parameter Sensitivity\\r\\nIn this section, we report the experimental results of parameter\\r\\nsensitivity on the financial prediction task on S&P500 dataset with\\r\\nvarious parameters in Figure 3.\\r\\nAccording to Figure 3 (a), we can observe that the performance\\r\\nof the model increases with the increase of embedded dimensions.\\r\\nThe performance of the model peaked at 256 and then degraded\\r\\nrapidly. This is because embedded information needs a suitable di\\x02mension to reduce information loss and noise. According to Figure 3\\r\\n(b), the performance of the model slowly improves as the coding\\r\\noutput dimension increases. The performance of the model begins\\r\\nto deteriorate after the dimension reaches 128. This is because the\\r\\ninput information dimension is low, so the low-dimensional out\\x02put can also achieve better performance. According to Figure 3 (c),\\r\\nthe performance of the model increases with the increase of the\\r\\nattention vector dimension. When the dimension reaches 256, the\\r\\nperformance of the model reaches its peak. Continuing to increase\\r\\nthe dimension will lead to overfitting, which will degrade the model.\\r\\nAccording to Figure 3 (d), we can observe that the fluctuation of\\r\\nthe model performance is low. We choose to pay attention to the\\r\\nnumber of force heads as 8. We also note that increasing the number\\r\\nof attention heads will make the model training more stable.\\nCIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sheng Xiang et al.\\r\\n(a) Dimension of final embedding 𝑑𝑎𝑡𝑡 (b) Dimension of encoding output 𝑑𝑒𝑛𝑐\\r\\n(c) Dimension of the attention vector q (d) Number of attention head ℎ\\r\\nFigure 3: Prediction performance of THGNN in terms of di\\x02mension of final embedding𝑑𝑎𝑡𝑡, dimension of encoding out\\x02put 𝑑𝑒𝑛𝑐 , dimension of the attention vector q, and number of\\r\\nattention head ℎ.\\r\\n4.6 Interpretability of Graph Neural Network\\r\\nThe stock price fluctuation of listed companies is not only related to\\r\\nthe historical price information of the company, but also affected by\\r\\nthe companies related to it. Therefore, we need to integrate dynamic\\r\\nand heterogeneous relevant information as input to the prediction\\r\\nmodel. In our technology, the relationship between each company\\r\\nchanges dynamically over time. The strength of the relationship\\r\\nbetween companies, that is, the proportion of contribution to the\\r\\nmessage also changes over time. Our time chart attention mecha\\x02nism can dynamically adjust the importance of each company in\\r\\nthe diagram. In addition, our heterogeneous attention mechanism\\r\\ncan dynamically adjust the importance of each source. Therefore,\\r\\nour model can help to predict stock price volatility more accurately,\\r\\nand the experimental results verify the superiority of our model\\r\\nperformance.\\r\\nThen, in order to explore the interpretability of our proposed\\r\\nmodel, we extract the attention weight of the graph in the process\\r\\nof the model prediction. We counted the attention weight of all\\r\\nnodes in the process of message delivery on the relational graph.\\r\\nUnder different daily returns and node degrees, we take the mean\\r\\nvalue of attention weights and visualize the statistical results, which\\r\\nare reported in Figure 4.\\r\\nFigure 4 (a) shows the attention weights on the 𝑝𝑜𝑠 diagram. We\\r\\ncan see (on the y-axis) that the nodes with higher degrees have\\r\\nhigher average attention weights. This shows that in the process\\r\\nof message delivery on the 𝑝𝑜𝑠 graph, the degree is higher, that\\r\\nis, the nodes with more neighbors will contribute more messages\\r\\nto their neighbors. We also found that (on the x-axis), companies\\r\\nwith larger fluctuations in daily returns also had higher average\\r\\nattention weights. This shows that more volatile price changes will\\r\\ncontribute more information to their neighbors, which also means\\r\\nthat price fluctuations will produce momentum spillover effects.\\r\\n(a) Graph Attention in 𝑝𝑜𝑠 Relationship\\r\\n(b) Graph Attention in 𝑛𝑒𝑔 Relationship\\r\\nFigure 4: Visualization of attention weights, X-axis denotes\\r\\nthe daily return of stocks. Y-axis denotes the average degree\\r\\nin each company relation graph.\\r\\n(a) ACC values on S&P500. (b) ACC values on CSI300.\\r\\nFigure 5: Prediction performance of single message source\\r\\nand corresponding attention value.\\r\\nAccording to Figure 4 (b), we can see that on the 𝑛𝑒𝑔 graph, with\\r\\na lower degree node, the average attention weight is higher. This\\r\\nindicates that during message delivery on the 𝑛𝑒𝑔 graph, nodes\\r\\nwith lower degrees contribute more messages to their neighbors.\\r\\nFor more interpretable experimental results, we also visualize\\r\\neach relationship’s attention weights and show the corresponding\\r\\nperformance when using only one relationship. Specifically, we\\r\\ntrained our proposed THGNN at two datasets, and counted the\\r\\nmean value of the attention weights in heterogeneous graph atten\\x02tion layer. Then, we used each single relationship’s message as the\\r\\ninput of the prediction model to obtain the prediction performance,\\r\\nas illustrated in Figure 5.\\r\\nIt is clear that 𝑠𝑒𝑙 𝑓 and 𝑝𝑜𝑠 message resources achieve better pre\\x02diction performance than 𝑛𝑒𝑔. Moreover, the 𝑝𝑜𝑠 message resource\\r\\ncontribute importantly to the prediction model, which proves the\\r\\ncontribution to the prediction model. The reason might be that the\\r\\ninfluence between companies in the similar price movement is rel\\x02atively useful for prediction future price movement. Although the\\r\\n𝑛𝑒𝑔 message resource has an unsatisfactory performance when pre\\x02dicting price movement single, it still contributes to our proposed\\r\\nTHGNN in achieving the state-of-the-art performance according to\\r\\nTable 3. We can see that temporal graph attention layer can reveal\\r\\nthe difference between nodes and weights then adequately, and\\r\\nthe heterogeneous graph attention can adjust the contribution of\\r\\neach message resource adaptively. The result demonstrates the ef\\x02fectiveness of graph-structure information and the interpretability\\r\\nof proposed graph neural network model.\\nTemporal and Heterogeneous Graph Neural Network for Financial Time Series Prediction CIKM ’22, October 17–21, 2022, Atlanta, GA, USA\\r\\nFigure 6: The desktop interface of investment portfolio based on our proposed THGNN method. It includes price-relevant\\r\\nlisted companies from historical data and visualization of how does our method makes predictions on buying and selling. The\\r\\npart (a) lists the stocks held by our method in order from highest to lowest. And part (b) shows the ’buy’ and ’sell’ signals\\r\\ngenerated by our trading protocols. Then part (c) lists the listed companies related to China National Petroleum Corporation\\r\\n(CBPC: 601857) and shows which ones have higher correlation to this company according to our generated stock correlation\\r\\ngraph.\\r\\n4.7 System Implementation\\r\\nIn this section, we introduce the implementation detail of our pro\\x02posed methods. We first show the settings of model employment\\r\\nand training strategy. Then we show the web-based application,\\r\\nwhich shows how our proposed method gives customers informa\\x02tive advice. Our proposed THGNN is re-trained every trading day.\\r\\nTo handle the large-scale dataset, we leverage mini-batch gradient\\r\\ndescent optimization with a batch size of 256 and a learning rate\\r\\nof 3e-4. The model is implemented by PyTorch and deployed in\\r\\nPython and Java. Besides, we use distributed Scrapy [32] to obtain\\r\\nhistorical stock data and utilize Neo4j [18] as the graph database to\\r\\nstore relational graphs.\\r\\nFigure 6 shows the interface of our desktop application. The\\r\\nupper left part of Figure 6 is the list of stocks to be held based\\r\\non the THGNN strategy. The lower left part of Figure 6 reports\\r\\nthe price change curve of China National Petroleum Corporation\\r\\n(CBPC: 601857). It contains buy and sell points suggested by our\\r\\nTHGNN. B denotes buying and S denotes selling. It can be seen\\r\\nthat our model provides three buy signals and two sell signals. The\\r\\nlower right part of Figure 6 reports the relevant companies of CBPC.\\r\\nCompanies with high stock price volatility correlations are marked\\r\\nwith red arrowhead. The results shows that our investment strategy\\r\\nprovides informative advice through a relational graph approach.\\r\\n5 CONCLUSION AND DISCUSSION\\r\\nIn this paper, a new temporal and heterogeneous graph neural net\\x02work model is proposed for financial time series prediction. Our\\r\\nmethod addresses the limitations of the existing works of graph\\r\\nneural networks by adjusting the message contribution ratio of\\r\\neach node through temporal and heterogeneous graph attention\\r\\nmechanism. We evaluate the effectiveness of the proposed method\\r\\ncomprehensively by comparing it with the most influential graph\\x02based and non-graph-based baselines. In addition, THGNN per\\x02forms better than other baselines in the actual investment strategy,\\r\\nand the results show that our approach based on dynamic hetero\\x02geneous graph can obtain a more profitable portfolio strategy than\\r\\nthose based on static or homogeneous graph structure.\\r\\nIn conclusion, this paper is the first time to model the inter\\x02company relationship into a heterogeneous dynamic graph, and\\r\\napply it to the financial time series prediction problem. This is bene\\x02ficial to the more extensive research and innovation of graph-based\\r\\ntechnology in the financial field. On the one hand, we model the\\r\\ncompany relation graph as the real dynamic heterogeneous graph;\\r\\non the other hand, we improve the financial time series predic\\x02tion model through the latest graph neural network technology.\\r\\nBesides, there is still room for improvement in our work on gener\\x02ating real-life company relation graphs. In the future, we will focus\\r\\non improving the modeling of the company’s relation to help the\\r\\nprediction model obtain more accurate training input graph data.\\nCIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sheng Xiang et al.\\r\\nACKNOWLEDGMENTS\\r\\nDawei Cheng is supported by the NSFC 62102287, Ying Zhang is\\r\\nsupported by ARC DP210101393.\\r\\nREFERENCES\\r\\n[1] Klaus Adam, J. Nicolini, and A. Marcet. 2008. Stock Market Volatility and Learning.\\r\\nIO: Theory (2008).\\r\\n[2] Michel Ballings, D. V. Poel, Nathalie Hespeels, and Ruben Gryp. 2015. Evaluating\\r\\nmultiple classifiers for stock price direction prediction. Expert Syst. Appl. 42\\r\\n(2015), 7046–7056.\\r\\n[3] Chi Chen, Li Zhao, Jiang Bian, Chunxiao Xing, and Tie-Yan Liu. 2019. Investment\\r\\nBehaviors Can Tell What Inside: Exploring Stock Intrinsic Properties for Stock\\r\\nTrend Prediction. Proceedings of the 25th ACM SIGKDD International Conference\\r\\non Knowledge Discovery & Data Mining (2019).\\r\\n[4] Yingmei Chen, Zhongyu Wei, and Xuanjing Huang. 2018. Incorporating Corpo\\x02ration Relationship via Graph Convolutional Neural Networks for Stock Price\\r\\nPrediction. Proceedings of the 27th ACM International Conference on Information\\r\\nand Knowledge Management (2018).\\r\\n[5] Dawei Cheng, Zhibin Niu, and Liqing Zhang. 2020. Delinquent events prediction\\r\\nin temporal networked-guarantee loans. IEEE Transactions on Neural Networks\\r\\nand Learning Systems (2020).\\r\\n[6] Dawei Cheng, Xiaoyang Wang, Ying Zhang, and Liqing Zhang. 2020. Graph\\r\\nNeural Network for Fraud Detection via Spatial-temporal Attention. IEEE Trans\\x02actions on Knowledge and Data Engineering (2020).\\r\\n[7] Dawei Cheng, Fangzhou Yang, Xiaoyang Wang, Y. Zhang, and Liqing Zhang. 2020.\\r\\nKnowledge Graph-based Event Embedding Framework for Financial Quantitative\\r\\nInvestments. Proceedings of the 43rd International ACM SIGIR Conference on\\r\\nResearch and Development in Information Retrieval (2020).\\r\\n[8] Dawei Cheng, Fangzhou Yang, Sheng Xiang, and Jin Liu. 2022. Financial time\\r\\nseries forecasting with multi-modality graph neural network. Pattern Recognition\\r\\n121 (2022), 108218. https://doi.org/10.1016/j.patcog.2021.108218\\r\\n[9] Dawei Cheng, Yiyi Zhang, Fangzhou Yang, Yi Tu, Zhibin Niu, and Liqing Zhang.\\r\\n2019. A dynamic default prediction framework for networked-guarantee loans.\\r\\nIn Proceedings of the 28th ACM International Conference on Information and\\r\\nKnowledge Management. 2547–2555.\\r\\n[10] Rui Cheng and Qing Li. 2021. Modeling the Momentum Spillover Effect for Stock\\r\\nPrediction via Attribute-Driven Graph Attention Networks. In AAAI.\\r\\n[11] Kyunghyun Cho, B. V. Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi\\r\\nBougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Represen\\x02tations using RNN Encoder–Decoder for Statistical Machine Translation. ArXiv\\r\\nabs/1406.1078 (2014).\\r\\n[12] Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. [n.d.]. Using Structured\\r\\nEvents to Predict Stock Price Movement: An Empirical Investigation. In Proceed\\x02ings of the 2014 Conference on Empirical Methods in Natural Language Processing,\\r\\n2014, October 25-29, 2014. 1415–1425.\\r\\n[13] M. Dixon, D. Klabjan, and J. Bang. 2017. Classification-based financial markets\\r\\nprediction using deep neural networks. Algorithmic Finance 6 (2017), 67–77.\\r\\n[14] Fuli Feng, Huimin Chen, Xiangnan He, Ji Ding, Maosong Sun, and Tat-Seng\\r\\nChua. 2019. Enhancing Stock Movement Prediction with Adversarial Training.\\r\\nIn IJCAI.\\r\\n[15] Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua.\\r\\n2019. Temporal Relational Ranking for Stock Prediction. ACM Transactions on\\r\\nInformation Systems (TOIS) 37 (2019), 1 – 30.\\r\\n[16] Gartheeban Ganeshapillai, John Guttag, and Andrew Lo. 2013. Learning connec\\x02tions in financial time series. In International Conference on Machine Learning.\\r\\nPMLR, 109–117.\\r\\n[17] S. Hochreiter and J. Schmidhuber. 1997. Long Short-Term Memory. Neural\\r\\nComputation 9 (1997), 1735–1780.\\r\\n[18] Florian Holzschuher and René Peinl. 2013. Performance of graph query languages:\\r\\ncomparison of cypher, gremlin and native access in Neo4j. In EDBT ’13.\\r\\n[19] Joel F Houston, Chen Lin, and Zhongyan Zhu. 2016. The financial implications\\r\\nof supply chain changes. Management Science 62, 9 (2016), 2520–2542.\\r\\n[20] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021.\\r\\nA survey on knowledge graphs: Representation, acquisition, and applications.\\r\\nIEEE Transactions on Neural Networks and Learning Systems (2021).\\r\\n[21] Zhigang Jin, Ya-Chi Yang, and Yuhong Liu. 2019. Stock closing price prediction\\r\\nbased on sentiment analysis and LSTM. Neural Computing and Applications 32\\r\\n(2019), 9713–9729.\\r\\n[22] Raehyun Kim, Chan Ho So, Minbyul Jeong, Sanghoon Lee, Jinkyu Kim, and\\r\\nJaewoo Kang. 2019. HATS: A Hierarchical Graph Attention Network for Stock\\r\\nMovement Prediction. ArXiv abs/1908.07999 (2019).\\r\\n[23] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti\\x02mization. CoRR abs/1412.6980 (2015).\\r\\n[24] Qing Li, Jinghua Tan, Jun Wang, and HsinChun Chen. 2020. A Multimodal Event\\x02driven LSTM Model for Stock Prediction Using Online News. IEEE Transactions\\r\\non Knowledge and Data Engineering (2020), 1–1. https://doi.org/10.1109/TKDE.\\r\\n2020.2968894\\r\\n[25] Wei Li, Ruihan Bao, Keiko Harimoto, Deli Chen, Jingjing Xu, and Qi Su. 2020.\\r\\nModeling the Stock Relation with Graph Network for Overnight Stock Movement\\r\\nPrediction. In IJCAI.\\r\\n[26] Zhige Li, Derek Yang, Li Zhao, Jiang Bian, Tao Qin, and Tie-Yan Liu. 2019. Indi\\x02vidualized Indicator for All: Stock-wise Technical Indicator Optimization with\\r\\nStock Embedding. Proceedings of the 25th ACM SIGKDD International Conference\\r\\non Knowledge Discovery & Data Mining (2019).\\r\\n[27] Xin Liang, Dawei Cheng, Fangzhou Yang, Yifeng Luo, Weining Qian, and Aoying\\r\\nZhou. 2020. F-HMTC: Detecting Financial Events for Investment Decisions Based\\r\\non Neural Hierarchical Multi-Label Text Classification.. In IJCAI. 4490–4496.\\r\\n[28] Qikai Liu, Xiang Cheng, Sen Su, and Shuguang Zhu. [n.d.]. Hierarchical Comple\\x02mentary Attention Network for Predicting Stock Price Movements with News. In\\r\\nProceedings of the 27th ACM International Conference on Information and Knowl\\x02edge Management, CIKM 2018, Torino, Italy, October 22-26, 2018. 1603–1606.\\r\\n[29] Weiwen Liu, Yin Zhang, Jianling Wang, Yun He, James Caverlee, Patrick PK\\r\\nChan, Daniel S Yeung, and Pheng-Ann Heng. 2021. Item Relationship Graph\\r\\nNeural Networks for E-Commerce. IEEE Transactions on Neural Networks and\\r\\nLearning Systems (2021).\\r\\n[30] Xiao-Yang Liu, Jingyang Rui, Jiechao Gao, Liuqing Yang, Hongyang Yang, Zhao\\x02ran Wang, Christina Dan Wang, and Guo Jian. 2021. FinRL-Meta: Data-Driven\\r\\nDeep ReinforcementLearning in Quantitative Finance. Data-Centric AI Workshop,\\r\\nNeurIPS (2021).\\r\\n[31] Simone Merello, Andrea Picasso Ratto, L. Oneto, and E. Cambria. 2019. Ensemble\\r\\nApplication of Transfer Learning and Sample Weighting for Stock Market Pre\\x02diction. 2019 International Joint Conference on Neural Networks (IJCNN) (2019),\\r\\n1–8.\\r\\n[32] Daniel N. Myers and James W. McGuffee. 2015. Choosing Scrapy. Journal of\\r\\nComputing Sciences in Colleges.\\r\\n[33] World Federation of Exchanges database. 2021. Market capitalization of listed\\r\\ndomestic companies. https://data.worldbank.org/indicator/CM.MKT.LCAP.CD\\r\\naccessed 25-July-2021.\\r\\n[34] Ashwin Paranjape, Austin R. Benson, and Jure Leskovec. 2017. Motifs in Temporal\\r\\nNetworks. Proceedings of the Tenth ACM International Conference on Web Search\\r\\nand Data Mining (2017).\\r\\n[35] Ramit Sawhney, S. Agarwal, Arnav Wadhwa, and R. Shah. 2020. Deep Attentive\\r\\nLearning for Stock Movement Prediction From Social Media Text and Company\\r\\nCorrelations. In EMNLP.\\r\\n[36] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Shah. 2020. Deep\\r\\nAttentive Learning for Stock Movement Prediction from Social Media Text and\\r\\nCompany Correlations. In Proceedings of the 2020 Conference on Empirical Methods\\r\\nin Natural Language Processing (EMNLP). 8415–8426.\\r\\n[37] Y. Tu, Li Niu, Junjie Chen, Dawei Cheng, and Liqing Zhang. 2020. Learning From\\r\\nWeb Data With Self-Organizing Memory Module. 2020 IEEE/CVF Conference on\\r\\nComputer Vision and Pattern Recognition (CVPR) (2020), 12843–12852.\\r\\n[38] Yi Tu, Li Niu, Weijie Zhao, Dawei Cheng, and Liqing Zhang. 2020. Image cropping\\r\\nwith composition and saliency aware aesthetic score map. In Proceedings of the\\r\\nAAAI Conference on Artificial Intelligence, Vol. 34. 12104–12111.\\r\\n[39] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\r\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\\r\\nNeed. ArXiv abs/1706.03762 (2017).\\r\\n[40] Yaowei Wang, Qing Li, Zhexue Huang, and Mark Junjie Li. [n.d.]. EAN: Event\\r\\nAttention Network for Stock Price Trend Prediction based on Sentimental Em\\x02bedding. In Proceedings of the 11th ACM Conference on Web Science, WebSci 2019,\\r\\nBoston, MA, USA, June 30 - July 03, 2019. 311–320.\\r\\n[41] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and\\r\\nS Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE\\r\\ntransactions on neural networks and learning systems 32, 1 (2020), 4–24.\\r\\n[42] Sheng Xiang, Dong Wen, Dawei Cheng, Ying Zhang, Lu Qin, Zhengping Qian,\\r\\nand Xuemin Lin. 2021. General graph generators: experiments, analyses, and\\r\\nimprovements. The VLDB Journal (2021).\\r\\n[43] Wentao Xu, Weiqing Liu, Chang Xu, Jiang Bian, Jian Yin, and Tie-Yan Liu. 2021.\\r\\nREST: Relational Event-driven Stock Trend Forecasting. Proceedings of the Web\\r\\nConference 2021 (2021).\\r\\n[44] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,\\r\\nYanming Shen, and Tie-Yan Liu. 2021. Do Transformers Really Perform Bad for\\r\\nGraph Representation?. In NeurIPS.\\r\\n[45] Dawei Zhou, Lecheng Zheng, Jiawei Han, and Jingrui He. 2020. A Data-Driven\\r\\nGraph Generative Model for Temporal Interaction Networks. Proceedings of\\r\\nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data\\r\\nMining (2020).'},\n",
       " {'name': '2312.01728v3.pdf',\n",
       "  'content': 'ImputeFormer: Low Rankness-Induced Transformers for\\r\\nGeneralizable Spatiotemporal Imputation\\r\\nTong Nie1, Guoyang Qin1, Wei Ma2, Yuewen Mei1, Jian Sun1,†\\r\\n1Tongji University, 2The Hong Kong Polytechnic University\\r\\nnietong@tongji.edu.cn, 2015qgy@tongji.edu.cn, wei.w.ma@polyu.edu.hk,\\r\\nmeiyuewen@tongji.edu.cn, sunjian@tongji.edu.cn\\r\\nABSTRACT\\r\\nMissing data is a pervasive issue in both scientific and engineering\\r\\ntasks, especially for the modeling of spatiotemporal data. This prob\\x02lem attracts many studies to contribute to data-driven solutions.\\r\\nExisting imputation solutions mainly include low-rank models and\\r\\ndeep learning models. The former assumes general structural pri\\x02ors but has limited model capacity. The latter possesses salient\\r\\nfeatures of expressivity but lacks prior knowledge of the underly\\x02ing spatiotemporal structures. Leveraging the strengths of both two\\r\\nparadigms, we demonstrate a low rankness-induced Transformer\\r\\nto achieve a balance between strong inductive bias and high model\\r\\nexpressivity. The exploitation of the inherent structures of spa\\x02tiotemporal data enables our model to learn balanced signal-noise\\r\\nrepresentations, making it generalizable for a variety of imputation\\r\\nproblems. We demonstrate its superiority in terms of accuracy, effi\\x02ciency, and versatility in heterogeneous datasets, including traffic\\r\\nflow, solar energy, smart meters, and air quality. Promising empiri\\x02cal results provide strong conviction that incorporating time series\\r\\nprimitives, such as low-rankness, can substantially facilitate the\\r\\ndevelopment of a generalizable model to approach a wide range of\\r\\nspatiotemporal imputation problems. The model implementation is\\r\\navailable at: https://github.com/tongnie/ImputeFormer.\\r\\nCCS CONCEPTS\\r\\n• Information systems → Spatial-temporal systems.\\r\\nKEYWORDS\\r\\nMissing Data, Data Imputation, Transformers, Low-Rank Modeling,\\r\\nSpatiotemporal Data, Time Series.\\r\\nACM Reference Format:\\r\\nTong Nie, Guoyang Qin, Wei Ma, Yuewen Mei, and Jian Sun. 2024. Im\\x02puteFormer: Low Rankness-Induced Transformers for Generalizable Spa\\x02tiotemporal Imputation. In Proceedings of the 30th ACM SIGKDD Conference\\r\\non Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,\\r\\nBarcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/xx\\r\\n1 INTRODUCTION\\r\\nMissing data is a common challenge in detection systems, especially\\r\\nin high-resolution monitoring systems. Factors such as inclement\\r\\n†\\r\\nJian Sun is the corresponding author.\\r\\nThis work is licensed under a Creative Commons Attribution\\r\\nInternational 4.0 License.\\r\\nKDD ’24, August 25–29, 2024, Barcelona, Spain.\\r\\n© 2024 Copyright held by the owner/author(s).\\r\\nACM ISBN xx.\\r\\nhttps://doi.org/xx\\r\\nweather, energy supply, and sensor service time can adversely affect\\r\\nthe quality of monitoring data [5]. Given these factors, data missing\\r\\nrates can be quite high. For example, the air quality measurements in\\r\\nthe Urban Air project [48] contain about 30% invalid records due to\\r\\nstation malfunction. Similarly, subsets of Uber Movement data, such\\r\\nas New York City, have an approximate 85% of missing records. This\\r\\nproblem encourages researchers to develop advanced models that\\r\\ncan exploit limited observations to impute missing values. Extensive\\r\\nresearch has contributed to data-driven methods for this purpose,\\r\\nespecially in the field of spatiotemporal data [5, 6, 28, 30, 40, 44].\\r\\nGenerally, there are two research paradigms for imputing miss\\x02ing data. The first paradigm uses low-rank and low-dimensional\\r\\nanalytical models, such as [4, 6, 20, 28, 40], which assumes the\\r\\ndata has a well-structured matrix or tensor form. These models\\r\\nutilize the algebraic properties of the assumed structure, such as\\r\\nlow-rankness, low nuclear norm, and spectrum sparsity (we use the\\r\\nterm “low-rank” as a proxy), to impute missing values [25]. While\\r\\nsimple low-rank models like matrix factorization and tensor com\\x02pletion can effectively handle incomplete data, they may struggle\\r\\nwith capturing complex patterns, such as nonlinearity and nonsta\\x02tionarity. With strong inductive bias, these models can excessively\\r\\nsmooth the reconstructed data, filtering out informative signals,\\r\\nand generating oversmoothing reconstructions in some cases [27].\\r\\nThe second paradigm uses deep learning-based imputation mod\\x02els. These models learn the dynamics of the data-generating process\\r\\nand demonstrate improved performance [1, 2, 7, 10, 19, 26]. How\\x02ever, despite the success of these models on various benchmarks,\\r\\nthere are still costs and difficulties that need further attention. First,\\r\\nsuch data-intensive methods require a substantial amount of train\\x02ing expenses due to the complex model structures in deep learning,\\r\\nsuch as probabilistic diffusion and bidirectional recurrence [1, 36].\\r\\nThis can consume significant computational memory and resources,\\r\\nmaking them less efficient for real-time deployment. Second, empir\\x02ical loss-based learning methods, without the guidance of physics or\\r\\ndata structures, are prone to overfitting and perform poorly when\\r\\napplied to tasks fall outside of the distribution of training data [47].\\r\\nWith the shift of focus in the field of deep imputation from RNNs\\r\\nand diffusion models to Transformers [10, 24, 26], Transformer\\x02related architectures have gained significant attention due to their\\r\\npotential to provide efficient generative output and high expressiv\\x02ity, enabling more effective imputations compared to autoregression\\x02based models. Additionally, Transformers are considered founda\\x02tional architectures for general time series forecasting [11]. How\\x02ever, the effectiveness of applying Transformers to general data\\r\\nimputation tasks requires further investigation. Modern deep learn\\x02ing techniques associated with these architectures, such as self\\x02attention and residual connections, can unintentionally preserve\\r\\narXiv:2312.01728v3 [cs.LG] 29 May 2024\\nKDD ’24, August 25–29, 2024, Barcelona, Spain. Tong Nie et al.\\r\\na.\\r\\nIncomplete\\r\\nSharp\\r\\nSmooth Ground truth\\r\\nSignal Noise Signal Noise\\r\\nw/ structural priors\\r\\nlow expressivity\\r\\nb. Paradigm 1: Low-rank/-dim imputation\\r\\nReconstructed\\r\\nSharp\\r\\nSmooth Signal Noise\\r\\nc. Paradigm 2: Deep learning imputation\\r\\nw/o structural priors\\r\\nhigh expressivity\\r\\nReconstructed\\r\\nSharp\\r\\nSmooth Signal Noise\\r\\nReconstructed\\r\\nw/ structural priors\\r\\nhigh expressivity\\r\\n(+model priors)\\r\\nd. Low rankness-induced deep learning\\r\\nOurs: ImputeFormer\\r\\nSharp\\r\\nBalanced\\r\\nSmooth\\r\\nSingular values\\r\\nSpatiotemporal matrix\\r\\nFigure 1: (a) The distribution of singular values in spatiotemporal data is long-tailed. The existence of missing data can increase\\r\\nits rank (or singular values). (b) Low-rank models can filter out informative signals and generate a smooth reconstruction,\\r\\nresulting in truncating too much energy in the left part of its spectrum. (c) Deep models can preserve high-frequency noise and\\r\\ngenerate sharp imputations, maintaining too much energy for the right part of the singular spectrum. With the generality of\\r\\nlow-rank models and the expressivity of deep models, ImputeFormer achieves a signal-noise balance for accurate imputation.\\r\\nhigh-frequency noise in data as informative signals [34]. This can\\r\\nlead the model to learn high-rank representations that violate the\\r\\nnatural distribution of data. Furthermore, the existence of missing\\r\\ndata can introduce spurious correlations between “tokens,” posing\\r\\nchallenges to these architectures. Considering the above-mentioned\\r\\nconcerns, incorporating a low-rank inductive bias into the Trans\\x02former framework seems to provide a chance to improve both the\\r\\neffectiveness and efficiency in spatiotemporal imputation.\\r\\nIn summary, matrix- and tensor-based models offer useful priors\\r\\nfor spatiotemporal data, such as low-rankness and sparsity. How\\x02ever, their ability to represent data is limited (see Fig. 1(b)). On\\r\\nthe other hand, deep learning models, particularly Transformers,\\r\\nexcel at learning representations but lack prior knowledge of data\\r\\ngeneration (see Fig. 1(c)). As the demand for a versatile and adapt\\x02able model that can handle various imputation problems in reality\\r\\nincreases, such as cross-domain datasets, different observation con\\x02ditions, highly sparse measurements, and different input patterns,\\r\\nit becomes apparent that existing advanced solutions, typically\\r\\nevaluated on limited tasks with simple settings, may not be general\\x02izable. Hence, there is a temptation to merge these two paradigms\\r\\nand utilize their respective strengths to investigate an alternative\\r\\nparadigm that can effectively handle complex imputation scenarios.\\r\\nTo this end, in this paper we leverage the structural priors of low\\x02rankness to generalize the canonical Transformer (see Fig. 1(d)) in\\r\\ngeneral spatiotemporal imputation tasks. Our approach, referred to\\r\\nas Imputation Transformers (ImputeFormer), imposes low-rankness\\r\\nand achieves attention factorization equivalently by introducing\\r\\na projected attention mechanism on the temporal dimension and\\r\\nan embedded attention on the spatial dimension. Additionally, we\\r\\npropose a Fourier sparsity loss to regularize the solution’s spectrum.\\r\\nBy inheriting the merits of both low-rank and deep learning models,\\r\\nit has achieved state-of-the-art imputation performance on various\\r\\nbenchmarks. Our main contributions are summarized as follows:\\r\\n(1) We are among the first to empower Transformers with low\\x02rankness inductive bias to achieve a balance between signal\\r\\nand noise for general spatiotemporal data imputation;\\r\\n(2) Compared to state-of-the-art benchmark models, we demon\\x02strate the advantages of ImputeFormer in accuracy, effi\\x02ciency, and versatility in diverse datasets, such as traffic\\r\\nflow, solar energy, electricity consumption, and air quality;\\r\\n(3) Comprehensive case studies reveal the model’s interpretabil\\x02ity and provide insights into the deep imputation paradigm.\\r\\n2 PRELIMINARY\\r\\nNotations. This section first introduces some basic notations fol\\x02lowing [26]. In a continuously working sensor system with 𝑁 static\\r\\ndetectors at some measurement positions, spatiotemporal data with\\r\\ncontext information can be obtained: (1) X𝑡:𝑡+𝑇 ∈ R\\r\\n𝑁 ×𝑇\\r\\n: The ob\\x02served data matrix containing missing values collected by all sen\\x02sors over a time interval T = {𝑡, . . . , 𝑡 +𝑇 }, where 𝑇 represents the\\r\\nobservation period; (2) Y𝑡:𝑡+𝑇 ∈ R\\r\\n𝑁 ×𝑇\\r\\n: The ground truth data ma\\x02trix used for evaluation; (3) U𝑡:𝑡+𝑇 ∈ R\\r\\n𝑇 ×𝑑u : Exogenous variables\\r\\ndescribe time series, such as the time of day, day of week, and week\\r\\nof month information; (4) V ∈ R\\r\\n𝑁 ×𝑑v\\r\\n: Meta information of sensors,\\r\\nsuch as detector ID and location of installation.\\r\\nProblem Formulation. The multivariate time series imputation\\r\\nproblem defines an inductive learning and inference process:\\r\\nLearning Θb = arg min\\r\\nΘ\\r\\n∑︁\\r\\n𝑡 ∈ T\\r\\nℓ (NN({x𝑡, u𝑡, m𝑡 }, V|Θ), x𝑡) ,\\r\\nInference bx𝑡\\r\\n′ = NN({x𝑡\\r\\n′, u𝑡′, m𝑡′ }, V|Θb), ∀{𝑡\\r\\n′\\r\\n. . . 𝑡′ + 𝑇 },\\r\\n(1)\\r\\nwhere NN(·|Θ) is the neural network model parameterized by Θ,\\r\\nand the indicator m𝑡 denotes the locations of the masked values\\r\\nfor training and the locations of the observed values for inference.\\r\\nAfter training the model on observed data, the imputation model\\r\\ncan act in a different time span than the training set.\\nImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation KDD ’24, August 25–29, 2024, Barcelona, Spain.\\r\\nL× Spatial-Temporal Layers Spatial interaction\\r\\nSpatial Attention\\r\\nLayerNorm\\r\\nLayerNorm\\r\\nFeedForward\\r\\nTemporal Attention\\r\\nLayerNorm\\r\\nLayerNorm\\r\\nFeedForward\\r\\nHidden States\\r\\nConcat\\r\\nInput \\r\\nEmb.\\r\\nPositional\\r\\nEnc.\\r\\nAdaptive\\r\\nEmb.\\r\\nTemporal Interaction\\r\\nReadout\\r\\nImputation\\r\\nLow-Rank Pattern in Spatial Attention Map\\r\\nRedundancy and Dominance in Time Series\\r\\nFourier Sparsity in Both Space and Time\\r\\nFigure 2: Low-rankness in time series and the induced ImputeFormer. (a) Redundancy in time series: PEMS08 data can be\\r\\nreasonably reconstructed using only five dominant patterns. (b) Low-rank spatial attention map: the singular values of the\\r\\nmultivariate attention map show a long-tailed distribution and most of them are small values. (c) Fourier sparsity in both space\\r\\nand time axes: both the spatial and temporal signals possess a sparse Fourier spectrum, with most amplitudes close to zero.\\r\\n3 RELATED WORK\\r\\nGenerally, there exist two series of studies on multivariate time\\r\\nseries imputation, i.e., 1) low-dimensional/rank models and 2) deep\\r\\nimputation models. We particularly discuss existing Transformer\\x02based solutions to clarify the connections between our models.\\r\\nLow-Dimensional/Rank Imputation. Early methods addressed\\r\\nthe data imputation problem by exploring statistical interpolation\\r\\ntools, such as MICE [37]. Recently, low-rank matrix factorization\\r\\n[20, 46] and tensor completion [4–6, 28, 30] have emerged as nu\\x02merically efficient techniques for spatiotemporal imputation. To in\\x02corporate series-related features, TRMF [46] imposed autoregressive\\r\\nregularization on the temporal manifold. TiDER [20] decomposed\\r\\nthe time series into trend, seasonality and bias components under\\r\\nthe factorization framework. Despite being conceptually intuitive\\r\\nand concise, limited capacity hinder their practical effectiveness.\\r\\nDeep Learning Imputation. Recent advances in neural time series\\r\\nanalysis open a new horizon to improve imputation performance.\\r\\nGenerally, deep imputation methods learn to reconstruct the distri\\x02bution of observed data or aggregate pointwise information pro\\x02gressively [12]. Representative methods include GRU-D [2], GRUI\\r\\n[22], BRITS [1], GAIN [45], E2GAN [23] NAOMI [21], CSDI [36], and\\r\\nPriSTI [19]. To exploit the multivariate nature of spatiotemporal\\r\\ndata, graph neural networks (GNNs) have been adopted to model\\r\\nsensor-wise correlations for more complicated missing patterns.\\r\\nFor example, MDGCN [15] and GACN [44] applied GNNs with RNNs\\r\\nfor traffic data imputation. IGNNK [42], STAR [14], and STCAGCN [31]\\r\\nfurther tackle the kriging problem, which is a special data impu\\x02tation scenario. As a SOTA model and an architectural template\\r\\nfor GNN-RNNs, GRIN [7] based on message-passing GRUs that pro\\x02gressively performed a two-stage forward and backward recurrent\\r\\nmessage aggregation with predefined relational biases.\\r\\nTransformers for Time Series Imputation. Transformers [38]\\r\\ncan aggregate abundant information from arbitrary input elements,\\r\\nbecoming a natural choice for sequential data imputation. In partic\\x02ular, CDSA [24] developed a cross-channel attention that utilizes cor\\x02relations in different dimensions. SAITS [10] combined the masked\\r\\nimputation task with an observed reconstruction task, and applied\\r\\na diagonally-masked self-attention to hierarchically reconstruct\\r\\nsparse data. SPIN [26] achieved SOTA imputation performance by\\r\\nconducting a sparse cross-attention and a temporal self-attention on\\r\\nall observed spatiotemporal points. However, the high complexity\\r\\nof cross-attention hinders its application in larger graphs.\\r\\n4 LOW RANKNESS-INDUCED TRANSFORMER\\r\\nThis section elaborates the ImputeFormer model. The major dif\\x02ference between our model and the canonical Transformer is the\\r\\nintegration of low-rank factorization. Unlike GNNs, ImputeFormer\\r\\ndoes not require a predefined graph due to the global adaptive\\r\\ninteraction between series. It also bypasses the use of intricate tem\\x02poral techniques, such as bidirectional recurrent aggregation, sparse\\r\\ncross-attention, and attention masking. Furthermore, it achieves\\r\\nlinear complexity with respect to spatial and temporal dimensions.\\r\\n4.1 Architectural Overview\\r\\nThe overall structure of the proposed ImputeFormer is shown in\\r\\nFig. 2. The input embedding layer projects sparse observations\\r\\nto hidden states in an additional dimension and introduces both\\r\\nfixed and learnable embedding into the inputs. Following a time\\x02and-graph template [8], TemporalInteraction and SpatialInteraction\\r\\nperform global message passing alternatively at all spatiotemporal\\r\\ncoordinates. Finally, a MLP readout is adopted to output the final\\r\\nimputation. This process can be summarized as follows:\\r\\nZ\\r\\n(0)\\r\\n𝑡:𝑡+𝑇\\r\\n=InputEmb(X𝑡:𝑡+𝑇 , U𝑡:𝑡+𝑇 , V),\\r\\nZ\\r\\n(ℓ+1)\\r\\n𝑡:𝑡+𝑇\\r\\n=TemporalInteraction(Z(ℓ)\\r\\n𝑡:𝑡+𝑇\\r\\n),\\r\\nZ\\r\\n(ℓ+1)\\r\\n𝑡:𝑡+𝑇\\r\\n=SpatialInteraction(Z(ℓ+1)\\r\\n𝑡:𝑡+𝑇\\r\\n), ∀ℓ ∈ {0, . . . 𝐿},\\r\\nXb𝑡:𝑡+𝑇 =Readout(Z(𝐿+1)\\r\\n𝑡:𝑡+𝑇\\r\\n).\\r\\n(2)\\r\\nThe canonical Transformer block [38] can be adopted to gather\\r\\nspatial-temporal information for imputation. However, we argue\\r\\nthat directly applying it to the imputation problem is questionable,\\nKDD ’24, August 25–29, 2024, Barcelona, Spain. Tong Nie et al.\\r\\nand there exist three key concerns: (1) Spurious correlations: Short\\x02term series within a window can be noisy and indistinguishable.\\r\\nModeling relational structures using sparse input can cause spuri\\x02ous and misleading correlations. (2) High-rank estimations: Time\\r\\nseries are typically low-rank in nature [16]. Full-attention compu\\x02tation on raw data can be overcorrelated and generate high-rank\\r\\nattention maps. (3) Scalability issue: All pairwise attention on large\\r\\ngraphs is memory intensive and computationally inefficient. To ad\\x02dress these issues, we start from time series primitives and enhance\\r\\nthe Transformer using these structural priors.\\r\\n4.2 Spatiotemporal Input Embedding\\r\\nInput Embedding. We adopt a dimension expansion strategy [43]\\r\\nto preserve the information density of the incomplete time series.\\r\\nIn practice, we expand an additional dimension of the input and\\r\\nproject it into a hidden state along this new dimension:\\r\\nZ\\r\\n(0)\\r\\n𝑡:𝑡+𝑇\\r\\n= MLP(Unsqueeze(X𝑡:𝑡+𝑇 , dim=-1), (3)\\r\\nwhere Unsqueeze(·) : R\\r\\n𝑁 ×𝑇 → R𝑁 ×𝑇 ×1\\r\\n, and Z\\r\\n(0)\\r\\n𝑡:𝑡+𝑇\\r\\n∈ R\\r\\n𝑁 ×𝑇 ×𝐷\\r\\nis the initial hidden representation. With this, we can aggregate mes\\x02sage from other time points by learning data-dependent weights:\\r\\nZ\\r\\n𝑖,(ℓ+1)\\r\\n𝑡:𝑡+𝑇\\r\\n= Fℓ (Z\\r\\n𝑖,(ℓ)\\r\\n𝑡:𝑡+𝑇\\r\\n)Z\\r\\n𝑖,(ℓ)\\r\\n𝑡:𝑡+𝑇\\r\\n, (4)\\r\\nwhere Fℓ (·) : R\\r\\n𝑇 ×𝐷 → R𝑇 ×𝑇\\r\\nrepresents a data-driven function at\\r\\nthe ℓ-th layer, such as self-attention. The rationale of this strategy\\r\\nis discussed in A.1.2.\\r\\nTime Stamp Encoding. Time stamp encoding is adopted to handle\\r\\nthe order-agnostic nature of Transformers [38]. As the input series\\r\\ncovers a relatively short range, we only consider the time-of-day\\r\\ninformation. We adopt the sinusoidal positional encoding in [38]\\r\\nto inject the time-of-day information of each time series:\\r\\n𝑝\\r\\n𝑡\\r\\nsine = sin(𝑝𝑡 ∗ 2𝜋/𝛿𝐷 ), 𝑝𝑡cosine = cos(𝑝𝑡 ∗ 2𝜋/𝛿𝐷 ),\\r\\nu𝑡 =\\r\\n\\x02\\r\\n𝑝\\r\\n𝑡\\r\\nsine ∥𝑝\\r\\n𝑡\\r\\ncosine\\r\\n\\x03\\r\\n,\\r\\n(5)\\r\\nwhere 𝑝𝑡is the index of 𝑡-th time-of-day point in the series, and\\r\\n𝛿𝐷 is the day-unit time mapping. We concatenate psine and pcosine\\r\\nas the final time stamp encoding U𝑡:𝑡+𝑇 ∈ R\\r\\n𝑇 ×2\\r\\n.\\r\\nNode Embedding. Previous work has demonstrated the impor\\x02tance of node identification in distinguishing different sensors for\\r\\nspatiotemporal forecasting [8, 17, 29, 33]. Here we also recommend\\r\\nthe use of learnable node embedding for imputation task. On the\\r\\none hand, it benefits the adaptation of local components [8] in\\r\\ngraph-based data structure. On the other hand, we highlight that\\r\\nnode embedding can be treated as an abstract and low-dimensional\\r\\nrepresentation of the incomplete series. To implement, we assign\\r\\neach series a randomly initialized parameter e\\r\\n𝑖 ∈ R𝐷𝑠\\r\\n. We then\\r\\nsplit the hidden dimension of the static node embedding equally\\r\\nby the length of the time window as a multi-head node embedding\\r\\nand unfold it to form a low-dimensional and time-varying repre\\x02sentation: E\\r\\n𝑖\\r\\n𝑡:𝑡+𝑇\\r\\n∈ R\\r\\n𝑇 ×𝐷𝑠 /𝑇\\r\\n. Implicit interactions between node\\r\\nembedding, input data, and modular components are involved in\\r\\nthe end-to-end gradient descent process. Finally, the spatiotemporal\\r\\ninput embedding for each node can be formulated as follows:\\r\\nZ\\r\\n𝑖,(1)\\r\\n𝑡:𝑡+𝑇\\r\\n= Concat(Z\\r\\n𝑖,(0)\\r\\n𝑡:𝑡+𝑇\\r\\n;U𝑡:𝑡+𝑇 ; E\\r\\n𝑖\\r\\n𝑡:𝑡+𝑇\\r\\n, dim=-1), (6)\\r\\nwhere Z\\r\\n𝑖,(1)\\r\\n𝑡:𝑡+𝑇\\r\\n∈ R\\r\\n𝑇 × (𝐷+𝐷𝑠 /𝑇 +2)\\r\\nis input to the following modules.\\r\\n4.3 Temporal Projected Attention\\r\\nAs is evident in Fig. 2, time series are supposed to be redundant\\r\\nin the time domain, that is, most of the information can be recon\\x02structed using only a few dominant modes. However, as the hidden\\r\\ndimension 𝐷\\r\\n′\\r\\nis practically much larger than the sequence length\\r\\n𝑇 , the attention score R\\r\\n𝑇 ×𝐷\\r\\n′\\r\\n× R\\r\\n𝐷\\r\\n′×𝑇 → R\\r\\n𝑇 ×𝑇\\r\\ncan be a high-rank\\r\\nmatrix, which is both adverse and inefficient to reconstruct incom\\x02plete hidden spaces. To address this concern, we propose a new\\r\\nprojected attention mechanism to impose a low-rank constraint\\r\\non the attentive process and efficiently model pairwise temporal\\r\\ninteractions between time points in linear complexity.\\r\\nTo utilize this structural bias, we first project the initial features\\r\\nto dense representations by attending to a low-dimensional vector.\\r\\nSpecifically, we first randomly initialize a learnable vector that is\\r\\nshared by all nodes with the gradient tractable as the projector\\r\\nPproj ∈ R\\r\\n𝐶×𝐷\\r\\n′\\r\\n, where 𝐶 < 𝑇 is the projected dimension. In order to\\r\\nrepresent the temporal message in a compact form, we then project\\r\\nthe hidden states Z\\r\\n𝑖,(ℓ) ∈ R𝑇 ×𝐷\\r\\n′\\r\\n(subscripts are omitted for brevity)\\r\\nto the projected space by attending to the query projector:\\r\\neZ\\r\\n𝑖,(ℓ)\\r\\nproj = SelfAtten(P\\r\\n(ℓ)\\r\\nproj, Z\\r\\n𝑖,(ℓ)\\r\\n, Z\\r\\n𝑖,(ℓ)\\r\\n)),\\r\\n= Softmax ©\\r\\n\\xad\\r\\n«\\r\\nP\\r\\n(ℓ)\\r\\nprojWQWT\\r\\nK\\r\\nZ\\r\\n𝑖,(ℓ),T\\r\\n√\\r\\n𝐷′\\r\\nª\\r\\n®\\r\\n¬\\r\\nZ\\r\\n𝑖,(ℓ)WV,\\r\\n(7)\\r\\nwhere eZ\\r\\n𝑖,(ℓ)\\r\\nproj ∈ R\\r\\n𝐶×𝐷\\r\\n′\\r\\nis the projected value, WQ, WK, WV ∈\\r\\nR\\r\\n𝐷\\r\\n′×𝐷′\\r\\nare linear weights. In particular, since the projector Pproj is\\r\\ndecoupled from the spatial dimension, the resulted attention map\\r\\nSoftmax(P\\r\\n(ℓ)\\r\\nprojWQWT\\r\\nK\\r\\nZ\\r\\n𝑖,(ℓ),T\\r\\n/\\r\\n√\\r\\n𝐷′) ∈ R\\r\\n𝐶×𝑇\\r\\ncan be interpreted as\\r\\nan indicator of how the incomplete information flow can be com\\x02pressed into a compact representation with smaller dimension, that\\r\\nis, an aggregation of available messages. More expositions on the\\r\\nprojector will be provided in Section 5.6.\\r\\neZ\\r\\n𝑖,(ℓ)\\r\\nproj stores the principal temporal patterns within the input\\r\\ndata. Then, we can recover the complete series with this compact\\r\\nrepresentation by dispersing the projected information to all other\\r\\nfull-length series by using the projector as a key dictionary:\\r\\nZ\\r\\n𝑖,(ℓ)\\r\\nhat = SelfAtten(Z\\r\\n𝑖,(ℓ)\\r\\n, P\\r\\n(ℓ)\\r\\nproj,eZ\\r\\n𝑖,(ℓ)\\r\\nproj )),\\r\\n= Softmax ©\\r\\n\\xad\\r\\n«\\r\\nZ\\r\\n𝑖,(ℓ)WQWT\\r\\nK\\r\\nP\\r\\n(ℓ),T\\r\\nproj\\r\\n√\\r\\n𝐷′\\r\\nª\\r\\n®\\r\\n¬\\r\\neZ\\r\\n𝑖,(ℓ)\\r\\nproj WV,\\r\\n(8)\\r\\nthen the above process is integrated into the Transformer encoder:\\r\\nbZ\\r\\n𝑖,(ℓ) = LayerNorm(Z𝑖,(ℓ) + Z\\r\\n𝑖,(ℓ)\\r\\nhat ),\\r\\nZ\\r\\n𝑖,(ℓ+1) = LayerNorm(bZ𝑖,(ℓ) + FeedForward(bZ𝑖,(ℓ)\\r\\n)),\\r\\n(9)\\r\\nwhere Z\\r\\n𝑖,(ℓ+1) ∈ R𝑇 ×𝐷\\r\\n′\\r\\nis the imputation by the ℓ-th temporal\\r\\ninteraction layer. Since the projector in Eqs. (7) and (8) can be\\r\\nobtained by end-to-end learning and is independent of the order in\\r\\nseries, it has the property of data-dependent model in Eq. (4). To\\r\\nindicate how the above process learns the low-rank representation\\r\\nof temporal attention, we develop the following remark.\\r\\nRemark (Difference between projected attention and\\r\\ncanonical self-attention). Given the query-key-value matrix\\nImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation KDD ’24, August 25–29, 2024, Barcelona, Spain.\\r\\nQ, K, V ∈ R\\r\\n𝑇 ×𝐷\\r\\n′\\r\\n, the canonical self-attention [38] in the temporal\\r\\naxis can be expressed compactly as: SelfAtten(Q, K, V) = 𝜎(QKT)V\\r\\nwith the rank 𝑟 ≤ min{𝑇 , 𝐷′}. For comparison, the two-step attentive\\r\\nprocess in Eqs. (7) and (8) are equivalent to the expanded form:\\r\\nbZ = SelfAtten(Q, P, SelfAtten(P, K, V)),\\r\\n= 𝜎(QPT)SelfAtten(P, K, V),\\r\\n= 𝜎(QPT)𝜎(PKT)V ≈\\r\\n1\\r\\n𝑁2\\r\\nQ(P\\r\\nT\\r\\nP)K\\r\\nTV.\\r\\nRecall that the projector P ∈ R\\r\\n𝐶×𝐷\\r\\n′\\r\\ncan have a small projection\\r\\ndimension 𝐶, it can be viewed as a channel-wise matrix factorization\\r\\nto reduce redundancy within each time series. The rank of the projected\\r\\nattention matrix is 𝑟 ≤ min{𝐶, 𝐷′}, which is theoretically lower than\\r\\nthe original rank. The projected attention guarantees expressivity by\\r\\nmaintaining a large hidden dimension 𝐷\\r\\n′\\r\\n, while at the same time\\r\\nadmitting a low-rank solution using a small projection dimension 𝐶.\\r\\nThe rank-reduced temporal attention matrix exploits the low-rankness\\r\\nof data in the temporal dimension, which is different from the low\\x02rank adaptation of model parameters developed recently [34].\\r\\nThe “projection-reconstruction\" process in Eqs. (7) and (8) re\\x02semble the low-rank factorization process X = UVT\\r\\n. Inflow in Eq.\\r\\n(7) controls the amount of message used to form a dense represen\\x02tation in a lower-dimensional space. Outflow in Eq. (8) determines\\r\\nhow hidden states can be reconstructed using only a few projected\\r\\ncoordinates. This mechanism also brings about efficiency benefits.\\r\\nThe canonical self-attention costs O (𝑇\\r\\n2\\r\\n) time complexity. The com\\x02plexity of the projected attention is O (𝑇𝐶), which scales linearly\\r\\n(see Section A.1.3) and is efficient for longer sequences. In addition,\\r\\nlow-rank attention preserves the dominating correlational struc\\x02tures and eliminates spurious correlations. The cleaned correlations\\r\\nallow the model to focus on the most relevant data as a reference.\\r\\n4.4 Spatial Embedded Attention\\r\\nThe availability of observed temporal information is not sufficient\\r\\nfor fine-grained imputation. In some cases, specific spatial events,\\r\\nsuch as traffic congestion, can lead to non-local patterns and un\\x02usual records. Therefore, it is reasonable to exploit the multivariate\\r\\nrelationships between series as a complement. A straightforward\\r\\nway to address this problem is to apply a Transformer block in spa\\x02tial dimension [17, 44]. Nevertheless, the three concerns discussed\\r\\nin Section 4.1 prevent the direct use of this technique.\\r\\nConsequently, we design an embedded attention as an alternative\\r\\nto spatial attention. We highlight that the node embedding in Eq. (6)\\r\\nsignifies not only the identity of series, but also a dense abstract of\\r\\neach individual. We then establish a correlation map using this low\\x02dimensional agent. Formally, we assume that the message passing\\r\\nhappens on a fully connected dense graph, and the edge weights\\r\\nare estimated by the pairwise correlating of node embedding:\\r\\nQ\\r\\n(ℓ)\\r\\n𝑒 = Linear(E), K\\r\\n(ℓ)\\r\\n𝑒 = Linear(E),\\r\\nA\\r\\n(ℓ) = Softmax \\r\\nQ\\r\\n(ℓ)\\r\\n𝑒 K\\r\\n(ℓ)T\\r\\n𝑒 √\\r\\n𝐷′\\r\\n!\\r\\n,\\r\\n(10)\\r\\nwhere A\\r\\n(ℓ) ∈ R𝑁 ×𝑁 denotes the pairwise correlation score of all\\r\\nsensors, and Q\\r\\n(ℓ)\\r\\n𝑒\\r\\n, K\\r\\n(ℓ)\\r\\n𝑒 ∈ R\\r\\n𝑁 ×𝐷emb are linearly projected from the\\r\\nspatiotemporal embedding set E = [e¯\\r\\n1\\r\\n∥e¯\\r\\n2\\r\\n∥ · · · ∥e¯\\r\\n𝑁 ] ∈ R𝑁 ×𝐷𝑠 /𝑇\\r\\n,\\r\\nwith e¯\\r\\n𝑖 being the static node embedding averaging over the tempo\\x02ral heads {𝑡 : 𝑡 + 𝑇 } from Eq. (6).\\r\\nGiven the graph representation over a period Z ∈ R\\r\\n𝑁 ×𝑇 ×𝐷\\r\\n′\\r\\n,\\r\\nthe complexity of obtaining a full spatial attention matrix costs\\r\\nO (𝑁\\r\\n2𝑇𝐷′\\r\\n). To alleviate scalability concerns on large graphs, we\\r\\nadopt the normalization trick in [35] to reparameterize Eq. (10).\\r\\nObserve that the main bottleneck in Eq. (10) happens in the mul\\x02tiplication of two large matrix Q\\r\\n(ℓ)\\r\\n𝑒 and K\\r\\n(ℓ)\\r\\n𝑒\\r\\n, we can reduce the\\r\\ncomplexity using the associative property of matrix multiplication\\r\\nif we can decouple the softmax function. To this end, we apply the\\r\\nsoftmax on separate side of the Q-K matrix and approximate A as:\\r\\nA\\r\\n(ℓ) ≈ 𝜎2 (Qe(ℓ)\\r\\n𝑒\\r\\n)𝜎1 (eK\\r\\n(ℓ)\\r\\n𝑒\\r\\n)\\r\\nT\\r\\n, (11)\\r\\nwhere 𝜎(·) is the abbreviation for softmax, and the subscript de\\x02notes the dimension that we perform normalization. The scaled\\r\\nQ-K functions Qe(ℓ)\\r\\n𝑒 = Q\\r\\n(ℓ)\\r\\n𝑒 /∥Q\\r\\n(ℓ)\\r\\n𝑒\\r\\n∥𝐹 , eK\\r\\n(ℓ)\\r\\n𝑒 = K\\r\\n(ℓ)\\r\\n𝑒 /∥K\\r\\n(ℓ)\\r\\n𝑒\\r\\n∥𝐹 are\\r\\nused to ensure numerical stability. On top of Eq. (11), the complete\\r\\nembedded attention can be reformulated as:\\r\\neZ\\r\\n(ℓ)\\r\\n𝑡\\r\\n= LayerNorm(Z\\r\\n(ℓ)\\r\\n𝑡\\r\\n+ 𝜎2 (Qe(ℓ)\\r\\n𝑒\\r\\n)𝜎1 (eK\\r\\n(ℓ)\\r\\n𝑒\\r\\n)\\r\\nTZ\\r\\n(ℓ)\\r\\n𝑡\\r\\n),\\r\\n= LayerNorm(Z\\r\\n(ℓ)\\r\\n𝑡\\r\\n+ 𝜎2 (Qe(ℓ)\\r\\n𝑒\\r\\n)\\r\\n\\x10\\r\\n𝜎1 (eK\\r\\n(ℓ)\\r\\n𝑒\\r\\n)\\r\\nTZ\\r\\n(ℓ)\\r\\n𝑡\\r\\n\\x11\\r\\n),\\r\\nZ\\r\\n(ℓ+1)\\r\\n𝑡\\r\\n= LayerNorm(eZ\\r\\n(ℓ)\\r\\n𝑡\\r\\n+ FeedForward(eZ\\r\\n(ℓ)\\r\\n𝑡\\r\\n)).\\r\\n(12)\\r\\nBy computing the multiplication of 𝜎1 (eK\\r\\n(ℓ)\\r\\n𝑒\\r\\n)\\r\\nT\\r\\nand Z\\r\\n(ℓ)\\r\\n𝑡\\r\\nat first,\\r\\nthe above process admits a O (𝑁𝐷emb) time complexity, which\\r\\nscales linearly with respect to the number of sensors. Since E is\\r\\ndecoupled from temporal information, it is robust to missing values\\r\\nand reliable to infer a correlation map for global imputation. The\\r\\nfull attention has the size R\\r\\n𝑁 × (𝑇 ×𝐷\\r\\n′\\r\\n) × R(𝑇 ×𝐷\\r\\n′\\r\\n)×𝑁 → R𝑁 ×𝑁 ,\\r\\nwhile the embedded attention has R\\r\\n𝑁 ×𝐷emb × R𝐷emb×𝑁 → R𝑁 ×𝑁 .\\r\\nIn this sense, the attention map in Eq. (10) act as a factorized low\\x02rank approximation of full attention. We highlight this property by\\r\\ncomparing the two formulations in the following analysis.\\r\\nRemark (Difference between embedded attention and canon\\x02ical self-attention). Given a hidden state Z ∈ R\\r\\n𝑁 ×𝑇 ×𝐷\\r\\n′\\r\\n, the\\r\\nself-attention can be computed on the folded matrix Z ∈ R\\r\\n𝑁 × (𝑇 𝐷′)\\r\\nas SelfAtten(Z, Z, Z) = 𝜎(ZW𝑄WT\\r\\n𝐾\\r\\nZ\\r\\nT\\r\\n)ZW𝑉 . The resulting rank\\r\\nof the attention matrix obeys 𝑟 ≤ min{𝑁 ,𝑇𝐷′}. While the embed\\x02ded attention has SelfAtten(E, E, Z) = 𝜎(EW𝐸)𝜎(EW𝐸)\\r\\nTZW𝑉 . If\\r\\nwe ignore the possible rank-increasing effect of softmax, the above\\r\\ncalculation generates the output with rank 𝑟 ≤ min{𝑁 , 𝐷emb}. Since\\r\\nthe dimension of the node embedding 𝐷emb is much smaller than the\\r\\nmodel dimension 𝐷\\r\\n′\\r\\n, the rank of the embedded attention map has a\\r\\nlower bound of rank than the full attention. In addition, the model\\r\\nstill has a large feedforward dimension 𝐷\\r\\n′\\r\\nto ensure capacity.\\r\\n4.5 Fourier Imputation Loss\\r\\nAs imputation model is typically optimized in a self-supervised\\r\\nmanner, previous studies proposed adopting accumulated and hi\\x02erarchical loss [7, 10, 26] to improve the supervision of layerwise\\r\\nimputation. However, we argue that such designs are not necessary\\r\\nand may generate overfitting. Instead, we propose a novel Fourier\\r\\nimputation loss (FIL) combined with a simple supervision loss as\\r\\ntask-specific biases to achieve effective training and generalization.\\nKDD ’24, August 25–29, 2024, Barcelona, Spain. Tong Nie et al.\\r\\nSelf-Supervised Masked Learning. To create supervised sam\\x02ples, we randomly whiten a proportion of incomplete observations\\r\\n(𝑝whiten) during model training. This operation ensures the gen\\x02eralizability of the imputation model (see Section 5.5). We use a\\r\\nmasking indicator Mwhiten to denote these locations where the\\r\\nmasked values are marked as ones and others as zeros. Note that\\r\\nthe supervision loss is only calculated on these manually whitened\\r\\npoints, and models are forbidden to have access to the masked\\r\\nmissing points used for evaluation. Therefore, the reconstruction\\r\\nloss of our model is a ℓ1 loss on the final imputation Xb:\\r\\nLrecon =\\r\\n1\\r\\n𝑁𝑇\\r\\n∑︁\\r\\n∥Mwhiten ⊙ (Xb − Y) ∥1, (13)\\r\\nwhere Y is the label used for training.\\r\\nFourier Sparsity Regularization. As discussed above, we can\\r\\nobtain a reasonable imputation by constraining the rank of the esti\\x02mated spatiotemporal tensor in the time domain. However, directly\\r\\noptimizing the rank of the tensor or matrix is challenging [18], as it\\r\\nincludes some non-trivial or non-differentiable computations, such\\r\\nas truncated singular value decomposition (SVD). And the SVD of a\\r\\n𝑁 ×𝑇 matrix costs O (min{𝑁\\r\\n2𝑇 , 𝑁𝑇 2\\r\\n}) complexity, which can be\\x02come a bottleneck when integrated with deep models. Fortunately,\\r\\nwe can simplify this process using the following lemma.\\r\\nLemma (Eqivalence between convolution nuclear norm\\r\\nand Fourier ℓ1 norm [3, 16]). Given a smooth or periodic time series\\r\\nx ∈ R\\r\\n𝑇\\r\\n, its circulant (convolution) matrix C (x) ∈ R\\r\\n𝑇 ×𝑇\\r\\nreflects the\\r\\nTucker low-rankness, depicted by the convolutional nuclear norm.\\r\\nThis property can be revealed by using the Discrete Fourier Transform\\r\\n(DFT). Let the DFT matrix be U ∈ C\\r\\n𝑇 ×𝑇\\r\\n, then the DFT is achieved by:\\r\\nDFT(x) =Ux = U(C (x) [:, 0]) = (UC (x)) [:, 0].\\r\\nAs DFT diagonalizes the circulant matrix by C (x) = U\\r\\nHdiag(𝜎1, . . . , 𝜎𝑇 )U\\r\\nand U is a unitary matrix with the first column being ones, we have:\\r\\n(UC (x)) [:, 0] = (diag(𝜎1, 𝜎2, . . . , 𝜎𝑇 )U) [:, 0],\\r\\n= [𝜎1, 𝜎2, . . . , 𝜎𝑇 ]\\r\\nT\\r\\n.\\r\\nTherefore, we have ∥DFT(x) ∥0 = ∥ [𝜎1, 𝜎2, . . . , 𝜎𝑇 ]\\r\\nT\\r\\n∥0 = rank(C (x)),\\r\\nand ∥DFT(x) ∥1 = ∥C (x) ∥∗.\\r\\nThis lemma means that we can efficiently obtain the singular\\r\\nvalues through DFT. The ℓ0 norm is exactly the matrix rank and\\r\\nthe ℓ1 norm is equal to the nuclear norm ∥C (x) ∥∗. Since ∥C (x) ∥∗\\r\\nserves as a convex surrogate of the rank of x in an approximate\\r\\nsense [16], we can equivalently achieve this goal by optimizing the\\r\\nFourierℓ1 norm. Considering the above equivalence, we can develop\\r\\na sparsity-constrained loss function in the frequency domain:\\r\\nX¯ = Mmissing ⊙ Xb + (1 − Mmissing) ⊙ Y,\\r\\nLFIL =\\r\\n1\\r\\n𝑁𝑇\\r\\n∑︁\\r\\n∥Flatten(FFT(X¯, dim = [0, 1])) ∥1,\\r\\n(14)\\r\\nwhere FFT(·) is the Fast Fourier Transform (FFT), Flatten(·) :\\r\\nR\\r\\n𝑁 ×𝑇 → R𝑁𝑇 rearranges the tensor form and ∥ · ∥1 is the vector ℓ1\\r\\nnorm. Since the spatiotemporal matrix can be regarded as a special\\r\\nRGB image from a global viewpoint, it also features a sparse Fourier\\r\\nspectrum in the space dimension [16]. We apply the FFT on both\\r\\nthe space and time axes and then flatten it into a long vector. LFIL\\r\\nis in fact a unsupervised loss that encourages the imputed values\\r\\nto be naturally compatible with the observed values globally.\\r\\nFinally, the total loss function is formulated as:\\r\\nL = Lrecon + 𝜆LFIL, (15)\\r\\nwhere 𝜆 is a weight hyperparameter. It is worth commenting that\\r\\nthe two loss functions complement each other: Lrecon prompts\\r\\nthe model to reconstruct the masked observations as precisely as\\r\\npossible in the space-time domain and LFIL generalizes on unob\\x02served points with regularization on the spectrum. This makes\\r\\nImputeFormer work effectively in highly sparse observations.\\r\\n5 EMPIRICAL EVALUATIONS\\r\\nIn this section, we evaluate our model on several well-known spa\\x02tiotemporal benchmarks, comparing it with state-of-the-art base\\x02lines, and testing its generality on different scenarios. Then compre\\x02hensive analysis and case studies are provided. A brief summary of\\r\\nthe adopted datasets is shown in Tab. 1. Detailed descriptions of ex\\x02perimental settings are provided in Section A.2. PyTorch implemen\\x02tations are available at https://github.com/tongnie/ImputeFormer.\\r\\nTable 1: Statistics of benchmark datasets.\\r\\nDatasets Type Steps Nodes Interval\\r\\nMETR-LA Traffic speed 34,272 207 5 min\\r\\nPEMS-BAY Traffic speed 52,128 325 5 min\\r\\nPEMS03 Traffic volume 26,208 358 5 min\\r\\nPEMS04 Traffic volume 16,992 307 5 min\\r\\nPEMS07 Traffic volume 28,224 883 5 min\\r\\nPEMS08 Traffic volume 17,856 170 5 min\\r\\nSOLAR Power production 52,560 137 10 min\\r\\nCER-EN Energy consumption 8,868 435 30 min\\r\\nAQI Air pollutant 8,760 437 60 min\\r\\nAQI36 Air pollutant 8,760 36 60 min\\r\\n5.1 Results on Traffic Benchmarks\\r\\nThe imputation results on traffic speed and volume data are given\\r\\nin Tab. 2. As can be seen, ImputeFormer consistently achieves the\\r\\nbest performance in all traffic benchmarks. Two strong competitors\\r\\nGRIN and SPIN show promising results in traffic speed data, which\\r\\nalign with the results of their respective papers [7, 26]. However,\\r\\ntheir performance is inferior on volume datasets and is surpassed\\r\\nby simple baselines such as ST-Transformer and Bi-MPGRU. Com\\x02pared to deep models, pure low-rank methods, such as matrix fac\\x02torization and tensor completion, are less effective due to limited\\r\\ncapacity. As for missing patterns, the structured block missing is\\r\\nmore challenging than the point missing pattern. For instance, the\\r\\nvanilla Transformer is competitive in the point missing case, while\\r\\nit is ineffective in block missing case. Generally, ImputeFormer\\r\\noutperforms others by a large margin in this tricky scenario.\\r\\n5.2 Results on Environmental and Energy Data\\r\\nBy exploiting the underlying low-rank structures, ImputeFormer\\r\\ncan serve as a general imputer in a variety of spatiotemporal data.\\r\\nTo demonstrate its versatility, we perform experiments on other\\r\\nspatiotemporal data, including energy and environmental data. Re\\x02sults are given in Tab. 3. It is observed that ImputeFormer exhibits\\r\\nsuperiority in other spatiotemporal datasets beyond traffic data.\\r\\nIn particular, the correlation of solar stations cannot be described\\nImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation KDD ’24, August 25–29, 2024, Barcelona, Spain.\\r\\nTable 2: Results (in terms of MAE) on METR-LA, PEMS-BAY, PEMS03, PEMS04, PEMS07 and PEMS08 traffic benchmarks.\\r\\nPoint missing Block missing\\r\\nModels PEMS-BAY METR-LA PEMS03 PEMS04 PEMS07 PEMS08 PEMS-BAY METR-LA PEMS03 PEMS04 PEMS07 PEMS08\\r\\nAverage 5.45 7.52 85.30 103.61 122.35 89.51 5.48 7.43 85.56 103.82 123.05 89.42\\r\\nMICE [37] 2.82 2.89 20.07 28.60 37.11 30.26 2.36 2.73 21.90 32.45 37.20 26.66\\r\\nTRMF [46] 2.10 3.51 18.80 24.34 29.06 20.27 2.09 3.36 18.71 24.47 29.42 19.80\\r\\nLRTC-AR [4] 0.94 2.14 15.52 22.11 27.60 19.33 4.05 5.35 17.59 24.08 27.82 19.95\\r\\nBi-MPGRU 0.72 2.00 11.23 15.84 15.66 11.90 1.41 2.33 13.87 19.81 21.12 15.89\\r\\nrGAIN [45] 1.90 2.81 13.32 22.86 24.41 16.33 2.21 2.95 14.85 23.26 26.69 27.12\\r\\nBRITS [1] 1.84 2.42 12.74 20.00 23.97 15.78 1.91 2.40 12.93 19.80 23.26 16.37\\r\\nSAITS [10] 1.33 2.25 12.40 20.23 22.81 15.12 1.58 2.32 12.43 20.35 22.82 16.80\\r\\nTransformer [38] 0.76 2.18 12.04 16.76 16.86 12.58 1.69 3.58 24.07 29.63 33.14 25.61\\r\\nST-Transformer 0.75 2.19 11.44 16.22 15.84 12.10 1.71 3.58 23.55 29.17 32.14 24.67\\r\\nTIDER [20] 1.43 2.68 15.02 22.17 21.38 18.46 2.46 4.95 21.12 23.74 28.66 21.00\\r\\nTimesNet [41] 1.47 2.93 14.99 20.40 22.00 16.53 2.73 4.79 44.85 51.05 60.90 45.78\\r\\nGRIN [7] 0.68 1.91 10.31 16.25 11.90 12.33 1.20 2.08 12.28 23.23 16.04 19.69\\r\\nSPIN [26] 0.79 1.93 12.85 18.96 17.61 15.02 1.13 2.02 14.68 19.85 16.99 16.81\\r\\nImputeFormer 0.64 1.80 8.23 14.92 11.38 11.01 0.95 1.86 9.02 16.83 13.82 12.50\\r\\n5.9% ↓ 5.8% ↓ 20.2% ↓ 5.8% ↓ 4.4% ↓ 7.5% ↓ 15.9% ↓ 7.9% ↓ 26.5% ↓ 15.0% ↓ 13.8% ↓ 21.3% ↓\\r\\nby physical distance and can be inferred from the data. After com\\x02paring the performance of SAITS, Transformer, ST-Transformer,\\r\\nand ImputeFormer, it can be concluded that direct attention compu\\x02tations on both temporal and spatial dimensions are less beneficial\\r\\nthan the low-rank attention. Furthermore, the spatial correlation of\\r\\nenergy production is less pronounced. Canonical attention on the\\r\\nspatial axis can be redundant and generate spurious correlations.\\r\\nThe use of embedded attention in our model can alleviate this issue.\\r\\nTable 3: Results (in terms of MAE) on AQI, Solar, and CER-EN\\r\\nbenchmarks. For Solar data, we compare the performances\\r\\nof baselines that are independent of the predefined graphs.\\r\\nSOLAR CER-EN Simulated faults\\r\\nModels Point\\r\\nmissing\\r\\nBlock\\r\\nmissing\\r\\nPoint\\r\\nmissing\\r\\nBlock\\r\\nmissing AQI36 AQI\\r\\nAverage 7.60 7.56 0.583 0.596 61.81 43.78\\r\\nMICE 1.59 1.58 0.535 0.555 38.90 29.12\\r\\nTRMF 2.44 2.35 0.557 0.559 41.91 27.67\\r\\nBi-MPGRU N.A. N.A. 0.247 0.349 12.02 15.41\\r\\nrGAIN 1.52 1.64 0.418 0.440 15.69 22.13\\r\\nBRITS 1.28 1.34 0.351 0.366 14.74 20.72\\r\\nSAITS 0.98 1.25 0.341 0.368 19.79 21.09\\r\\nTransformer 2.19 3.58 0.254 0.353 14.99 17.04\\r\\nST-Transformer 2.17 3.57 0.251 0.351 13.27 18.55\\r\\nTIDER 2.84 3.87 0.336 0.377 32.85 18.11\\r\\nTimesNet 2.93 4.73 0.328 0.460 32.30 28.99\\r\\nGRIN N.A. N.A. 0.235 0.341 12.08 14.51\\r\\nSPIN N.A. N.A. OOM OOM 11.89 14.31\\r\\nImputeFormer 0.51 0.89 0.236 0.296 11.58 13.40\\r\\n48.0% ↓ 28.8% ↓ 0.4% ↑ 13.2% ↓ 2.6% ↓ 6.4% ↓\\r\\n5.3 Ablation Study\\r\\nTo justify the rationale of model designs, we conduct ablation stud\\x02ies on the model structure. Results are shown in Tab. 4. Several\\r\\nintriguing findings can be observed: (1) After removing any of the\\r\\ntemporal and spatial attention modules, the performance degen\\x02erates substantially; especially, the spatial interaction contributes\\r\\nto the inference of block missing patterns significantly, while the\\r\\ntemporal modules are crucial for point missing scenarios. (2) The in\\x02corporation of MLP benefits little for the imputation, which validates\\r\\nour argument in Section A.1.2. (3) Compared to hierarchical loss\\r\\non supervised points, FIL generalizes on the unobserved points and\\r\\neffectively reduces the estimation errors.\\r\\nTable 4: Ablations studies on ImputeFormer.\\r\\nVariation Component PEMS08 METR-LA\\r\\nSpatial Temporal Point Block Point Block\\r\\nImputeFormer Attention Attention 11.24 12.86 1.80 1.88\\r\\nReplace\\r\\nAttention MLP 16.95 17.11 2.39 2.28\\r\\nMLP Attention 12.84 17.42 2.20 2.92\\r\\nMLP MLP 34.72 34.41 5.80 5.79\\r\\nw/o Attention w/o 17.06 17.13 2.39 2.28\\r\\nw/o Attention 12.87 17.44 2.21 2.93\\r\\nLoss function w/o FIL 11.63 13.35 1.85 1.93\\r\\nHierarchical loss 11.35 13.07 1.84 1.92\\r\\nArchitecture Order T-S 11.26 13.16 1.80 1.87\\r\\nS-T 11.30 13.13 1.80 1.88\\r\\nJoint ST 17.50 20.00 1.94 2.58\\r\\nFigure 3: Comparison of computational efficiency.\\r\\n5.4 Model Efficiency\\r\\nWe evaluate the computational efficiency of different architectures\\r\\nin Fig. 3. Intuitively, ImputeFormer exhibits high training efficiency.\\r\\nDue to the low-rank design philosophy, ImputeFormer is approxi\\x02mately 15 times faster than the state-of-the-art Transformer base\\x02line (SPIN). It is also cost-effective in GPU memory consumption.\\r\\n5.5 Robustness and Versatility Analysis\\r\\nInference under Different Missing Rates. Deep imputation mod\\x02els are subject to the distribution shift problem between training\\nKDD ’24, August 25–29, 2024, Barcelona, Spain. Tong Nie et al.\\r\\nand testing datasets. A desirable characteristic is that a model can\\r\\ndeal with different missing patterns during inference. Therefore, we\\r\\nconsider a challenging scenario in which a model is trained with a\\r\\nfixed missing rate but evaluated on different scenarios with varying\\r\\nmissing rates. This constructs a zero-shot transfer evaluation. It is\\r\\nnoteworthy in Tab. 5 that both ImputeFormer and SPIN are more\\r\\nrobust than other baselines in these scenarios. RNNs and vanilla\\r\\nTransformers can overfit the training data with a fixed data missing\\r\\npattern, thereby showing inferior generalization ability.\\r\\nTable 5: Inference under varying missing rate with a single\\r\\ntrained model (Zero-shot).\\r\\nPEMS08 METR-LA\\r\\nModels Missing rate Missing rate\\r\\n50% 75% 95% 50% 75% 95%\\r\\nBRITS 17.21 22.01 52.78 2.61 3.04 5.11\\r\\nSAITS 16.03 31.32 83.79 2.44 3.37 6.80\\r\\nST-Transformer 11.65 13.11 39.95 2.32 2.72 5.16\\r\\nGRIN 13.25 16.06 42.61 2.06 2.39 4.07\\r\\nSPIN 15.13 15.51 18.30 2.11 2.34 3.03\\r\\nImputeFormer 11.52 12.18 17.35 1.96 2.17 2.79\\r\\nInference with Varying Sequence Length. In reality, the imputa\\x02tion model can face time series with different lengths and sampling\\r\\nfrequencies. We can adopt a well-trained model to perform infer\\x02ence on varying sequence length. Results are shown in Fig. 4. It\\r\\nis obvious that ImputeFormer can readily generalize to sequences\\r\\nwith different lengths and more robust than other models.\\r\\nInference Error\\r\\nInput Length\\r\\n(a) PEMS08\\r\\nInference Error\\r\\nInput Length\\r\\n(b) METR-LA\\r\\nFigure 4: Inference under different lengths of input sequence\\r\\nwith a single trained model (zero-shot).\\r\\nDealing with Highly Sparse Observation. To evaluate the per\\x02formance on highly sparse data, we further train and test models\\r\\nwith lower observation rates. Results are shown in Tab. 6. Generally\\r\\nspeaking, both Transformer- and RNN-based models are suscepti\\x02ble to sparse training data. Due to the low-rank constraints on the\\r\\nattention matrix and loss function, our model is more robust with\\r\\nhighly sparse data. Since the attention map in SPIN is calculated\\r\\nonly at the observed points, it is more stable than other baselines.\\r\\nBut our model consistently achieves lower imputation errors.\\r\\nRandom Masking in Training. Random masking strategy is\\r\\nused to create supervised samples for model training. Therefore,\\r\\nthe distribution of masking samples of training data and missing\\r\\nobservations of testing data should be close to ensure good per\\x02formance [31]. However, it can be difficult to know exactly the\\r\\nmissing patterns or missing rates in advance in many scenarios.\\r\\nTherefore, a proper masking strategy is of vital importance. To\\r\\nTable 6: Results on PEMS08 data with sparse observations\\r\\n(Training from scratch).\\r\\nMissing rate\\r\\nModels 60% 70% 80% 90%\\r\\nBRITS 18.60 19.75 21.44 24.17\\r\\nSAITS 17.53 18.24 19.39 21.27\\r\\nST-Transformer 13.67 14.32 15.86 23.98\\r\\nGRIN 14.04 15.01 17.26 25.47\\r\\nSPIN 13.64 14.30 15.19 17.13\\r\\nImputeFormer 12.57 13.17 13.98 15.94\\r\\nTable 7: Results on PEMS03 with various masking strategies.\\r\\nPoint missing Masking Probability\\r\\nModels 25% 50% 75% Combined\\r\\nBi-MPGRU 11.30 11.52 12.20 11.48\\r\\nBRITS 13.06 13.86 16.06 13.70\\r\\nSAITS 12.42 16.13 21.63 12.61\\r\\nST-Transformer 11.19 11.43 12.22 11.39\\r\\nGRIN 9.55 9.74 10.39 9.72\\r\\nSPIN 11.08 11.21 13.85 12.04\\r\\nImputeFormer 7.66 8.16 11.44 8.45\\r\\nBlock missing Masking Probability\\r\\nModels 25% 50% 75% Combined\\r\\nBi-MPGRU 13.32 13.36 13.96 13.33\\r\\nBRITS 12.26 13.01 15.58 12.63\\r\\nSAITS 12.35 15.73 20.14 12.32\\r\\nST-Transformer 23.51 23.76 23.90 23.26\\r\\nGRIN 11.94 12.05 12.68 11.99\\r\\nSPIN 13.10 13.68 13.84 13.97\\r\\nImputeFormer 8.89 9.23 16.96 8.80\\r\\nevaluate the impact of the masking rate in training data, we further\\r\\nconsider four different masking strategies during model training:\\r\\nthe masking rates are set to 0.25, 0.5, 0.75, and a combination of them\\r\\n[0.25, 0.5, 0.75] respectively. As shown in Tab. 7, most models per\\x02form best when the masking rate is close to the missing rate in the\\r\\npoint missing scenario (e.g., 25%). However, when the missing rate\\r\\nis unclear due to randomness within the failure generation process,\\r\\nsuch as the block missing, the combination strategy is more advan\\x02tageous. For example, Transformers including ST-Transformer,\\r\\nSAITS, and ImputeFormer can benefit from this strategy. More im\\x02portantly, such a hybrid masking method enables the model to work\\r\\nsuccessfully on varying observation rates during inference.\\r\\n0 0.005 0.01 0.05 0.1 0.2\\r\\nlambda\\r\\n11.25\\r\\n11.50\\r\\n11.75\\r\\n12.00\\r\\n12.25\\r\\n12.50\\r\\n12.75\\r\\n13.00\\r\\n13.25\\r\\nImputation Error\\r\\nPEMS08-Point\\r\\nImputeFormer\\r\\n(a) PEMS08, point missing\\r\\n0 0.005 0.01 0.05 0.1 0.2\\r\\nlambda\\r\\n13\\r\\n14\\r\\n15\\r\\n16\\r\\n17\\r\\n18\\r\\n19\\r\\nImputation Error\\r\\nPEMS08-Block\\r\\nImputeFormer\\r\\n(b) PEMS08, block missing\\r\\nFigure 5: Impact of 𝜆 in FIL.\\r\\nImpact of Fourier Imputation Loss. We study the impact of\\r\\nhyperparameter 𝜆 in Eq. (15). Fig. 5 displays the imputation error\\r\\nunder different 𝜆 values. The imputation model can benefit from the\\r\\nFIL design, and a too large penalty on the sparsity of the spectrum\\r\\ncan lead to a smooth and inaccurate reconstruction.\\nImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation KDD ’24, August 25–29, 2024, Barcelona, Spain.\\r\\n5.6 Case Studies: Interpretability\\r\\nThis section studies the interpretability using data examples.\\r\\nSpectrum Analysis. To corroborate the hypothesis that our model\\r\\nhas the merits of both deep learning and low-rank methods, we\\r\\nanalyze the singular value (SV) spectrum of the imputations. Fig.\\r\\n6 shows the cumulative SV distribution of different competing\\r\\nmodels. ImputeFormer has a close SV cumulative distribution to\\r\\ncomplete data, and the first 85 SVs can account for 80% of the energy.\\r\\nThere exist two additional interesting observations: (1) deep learn\\x02ing models without explicit low-rank modeling such as canonical\\r\\nTransformers downplay the role of the first few dominant SVs; (2)\\r\\npure low-rank models such as MF generate an oversmoothing result\\r\\nthat too much energy is constrained to the first part of spectrum.\\r\\nThus, we can ascribe the desirable performance of our model to the\\r\\ngood balance of significant signals and high-frequency noise.\\r\\n0 50 100 150 200 250 300 350\\r\\nSingular Value Index\\r\\n0.4\\r\\n0.5\\r\\n0.6\\r\\n0.7\\r\\n0.8\\r\\n0.9\\r\\n1.0\\r\\nNormalized Cumulative Singular Value\\r\\nCDF of Singular Value\\r\\nMatrixFactorization\\r\\nTransformer\\r\\nImputeFormer\\r\\nGroundTruth\\r\\nFigure 6: Cumulative distribu\\x02tion of singular values.\\r\\n0 25 50 75 100 125 150 175\\r\\n# of Singular Value\\r\\n0\\r\\n1000\\r\\n2000\\r\\n3000\\r\\n4000\\r\\n5000\\r\\n6000\\r\\nSingular Value\\r\\n0\\r\\n5\\r\\n10\\r\\n15\\r\\n20\\r\\n25\\r\\n30\\r\\nSingular Value\\r\\nComplete Data\\r\\nIncomplete Data\\r\\nNode Embedding\\r\\nFigure 7: Singular spectrum of\\r\\ndata and node embedding.\\r\\nInterpretations on Spatial Embedding. To illustrate the role of\\r\\nnode embedding, we analyze the SV spectrum of the PEMS08 data\\r\\nin Fig. 7. The complete data show a prominent low-rank property,\\r\\nbut the SVs of incomplete data dramatically expand. In contrast,\\r\\nthe node embedding also displays a similar low-rank distribution,\\r\\nwhich can act as a dense surrogate for each sensor. Furthermore,\\r\\nwe analyze the multivariate attention map obtained by correlating\\r\\nthe node embedding in Fig. 8. It is evident that as the embedded at\\x02tention layers become deeper, the learned attention maps approach\\r\\nthe actual ones. However, incomplete data produce noisy correlations\\r\\nwith little informative pattern. Fig. 9 displays the t-SNE visualization\\r\\nof each node embedding with two projected coordinates in PEMS08.\\r\\nThe embeddings tend to form clusters, and different clusters are\\r\\napart from others. This phenomenon is in accordance with highway\\r\\ntraffic sensor systems that proximal sensors share similar readings.\\r\\nAttention Map at 1-st EA Layer\\r\\n# of Sensor\\r\\n# of Sensor\\r\\n# of Sensor\\r\\nAttention Map at 3-rd EA Layer\\r\\n# of Sensor\\r\\nAttention Map of Complete Data\\r\\n# of Sensor\\r\\nAttention Map of Sparse Data Figure 8: Multivariate attention maps of PEMS08 data.\\r\\nInterpretations on Temporal Projector. To illustrate the mecha\\x02nism of the temporal projected attention in Eqs. (7) and (8), inflow\\r\\nand outflow attention maps are shown in Figs. 11. It can be seen\\r\\nthat these matrices quantify how the information of incomplete\\r\\nstates flows into compact representations and then is recovered to\\r\\n75 50 25 0 25 50\\r\\nCoordinate 1\\r\\n75\\r\\n50\\r\\n25\\r\\n0\\r\\n25\\r\\n50\\r\\n75\\r\\nCoordinate 2\\r\\nt-SNE Visualization of Node Embedding\\r\\nFigure 9: The t-SNE visualiza\\x02tion of node embedding.\\r\\n0 5 10 15 20\\r\\n# of Singular Value\\r\\n0\\r\\n10000\\r\\n20000\\r\\n30000\\r\\n40000\\r\\nSingular Value\\r\\nComplete Hidden States\\r\\nIncomplete Hidden States\\r\\nProjected Hidden States\\r\\nFigure 10: Singular values of\\r\\ndifferent hidden states in the\\r\\ntemporal attention layer.\\r\\ncomplete states. Inflows show that only a fraction of the message is\\r\\ndirected towards the projector, while different attention heads can\\r\\nprovide varying levels of information density. Meanwhile, outflows\\r\\nindicate that a small number of temporal modes can reconstruct\\r\\nuseful neural representations for imputation. This can be analogous\\r\\nto the low-rank reconstruction process, which serves as an inductive\\r\\nbias for time series with low information density. We further examine\\r\\nthe SV distribution of different hidden states in the last temporal\\r\\nattention layer. As evidenced by Fig. 10, after flow through the\\r\\nprojected attention layer, the hidden states have lower SVs than the\\r\\nincomplete inputs and are closer to the complete representations.\\r\\nInflow at Different Attention Heads Outflow at Different Attention Heads\\r\\nProjected Attention at Head 0 Projected Attention at Head 1\\r\\nProjected Attention at Head 2 Projected Attention at Head 3\\r\\n# of Step\\r\\n# of Coordinate\\r\\n# of Step\\r\\n# of Coordinate\\r\\nProjected Attention at Head 0 Projected Attention at Head 1\\r\\nProjected Attention at Head 2 Projected Attention at Head 3\\r\\n# of Coordinate # of Coordinate\\r\\n# of Step # of Step\\r\\nFigure 11: Inflow and outflow in the projected attention layer.\\r\\n6 CONCLUSION\\r\\nThis paper demonstrates a low rankness-induced Transformer\\r\\nmodel termed ImputeFormer to address the missing spatiotemporal\\r\\ndata imputation problem. Taking advantage of the low-rank fac\\x02torization, we design projected temporal attention and embedded\\r\\nspatial attention to incorporate structural priors into the Trans\\x02former model. Furthermore, a Fourier sparsity loss is developed to\\r\\nregularize the solution’s spectrum. The evaluation results on vari\\x02ous benchmarks indicate that ImputeFormer not only consistently\\r\\nachieves state-of-the-art imputation accuracy, but also exhibits high\\r\\ncomputational efficiency, generalizability across various datasets,\\r\\nversatility for different scenarios, and interpretability. Therefore,\\r\\nwe believe that it has the potential to advance research on spa\\x02tiotemporal data for general imputation tasks. Future work can\\r\\nadopt it to achieve time series representation learning task and\\r\\nexplore the multipurpose pretraining problem for time series.\\r\\nACKNOWLEDGMENTS\\r\\nThe work was supported by research grants from the National Nat\\x02ural Science Foundation of China (52125208), National Key R&D\\r\\nPrograms of China (2022YFB2602100), the China National Postdoc\\x02toral Program for Innovative Talents (BX20220231), the China Post\\x02doctoral Science Foundation (2022M712409), and the Science and\\r\\nTechnology Commission of Shanghai Municipality (22dz1203200).\\nKDD ’24, August 25–29, 2024, Barcelona, Spain. Tong Nie et al.\\r\\nREFERENCES\\r\\n[1] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. Brits:\\r\\nBidirectional recurrent imputation for time series. Advances in neural information\\r\\nprocessing systems 31 (2018).\\r\\n[2] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan\\r\\nLiu. 2018. Recurrent neural networks for multivariate time series with missing\\r\\nvalues. Scientific reports 8, 1 (2018), 6085.\\r\\n[3] Xinyu Chen, Zhanhong Cheng, Nicolas Saunier, and Lijun Sun. 2022. Laplacian\\r\\nconvolutional representation for traffic time series imputation. arXiv preprint\\r\\narXiv:2212.01529 (2022).\\r\\n[4] Xinyu Chen, Mengying Lei, Nicolas Saunier, and Lijun Sun. 2021. Low-rank\\r\\nautoregressive tensor completion for spatiotemporal traffic data imputation. IEEE\\r\\nTransactions on Intelligent Transportation Systems 23, 8 (2021), 12301–12310.\\r\\n[5] Xinyu Chen and Lijun Sun. 2021. Bayesian temporal factorization for multi\\x02dimensional time series prediction. IEEE Transactions on Pattern Analysis and\\r\\nMachine Intelligence 44, 9 (2021), 4659–4673.\\r\\n[6] Xinyu Chen, Jinming Yang, and Lijun Sun. 2020. A nonconvex low-rank tensor\\r\\ncompletion model for spatiotemporal traffic data imputation. Transportation\\r\\nResearch Part C: Emerging Technologies 117 (2020), 102673.\\r\\n[7] Andrea Cini, Ivan Marisca, and Cesare Alippi. 2021. Filling the g_ap_s: Mul\\x02tivariate time series imputation by graph neural networks. arXiv preprint\\r\\narXiv:2108.00298 (2021).\\r\\n[8] Andrea Cini, Ivan Marisca, Daniele Zambon, and Cesare Alippi. 2024. Taming\\r\\nlocal effects in graph-based spatiotemporal forecasting. Advances in Neural\\r\\nInformation Processing Systems 36 (2024).\\r\\n[9] Commission for Energy Regulation (CER). 2012. CER Smart Metering Project -\\r\\nElectricity Customer Behaviour Trial, 2009-2010. Dataset. https://www.ucd.ie/\\r\\nissda/data/commissionforenergyregulationcer/ SN: 0012-00.\\r\\n[10] Wenjie Du, David Côté, and Yan Liu. 2023. Saits: Self-attention-based imputation\\r\\nfor time series. Expert Systems with Applications 219 (2023), 119619.\\r\\n[11] Azul Garza and Max Mergenthaler-Canseco. 2023. TimeGPT-1. arXiv preprint\\r\\narXiv:2310.03589 (2023).\\r\\n[12] Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geof\\x02frey I Webb, Irwin King, and Shirui Pan. 2023. A survey on graph neural networks\\r\\nfor time series: Forecasting, classification, imputation, and anomaly detection.\\r\\narXiv preprint arXiv:2307.03759 (2023).\\r\\n[13] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2017. Diffusion convolu\\x02tional recurrent neural network: Data-driven traffic forecasting. arXiv preprint\\r\\narXiv:1707.01926 (2017).\\r\\n[14] Wei Liang, Yuhui Li, Kun Xie, Dafang Zhang, Kuan-Ching Li, Alireza Souri, and\\r\\nKeqin Li. 2023. Spatial-Temporal Aware Inductive Graph Neural Network for\\r\\nC-ITS Data Recovery. IEEE Transactions on Intelligent Transportation Systems 24,\\r\\n8 (2023), 8431–8442.\\r\\n[15] Yuebing Liang, Zhan Zhao, and Lijun Sun. 2022. Memory-augmented dynamic\\r\\ngraph convolution networks for traffic data imputation with diverse missing pat\\x02terns. Transportation Research Part C: Emerging Technologies 143 (2022), 103826.\\r\\n[16] Guangcan Liu and Wayne Zhang. 2022. Recovery of future data via convolution\\r\\nnuclear norm minimization. IEEE Transactions on Information Theory 69, 1 (2022),\\r\\n650–665.\\r\\n[17] Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quan\\x02jun Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makes\\r\\nvanilla transformer sota for traffic forecasting. In Proceedings of the 32nd ACM\\r\\nInternational Conference on Information and Knowledge Management. 4125–4129.\\r\\n[18] Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. 2012. Tensor comple\\x02tion for estimating missing values in visual data. IEEE transactions on pattern\\r\\nanalysis and machine intelligence 35, 1 (2012), 208–220.\\r\\n[19] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. 2023.\\r\\nPriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation. arXiv\\r\\npreprint arXiv:2302.09746 (2023).\\r\\n[20] Shuai Liu, Xiucheng Li, Gao Cong, Yile Chen, and Yue Jiang. 2022. Multivariate\\r\\nTime-series Imputation with Disentangled Temporal Representations. In The\\r\\nEleventh International Conference on Learning Representations.\\r\\n[21] Yukai Liu, Rose Yu, Stephan Zheng, Eric Zhan, and Yisong Yue. 2019. Naomi:\\r\\nNon-autoregressive multiresolution sequence imputation. Advances in neural\\r\\ninformation processing systems 32 (2019).\\r\\n[22] Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et al. 2018. Multivariate time\\r\\nseries imputation with generative adversarial networks. Advances in neural\\r\\ninformation processing systems 31 (2018).\\r\\n[23] Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. 2019. E2gan: End\\x02to-end generative adversarial network for multivariate time series imputation.\\r\\nIn Proceedings of the 28th international joint conference on artificial intelligence.\\r\\nAAAI Press Palo Alto, CA, USA, 3094–3100.\\r\\n[24] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\r\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\r\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\r\\n[25] Wei Ma and George H Chen. 2019. Missing not at random in matrix completion:\\r\\nThe effectiveness of estimating missingness probabilities under a low nuclear\\r\\nnorm assumption. Advances in neural information processing systems 32 (2019).\\r\\n[26] Ivan Marisca, Andrea Cini, and Cesare Alippi. 2022. Learning to reconstruct\\r\\nmissing data from spatiotemporal graphs with sparse observations. Advances in\\r\\nNeural Information Processing Systems 35 (2022), 32069–32082.\\r\\n[27] Nazeer Muhammad, Nargis Bibi, Adnan Jahangir, and Zahid Mahmood. 2018.\\r\\nImage denoising with norm weighted fusion estimators. Pattern Analysis and\\r\\nApplications 21 (2018), 1013–1022.\\r\\n[28] Tong Nie, Guoyang Qin, and Jian Sun. 2022. Truncated tensor Schatten p-norm\\r\\nbased approach for spatiotemporal traffic data imputation with complicated\\r\\nmissing patterns. Transportation research part C: emerging technologies 141 (2022),\\r\\n103737.\\r\\n[29] Tong Nie, Guoyang Qin, Lijun Sun, Wei Ma, Yu Mei, and Jian Sun. 2023. Contex\\x02tualizing MLP-Mixers Spatiotemporally for Urban Data Forecast at Scale. arXiv\\r\\npreprint arXiv:2307.01482 (2023).\\r\\n[30] Tong Nie, Guoyang Qin, Yunpeng Wang, and Jian Sun. 2023. Correlating sparse\\r\\nsensing for large-scale traffic speed estimation: A Laplacian-enhanced low-rank\\r\\ntensor kriging approach. Transportation Research Part C: Emerging Technologies\\r\\n152 (2023), 104190.\\r\\n[31] Tong Nie, Guoyang Qin, Yunpeng Wang, and Jian Sun. 2023. Towards better traffic\\r\\nvolume estimation: Jointly addressing the underdetermination and nonequilib\\x02rium problems with correlation-adaptive GNNs. Transportation Research Part C:\\r\\nEmerging Technologies 157 (2023), 104402.\\r\\n[32] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.\\r\\nA time series is worth 64 words: Long-term forecasting with transformers. arXiv\\r\\npreprint arXiv:2211.14730 (2022).\\r\\n[33] Zezhi Shao, Zhao Zhang, Fei Wang, Wei Wei, and Yongjun Xu. 2022. Spatial\\x02Temporal Identity: A Simple yet Effective Baseline for Multivariate Time Series\\r\\nForecasting. In Proceedings of the 31st ACM International Conference on Informa\\x02tion & Knowledge Management. 4454–4458.\\r\\n[34] Pratyusha Sharma, Jordan T Ash, and Dipendra Misra. 2023. The Truth is in\\r\\nThere: Improving Reasoning in Language Models with Layer-Selective Rank\\r\\nReduction. arXiv preprint arXiv:2312.13558 (2023).\\r\\n[35] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li.\\r\\n2021. Efficient attention: Attention with linear complexities. In Proceedings of the\\r\\nIEEE/CVF winter conference on applications of computer vision. 3531–3539.\\r\\n[36] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. Csdi: Con\\x02ditional score-based diffusion models for probabilistic time series imputation.\\r\\nAdvances in Neural Information Processing Systems 34 (2021).\\r\\n[37] Stef Van Buuren and Karin Groothuis-Oudshoorn. 2011. mice: Multivariate\\r\\nimputation by chained equations in R. Journal of statistical software 45 (2011),\\r\\n1–67.\\r\\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\r\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\r\\nyou Need. In Advances in Neural Information Processing Systems.\\r\\n[39] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin\\x02former: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\r\\n(2020).\\r\\n[40] Xudong Wang, Yuankai Wu, Dingyi Zhuang, and Lijun Sun. 2023. Low-rank\\r\\nHankel tensor completion for traffic speed estimation. IEEE Transactions on\\r\\nIntelligent Transportation Systems 24, 5 (2023), 4862–4871.\\r\\n[41] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng\\r\\nLong. 2022. Timesnet: Temporal 2d-variation modeling for general time series\\r\\nanalysis. arXiv preprint arXiv:2210.02186 (2022).\\r\\n[42] Yuankai Wu, Dingyi Zhuang, Aurelie Labbe, and Lijun Sun. 2021. Inductive\\r\\ngraph neural networks for spatiotemporal kriging. In Proceedings of the AAAI\\r\\nConference on Artificial Intelligence, Vol. 35. 4478–4485.\\r\\n[43] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi\\r\\nZhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with\\r\\nGraph Neural Networks. In Proceedings of the 26th ACM SIGKDD International\\r\\nConference on Knowledge Discovery and Data Mining. 753–763.\\r\\n[44] Yongchao Ye, Shiyao Zhang, and James JQ Yu. 2021. Spatial-temporal traffic\\r\\ndata imputation via graph attention convolutional network. In International\\r\\nConference on Artificial Neural Networks. Springer, 241–252.\\r\\n[45] Jinsung Yoon, James Jordon, and Mihaela Schaar. 2018. Gain: Missing data impu\\x02tation using generative adversarial nets. In International conference on machine\\r\\nlearning. PMLR, 5689–5698.\\r\\n[46] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. 2016. Temporal regularized\\r\\nmatrix factorization for high-dimensional time series prediction. Advances in\\r\\nneural information processing systems 29 (2016).\\r\\n[47] Ruiyang Zhang, Yang Liu, and Hao Sun. 2020. Physics-guided convolutional neu\\x02ral network (PhyCNN) for data-driven seismic response modeling. Engineering\\r\\nStructures 215 (2020), 110704.\\r\\n[48] Yu Zheng, Xiuwen Yi, Ming Li, Ruiyuan Li, Zhangqing Shan, Eric Chang, and\\r\\nTianrui Li. 2015. Forecasting fine-grained air quality based on big data. In Pro\\x02ceedings of the 21th ACM SIGKDD international conference on knowledge discovery\\r\\nand data mining. 2267–2276.\\nImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation KDD ’24, August 25–29, 2024, Barcelona, Spain.\\r\\nA APPENDIX\\r\\nA.1 Additional Discussions\\r\\nA.1.1 Architectural Comparison. We provide an illustration to show\\r\\nthe structural difference between different paradigms in Fig. 12.\\r\\nTime Space\\r\\n(b) ST-Transformer\\r\\n(a) Bi-MPGRU Proximity-based GCN\\r\\nSensor\\r\\nImputation\\r\\nPredefined Graph\\r\\nSpatial Attention\\r\\nImputation\\r\\nSensor\\r\\n(c) ImputeFormer\\r\\nSensor\\r\\nNode Emb.\\r\\nImputation\\r\\nEmb. Attn.\\r\\nHidden States\\r\\nTime Step\\r\\nBi-Directional RNN\\r\\nImputation\\r\\nTemporal Attention\\r\\nTime Step\\r\\nImputation\\r\\nTime Step\\r\\nProj. \\r\\nAttn.\\r\\nImputation\\r\\nProjector\\r\\nHidden States Hidden States\\r\\nFigure 12: (a) MPGRU adopts Bi-RNNs to gather available\\r\\nreadings from consecutive time points and GCNs to collect\\r\\nneighborhood data on predefined graphs. (b) Transformers\\r\\ncompute all pairwise correlations of the raw data in both\\r\\nspatial and temporal axes. (c) ImputeFormer utilizes projected\\r\\nattention along the temporal axis and embedded attention\\r\\nbased on the node representation in the spatial axis.\\r\\nA.1.2 Effective Input Embedding. Unlike images or languages, time\\r\\nseries have low semantic densities [32]. Therefore, many forecasting\\r\\nmodels flatten and abstract the input series to reduce information\\r\\nredundancy [29, 33]. Specifically, given the input X𝑡:𝑡+𝑇 ∈ R\\r\\n𝑁 ×𝑇\\r\\n,\\r\\neach series can be processed by a MLP shared by all series: z\\r\\n𝑖,(0) =\\r\\nMLP(x\\r\\n𝑖\\r\\n), where MLP(·) : R\\r\\n𝑇 → R𝐷 . However, we claim that this\\r\\ntechnique is not suitable for imputation. If we express it as follows:\\r\\n𝑥𝑡+ℎ = 𝜎\\r\\n ∑︁\\r\\n𝑇\\r\\n𝑘=0\\r\\n𝑤𝑘,ℎ𝑥𝑡+𝑘 + 𝑏𝑘,ℎ!, ℎ ∈ {0, . . . ,𝑇 }, (16)\\r\\nit is evident that the linear weights only depends on the relative\\r\\nposition in the sequence and are agnostic to the data flow. Since\\r\\nmissing data points and intervals can occur at arbitrary locations in\\r\\nthe series, fixed weights can learn spurious relationships between\\r\\neach time step, thus overfitting the missing patterns in the training\\r\\ndata. Therefore, we suggest not to use linear mappings on the time\\r\\naxis to account for the varying missing time points.\\r\\nA.1.3 Low-Rankness in Self-Attention. Wang et al. [39] studied\\r\\nthe observation that the self-attention matrix in Transformer is\\r\\nlow-rank and proposed a linear attention. Our proposed temporal\\r\\nprojected attention model shares a similar idea as this work but\\r\\nhas different mechanisms and formulations. We indicate the differ\\x02ences in the following exposition. Given Q, K, V ∈ R\\r\\n𝑇 ×𝐷 , and the\\r\\nprojector P ∈ R\\r\\n𝐶×𝐷 , temporal projected attention is formulated as:\\r\\nSelfAtten(Q, P, SelfAtten(P, K, V)),\\r\\n= 𝜎(QPT)SelfAtten(P, K, V) = 𝜎(QPT)\\r\\n| {z }\\r\\n𝑇 ×𝐶\\r\\n𝜎(PKT)\\r\\n| {z }\\r\\n𝐶×𝑇\\r\\nV, (17)\\r\\nwhile the linear attention [39] assigns two learnable matrices E, F ∈\\r\\nR\\r\\n𝐶×𝑇\\r\\nand has the following form:\\r\\nSelfAtten(Q, EK, FV)), = 𝜎( QKT\\r\\n|{z}\\r\\n𝑇 ×𝑇\\r\\nE\\r\\nT\\r\\n|{z}\\r\\n𝑇 ×𝐶\\r\\n)FV. (18)\\r\\nAs for complexity, we can compute Eq. (17) in the order: 𝜎(QPT) >\\r\\n𝜎(PKT) > 𝜎(PKT)V > 𝜎(QPT\\r\\n)𝜎(PKT)V, which admits 𝑂(4𝑇𝐷𝐶)\\r\\ncomplexity. Similarly, Eq. (18) has the same complexity. Although\\r\\nof the same complexity, our model has an explicit and symmetric\\r\\nformulation that brings improved model expressivity. The advan\\x02tages are threefold: (1) explicit low-rank factorization: Linformer\\r\\n[39] does not directly achieve a low-rank factorization of the atten\\x02tion matrix. It first computes the full attention matrix QKT with\\r\\nsize 𝑇 ×𝑇 and then compresses it to 𝑇 ×𝐶. Instead, ImputeFormer\\r\\ndirectly factorizes the full attention matrix from𝑇 ×𝑇 to𝑇 ×𝐶,𝐶×𝑇 ,\\r\\nwhich is beneficial for dealing with redundancy and missingness\\r\\nin the attention matrix; (2) pattern adaptation: Linformer sets the\\r\\ncompression matrix E, F ∈ R\\r\\n𝐶×𝑇\\r\\ncompletely learnable, which is\\r\\nagnostic to the missing patterns of Q, K, and V. These static pa\\x02rameters cannot account for the varying missing patterns. Instead,\\r\\nwe obtain the 𝑇 × 𝐶 factor matrix through the query QPT, which\\r\\nis pattern-adaptive; (3) increased capacity: Linformer has 𝑇 × 𝐶\\r\\nlearnable parameters, while ours has 𝐶 × 𝐷 parameters, which has\\r\\na larger model capacity while having the same time complexity.\\r\\nA.2 Reproducibility\\r\\nA.2.1 Implementations. We build our model and baselines based on\\r\\nthe SPIN repository (https://github.com/Graph-Machine-Learning\\x02Group/spin). All experiments were performed on a single NVIDIA\\r\\nRTX A6000 GPU (48 GB). For the hyperparameters of ImputeFormer,\\r\\nwe set the hidden size to 256, the input projection size to 32, the\\r\\nnode embedding size to 64, the projected size to 6, the number of\\r\\nattention layers to 3, and the sequence length to 24 for all data.\\r\\nWe also keep the same training, validation and evaluation split as\\r\\n[7, 26] and report the metrics on the masked evaluation points.\\r\\nA.2.2 Dataset Descriptions. We adopt heterogeneous spatiotempo\\x02ral benchmark datasets to evaluate the imputation performance.\\r\\nTraffic Speed Data. Our experiments include two commonly used\\r\\ntraffic speed datasets, named METR-LA and PEMS-BAY. METR-LA con\\x02tains spot speed data from 207 loop sensors over a period of 4\\nKDD ’24, August 25–29, 2024, Barcelona, Spain. Tong Nie et al.\\r\\nDay.1 Day.2 Day.3 Day.4 Day.5 Day.6 Day.7\\r\\nSensor No.48\\r\\n0\\r\\n200\\r\\n400\\r\\n600\\r\\nVolume\\r\\nPEMS08 with 75% point missing\\r\\nGroundTruth\\r\\nImputation\\r\\nObservation\\r\\n(a) PEMS08, 75% point missing\\r\\nDay.1 Day.2 Day.3 Day.4 Day.5 Day.6 Day.7\\r\\nSensor No.157\\r\\n0\\r\\n200\\r\\n400\\r\\n600\\r\\nVolume\\r\\nPEMS08 with 95% point missing\\r\\nGroundTruth\\r\\nImputation\\r\\nObservation\\r\\n(b) PEMS08, 95% point missing\\r\\nDay.1 Day.2 Day.3 Day.4 Day.5 Day.6 Day.7\\r\\nSensor No.48\\r\\n0\\r\\n20\\r\\n40\\r\\n60\\r\\n80\\r\\nSpeed\\r\\nMETR-LA with 75% point missing\\r\\nGroundTruth\\r\\nImputation\\r\\nObservation\\r\\n(c) METR-LA, 75% point missing\\r\\nDay.1 Day.2 Day.3 Day.4 Day.5 Day.6 Day.7\\r\\nSensor No.157\\r\\n0\\r\\n20\\r\\n40\\r\\n60\\r\\n80\\r\\nSpeed\\r\\nMETR-LA with 95% point missing\\r\\nGroundTruth\\r\\nImputation\\r\\nObservation\\r\\n(d) METR-LA, 95% point missing\\r\\nDay.1 Day.2 Day.3 Day.4 Day.5 Day.6 Day.7\\r\\nSensor No.126\\r\\n0\\r\\n200\\r\\n400\\r\\n600\\r\\nVolume\\r\\nPEMS03 with 10.0% block missing\\r\\nGroundTruth\\r\\nImputation\\r\\nObservation\\r\\n(e) PEMS03, 10% block missing\\r\\nDay.1 Day.2 Day.3 Day.4 Day.5 Day.6 Day.7\\r\\nSensor No.126\\r\\n0\\r\\n20\\r\\n40\\r\\n60\\r\\n80\\r\\nSpeed\\r\\nPEMS-BAY with 10.0% block missing\\r\\nGroundTruth\\r\\nImputation\\r\\nObservation\\r\\n(f) PEMS-BAY, 10% block missing\\r\\nFigure 13: Visualization examples of imputation for traffic speed and volume data under different missing patterns.\\r\\nmonths from Mar 2012 to Jun 2012, located at the Los Angeles\\r\\nCounty highway network. PEMS-BAY records 6 months of speed\\r\\ndata from 325 static sensors in the San Francisco South Bay Area.\\r\\nTraffic Volume Data. We adopt four traffic volume data, including\\r\\nPEMS03, PEMS04, PEMS07, and PEMS08. They contain the highway\\r\\ntraffic volume record collected by the Caltrans Performance Mea\\x02surement System (PeMS) and aggregated into 5-minute intervals.\\r\\nEnergy and Environmental Data. Four energy and environmen\\x02tal data are selected to evaluate the generality of models, including:\\r\\n(1) Solar: solar power production records from 137 synthetic PV\\r\\nfarms in Alabama state in 2006, which are sampled every 10 minutes;\\r\\n(2) CER-EN: smart meters measuring energy consumption from the\\r\\nIrish Commission for Energy Regulation Smart Metering Project\\r\\n[9]. Following the setting in [7], we select 435 time series aggre\\x02gated at 30 minutes for evaluation. (3) AQI: PM2.5 pollutant records\\r\\ncollected by 437 air quality monitoring stations in 43 Chinese cities\\r\\nfrom May 2014 to April 2015 with the aggregation interval of 1 hour.\\r\\nNote that AQI data contains nearly 26% missing data. (4) AQI36: a\\r\\nsubset of AQI data which contains 36 sensors in Beijing distinct.\\r\\nA.2.3 Experimental Settings and Baseline Methods. This section\\r\\ndescribes the detailed information on experimental setups.\\r\\nMissing patterns. For traffic, Solar and CER-EN, we consider two\\r\\nscenarios discussed in [7, 26]: (1) Point missing: randomly remove\\r\\nobserved points with 25% probability; (2) Block missing: randomly\\r\\ndrop 5% of the available data and at the same time simulate a sensor\\r\\nfailure lasting for L ∼ U (12, 48) steps with 0.15% probability.\\r\\nWe keep the above missing rates the same as in the previous work\\r\\n[7, 26]. In addition, we also evaluated the performance under sparser\\r\\nconditions. For example, the block missing with 10% probability\\r\\ncorresponds to a total missing rate of ≈ 90 ∼ 95%. Note that matrix\\r\\nor tensor models can only handle in-sample imputation, where the\\r\\nobserved training data and the test data are in the same time period.\\r\\nHowever, deep models can work in out-of-sample scenarios [7]\\r\\nwhere the training and test sequences are disjoint. We adopt the\\r\\nout-of-sample tests for deep models and in-sample tests for others.\\r\\nBaseline Methods. We compare our model with SOTA deep\\x02learning and low-rank imputation methods. For statistical and opti\\x02mization models, we consider: (1) Observation average (Average);\\r\\n(2) Temporal regularized matrix factorization (TRMF) [46]; (3) Low\\x02rank autoregressive tensor completion (LRTC-AR) [4]; (4) MICE [37].\\r\\nFor deep imputation models, we select several competitive baselines:\\r\\n(1) SPIN [26]: sparse spatiotemporal attention model with state-of\\x02the-art imputation performance; (2) GRIN [7]: message-passing\\x02based bidirectional RNN model with competitive performance; (3)\\r\\nSAITS [10]: Temporal Transformer model with diagonally masked\\r\\nattention; (4) BRITS [1]: bidirectional RNN model for imputation; (5)\\r\\nrGAIN [45]: GAIN model with bidirectional recurrent encoder and\\r\\ndecoder; (6) Transformer/ST-Transformer [38]: canonical Trans\\x02former with self-attention in temporal or spatial-temporal dimen\\x02sions; (7) TiDER [20]: matrix factorization with disentangled neural\\r\\nrepresentations; (8) TimesNet [41]: 2D convolution-based general\\r\\ntime series analysis model; (9) BiMPGRU: a bidirectional RNN-based\\r\\nGCN model, which is similar to DCRNN [13].\\r\\nAblation Studies. Particularly, we examine the following vari\\x02ations: (a) Temporal blocks: we replace the temporal interaction\\r\\nmodule with MLP or directly remove it; (b) Spatial blocks: we replace\\r\\nthe spatial interaction module with MLP or directly remove it; (c)\\r\\nLoss function: We remove the FIL or replace it with a hierarchical\\r\\nloss used in [10, 26]; (d) Architecture: we evaluate the impacts of\\r\\nthe order of spatial-temporal blocks and the joint attention strategy.\\r\\nWe adopt two data to evaluate and other settings remain the same.\\r\\nEvaluation Metrics. To evaluate the model, we simulate different\\r\\nobservation conditions by removing parts of the raw data to con\\x02struct incomplete samples based on different missing rates (𝑝missing).\\r\\nEvaluation metrics are then calculated for these simulated missing\\r\\npoints. We use a masking indicator Mmissing to denote these loca\\x02tions in which the unobserved (missing) values are marked as ones,\\r\\nobserved as zeros. Note that the masked points for evaluation are\\r\\nnot available for the models during all stages. The mean absolute\\r\\nerror (MAE) is adopted to report the results.\\r\\nA.3 Imputation Visualization\\r\\nWe provide several visualization examples in Fig. 13. As evidenced,\\r\\nImputeFormer can generate reasonable imputations by learning\\r\\nthe inherent structures of spatiotemporal data. Previous studies\\r\\nhave discovered that low-rank models can cause oversmoothing\\r\\nestimation [4, 30]. Due to the representation power of deep architec\\x02tures, our model can provide a detailed reconstruction. In particular,\\r\\nalthough only limited temporal information is available in the block\\r\\nmissing case, it can resort to the node embedding as the query to\\r\\nspatial relations, thereby generating an effective imputation.'},\n",
       " {'name': '1903.02787v2.pdf',\n",
       "  'content': 'GRATIS: GeneRAting TIme Series with diverse and\\r\\ncontrollable characteristics\\r\\nYanfei Kang∗, Rob J Hyndman†, and Feng Li‡\\r\\nAbstract\\r\\nThe explosion of time series data in recent years has brought a flourish of new time series\\r\\nanalysis methods, for forecasting, clustering, classification and other tasks. The evaluation\\r\\nof these new methods requires either collecting or simulating a diverse set of time series\\r\\nbenchmarking data to enable reliable comparisons against alternative approaches. We pro\\x02pose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS,\\r\\nwith the use of mixture autoregressive (MAR) models. We simulate sets of time series using\\r\\nMAR models and investigate the diversity and coverage of the generated time series in a time\\r\\nseries feature space. By tuning the parameters of the MAR models, GRATIS is also able\\r\\nto efficiently generate new time series with controllable features. In general, as a costless\\r\\nsurrogate to the traditional data collection approach, GRATIS can be used as an evaluation\\r\\ntool for tasks such as time series forecasting and classification. We illustrate the usefulness\\r\\nof our time series generation process through a time series forecasting application.\\r\\nKeywords: Time series features; Time series generation; Mixture autoregressive models;\\r\\nTime series forecasting; Simulation.\\r\\n1 Introduction\\r\\nWith the widespread collection of time series data via scanners, monitors and other automated\\r\\ndata collection devices, there has been an explosion of time series analysis methods developed in\\r\\nthe past decade or two. Paradoxically, the large datasets are often also relatively homogeneous\\r\\nin the industry domain, which limits their use for evaluation of general time series analysis\\r\\nmethods (Keogh and Kasetty, 2003; Mu˜noz et al., 2018; Kang et al., 2017). The performance\\r\\n∗School of Economics and Management, Beihang University, Beijing 100191, China. Email:\\r\\nyanfeikang@buaa.edu.cn, ORCID: https://orcid.org/0000-0001-8769-6650.\\r\\n†Department of Econometrics and Business Statistics, Monash University, Clayton, Victoria, 3800, Australia.\\r\\nEmail: Rob.Hyndman@monash.edu, ORCID: https://orcid.org/0000-0002-2140-5352.\\r\\n‡School of Statistics and Mathematics, Central University of Finance and Economics, 100081 Beijing, China.\\r\\nEmail: feng.li@cufe.edu.cn, ORCID: https://orcid.org/0000-0002-4248-9778, Corresponding author.\\r\\n1\\nof any time series mining algorithm depends on the diversity of the training data, so that the\\r\\nevaluation of the algorithm can be generalized to a wide range of future data (Smith-Miles and\\r\\nBowly, 2015).\\r\\nAs Keogh and Kasetty (2003) argue, after extensive analysis on highly diverse datasets,\\r\\nthere is “a need for more comprehensive time series benchmarks and more careful evaluation in\\r\\nthe data mining community”. Although some attempts have been made to alleviate these issues\\r\\nin certain time series tasks, such as the widely used UCR archive (Dau et al., 2018) for time\\r\\nseries classification (see e.g., Ismail Fawaz et al. (2019)), and the M4 dataset for the most recent\\r\\ntime series forecasting competition (Makridakis et al., 2018), the time series area lacks diverse\\r\\nand controllable benchmarking data for algorithm evaluation, compared to the popularly used\\r\\nbenchmarking datasets in other data mining domains like ImageNet (Deng et al., 2009) for\\r\\nimaging analysis and UCI machine learning repository (Dua and Karra Taniskidou, 2017) for\\r\\ngeneral machine learning algorithms evaluation.\\r\\nIn recent years, research shows that it is also possible to use generated data for algorithm\\r\\nlearning under certain application domains, which give great potential for exploring simulated\\r\\ndata. One such example is the well-known “Alpha Zero” (Silver et al., 2017), being able to learn\\r\\nfrom simulated games based on self-play without human input for guidance. Simulated datasets\\r\\nare also extremely useful when a researcher needs good control over the generated data and a\\r\\ncareful design of the data generator in order to test the generalizability and weaknesses of their\\r\\nmethods. Such examples can be found in evaluating and comparing statistical methods for early\\r\\ntemporal detection of outbreaks based on simulated data (Lotze et al., 2010; B´edubourg and\\r\\nLe Strat, 2017), examining neural network forecasting through a simulated computer experi\\x02ment (Zhang et al., 2001), detecting the impact of autocorrelation on detection and timeliness\\r\\nperformance (Lotze and Shmueli, 2009), etc. In this paper, we present a tool of GeneRAting\\r\\nTIme Series with diverse and controllable characteristics, named GRATIS, by exploring the\\r\\npossible time series features and the nature of time dependence. Moreover, in order to show\\r\\nthe usefulness of our generation scheme, we develop a novel forecasting selection method based\\r\\non our generated data in the application section.\\r\\nSome prior approaches have focused on the shapes of one or more given time series, or on\\r\\nsome predefined “types” of time series, in order to generate new time series. Vinod, L´opez-de\\x02Lacalle, et al. (2009) use a maximum entropy bootstrap method to generate ensembles for time\\r\\nseries data. The generated samples retain the shape, or local peaks and troughs, of the original\\r\\ntime series. They are not exactly the same, but ‘strongly dependent’, and thus can be used for\\r\\nconvenient statistical inference. Bagnall et al. (2017) simulate time series data from different\\r\\n2\\nshape settings. The simulators are created by placing one or more shapes on a white noise\\r\\nseries. Time series classification algorithms are then evaluated on different representations of\\r\\nthe data, which helps understand why one algorithm works better than another on a particular\\r\\nrepresentation of the data. The obvious drawback in these generation methods is that it is\\r\\nimpossible to create simulated time series that comprehensively cover the possible space of time\\r\\nseries, which limits the reproducibility and applicability of the tested methodology.\\r\\nAn appealing approach is to generate new instances with controllable characteristics, a\\r\\nmethod that has been used in several other areas of analysis including graph coloring (Smith\\x02Miles and Bowly, 2015), black-box optimization (Mu˜noz and Smith-Miles, 2017) and machine\\r\\nlearning classification (Mu˜noz et al., 2018). Kang et al. (2017) adapt the idea to time series,\\r\\nand show that it is possible to “fill in” the space of a large collection of real time series data\\r\\nby generating artificial time series with desired characteristics. Each time series is represented\\r\\nusing a feature vector which is projected on to a two-dimensional “instance space” that can be\\r\\nvisually inspected for diversity. Kang et al. (2017) use a genetic algorithm to evolve new time\\r\\nseries to fill in any gaps in the two-dimensional instance space. In a later related paper, Kegel\\r\\net al. (2017) use STL (an additive decomposition method) to estimate the trend and seasonal\\r\\ncomponent of a series, which they then modify using multiplicative factors to generate new\\r\\ntime series. The evolutionary algorithm approach of Kang et al. (2017) is quite general, but\\r\\ncomputationally slow, while the STL approach of Kegel et al. (2017) is much faster but only\\r\\ngenerates series that are additive in trend and seasonality. In the meta-learning framework of\\r\\nTalagala et al. (2018), a random forest classifier is used to select the best forecasting method\\r\\nbased on time series features. The observed time series are augmented by simulating new time\\r\\nseries similar to the observed series, which helps to form a larger dataset to train the model\\x02selection classifier. The simulated series rely on the assumed data generating processes (DGPs),\\r\\nwhich are exponential smoothing models and ARIMA models.\\r\\nIn this paper, we propose a new efficient and general approach to time series generation,\\r\\nGRATIS, based on Gaussian mixture autoregressive (MAR) models to generate a wide range of\\r\\nnon-Gaussian and nonlinear time series. Our generated dataset can be used as benchmarking\\r\\ndata in the time series domain, functioning similarly to ImageNet in image processing but with\\r\\na minimal input of human efforts and computational resources. Mixture transition distribution\\r\\nmodels were first developed by Le et al. (1996) to capture many general non-Gaussian and\\r\\nnonlinear features, which were later generalized to MAR models (Wong and Li, 2000). We\\r\\nexplore generating data from a random population of MAR models, as well as generating data\\r\\nwith specified features, which is particularly useful in time series classification or in certain\\r\\n3\\nareas where only some features are of interest. In this way, we provide a solution to the need\\r\\nfor a heterogeneous set of time series to use for time series analysis.\\r\\nFinite mixture models have proven useful in many other contexts as well. Different spec\\x02ifications of finite mixtures have been shown to be able to approximate large nonparametric\\r\\nclasses of conditional multivariate densities (Jiang and Tanner, 1999; Norets, 2010). More gen\\x02eral models to flexibly estimate the density of a continuous response variable conditional on a\\r\\nhigh-dimensional set of covariates have also been proposed (Li et al., 2010; Villani et al., 2009).\\r\\nMu˜noz and Smith-Miles (2017) generate general classification instances with a desired feature\\r\\nvector by fitting Gaussian mixture models. Our GRATIS approach is consistent with this line\\r\\nof literature but we reverse the procedure for generating new time series, in that we use finite\\r\\nmixtures of Gaussian processes to produce time series with specified features.\\r\\nWe first simulate a large time series dataset using MAR models and calculate their fea\\x02tures, and the corresponding embedded two-dimensional instance space. The coverage of the\\r\\nsimulated data can then be compared with existing benchmarking time series datasets in the\\r\\ntwo-dimensional space. At the same time, new time series instances with desired features can\\r\\nbe generated by tuning a MAR model.\\r\\nThe rest of the paper is organized as follows. Section 2 generates time series from MAR mod\\x02els. Section 3 investigates the diversity and coverage of the generated data in two-dimensional\\r\\ninstance spaces. In Section 4, we tune a MAR model efficiently using a genetic algorithm to\\r\\ngenerate time series data with some specific feature targets. We also study the efficiency of our\\r\\nalgorithm. In Section 5, we present a novel time series forecasting approach by exploiting the\\r\\nfeature space of generated time series. We show that our scheme can serve as a useful resource\\r\\nfor time series applications such as forecasting competitions. Section 6 concludes the paper and\\r\\ndiscusses possible directions for further studies.\\r\\n2 Time series generation from MAR models\\r\\n2.1 Mixture autoregressive models\\r\\nMixture autoregressive models consist of multiple stationary or non-stationary autoregressive\\r\\ncomponents. Being able to capture a great variety of shape-changing distributions, MAR models\\r\\ncan handle nonlinearity, non-Gaussianity, cycles and heteroskedasticity in the time series (Wong\\r\\nand Li, 2000). We define a K-component MAR model as:\\r\\nF(xt|F−t) = X\\r\\nK\\r\\nk=1\\r\\nαkΦ\\r\\n\\x12\\r\\nxt − φk0 − φk1xt−1 − · · · − φkpkxt−pk\\r\\nσk\\r\\n\\x13\\r\\n, (1)\\r\\n4\\nwhere F(xt\\r\\n|F−t) is the conditional cumulative distribution of xt given the past information\\r\\nF−t ⊆ {xt−1, . . . , xt−pk}, Φ(·) is the cumulative distribution function of the standard normal\\r\\ndistribution, xt − φk0 − φk1xt−1 − · · · − φkpkxt−pkis the autoregressive term in each mixing\\r\\ncomponent, σk > 0 is the standard error, PK\\r\\nk=1 αk = 1, and αk > 0 for k = 1, 2, . . . , K.\\r\\nDenoted as MAR(K; p1, p2, . . . , pk), it is actually finite mixtures of K Gaussian AR models.\\r\\nThe MAR models have appealing properties as they are based on finite mixture models.\\r\\nFor example, the conditional expectation, variance and the mth central moment of xt can be\\r\\nwritten respectively as:\\r\\nE(xt|F−t) = X\\r\\nK\\r\\nk=1\\r\\nαk(φk0 + φk1xt−1 + · · · + φkpkxt−pk) = X\\r\\nK\\r\\nk=1\\r\\nαkµk,t , (2)\\r\\nVar(xt|F−t) = X\\r\\nK\\r\\nk=1\\r\\nαkσ\\r\\n2\\r\\nk +\\r\\nX\\r\\nK\\r\\nk=1\\r\\nαkµ\\r\\n2\\r\\nk,t −\\r\\n X\\r\\nK\\r\\nk=1\\r\\nαkµk,t!2, (3)\\r\\nE((xt − E(xt|F−t))m) = X\\r\\nK\\r\\nk=1\\r\\nXm\\r\\ni=1\\r\\nαk\\r\\n \\r\\nm\\r\\ni\\r\\n!\\r\\nE((xt − µk,t)\\r\\ni\\r\\n). (4)\\r\\nThus, the MAR model gives a description of the conditional distribution of the time series,\\r\\nand the shape changing feature of the conditional distributions allow the MAR models to\\r\\ndescribe processes with heteroskedasticity (Wong and Li, 2000). Furthermore, unlike standard\\r\\nAR models, higher order moments (Equation (4)) are also available in MAR densities. To adapt\\r\\nthe MAR model to deal with non-stationarity, one can simply include a unit root in each of the\\r\\nK components in Equation (1). Since a seasonal ARIMA model with seasonal effects and unit\\r\\nroots, denoted as ARIMA(p, d, 0)(P, D, 0)Period, can be simply expanded to AR models, one can\\r\\nflexibly include seasonal effects and non-stationarity in any component in the MAR models.\\r\\nSimulating time series with recurrent extreme values is possible when the MAR model has two\\r\\ndifferent components with distinct means (one for normal time and one for extreme time) and\\r\\nunbalanced weights (e.g., 0.99 and 0.01), where the AR process within each component is a\\r\\nseasonal process.\\r\\nIn principle, one can extend the MAR models to mixtures of both autoregressive and moving\\r\\naverage models, but we will keep the MAR form as in Equation (1) and not introduce the\\r\\nunnecessary complexity, because both autoregressive and moving average models can be written\\r\\nin terms of autoregressive models. The model in Equation (1) is in univariate form, but it can\\r\\nbe extended to the multivariate case by introducing a multivariate normal CDF and vector\\r\\nautoregressive terms. Our approach is fully probabilistic, and incorporates the uncertainty of\\r\\nthe parameters in MAR to allow for specific scenarios in time series. This is an alternative\\r\\nto previous studies in Lotze et al. (2010) and B´edubourg and Le Strat (2017) that simulate\\r\\n5\\nmultivariate time series by adding specific events on simulated baseline data.\\r\\nIn real data, the distribution of the time series can be multi-modal and/or heavy tailed, and\\r\\nso the expectation may not be the best prediction of the future. This is handled nicely with the\\r\\nmixture distribution F(xt|F−t). From Equation (3), the conditional variance of xt changes with\\r\\nconditional means of different components. The larger the difference among conditional means\\r\\nµk,t (k = 1, 2, . . . , K), the larger the conditional variance of xt. The value of PK\\r\\nk=1 αkµ\\r\\n2\\r\\nk,t −\\r\\n(\\r\\nPK\\r\\nk=1 αkµk,t)\\r\\n2\\r\\nis equal to zero only when µ1,t = µ2,t = · · · = µk,t which also yields a heavy-tailed\\r\\ndistribution; otherwise, it is larger than zero. The baseline conditional variance is PK\\r\\nk=1 αkσ\\r\\n2\\r\\nk\\r\\n.\\r\\nThe key merits of MAR models for nonlinear time series modeling are: (i) for a sufficiently\\r\\ndiverse parameters space and finite number of components, MAR models are able to capture\\r\\nextensive time series features in principle (Li et al., 2010), (ii) one can simply include seasonal\\r\\neffects and non-stationary in each component (see Section 2.3), (iii) there is no need to treat sta\\x02tionary and non-stationary time series separately as mixtures of stationary and non-stationary\\r\\ncomponents can yield a both stationary and non-stationary process with MAR (Wong and Li,\\r\\n2000), (iv) the conditional distributions of the time series given the past information change\\r\\nwith time which allows for meaningful time series evolving with historical information, and (v)\\r\\nthe MAR models can handle complicated univariate and multivariate time series with different\\r\\nfrequencies and seasonalities. The MAR models is also capable of capturing features such as\\r\\nmultimodality, heavy tails and heteroskedasticity.\\r\\nIn principle, one may use other types of flexible models as the generator. Nonetheless, our\\r\\ngenerator based on mixture autoregressive models with minimal parameters setting efficiently\\r\\ngenerate time series data with diverse features. We describe our time series generation approach\\r\\nand analyze the diversity and coverage of the generated time series in the following sections.\\r\\n2.2 Diverse time series generation\\r\\nDue to the flexibility of mixture models, they have been successfully applied in many statistical\\r\\ndomains such as Bayesian nonparametrics (Escobar and West, 1995), forecasting with high\\r\\nfrequency and heavy-tailed time series (Li et al., 2010), model selection (Constantinopoulos et\\r\\nal., 2006) and averaging (Villani et al., 2009), classification methods (Povinelli et al., 2004) and\\r\\ntext modeling (Griffiths et al., 2004). Nevertheless, in the extensive literature, little attention\\r\\nhas been given to data generation which is crucially important for evaluating the performance of\\r\\nall the tasks mentioned above. Data generating processes do not require sophisticated modeling\\r\\ntechniques, but they do require a priori knowledge of the target data space. This space is usually\\r\\nhuge and extremely difficult to simulate in a non-time-series context. However, generating\\r\\n6\\ndiverse time series is possible if one can explore a wide range of time dependencies in time\\r\\nseries. In this section we demonstrate how to generate a set of diverse time series data based\\r\\non the nature of time series dependence.\\r\\nWe design a simulation study to provide insights into the time series simulated from mixture\\r\\nautoregressive models. A significant difference in our data generation process compared to\\r\\ntypical simulation processes used in the statistical literature (where the data are generated\\r\\nfrom models with fixed parameter values), is that we use distributions (see Table 1) instead of\\r\\nfixed values for the parameters in the underlying models. This allows us to generate diverse\\r\\ntime series instances. Table 1 shows the parameter settings used in the simulation. These are\\r\\nanalogous to non-informative priors (Gelman et al., 2013) in the Bayesian contexts, i.e., the\\r\\ndiversity of the generated time series should not rely on the parameter settings.\\r\\nThe periods of the simulated time series are set to be 1, 4, 12 or 52 to match annual,\\r\\nquarterly, monthly and weekly time series. Their lengths are randomly chosen from the lengths\\r\\nof the M4 data (Makridakis et al., 2018). We randomly draw the number of components, K,\\r\\nfrom a uniform distribution on {1, 2, 3, 4, 5}. Although, it may be desirable to have a larger\\r\\nK, Villani et al. (2009) and Li et al. (2010) show that mixture models with comprehensive\\r\\nmean structures are flexible enough with less than five components. The weights of the mixing\\r\\ncomponents, αk, can be obtained as βk/\\r\\nPK\\r\\ni=1 βi for k = 1, 2, . . . , K, where the βs follow uniform\\r\\ndistributions on (0,1). Assuming that the kth component follows a standard seasonal ARIMA\\r\\nmodel as ARIMA(pk, dk, 0)(Pk, Dk, 0)Period, the coefficients of the AR and seasonal AR parts,\\r\\nθki, i = 1, 2, . . . , pk and Θkj , j = 1, 2, . . . , Pk, follow normal distributions with given mean\\r\\nand variance values. In principle, both the coefficients θki and Θkj could be unbounded, but\\r\\nthis limitation is necessary to keep the features of the simulated data as realistic as possible.\\r\\nFor the kth mixing component, we perform dk differences and Dk seasonal differences, where\\r\\ndk ∼ Bernoulli(0.9) and Dk ∼ Bernoulli(0.4), respectively.\\r\\nFor the parameter settings given in Table 1, we generate 20,000 yearly, 20,000 quarterly,\\r\\n40,000 monthly and 10,000 weekly time series based on the MAR models. For each generated\\r\\ntime series, we discard the first Period × 10 samples as burn-in. Figure 1 shows examples of\\r\\nsimulated yearly, quarterly, monthly and weekly data using GRATIS. The lengths of the simu\\x02lated series are set to be similar to those of the M4 time series data, which is the largest dataset\\r\\npublicly available to be compared in Section 3. Each of the time series can be summarized with\\r\\na feature vector described in Section 3.1. For each frequency and each simulated example, we\\r\\nalso show (using the same color) the closest real time series from M4 in feature spaces to give\\r\\nan evidence of the realisticity of the generated time series using GRATIS.\\r\\n7\\nGRATIS\\r\\n0 10 20 30 40 50\\r\\nTime\\r\\nYearly\\r\\nM4\\r\\n0 10 20 30 40 50\\r\\nTime\\r\\nGRATIS\\r\\n0 10 20 30 40\\r\\nTime\\r\\nQuarterly\\r\\nM4\\r\\n0 10 20 30 40\\r\\nTime\\r\\nGRATIS\\r\\n0 10 20 30 40\\r\\nTime\\r\\nMonthly\\r\\nM4\\r\\n0 10 20 30 40\\r\\nTime\\r\\nGRATIS\\r\\n0 10 20 30 40 50\\r\\nTime\\r\\nWeekly\\r\\nM4\\r\\n0 10 20 30 40\\r\\nTime\\r\\nFigure 1: Examples of simulated yearly, quarterly, monthly and weekly time series of different\\r\\nlengths. For each frequency and each simulated example, we also show (using the same color)\\r\\nthe closest real time series from M4 in feature spaces. The time series are standardized with\\r\\ncentering and scaling and thus the y-axis is omitted. The x-axis for each plot is in years.\\r\\n8\\nTable 1: Parameter settings in our generator for simulating time series using mixtures of ARIMA\\r\\nmodels.\\r\\nParameter Description Values\\r\\nPeriod Period of time series 1, 4, 12 or 52\\r\\nn Length of time series Randomly chosen from the lengths of\\r\\nM4 data\\r\\nK Number of components U{1, 2, 3, 4, 5}\\r\\nαk Weights of mixture components αk = βk/\\r\\nPK\\r\\ni=1 βi\\r\\n, where βi ∼ U(0, 1)\\r\\nθki Coefficients of the AR parts N(0, 0.5)\\r\\nΘkj Coefficients of the seasonal AR parts N(0, 0.5)\\r\\nσk Variance of the error terms logNormal(mean = 0.1,sd = 0.1)\\r\\ndk Number of differences in each component Bernoulli(0.9)\\r\\nDk Number of seasonal differences in each\\r\\ncomponent\\r\\nBernoulli(0.4)\\r\\nTable 2: Computational time (in seconds) for simulation of 1,000 yearly, quarterly, monthly\\r\\nand weekly time series. Different lengths are considered for each seasonal pattern according to\\r\\nthe 20%, 50% and 75% quantiles of the time series lengths in M4 data.\\r\\nYearly Quarterly Monthly Weekly\\r\\nLength Time (s) Length Time (s) Length Time (s) Length Time (s)\\r\\n20 3 60 7 80 13 350 65\\r\\n30 3 90 10 200 26 900 156\\r\\n40 4 120 13 300 39 1600 267\\r\\nTable 2 shows the computational time for simulation of 1,000 yearly, quarterly, monthly\\r\\nand weekly time series. Different lengths are considered for each seasonal pattern according\\r\\nto the 20%, 50% and 75% quantiles of the time series lengths in the M4 data. We have\\r\\ndeveloped an R package gratis for the time series generation which is available from https:\\r\\n//github.com/ykang/gratis. The code is written in R, and we run it on a laptop with a 2.6\\r\\nGHz, 8 cores CPU and 16G RAM.\\r\\n2.3 Multi-seasonal time series generation\\r\\nSo far, we have focused on time series in which there is only one seasonal pattern. However,\\r\\nmany time series exhibit multiple seasonal patterns of different lengths, especially those series\\r\\nobserved at a high frequency (such as daily or hourly data). For example, Figure 2 shows the\\r\\nhalf-hourly electricity demand for the state of Victoria, Australia, for 5 weeks in late 2014.\\r\\nThere is a clear daily pattern of frequency 48, and a weekly pattern of frequency 48 × 7 = 336.\\r\\nWith a longer time series, an annual pattern would also become obvious.\\r\\nSimulation of multi-seasonal time series involves weighted aggregation of simulated time\\r\\nseries with the corresponding frequencies. A simulated multi-seasonal time series xt with M\\r\\n9\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n1 2 3 4 5 6\\r\\nWeek\\r\\nTotal electricity demand (GW)\\r\\n−2\\r\\n−1\\r\\n0\\r\\n1\\r\\n1 2 3 4 5 6\\r\\nWeek\\r\\nSimulated\\r\\nFigure 2: Top panel: Half-hourly electricity demand for Victoria, Australia, with a daily pattern\\r\\nof frequency 48 and a weekly pattern of frequency 336. Bottom panel: Simulated time series\\r\\nwith the same seasonal periods and the closest Euclidean distance in features to the real data\\r\\nshown in the top panel.\\r\\n10\\nseasonal patterns can be written as\\r\\nxt =\\r\\nX\\r\\nM\\r\\nm=1\\r\\nωmxFm,t,\\r\\nwhere m = 1, 2, . . . , M, xFm,t is the mth simulated time series with frequency Fm, and weight\\r\\nωm satisfies PM\\r\\nm=1 ωm = 1 and 0 < ωm < 1. The weights can be obtained by\\r\\nωm =\\r\\nγm PM\\r\\nr=1 γr\\r\\n, where γm ∼ U(0, 1).\\r\\n3 Diversity and coverage of generated time series\\r\\n3.1 Time series features\\r\\nA feature Fk can be any kind of function computed from a time series {x1, . . . , xn}. Examples\\r\\ninclude a simple mean, the parameter of a fitted model, or some statistic intended to highlight\\r\\nan attribute of the data.\\r\\nA unique “best” feature representation of a time series does not exist (Fulcher, 2018). What\\r\\nfeatures are used depends on both the nature of the time series being analyzed, and the purpose\\r\\nof the analysis. For example, consider the mean as a simple time series feature. If some time\\r\\nseries contain unit roots, then the mean is not a meaningful feature without some additional\\r\\nconstraints on the initial values of the time series. Even if the series are all stationary, if the\\r\\npurpose of our analysis is to identify the best forecasting method, then the mean is probably\\r\\nof no value. On the other hand suppose we are monitoring the CPU usage every minute for\\r\\nnumerous servers, and we observe a daily seasonality. Then provided all our time series begin\\r\\nat the same time and are of the same length, the mean provides useful comparative information\\r\\nabout the CPU over- or under-utilization in a system despite the time series not being stationary.\\r\\nHowever, if one is interested in the peak performance of a super computer, the maximum of\\r\\nCPU load is a more desirable feature. This example shows that it is difficult to formulate\\r\\ngeneral desirable properties of features without knowledge of both the time series properties\\r\\nand the required analysis. We encourage analysts using time series features to consider these\\r\\nthings before computing many possibly unhelpful or even misleading features.\\r\\nBecause we are studying collections of time series of different lengths, on different scales,\\r\\nand with different properties, we restrict our features to be ergodic, stationary and independent\\r\\nof scale. Specifically, we consider the set of 26 diverse features shown in Table 3. Some features\\r\\nare from previous studies (Wang et al., 2006; Fulcher and Jones, 2014; Kang et al., 2014,\\r\\n2015; Hyndman et al., 2015; Kang et al., 2017), and some are new features that we believe\\r\\n11\\nTable 3: The features we use to characterize a time series.\\r\\nFeature Name Description Range\\r\\nF1 length Length of the time series [1, ∞)\\r\\nF2 nPeriods Number of seasonal periods [1, ∞)\\r\\nF3 Periods Vector of seasonal periods {1, 2, 3, . . . }\\r\\nF4 ndiffs Number of differences for stationarity {0, 1, 2, . . . }\\r\\nF5 nsdiffs Number of seasonal differences for\\r\\nstationarity\\r\\n{0, 1, 2, . . . }\\r\\nF6 (x.acf1, x.acf10, diff1.acf1,\\r\\ndiff1.acf10, diff2.acf1, diff2.acf10,\\r\\nseas.acf1)\\r\\nVector of autocorrelation coefficients (-1, 1) or (0, ∞)\\r\\nF7 (x.pacf5, diff1.pacf5, diff2.pacf5,\\r\\nseas.pacf)\\r\\nVector of partial autocorrelation coefficients (-1, 1) or (0, ∞)\\r\\nF8 entropy Spectral entropy (0, 1)\\r\\nF9 nonlinearity Nonlinearity coefficient [0, ∞)\\r\\nF10 hurst Long-memory coefficient [0.5, 1]\\r\\nF11 stability Stability (0, ∞)\\r\\nF12 lumpiness Lumpiness [0, ∞)\\r\\nF13 (unitroot.kpss, unitroot.pp) Vector of unit root test statistics to indicate\\r\\nthe strength of non-stationarity\\r\\n(0, ∞) or (−∞, ∞)\\r\\nF14 (max.level.shift, time.level.shift) Maximum level shift (0, ∞)\\r\\nF15 (max.var.shift, time.var.shift) Maximum variance shift (0, ∞)\\r\\nF16 (max.kl.shift, time.kl.shift) Maximum shift in Kulback-Leibler divergence (0, ∞)\\r\\nF17 trend Strength of trend [0, 1)\\r\\nF18 seasonal.strength Strength of seasonality [0, 1)\\r\\nF19 spike Spikiness [0, 1)\\r\\nF20 linearity Linearity (−∞, ∞)\\r\\nF21 curvature Curvature (−∞, ∞)\\r\\nF22 (e.acf1, e.acf10) Vector of autocorrelation coefficients of\\r\\nremainder\\r\\n(-1, 1) or (0, ∞)\\r\\nF23 arch.acf Heterogeneity measure by ARCH ACF\\r\\nstatistic to indicate ARCH effects\\r\\n(0, ∞)\\r\\nF24 garch.acf Heterogeneity measure by GARCH ACF\\r\\nstatistic to indicate GARCH effects\\r\\n(0, ∞)\\r\\nF25 arch.r2 Heterogeneity measure by ARCH R2 statistic\\r\\nto indicate ARCH effects\\r\\n[0, 1]\\r\\nF26 garch.r2 Heterogeneity measure by GARCH R2\\r\\nstatistic to indicate GARCH effects\\r\\n[0, 1]\\r\\nprovide useful information about our data. Our new features are intended to measure attributes\\r\\nassociated with multiple seasonality, non-stationarity and heterogeneity of the time series. The\\r\\nfeatures are defined in the Appendix A.1. All features are computed using the tsfeatures\\r\\npackage (Hyndman et al., 2019) in R (R Core Team, 2018), which nicely handles missing values\\r\\nwhile calculating the feature vector for a given time series.\\r\\nLittle previous study has used features for multiple seasonal time series. In multiple seasonal\\r\\ntime series, there is more than one seasonal period present in the data; for example, hourly\\r\\nelectricity demand data contains a time-of-day pattern (with seasonal period 24), a time-of\\x02week pattern (with seasonal period 7 × 24 = 168) and a time-of-year pattern (with seasonal\\r\\nperiod 365 × 24 = 8760). If there are M possible seasonal periods, then F2 = M and F3 is\\r\\nan M-vector containing the seasonal periods. For example, with monthly data F3 = 12, and\\r\\nwith hourly data F3 = (24, 168, 8760)0. The strength of seasonality (F18) is also an M-vector\\r\\n12\\nTable 4: Benchmarking datasets used for comparison with the simulated series from MAR\\r\\nmodels. The number of series is shown per dataset and seasonal pattern.\\r\\nDataset R package Yearly Quarterly Monthly Weekly Daily\\r\\nM1 Mcomp 181 203 617 – –\\r\\nM3 Mcomp 645 756 1428 –\\r\\nM4 M4comp2018 23000 24000 48000 359 4227\\r\\nTourism Tcomp 518 427 366 – –\\r\\nNNGC1 tscompdata 11 11 11 11 11\\r\\ncontaining separate measures of the strength of seasonality for each of the seasonal periods.\\r\\nWe investigate the diversity and coverage of the generated time series data based on MAR\\r\\nmodels by comparing the feature space of the simulated data with feature spaces of several\\r\\nbenchmarking time series datasets, including those from the M1, M3, M4, Tourism and NNGC1\\r\\nforecasting competitions. The R packages they come from, and the numbers of yearly, quarterly,\\r\\nmonthly, weekly and daily time series in each dataset are shown in Table 4.\\r\\n3.2 Diversity and coverage analysis\\r\\nFirst, we analyze the feature diversity from a marginal perspective. Figure 3 depicts the feature\\r\\ndiversity and coverage for simulated yearly, quarterly, monthly and weekly time series compared\\r\\nto the benchmarks for all the possible features we use in the paper. The features in our generated\\r\\ndata are diverse in the sense that (i) the shapes of the feature plots for the simulated data widely\\r\\nmatch to the theoretical ranges of features given in Table 3, and (ii) the quantiles of the features\\r\\nfor the simulated data cover the shapes of all features in the benchmarking data.\\r\\nTo better understand the feature space, Kang et al. (2017) use principal component analysis\\r\\n(PCA) to project the features to a 2-dimensional “instance space” for visualization. A major\\r\\nlimitation of any linear dimension reduction method is that it puts more emphasis on keeping\\r\\ndissimilar data points far apart in the low dimensional space. But in order to represent high\\r\\ndimensional data in a low dimensional, nonlinear manifold, it is also important that similar data\\r\\npoints are placed close together. When there are many features and nonlinear correlations are\\r\\npresent, using a linear transformation of the features may be misleading. Therefore, we use t\\x02Stochastic Neighbor Embedding (t-SNE), a nonlinear technique capable of retaining both local\\r\\nand global structure of the data in a single map, to conduct the nonlinear dimension reduction\\r\\nof the high dimensional feature space (Maaten and Hinton, 2008).\\r\\nFigure 4 shows a comparison of PCA and t-SNE for the M3 data. The top row of the plot\\r\\nshows the distribution of the seasonal period period in the t-SNE and PCA spaces, while the\\r\\nbottom row shows that of the spectral entropy feature entropy. It can be seen that t-SNE is\\r\\n13\\nFigure 3: Plots showing feature diversity and coverage of the simulated yearly, quarterly,\\r\\nmonthly, and weekly time series compared with the benchmarking data M1, M3, M4, NNGC1\\r\\nand Tourism. Boxplots are used for features with continuous values while percentage bar charts\\r\\nfor discrete cases. In all the plots, the same order of the datasets is used.\\r\\n14\\nFigure 4: Two-dimensional spaces of M3 data based on t-SNE (left) and PCA (right). Comp1\\r\\nand Comp2 are the first two components after dimension reduction using t-SNE and PCA. The\\r\\ntop row shows the distribution of period in the two spaces, while the bottom row shows that of\\r\\nentropy.\\r\\n15\\nFigure 5: Two-dimensional t-SNE spaces of the simulated yearly, quarterly, monthly, and weekly\\r\\ntime series, together with the time series with the same seasonal patterns from the M1, M3,\\r\\nM4, NNGC1 and Tourism datasets. Comp1 and Comp2 are the first two components after\\r\\ndimension reduction using t-SNE.\\r\\nbetter able to capture the nonlinear structure in the data, while the linear transformation of\\r\\nPCA leads to a large proportion of information loss, especially when more features are used.\\r\\nThe distribution of other features can be studied similarly.\\r\\nThe simulated yearly, quarterly, monthly and weekly time series are projected into a two\\x02dimensional feature space together with the yearly, quarterly, monthly and weekly time series in\\r\\nthe benchmarking datasets, as shown in Figure 5. Time series with different seasonal patterns\\r\\nare shown in separate panels of Figure 5 to make the comparisons easier.\\r\\nGiven the two-dimensional feature spaces of dataset A and dataset B, we quantify the\\r\\nmiscoverage of dataset A over dataset B in the following steps:\\r\\n1. Find the maximum ranges of the x and y axes reached by the combined datasets A and\\r\\nB, and cut the x and y dimensions into Nb = 30 bins.\\r\\n16\\n2. In the constructed two-dimensional grid with N2\\r\\nb = 900 subgrids, we denote\\r\\nIi,A =\\r\\n\\uf8f1\\r\\n\\uf8f4\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f4\\uf8f3\\r\\n0 if no time series in dataset A fall into the ith subgrid;\\r\\n1 otherwise.\\r\\nAn analogous definition of Ii,B applies for dataset B.\\r\\n3. The miscoverage of dataset A over dataset B is defined as\\r\\nmiscoverageA/B = N\\r\\n−2\\r\\nb\\r\\nX\\r\\nNb\\r\\ni=1\\r\\n[(1 − Ii,A) × Ii,B]. (5)\\r\\nOne could use (1 − miscoverageA/B) to infer the relative diversity of dataset A over the\\r\\ndataset B. Specifically, if dataset A is the simulated time series and dataset B is a rich collection\\r\\nof real time series, (1 − miscoverageA/B) can also be used as a measure of how realistic of the\\r\\nsimulated time series set A by taking the real dataset B as the ground truth.\\r\\nTable 5 shows the pairwise miscoverage values of benchmarking dataset A over B. Again,\\r\\ntime series with different seasonal patterns are shown separately. The miscoverage values of\\r\\nthe simulated dataset from the DGP over others are always smaller than that of others over\\r\\nthe DGP. Focusing on the M4 data, the most comprehensive time series competition data to\\r\\ndate, the miscoverage values of the DGP over M4 are 0.017, 0.013, 0.030 and 0.001 for yearly,\\r\\nquarterly, monthly and weekly data, respectively. On the other hand, the miscoverage values\\r\\nof M4 over DGP are substantially higher. Therefore, together with Figure 5, we find that\\r\\nthe simulated data from MAR models bring more diversity than the existing benchmarking\\r\\ndatasets. Therefore, Figure 5 and Table 5 present two useful tools for a researcher to screen\\r\\nthe diversity of simulated time series and their realisticity compared to a collection of real time\\r\\nseries.\\r\\nThe coverage analysis indicates the possibility of generating time series in a certain feature\\r\\nspace. From Figure 5, one may notice that some areas of the feature space contain more\\r\\ngenerated time series than other areas with the default parameter settings in Table 1. Possible\\r\\nways to allow the time series to be more evenly distributed in the feature space are either to\\r\\nadapt the parameter setting in Table 1 to make the generator focus more on the sparse area,\\r\\nor enlarge the number of simulations and incorporate with an accept-reject scheme, or use the\\r\\ntime series generation method with target features proposed in Section 4.\\r\\n17\\nTable 5: Miscoverage of dataset A over dataset B. Take the yearly section for example, the\\r\\nmiscoverage of simulated data over M4 is 0.017, while the miscoverage of M4 over simulated\\r\\ndata is 0.053.\\r\\nDataset A Dataset B\\r\\nDGP M4 M3 M1 Tourism NNGC1\\r\\nYearly\\r\\nDGP 0.000 0.017 0.001 0.001 0.000 0.001\\r\\nM4 0.053 0.000 0.001 0.001 0.000 0.000\\r\\nM3 0.609 0.550 0.000 0.041 0.106 0.008\\r\\nM1 0.680 0.629 0.113 0.000 0.107 0.008\\r\\nTourism 0.622 0.568 0.108 0.033 0.000 0.007\\r\\nNNGC1 0.720 0.669 0.126 0.052 0.124 0.000\\r\\nQuarterly\\r\\nDGP 0.000 0.013 0.000 0.000 0.000 0.000\\r\\nM4 0.079 0.000 0.001 0.000 0.001 0.000\\r\\nM3 0.567 0.536 0.000 0.043 0.096 0.009\\r\\nM1 0.651 0.616 0.149 0.000 0.110 0.009\\r\\nTourism 0.660 0.621 0.200 0.112 0.000 0.010\\r\\nNNGC1 0.769 0.729 0.243 0.132 0.137 0.000\\r\\nMonthly\\r\\nDGP 0.000 0.030 0.001 0.000 0.002 0.000\\r\\nM4 0.061 0.000 0.003 0.001 0.000 0.000\\r\\nM3 0.488 0.446 0.000 0.020 0.052 0.002\\r\\nM1 0.566 0.523 0.131 0.000 0.063 0.004\\r\\nTourism 0.654 0.602 0.251 0.162 0.000 0.006\\r\\nNNGC1 0.716 0.669 0.288 0.204 0.084 0.000\\r\\nWeekly\\r\\nDGP 0.000 0.001 – – – 0.000\\r\\nM4 0.456 0.000 – – – 0.007\\r\\nM3 – – – – – –\\r\\nM1 – – – – – –\\r\\nTourism – – – – – –\\r\\nNNGC1 0.576 0.138 – – – 0.000\\r\\n18\\n4 Efficient time series generation with target features\\r\\n4.1 Tuning a MAR model with target features\\r\\nIn time series analysis, researchers with a particular focus may be only interested in a certain\\r\\narea of the feature space, or a subset of features, e.g., heteroscedasticity and volatility in financial\\r\\ntime series, trend and entropy in time series forecasting, or peaks and spikes in energy time\\r\\nseries. Practitioners may also want to mimic more time series from their limited collection of\\r\\nreal data such as sales records for new products and health metrics for a rare disease. Therefore,\\r\\nefficient generation of time series with target features of interest is another important problem\\r\\nto address. For a review of the relevant literature, see Kang et al. (2017).\\r\\nKang et al. (2017) use a genetic algorithm (GA) to evolve time series of length n that project\\r\\nto the target feature point F˜ in the two-dimensional instance space as closely as possible. At\\r\\neach iteration of the GA, a combination of selection, crossover and mutation is applied over\\r\\nthe corresponding population to optimize the n points of the time series to be evolved. The\\r\\ncomputational complexity grows linearly as the length of time series increases. We document\\r\\nthe details of the genetic algorithm in Appendix A.4.\\r\\nIn this paper, instead of evolving time series of length n (i.e., optimization in an n-dimension\\r\\nspace), we use a GA to tune the MAR model parameters until the distance between the target\\r\\nfeature vector and the feature vector of a sample of time series simulated from the MAR is\\r\\nclose to zero. The parameters for the MAR model, the underlying DGP, can be represented\\r\\nas a vector Θ = {αk, φi} for k = 1, . . . , K and i = k0, . . . , kpk in Equation (1). A significant\\r\\nimprovement compared to Kang et al. (2017) is that the length of the vector Θ is much smaller\\r\\nthan n, so that the tuning process can be performed efficiently in a much lower-dimensional\\r\\nparameter space. The GA optimization steps are summarized below, for a specified period\\r\\nPeriod and length n for the desired time series.\\r\\n1. Select a target feature point F˜ in the feature space or let F˜ be the extracted features\\r\\nfrom existing time series that one wants to match. Now we aim to find a parameter vector\\r\\nΘ∗that can evolve a time series XF˜ with its feature vector F as close as possible to the\\r\\ntarget feature point F˜.\\r\\n2. Generate an initial population of size NP for the parameter vector Θ, in which each\\r\\nparameter is chosen from a uniform distribution as given in Table 1. That allows the\\r\\nentire range of possible solutions of Θ to be reached randomly.\\r\\n3. For each iteration, repeat the following steps until some stopping criteria are met.\\r\\n19\\n(a) For each member in the current population, simulate a time series j and calculate\\r\\nits feature vector Fj .\\r\\n(b) Calculate the fitness value for each member:\\r\\nFitness(j) = −\\r\\n1\\r\\nc\\r\\nkFj − F˜k,\\r\\nwhere c is a scaling constant usually defined as c = kF˜k and k·k is a distance measure\\r\\nfor the feature space. We find that Euclidean distance works well in our algorithm.\\r\\n(c) Produce the new generation based on the crossover, mutation and the survival of the\\r\\nfittest individual to improve the average fitness value of each generation.\\r\\n4. Keep the time series that is closest to the target feature point (i.e., has the largest fitness\\r\\nvalue) to be the newly generated time series for the corresponding target.\\r\\nOne may want to mimic very short time series. In principle, GRATIS could be equally\\r\\nwell used if the features of target series are available. Nonetheless, if the feature calculation\\r\\ninvolves lagged values, the length of time series should be at least greater than the length of\\r\\nlags. Besides, expert information can be also provided through the settings in Table 1, which is\\r\\nparticularly useful when one has very limited historical data or one is interested in a particular\\r\\nsetting of features.\\r\\n4.2 Efficiency analysis\\r\\nTable 6 shows the computational time for generating different time series with different sets of\\r\\nfeatures. Lengths are chosen in the same way as for Table 2. The target feature vectors used\\r\\nare median values of the selected features of the simulated data with the same seasonal pattern\\r\\nand similar lengths. The algorithm used in Kang et al. (2017) takes about 22,000 seconds to\\r\\nevolve 100 time series of length 100 with 6 features. For this similar task, but with twice as\\r\\nmany features, our algorithm is about 40 times faster on average. This speedup allows us to\\r\\ngenerate time series with controllable features in a reasonable time.\\r\\n20\\nTable 6: Computational time (in seconds) for generation of 100 yearly, quarterly, monthly and\\r\\nweekly time series. Feature set A consists of ndiffs, x acf1, entropy and trend. Feature set\\r\\nB consists of Feature set A, diff1 acf1, seasonal strength, seas pacf and e acf1. Feature set C\\r\\nconsists of Feature set B, e acf10, unitroot kpss, linearity and garch r2. Median values of the\\r\\nselected features of the simulated data with the same seasonal pattern and similar lengths are\\r\\nused as the targets.\\r\\nYearly Quarterly Monthly Weekly\\r\\nLength Time (s) Length Time (s) Length Time (s) Length Time (s)\\r\\nFeature set A (four features)\\r\\n20 53 60 78 80 62 350 124\\r\\n30 40 90 71 200 74 900 182\\r\\n40 40 120 87 300 132 1600 265\\r\\nFeature set B (eight features)\\r\\n20 43 60 130 80 524 350 3001\\r\\n30 119 90 319 200 480 900 3395\\r\\n40 101 120 405 300 1340 1600 3674\\r\\nFeature set C (twelve features)\\r\\n20 180 60 650 80 1190 350 1655\\r\\n30 550 90 530 200 349 900 1360\\r\\n40 202 120 1160 300 725 1600 1573\\r\\n5 Application to time series forecasting\\r\\nIn general, our time series generation scheme, GRATIS, can serve as a useful resource for various\\r\\nadvanced time series analysis, such as time series forecasting and classification. For illustration\\r\\npurposes, we present a novel time series forecasting approach by exploiting the generated time\\r\\nseries. It is worth mentioning that the construction of such a large-scale and high-quality\\r\\ndatabase is efficient and does not rely on traditional data collection methods.\\r\\n5.1 Forecasting based on feature spaces in generated time series\\r\\nThe No-Free-Lunch theorem states there is never universally a best method that fits in all\\r\\nsituations in machine learning and optimization (Wolpert, 1996; Wolpert and Macready, 1997).\\r\\nThis idea also applies to the context of time series forecasting, in which no single forecasting\\r\\nmethod stands out as best for any type of time series, e.g., (Adam, 1973; Collopy and Armstrong,\\r\\n1992; Wang et al., 2009; Petropoulos et al., 2014; Kang et al., 2017). Adam (1973) showed that\\r\\nthe statistical characteristics of each time series are related to the accuracy of each forecasting\\r\\nmethod. Later literature, e.g., (Adam, 1973; Shah, 1997; Meade, 2000) tend to support this\\r\\nargument. An ideal case is that one can select the best forecast model for each series in a dataset\\r\\naccording to its features. Another way is to calculate the averaging weights of the forecasts from\\r\\nindividual forecast models for each time series based on its features.\\r\\nWe aim to examine how those features influence forecasting method performance through\\r\\n21\\nsimulations from mixture autoregressive models, which enables us to predict the performances\\r\\nof the forecasting methods on the candidate time series data, and select the best forecasting\\r\\nmethod or average different forecasting methods with proper weights. We describe the complete\\r\\ndiagram in Figure 6. The task details are partitioned into training and testing procedures as\\r\\nbelow.\\r\\n1. Training on generated time series:\\r\\n(a) Simulate 10, 000 yearly, quarterly and monthly time series as the training time series\\r\\nwith GRATIS. The forecasting horizon h are set as 6, 8, 18 for yearly, quarterly and\\r\\nmonthly data, respectively.\\r\\n(b) Calculate the training feature matrix F\\r\\n(train)\\r\\nfor the historical data.\\r\\n(c) Calculate the out-of-sample MASE(train)(Mean Absolute Scaled Error; Hyndman\\r\\nand Koehler, 2006) values on the forecasting horizon for nine commonly used time\\r\\nseries forecasting methods.\\r\\n(d) Model the relationship between the feature matrix F\\r\\n(train) and MASE(train) with\\r\\nnonlinear regression models.\\r\\n2. Forecasting on testing time series:\\r\\n(a) Calculate the feature matrix F\\r\\n(test)\\r\\n.\\r\\n(b) Predict the best forecasting method that minimizes the predicted MASE d (test)for\\r\\neach time series. Or use a model averaging procedure, that is, calculate the weight\\r\\nof mth model (ϑm) as\\r\\nϑm =\\r\\nexp (1/MASE d 3\\r\\nk\\r\\n)\\r\\nPM\\r\\nm=1 exp (1/MASE d 3\\r\\nm)\\r\\n.\\r\\nwhere MASE d m is the predicted MASE value from the nonlinear regression model in\\r\\nFigure 6.\\r\\nWe use MASE as the measure of forecasting accuracy because it is stated in Hyndman\\r\\nand Koehler (2006) as a “ generally applicable measurement of forecast accuracy without the\\r\\nproblems seen in the other measurements”. It has also been shown that MASE does not sub\\x02stantially affect the main conclusions about the best-performing methods in the M3 competition\\r\\ndata (Hyndman and Koehler, 2006). It is worth mentioning that, in principle, our method is\\r\\ngeneral and can be incorporated with any other forecasting measures. All steps except step 1.(d)\\r\\n22\\n\"×$\\r\\n(&\\'()*)\\r\\n*×$\\r\\n(&,-&)  2 *×3\\r\\n(&,-&) = 6 *×$\\r\\n&,-&\\r\\nh\\r\\n\"×3\\r\\n(&\\'()*)\\r\\nFor the i-th time series, forecast model selection\\r\\nand model averaging are performed based on the i\\x02th row of  2 *×3\\r\\n(&,-&)\\r\\n.\\r\\n\"×3\\r\\n(&\\'()*)\\r\\n=\\t \"×$\\r\\n&\\'()* + \\r\\nh\\r\\nTraining on generated N time series Forecasting on n testing time series\\r\\nM forecasting\\r\\nmethods\\r\\nARIMA\\r\\nETS\\r\\nNNET-AR\\r\\nTBATS\\r\\nSTL-AR\\r\\nRW-DRIFT\\r\\nTHETA\\r\\nNAIVE\\r\\nSNAIVE\\r\\n…\\r\\nFigure 6: Forecasting diagram based on generated time series. The notion N is the number of\\r\\ntraining time series, P is the number of time series features, M is the number of forecasting\\r\\nmethods, n is the number of testing series, and g(·) describes the nonlinear relationship between\\r\\nMASE values and time series features.\\r\\ncan be parallelized to reduce the computation burden and our R package gratis is designed for\\r\\nparallel computing. In step 1.(d), we find that quantile regression with a lasso penalty works\\r\\nthe best for seasonal data, i.e., quarterly and monthly data. We document the model details\\r\\nin Appendix A.3, whilst multivariate adaptive spline regression models (Friedman et al., 1991)\\r\\nwork well for non-seasonal data, i.e., yearly data in our case.\\r\\nThis modeling approach with GRATIS is novel in several ways: (i) the training process\\r\\nis done purely on generated time series and does not involve the candidate time series, (ii) it\\r\\ndoes not require us to try all potential models on the testing time series which makes it highly\\r\\ncomputationally efficient when the testing data is very large, (iii) it does not require real data\\r\\ninput in the training process, and our trained model can serve as a general pre-trained time series\\r\\nforecasting algorithm selector, and (iv) most importantly, the whole procedure only requires the\\r\\ntransformed nonsensitive time series features instead of real time series as the forecasting input.\\r\\nThis is particularly useful when privacy is a concern.\\r\\n5.2 Application to M3 data\\r\\nFor demonstration purposes, we apply our pretrained models to the yearly, quarterly and\\r\\nmonthly data in M3. We consider nine commonly used forecasting methods (Hyndman and\\r\\nAthanasopoulos, 2018): automated ARIMA algorithm (ARIMA), automated exponential smooth\\x0223\\nTable 7: Comparison of the MASE values from feature-based forecasting using GRATIS and\\r\\nthe other individual nine methods on the M3 data. Simple averaging shows the MASE values\\r\\nbased on the mean of the nine forecast values from the individual methods.\\r\\nForecasting Method Yearly Quarterly Monthly Overall\\r\\nBenchmarking\\r\\nARIMA 1.898 0.841 0.710 0.855\\r\\nETS 1.907 0.855 0.712 0.869\\r\\nNNET-AR 2.338 1.021 0.828 1.037\\r\\nTBATS 1.900 0.914 0.699 0.869\\r\\nSTLM-AR 2.625 1.688 1.010 1.326\\r\\nRW-DRIFT 1.929 0.988 0.894 1.046\\r\\nTHETA 1.985 0.831 0.721 0.869\\r\\nNAIVE 2.267 1.044 0.927 1.135\\r\\nSNAIVE 2.267 1.176 0.969 1.146\\r\\nSimple Averaging 1.964 0.834 0.736 0.872\\r\\nGRATIS\\r\\nModel Selection 1.713 0.792 0.694 0.831\\r\\nModel Averaging 1.759 0.784 0.688 0.807\\r\\ning algorithm (ETS), NNET-AR model applying a feed-forward neural network using autore\\x02gressive inputs (NNET-AR), TBATS model (Exponential Smoothing State Space Model With\\r\\nBox-Cox Transformation, ARMA Errors, Trend And Seasonal Components), Seasonal and\\r\\nTrend decomposition using Loess with AR modeling of the seasonally adjusted series (STL\\x02AR), random walk with drift (RW-DRIFT), theta method (THETA), naive (NAIVE), and\\r\\nseasonal naive (SNAIVE).\\r\\nFor each time series, forecasting method selection is performed according to the smallest\\r\\npredicted MASE values. For comparison purposes, we also calculate the MASE values for all\\r\\nnine forecasting algorithms applied to each of M3 time series. Table 7 depicts the median of\\r\\nthe MASE values for our method selection and averaging procedures, and the nine individual\\r\\nmethods on each group of time series in M3. The MASE values from model selection show\\r\\nthat our feature-based forecasting scheme trained with generated time series gives the lowest\\r\\nforecasting MASE in all individual groups and the overall dataset. Under the model averaging\\r\\nscheme, our feature-based forecasting scheme trained with generated time series further increases\\r\\nforecasting accuracy in terms of MASE. Table 8 shows the model selection proportions and\\r\\nmodel averaging weights on M3 with GRATIS.\\r\\nFigure 7 depicts the distribution of MASE with boxplots for yearly, quarterly, monthly and\\r\\noverall data in M3. Our proposed method is not only the best on average but also avoids very\\r\\nlarge deviations, compared to other nine methods. Figure 8 shows the projection of the best\\r\\npredicted forecasting method by our feature-based model selection algorithm on two-dimensional\\r\\nfeature spaces for the yearly, quarterly, monthly data in M3, from which one may observe distinct\\r\\n24\\nTable 8: Model selection proportions and model averaging weights on M3 with GRATIS.\\r\\nIn each column, “% ” means the percentage of times the corresponding method is selected in\\r\\nfeature-based model selection and ϑ means the median weight for each method used in feature\\x02based model averaging.\\r\\nYearly Quarterly Monthly Overall\\r\\nForecasting Method Selection (%) Averaging (ϑ) Selection (%) Averaging (ϑ) Selection (%) Averaging (ϑ) Selection (%) Averaging (ϑ)\\r\\nARIMA 26.8 0.157 41.3 0.180 55.6 0.188 45.2 0.180\\r\\nETS 14.3 0.131 7.1 0.155 4.3 0.138 7.4 0.139\\r\\nNNET-AR 1.2 0.064 1.3 0.060 0.7 0.096 1.0 0.076\\r\\nTBATS 9.9 0.128 5.3 0.139 9.0 0.130 8.2 0.131\\r\\nSTLM-AR 3.1 0.045 1.2 0.024 4.6 0.058 3.3 0.045\\r\\nRW-DRIFT 20.5 0.131 29.1 0.138 13.9 0.077 19.4 0.106\\r\\nTHETA 16.7 0.120 5.4 0.087 2.0 0.084 6.3 0.092\\r\\nNAIVE 7.4 0.078 7.9 0.080 6.6 0.080 7.1 0.080\\r\\nSNAIVE – 0.078 1.3 0.057 3.4 0.064 2.1 0.065\\r\\npatterns exist. For example, random walk with drift (RW-DRIFT) performs the best for the\\r\\nquarterly time series lying in the middle of the feature space, while ARIMA and TBATS models\\r\\nworks well for the monthly data lying in the left region of the feature space.\\r\\nIt is worth mentioning that although our generated time series dataset is based on mixtures\\r\\nof autoregressive models, the ARIMA models are not always selected as the best in the generated\\r\\ndata, providing further evidence that our generated time series are diverse. This is due to the\\r\\nflexibility of mixtures and our random parameter settings in the generator in Table 1. In order to\\r\\ngain further insight into our forecasting approach, we fit a MAR to each candidate series of the\\r\\nyearly data from M3 and estimate the model parameters using maximum likelihood estimation.\\r\\nThe median of the resulting MASE is 2.311 which is much greater than the MASE values from\\r\\nmodel selection and model averaging in Table 7. This indicates that MAR models may not be\\r\\nadequate to model each series, but they are capable of producing a suitably rich collection of\\r\\ntime series to allow one to predict when a given forecasting method is likely to perform well\\r\\naccording to the time series characteristics.\\r\\n25\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n−4\\r\\n−2\\r\\n0\\r\\n2\\r\\n4\\r\\nARIMA\\r\\nETS\\r\\nNNET−AR\\r\\nTBATS\\r\\nSTLM−AR\\r\\nRW−DRIFT\\r\\nTHETA\\r\\nNAIVE\\r\\nSNAIVE\\r\\nSimple Averaging\\r\\nModel Selection\\r\\nModel Averaging\\r\\nlog(MASE)\\r\\nYearly\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n−4\\r\\n−2\\r\\n0\\r\\n2\\r\\n4\\r\\nARIMA\\r\\nETS\\r\\nNNET−AR\\r\\nTBATS\\r\\nSTLM−AR\\r\\nRW−DRIFT\\r\\nTHETA\\r\\nNAIVE\\r\\nSNAIVE\\r\\nSimple Averaging\\r\\nModel Selection\\r\\nModel Averaging\\r\\nQuarterly\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n−4\\r\\n−2\\r\\n0\\r\\n2\\r\\n4\\r\\nARIMA\\r\\nETS\\r\\nNNET−AR\\r\\nTBATS\\r\\nSTLM−AR\\r\\nRW−DRIFT\\r\\nTHETA\\r\\nNAIVE\\r\\nSNAIVE\\r\\nSimple Averaging\\r\\nModel Selection\\r\\nModel Averaging\\r\\nlog(MASE)\\r\\nMonthly\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n−4\\r\\n−2\\r\\n0\\r\\n2\\r\\n4\\r\\nARIMA\\r\\nETS\\r\\nNNET−AR\\r\\nTBATS\\r\\nSTLM−AR\\r\\nRW−DRIFT\\r\\nTHETA\\r\\nNAIVE\\r\\nSNAIVE\\r\\nSimple Averaging\\r\\nModel Selection\\r\\nModel Averaging\\r\\nOverall\\r\\nFigure 7: Boxplot of log scaled MASE values for yearly, quarterly, monthly and overall data in\\r\\nM3.\\r\\n26\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n● −2 ●\\r\\n−1\\r\\n0\\r\\n1\\r\\n2\\r\\n−2 −1 0 1\\r\\nComp2\\r\\nYearly\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n● ●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n−2\\r\\n−1\\r\\n0\\r\\n1\\r\\n2\\r\\n−2 −1 0 1 2\\r\\nComp1\\r\\nQuarterly\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ● ●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n● ●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●● ●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●●\\r\\n●\\r\\n●●\\r\\n●●●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n● ●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●\\r\\n●●\\r\\n●\\r\\n●\\r\\n●\\r\\n−2\\r\\n−1\\r\\n0\\r\\n1\\r\\n2\\r\\n−2 −1 0 1 2\\r\\nMonthly\\r\\nMethod\\r\\n● ARIMA ETS NAIVE NNET−AR RW−DRIFT SNAIVE STLM−AR TBATS THETA ● ● ● ● ● ● ● ●\\r\\nFigure 8: Projection of best predicted forecasting method by our feature-based model selection\\r\\nalgorithm on two-dimensional feature spaces for the yearly, quarterly, monthly data in M3.\\r\\n6 Conclusions and discussion\\r\\nWe have proposed an efficient simulation method, GRATIS, for generating time series with\\r\\ndiverse characteristics requiring minimal input of human effort and computational resources.\\r\\nOur generated dataset can be used as benchmarking data in the time series domain, which\\r\\nfunctions similarly to other machine learning data repositories. The simulation method is\\r\\nbased on mixture autoregressive models where the parameters are assigned with statistical\\r\\ndistributions. In such a way, we provide a general benchmarking tool serving for advanced time\\r\\nseries analysis where a large collection of benchmarking data is required, including forecasting\\r\\ncomparison, model averaging, and time series model training with self-generated data. To the\\r\\nbest of our knowledge, this is the first paper that thoroughly studies the possibility of generating\\r\\na rich collection of time series. Our method not only generates realistic time series data but\\r\\nalso gives a higher coverage of the feature space than existing time series benchmarking data.\\r\\nThe GRATIS approach is also able to efficiently generate new time series with controllable\\r\\ntarget features, by tuning the parameters of MAR models. This is particularly useful in time\\r\\nseries classification or specific areas where only some features are of interest. This procedure\\r\\nis the inverse of feature extraction which usually requires much computational power. Our\\r\\napproach of generating new time series from given features can scale up the computation time\\r\\nby 40 times (compared to Kang et al., 2017) making feature-driven time series analysis tasks\\r\\nfeasible.\\r\\nWe further show that the GRATIS scheme can serve as a useful resource for time series\\r\\napplications. In particular, we present a novel time series forecasting approach by exploiting\\r\\nthe time series features of current generated time series. Our application also sheds light on a\\r\\n27\\npotential direction to forecasting with private data where the model training could be purely\\r\\nbased on our generated data. Other potential extensions include: (i) GRATIS with exogenous\\r\\ninformation via mixture of ARIMA with explanatory variables (ARIMAX) to allow for local\\r\\npatterns due to external events, (ii) GRATIS with multivariate time series by exploring mixtures\\r\\nof vector autoregression models, (iii) GRATIS with cross-sectional information about the time\\r\\nseries by exploring the approaches in Wang et al. (2008) and Ashouri et al. (2019), (iv) extending\\r\\nGRATIS to discrete time series by investigating the mixture of integer-valued autoregressive\\r\\nprocesses Latour, 1998 or Poisson autoregression (Freeland and McCabe, 2004), and (v) using\\r\\nGRATIS to serve as a pre-training process of deep learning methods to save time and improve\\r\\naccuracy.\\r\\n7 Acknowledgments\\r\\nThe authors are grateful to Associate Editor, two anonymous reviewers for their helpful com\\x02ments that improved the contents of this paper. Yanfei Kang and Feng Li are supported by\\r\\nthe National Natural Science Foundation of China (No. 11701022 and No. 11501587, respec\\x02tively). Rob J Hyndman is supported by the Australian Centre of Excellence in Mathematical\\r\\nand Statistical Frontiers.\\r\\nA Appendix\\r\\nA.1 Description of time series features\\r\\nIn this section, we document the feature details in Table 3. We have also developed an R\\r\\npackage tsfeatures to provide methods for extracting various features from time series data\\r\\nwhich is available at https://CRAN.R-project.org/package=tsfeatures.. Note that all of\\r\\nour features are ergodic for stationary and difference-stationary processes, and not dependent\\r\\non the scale of the time series. Thus, they are well-suited for applying to a large diverse set of\\r\\ntime series.\\r\\nEach time series, of any length, can be summarized as a feature vector F = (F1, F2, . . . , F26)\\r\\n0\\r\\n.\\r\\nThe length of this vector will be 42 for non-seasonal time series, and for seasonal time series with\\r\\na single seasonal period. For multiple seasonal time series, F will have a few more elements.\\r\\nThe first five features in Table 3 are all positive integers. F1 is the time series length. F2 is\\r\\nthe number of seasonal periods in the data (determined by the frequency of observation, not the\\r\\nobservations themselves) and set to 1 for non-seasonal data. F3 is a vector of seasonal periods\\r\\nand set to 1 for non-seasonal data. F4: the number of first-order differences required before the\\r\\n28\\ndata pass a KPSS stationarity test (Kwiatkowski et al., 1992) at the 5% level. F5 is the number\\r\\nof seasonal differences required before the data pass an OCSB test (Osborn et al., 1988) at the\\r\\n5% level. For multiple seasonal time series, we compute F5 using the largest seasonal period.\\r\\nFor non-seasonal time series (when F1 = 1), we set F5 = 0.\\r\\nWe compute the autocorrelation function of the series, the differenced series, and the twice\\x02differenced series. Then F6 is a vector comprising the first autocorrelation coefficient in each\\r\\ncase, and the sum of squares of the first 10 autocorrelation coefficients in each case. The\\r\\nautocorrelation coefficient of the original series at the first seasonal lag is also computed. For\\r\\nnon-seasonal data, this is set to 0.\\r\\nWe compute the partial autocorrelation function of the series, the differenced series, and the\\r\\nsecond-order differenced series. Then F7 is a vector comprising the sum of squares of the first\\r\\n5 partial autocorrelation coefficients in each case. The partial autocorrelation coefficient of the\\r\\noriginal series at the first seasonal lag is also computed. For non-seasonal data, this is set to 0.\\r\\nThe spectral entropy is the Shannon entropy\\r\\nF8 = −\\r\\nZ π\\r\\n−π\\r\\nˆf(λ) log ˆf(λ)dλ,\\r\\nwhere ˆf(λ) is an estimate of the spectral density of the data. This measures the “forecastability”\\r\\nof a time series, where low values of F8 indicate a high signal-to-noise ratio, and large values of\\r\\nF8 occur when a series is difficult to forecast.\\r\\nThe nonlinearity coefficient (F9) is computed using a modification of the statistic used in\\r\\nTer¨asvirta’s nonlinearity test. Ter¨asvirta’s test uses a statistic X2 = T log(SSE1/SSE0) where\\r\\nSSE1 and SSE0 are the sum of squared residuals from a nonlinear and linear autoregression\\r\\nrespectively. This is non-ergodic, so is unsuitable for our purposes. Instead, we define F9 =\\r\\nX2/T which will converge to a value indicating the extent of nonlinearity as T → ∞.\\r\\nWe use a measure of the long-term memory of a time series (F10), computed as 0.5 plus\\r\\nthe maximum likelihood estimate of the fractional differencing order d given by Haslett and\\r\\nRaftery (1989). We add 0.5 to make it consistent with the Hurst coefficient. Note that the\\r\\nfractal dimension can be estimated as D = 2 − F9.\\r\\nF11 and F12 are two time series features based on tiled (non-overlapping) windows. Means\\r\\nor variances are produced for all tiled windows. Then stability is the variance of the means,\\r\\nwhile lumpiness is the variance of the variances.\\r\\nF13 is a vector comprising the statistic for the KPSS unit root test with linear trend and lag\\r\\none, and the statistic for the “Z-alpha” version of PP unit root test with constant trend and\\r\\nlag one.\\r\\n29\\nThe next three features (F14, F15, F16) compute features of a time series based on sliding\\r\\n(overlapping) windows. F14 finds the largest mean shift between two consecutive windows. F15\\r\\nfinds the largest variance shift between two consecutive windows. F16 finds the largest shift in\\r\\nKulback-Leibler divergence between two consecutive windows.\\r\\nThe following six features (F17– F22) are modifications of features used in Kang et al.\\r\\n(2017). We extend the STL decomposition approach (Cleveland et al., 1990) to handle multiple\\r\\nseasonalities. Thus, the decomposition contains a trend, up to M seasonal components, and a\\r\\nremainder component:\\r\\nxt = ft + s1,t + · · · + sM,t + et\\r\\nwhere ftis the smoothed trend component, si,t is the ith seasonal component and etis a\\r\\nremainder component. The components are estimated iteratively. Let s\\r\\n(k)\\r\\ni,t be the estimate\\r\\nof si,t at the kth iteration, with initial values given as s\\r\\n(0)\\r\\ni,t = 0. Then we apply an STL\\r\\ndecomposition to xt −\\r\\nP j=1\\r\\nj6=i\\r\\nM\\r\\ns\\r\\n(k−1)\\r\\nj,t to obtained updated estimates s\\r\\n(k)\\r\\ni,t for k = 1, 2, . . .. In\\r\\npractice, this converges quickly and only two iterations are required. To allow the procedure to\\r\\nbe applied automatically, we set the seasonal window span for STL to be 21 in all cases. For a\\r\\nnon-seasonal time series (when F1 = 1), we simply estimate xt = ft + et where ftis computed\\r\\nusing Friedman’s “super smoother” (Friedman, 1984).\\r\\n• F17 and F18 are defined as\\r\\nF17 = 1 −\\r\\nVar(et)\\r\\nVar(ft + et)\\r\\nand F18,i = 1 −\\r\\nVar(et)\\r\\nVar(si,t + et)\\r\\n.\\r\\nIf their values are less than 0, they are set to 0, while values greater than 1 are set to\\r\\n1. For non-seasonal time series F18 = 0. For seasonal time series, F18 is an M-vector,\\r\\nwhere M is the number of periods. This is analogous to the way the strength of trend and\\r\\nseasonality were defined in Wang et al. (2006), Hyndman et al. (2015) and Kang et al.\\r\\n(2017).\\r\\n• F19 measures the “spikiness” of a time series, and is computed as the variance of the\\r\\nleave-one-out variances of the remainder component et.\\r\\n• F20 and F21 measures the linearity and curvature of a time series calculated based on the\\r\\ncoefficients of an orthogonal quadratic regression.\\r\\n• We compute the autocorrelation function of et, and F22 is a 2-vector containing the first\\r\\nautocorrelation coefficient and the sum of the first ten squared autocorrelation coefficients.\\r\\n30\\nThe remaining features measure the heterogeneity of the time series. First, we pre-whiten\\r\\nthe time series to remove the mean, trend, and autoregressive (AR) information (Barbour and\\r\\nParker, 2014). Then we fit a GARCH(1,1) model to the pre-whitened time series, xt, to measure\\r\\nfor autoregressive conditional heteroskedasticity (ARCH) effects. The residuals from this model,\\r\\nzt, are also measured for ARCH effects using a second GARCH(1,1) model.\\r\\n• F23 is the sum of squares of the first 12 autocorrelations of {x\\r\\n2\\r\\nt }.\\r\\n• F24 is the sum of squares of the first 12 autocorrelations of {z\\r\\n2\\r\\nt }.\\r\\n• F25 is the R2 value of an AR model applied to {x\\r\\n2\\r\\nt }.\\r\\n• F26 is the R2 value of an AR model applied to {z\\r\\n2\\r\\nt }.\\r\\nThe statistics obtained from {x\\r\\n2\\r\\nt } are the ARCH effects, while those from {z\\r\\n2\\r\\nt } are the\\r\\nGARCH effects. Note that the two R2 values are used in the Lagrange-multiplier test of Engle\\r\\n(1982), and the sum of squared autocorrelations are used in the Ljung-Box test proposed by\\r\\nLjung and Box (1978).\\r\\nA.2 Web application for time series generation\\r\\nWe implement our approach in a shiny app (Chang et al., 2018) as shown in Figure 9. Users\\r\\ncan first choose the features that they are interested in. After setting their desired time se\\x02ries seasonal pattern, length, the number of time series, and the feature values required, the\\r\\ngenerated series are displayed. In Figure 9, we aim to generate ten monthly time series with\\r\\nlength 120. The selected features are nsdiffs, x acf1, entropy, stability, trend, seasonal strength\\r\\nand garch r2. The corresponding target vector is (1, 0.85, 0.55, 0.73, 0.91, 0.95, 0.07)0. Following\\r\\nthe GA process, the generated time series are shown at the bottom of Figure 9. The simulated\\r\\ndata can also be downloaded to a local computer.\\r\\nA.3 Penalized linear quantile regression\\r\\nConsider a sample {yi, xi} for i = 1, ..., n where yiis the MASE values and xiis the feature\\r\\nvector in our framework. The conditional τ th quantile function is defined as fτ (xi) where\\r\\nτ = p(yi < fτ (Xi)|Xi = xi). Following Koenker and Bassett Jr (1978), we define the check\\r\\nfunction\\r\\nρτ (r) =\\r\\n\\uf8f1\\r\\n\\uf8f4\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f4\\uf8f3\\r\\nτ r r > 0;\\r\\n−(1 − τ )r otherwise.\\r\\n31\\nFigure 9: The shiny application generating time series with controllable features. The applica\\x02tion is designed with four tabs, named ‘Structure’, ‘Features’, ‘Visualise’ and ‘Download series’.\\r\\nThe top panel shows the options for the basic structure of the desired time series. For illustra\\x02tion, this interface illustrates the generation of ten monthly time series with length 120. The\\r\\nmiddle panel shows specification of the target feature vector, which consists of the non-zero val\\x02ues set for the features. The generated ten time series with the target features are shown at the\\r\\nbottom panel. The app is available at https://ebsmonash.shinyapps.io/tsgeneration/.\\r\\n32\\nThus the τ th conditional quantile regression with the adaptive LASSO penalty can be estimated\\r\\nby solving the following minimization problem\\r\\nminXn\\r\\ni=1\\r\\nρτ (yi − x\\r\\n0\\r\\niβ) + λ\\r\\nX\\r\\nd\\r\\nj=1\\r\\nωj |βj |\\r\\nfor some appropriately chosen λ and weights ωj .\\r\\nWith the aid of slack variables, the convex optimization problem can be equivalently written\\r\\ninto a linear programming problem which can be easily solved by standard optimization software.\\r\\nA comparison of existing algorithms can be found in Peng and Wang (2015).\\r\\nA.4 Genetic Algorithms\\r\\nGenetic algorithms are stochastic search algorithms which are able to solve optimization prob\\x02lems. GAs use evolutionary strategies inspired by the principles of biological evolution where\\r\\nparameters are treated as population. At a certain stage of evolution a population is composed\\r\\nof a number of chromosomes. Applying some mapping from the chromosome representation into\\r\\nthe decision variable space, which represents potential solutions to an optimization problem.\\r\\nA typical genetic algorithm requires three inputs, (i) type of variables (ii) fitness function,\\r\\nand (iii) convergence criteria which is usually the improvement of fitness function. Algorithm 1\\r\\ndocuments the evolution process of the genetic algorithms.\\r\\nAlgorithm 1: Genetic algorithms\\r\\nOutput: Evolved population\\r\\nInput : Initial population; Fitness function; Convergence criteria\\r\\nwhile the convergence criteria not met do\\r\\n• Run the genetic operators\\r\\n1. Selection: mimics the behavior of natural organisms in a competitive\\r\\nenvironment, in which only the most qualified and their offspring survive. It\\r\\nsamples the population with replacement where each individual has a given\\r\\nprobability of surviving.\\r\\n2. Form the new population\\r\\n(a) Crossover: forms new offsprings from two parent chromosomes by combining\\r\\npart of the genetic information from each.\\r\\n(b) Mutation: randomly alters the values of genes in a parent chromosome.\\r\\n• Fitness evaluation with the fitness function.\\r\\nend\\r\\n33\\nReferences\\r\\nAdam, Everett E. (Oct. 1973). “Individual item forecasting model evaluation”. In: Decision\\r\\nSciences 4.4, pp. 458–470. issn: 1540-5915.\\r\\nAshouri, Mahsa, Galit Shmueli, and Chor-Yiu Sin (2019). “Tree-based methods for clustering\\r\\ntime series using domain-relevant attributes”. In: Journal of Business Analytics 2.1, pp. 1–\\r\\n23.\\r\\nBagnall, Anthony et al. (2017). “Simulated data experiments for time series classification Part\\r\\n1: accuracy comparison with default settings”. In: arXiv preprint arXiv:1703.09480.\\r\\nBarbour, Andrew J and Robert L Parker (Feb. 2014). “psd: Adaptive, sine multitaper power\\r\\nspectral density estimation for R”. In: Computers & Geosciences 63, pp. 1–8.\\r\\nB´edubourg, Gabriel and Yann Le Strat (2017). “Evaluation and comparison of statistical meth\\x02ods for early temporal detection of outbreaks: A simulation-based study”. In: PloS one 12.7,\\r\\ne0181227.\\r\\nChang, Winston et al. (2018). shiny: Web Application Framework for R. R package version\\r\\n1.1.0. url: https://CRAN.R-project.org/package=shiny.\\r\\nCleveland, Robert B et al. (1990). “STL: A seasonal-trend decomposition procedure based on\\r\\nloess”. In: Journal of Official Statistics 6.1, pp. 3–73.\\r\\nCollopy, Fred and J Scott Armstrong (1992). “Rule-based forecasting: development and valida\\x02tion of an expert systems approach to combining time series extrapolations”. In: Management\\r\\nScience 38.10, pp. 1394–1414.\\r\\nConstantinopoulos, Constantinos, Michalis K Titsias, and Aristidis Likas (2006). “Bayesian\\r\\nfeature and model selection for Gaussian mixture models”. In: IEEE Transactions on Pattern\\r\\nAnalysis & Machine Intelligence 6, pp. 1013–1018.\\r\\nDau, Hoang Anh et al. (Oct. 2018). The UCR Time Series Classification Archive. https :\\r\\n//www.cs.ucr.edu/˜eamonn/time_series_data_2018/.\\r\\nDeng, Jia et al. (2009). “Imagenet: A large-scale hierarchical image database”. In: Computer\\r\\nVision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. Ieee, pp. 248–255.\\r\\nDua, Dheeru and Efi Karra Taniskidou (2017). UCI Machine Learning Repository. url: http:\\r\\n//archive.ics.uci.edu/ml.\\r\\nEllis, Peter (2016). Tcomp: Data from the 2010 Tourism Forecasting Competition. R package\\r\\nversion 1.0.0. url: https://CRAN.R-project.org/package=Tcomp.\\r\\nEngle, Robert F (1982). “Autoregressive conditional heteroscedasticity with estimates of the\\r\\nvariance of United Kingdom inflation”. In: Econometrica 50, pp. 987–1007.\\r\\n34\\nEscobar, Michael D and Mike West (1995). “Bayesian density estimation and inference using\\r\\nmixtures”. In: Journal of the American Statistical Association 90.430, pp. 577–588.\\r\\nFreeland, RK and Brendan PM McCabe (2004). “Analysis of low count time series data by\\r\\nPoisson autoregression”. In: Journal of Time Series Analysis 25.5, pp. 701–722.\\r\\nFriedman, Jerome H (1984). A variable span scatterplot smoother. Technical Report 5. Labora\\x02tory for Computational Statistics, Stanford University.\\r\\nFriedman, Jerome H et al. (1991). “Multivariate adaptive regression splines”. In: The Annals of\\r\\nStatistics 19.1, pp. 1–67.\\r\\nFulcher, B.D. and N.S. Jones (Dec. 2014). “Highly comparative feature-based time-series classi\\x02fication”. In: IEEE Transactions on Knowledge and Data Engineering 26.12, pp. 3026–3037.\\r\\nissn: 1041-4347.\\r\\nFulcher, Ben D (2018). “Feature-based time-series analysis”. In: Feature engineering for machine\\r\\nlearning and data analytics. CRC Press, pp. 87–116.\\r\\nGelman, Andrew et al. (2013). Bayesian data analysis. Chapman and Hall/CRC. isbn: 9781439840955.\\r\\nGriffiths, Thomas L et al. (2004). “Hierarchical topic models and the nested chinese restaurant\\r\\nprocess”. In: Advances in neural information processing systems, pp. 17–24.\\r\\nHaslett, John and Adrian E Raftery (1989). “Space-time modelling with long-memory depen\\x02dence: assessing Ireland’s wind power resource”. In: Journal of the Royal Statistical Society\\r\\nSeries C 38.1, pp. 1–50.\\r\\nHyndman, Rob J (2018). tscompdata: Time series data from various forecasting competitions.\\r\\nVersion 0.0.1. url: https://github.com/robjhyndman/tscompdata.\\r\\nHyndman, Rob J and George Athanasopoulos (2018). Forecasting: principles and practice. Mel\\x02bourne, Australia: OTexts. url: OTexts.org/fpp2.\\r\\nHyndman, Rob J and Anne B Koehler (2006). “Another look at measures of forecast accuracy”.\\r\\nIn: International Journal of Forecasting 22.4, pp. 679–688.\\r\\nHyndman, Rob J, Earo Wang, and Nikolay Laptev (2015). “Large-scale unusual time series\\r\\ndetection”. In: Proceedings of the IEEE International Conference on Data Mining. 14–17\\r\\nNovember 2015. Atlantic City, NJ, USA.\\r\\nHyndman, Rob J et al. (2018). Mcomp: Data from the M-Competitions. Version 2.7. url: https:\\r\\n//CRAN.R-project.org/package=Mcomp.\\r\\nHyndman, Rob et al. (2019). tsfeatures: Time Series Feature Extraction. R package version\\r\\n1.0.1. url: https://CRAN.R-project.org/package=tsfeatures.\\r\\nIsmail Fawaz, Hassan et al. (Mar. 2019). “Deep learning for time series classification: a review”.\\r\\nIn: Data Mining and Knowledge Discovery. issn: 1573-756X.\\r\\n35\\nJiang, Wenxin and Martin A Tanner (1999). “On the approximation rate of hierarchical mixtures\\x02of-experts for generalized linear models”. In: Neural Computation 11.5, pp. 1183–1198.\\r\\nKang, Yanfei, Danijel Beluˇsi´c, and Kate Smith-Miles (2014). “Detecting and classifying events\\r\\nin noisy time series”. In: Journal of the Atmospheric Sciences 71.3, pp. 1090–1104.\\r\\n— (2015). “Classes of structures in the stable atmospheric boundary layer”. In: Quarterly Jour\\x02nal of the Royal Meteorological Society 141.691, pp. 2057–2069.\\r\\nKang, Yanfei, Rob J Hyndman, and Kate Smith-Miles (2017). “Visualising forecasting algorithm\\r\\nperformance using time series instance spaces”. In: International Journal of Forecasting 33.2,\\r\\npp. 345–358.\\r\\nKegel, Lars, Martin Hahmann, and Wolfgang Lehner (2017). “Generating What-if scenarios for\\r\\ntime series data”. In: Proceedings of the 29th International Conference on Scientific and\\r\\nStatistical Database Management. ACM, p. 3.\\r\\nKeogh, Eamonn and Shruti Kasetty (2003). “On the need for time series data mining bench\\x02marks: a survey and empirical demonstration”. In: Data Mining and knowledge discovery\\r\\n7.4, pp. 349–371.\\r\\nKoenker, Roger and Gilbert Bassett Jr (1978). “Regression quantiles”. In: Econometrica: journal\\r\\nof the Econometric Society, pp. 33–50.\\r\\nKwiatkowski, Denis et al. (1992). “Testing the null hypothesis of stationarity against the al\\x02ternative of a unit root How sure are we that economic time series have a unit root?” In:\\r\\nJournal of Econometrics 54, pp. 159–178.\\r\\nLatour, Alain (1998). “Existence and stochastic structure of a non-negative integer-valued au\\x02toregressive process”. In: Journal of Time Series Analysis 19.4, pp. 439–455.\\r\\nLe, Nhu D, R Douglas Martin, and Adrian E Raftery (1996). “Modeling flat stretches, bursts\\r\\noutliers in time series using mixture transition distribution models”. In: Journal of the\\r\\nAmerican Statistical Association 91.436, pp. 1504–1515.\\r\\nLi, Feng, Mattias Villani, and Robert Kohn (2010). “Flexible modeling of conditional distribu\\x02tions using smooth mixtures of asymmetric student-t densities”. In: Journal of Statistical\\r\\nPlanning and Inference 140.12, pp. 3638–3654.\\r\\nLjung, G M and George E P Box (1978). “On a measure of lack of fit in time series models”.\\r\\nIn: Biometrika 65.2, pp. 297–303.\\r\\nLotze, Thomas H and Galit Shmueli (2009). “How does improved forecasting benefit detection?\\r\\nAn application to biosurveillance”. In: International Journal of Forecasting 25.3, pp. 467–\\r\\n483.\\r\\n36\\nLotze, Thomas et al. (2010). “Simulating and evaluating biosurveillance datasets”. In: Bio\\x02surveillance: Methods and Case Studies; Chapman and Hall: Boca Raton, FL 2352.\\r\\nMaaten, Laurens van der and Geoffrey Hinton (2008). “Visualizing data using t-SNE”. In:\\r\\nJournal of Machine Learning Research 9.Nov, pp. 2579–2605.\\r\\nMakridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos (2018). “The M4 Com\\x02petition: Results, findings, conclusion and way forward”. In: International Journal of Fore\\x02casting 34.4, 802–=808.\\r\\nMeade, Nigel (2000). “Evidence for the selection of forecasting methods”. In: Journal of Fore\\x02casting 19.6, pp. 515–535.\\r\\nMu˜noz, Mario A and Kate Smith-Miles (2017). “Performance analysis of continuous black-box\\r\\noptimization algorithms via footprints in instance space”. In: Evolutionary Computation\\r\\n25.4, pp. 529–554.\\r\\nMu˜noz, Mario A et al. (2018). “Instance spaces for machine learning classification”. In: Machine\\r\\nLearning 1, pp. 109–147.\\r\\nNorets, Andriy (2010). “Approximation of conditional densities by smooth mixtures of regres\\x02sions”. In: Annals of Statistics 38.3, pp. 1733–1766.\\r\\nOsborn, Denise R et al. (1988). “Seasonality and the order of integration for consumption”. In:\\r\\nOxford Bulletin of Economics and Statistics 50.4, pp. 361–377.\\r\\nPeng, Bo and Lan Wang (2015). “An iterative coordinate descent algorithm for high-dimensional\\r\\nnonconvex penalized quantile regression”. In: Journal of Computational and Graphical Statis\\x02tics 24.3, pp. 676–694.\\r\\nPetropoulos, F et al. (2014). “‘Horses for Courses’ in demand forecasting”. In: European Journal\\r\\nof Operational Research 237.1, pp. 152–163.\\r\\nPovinelli, Richard J et al. (2004). “Time series classification using Gaussian mixture models of\\r\\nreconstructed phase spaces”. In: IEEE Transactions on Knowledge and Data Engineering\\r\\n16.6, pp. 779–783.\\r\\nR Core Team (2018). R: A Language and Environment for Statistical Computing. R Foundation\\r\\nfor Statistical Computing. Vienna, Austria. url: https://www.R-project.org/.\\r\\nShah, Chandra (1997). “Model selection in univariate time series forecasting using discriminant\\r\\nanalysis”. In: International Journal of Forecasting 13.4, pp. 489–500.\\r\\nSilver, David et al. (2017). “Mastering the game of Go without human knowledge”. In: Nature\\r\\n550.7676, p. 354.\\r\\nSmith-Miles, Kate and Simon Bowly (2015). “Generating New Test Instances by Evolving in\\r\\nInstance Space”. In: Computers & Operations Research 63, pp. 102–113.\\r\\n37\\nTalagala, Thiyanga S, Rob J Hyndman, and George Athanasopoulos (2018). Meta-learning how\\r\\nto forecast time series. Working paper 6/18. Monash University, Department of Econometrics\\r\\nand Business Statistics.\\r\\nVillani, Mattias, Robert Kohn, and Paolo Giordani (2009). “Regression density estimation using\\r\\nsmooth adaptive Gaussian mixtures”. In: Journal of Econometrics 153.2, pp. 155–173.\\r\\nVinod, Hrishikesh D, Javier L´opez-de-Lacalle, et al. (2009). “Maximum entropy bootstrap for\\r\\ntime series: the meboot R package”. In: Journal of Statistical Software 29.5, pp. 1–19.\\r\\nWang, Shanshan, Wolfgang Jank, and Galit Shmueli (2008). “Explaining and forecasting online\\r\\nauction prices and their dynamics using functional data analysis”. In: Journal of Business\\r\\n& Economic Statistics 26.2, pp. 144–160.\\r\\nWang, Xiaozhe, Kate A Smith-Miles, and Rob J Hyndman (2009). “Rule induction for fore\\x02casting method selection: meta-learning the characteristics of univariate time series”. In:\\r\\nNeurocomputing 72.10-12, pp. 2581–2594.\\r\\nWang, Xiaozhe, Kate A. Smith, and Rob J. Hyndman (2006). “Characteristic-based clustering\\r\\nfor time series data”. In: Data Mining and Knowledge Discovery 13.3, pp. 335–364.\\r\\nWolpert, David H (1996). “The lack of a priori distinctions between learning algorithms”. In:\\r\\nNeural Computation 8.7, pp. 1341–1390.\\r\\nWolpert, David H and William G Macready (1997). “No free lunch theorems for optimization”.\\r\\nIn: IEEE Transactions on Evolutionary Computation 1.1, pp. 67–82.\\r\\nWong, Chun Shan and Wai Keung Li (2000). “On a mixture autoregressive model”. In: Journal\\r\\nof the Royal Statistical Society: Series B (Statistical Methodology) 62.1, pp. 95–115.\\r\\nZhang, G Peter, B Eddy Patuwo, and Michael Y Hu (2001). “A simulation study of artificial\\r\\nneural networks for nonlinear time-series forecasting”. In: Computers & Operations Research\\r\\n28.4, pp. 381–396.\\r\\n38'},\n",
       " {'name': '2409.15662v1.pdf',\n",
       "  'content': 'Double-Path Adaptive-correlation Spatial-Temporal Inverted\\r\\nTransformer for Stock Time Series Forecasting\\r\\nWenbo Yan\\r\\nSchool of Intelligence Science and Technology\\r\\nComputational Intelligence Laboratory\\r\\nPeking University, China\\r\\nwenboyan@stu.pku.edu.cn\\r\\nYing Tan\\r\\nSchool of Intelligence Science and Technology\\r\\nInstitute for Artificial Intellignce\\r\\nNational Key Laboratory of General Artificial Intelligence\\r\\nKey Laboratory of Machine Perceptron (MOE)\\r\\nPeking University, China\\r\\nytan@pku.edu.cn\\r\\nAbstract\\r\\nSpatial-temporal graph neural networks (STGNNs) have achieved\\r\\nsignificant success in various time series forecasting tasks. However,\\r\\ndue to the lack of explicit and fixed spatial relationships in stock\\r\\nprediction tasks, many STGNNs fail to perform effectively in this\\r\\ndomain. While some STGNNs learn spatial relationships from time\\r\\nseries, they often lack comprehensiveness. Research indicates that\\r\\nmodeling time series using feature changes as tokens reveals en\\x02tirely different information compared to using time steps as tokens.\\r\\nTo more comprehensively extract dynamic spatial information from\\r\\nstock data, we propose a Double-Path Adaptive-correlation Spatial\\x02Temporal Inverted Transformer (DPA-STIFormer). DPA-STIFormer\\r\\nmodels each node via continuous changes in features as tokens\\r\\nand introduces a Double Direction Self-adaptation Fusion mecha\\x02nism. This mechanism decomposes node encoding into temporal\\r\\nand feature representations, simultaneously extracting different\\r\\nspatial correlations from a double path approach, and proposes a\\r\\nDouble-path gating mechanism to fuse these two types of corre\\x02lation information. Experiments conducted on four stock market\\r\\ndatasets demonstrate state-of-the-art results, validating the model’s\\r\\nsuperior capability in uncovering latent temporal-correlation pat\\x02terns.\\r\\nCCS Concepts\\r\\n• Information systems → Spatial-temporal systems.\\r\\nKeywords\\r\\nSpatial-Temporal Graph Neural Network; Pre-training Model; Time\\r\\nSeries Forecasting\\r\\nACM Reference Format:\\r\\nWenbo Yan and Ying Tan. 2024. Double-Path Adaptive-correlation Spatial\\x02Temporal Inverted Transformer for Stock Time Series Forecasting. In Pro\\x02ceedings of the 31th ACM SIGKDD Conference on Knowledge Discovery and\\r\\nData Mining (KDD ’25). ACM, New York, NY, USA, 9 pages. https://doi.org/\\r\\nXXXXXXX.XXXXXXX\\r\\nPermission to make digital or hard copies of all or part of this work for personal or\\r\\nclassroom use is granted without fee provided that copies are not made or distributed\\r\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\r\\non the first page. Copyrights for components of this work owned by others than the\\r\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\r\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\r\\nand/or a fee. Request permissions from permissions@acm.org.\\r\\nKDD ’25, August 3-7, 2025, Toronto, ON, Canada\\r\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\r\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\r\\nhttps://doi.org/XXXXXXX.XXXXXXX\\r\\n1 INTRODUCTION\\r\\nTime series data is ubiquitous in various aspects of life, including\\r\\nhealthcare, transportation, weather, and finance, where a substan\\x02tial amount of time series data is generated daily. If historical data\\r\\ncan be utilized to uncover potential patterns and predict future\\r\\nchanges, it could greatly enhance our quality of life. Initially, it was\\r\\nassumed that time series data is generated from certain processes,\\r\\nleading to the development of models such as Box and Jenkins [2],\\r\\nEngle [6] to approximate these processes. In recent years, numer\\x02ous deep learning methods have been applied to the field of time\\r\\nseries forecasting. To better model time series data, models based\\r\\non recurrent neural networks (RNNs), such as Qin et al. [15], as well\\r\\nas models based on convolutional neural networks (CNNs), such as\\r\\nDai et al. [5], have been proposed. With the rise of the Transformer\\r\\nmodel in the field of natural language processing, Transformer\\x02based time series forecasting models, such as Zhang and Yan [24],\\r\\nhave been introduced. As research has progressed, it has been found\\r\\nthat for certain time series problems, such as traffic and climate fore\\x02casting, employing spatial-temporal prediction methods can better\\r\\npredict future changes. Consequently, many spatial-temporal graph\\r\\nneural networks (STGNNs) have been proposed and have achieved\\r\\nexcellent results in fields like transportation. Moreover, to address\\r\\nthe issue that Transformer-based methods cannot utilize spatial re\\x02lationships, Spatial-Temporal Transformers (STTransformers) such\\r\\nas Lim et al. [13], Xu et al. [21] have been proposed.\\r\\nIn fact, these time series modeling methods often fail to achieve\\r\\nsatisfactory results in stock market prediction. Firstly, whether\\r\\nbased on RNNs or temporal convolution methods, they typically\\r\\nfocus on modeling the time series of individual nodes independently,\\r\\nwithout considering the interdependencies between different time\\r\\nseries. However, in the stock market, the entire market functions as\\r\\nan integrated whole, and there are significant interactions between\\r\\nthe time series of different stocks at any given moment. Therefore,\\r\\nignoring the correlations between different nodes fails to effectively\\r\\naddress the problem of stock prediction. Secondly, for Transformer\\x02based methods, since the features of time series often lie in the\\r\\nvariations between time steps and each time step itself contains low\\r\\ninformation content or even no actual meaning, using time steps\\r\\nas tokens to model time series with Transformers often results in\\r\\npoor performance.\\r\\nWhile STGNNs can effectively utilize the correlations between\\r\\nnodes and integrate changes in other time series for comprehensive\\r\\nmodeling, most spatial-temporal graph neural networks require pre\\x02defined adjacency relationships, such as spatial distances in climate\\r\\narXiv:2409.15662v1 [cs.LG] 24 Sep 2024\\nKDD ’25, August 3-7, 2025, Toronto, ON, Canada Wenbo Yan and Ying Tan\\r\\ndata or road connections in transportation networks. However,\\r\\nsuch relationships are not explicitly present in the stock market.\\r\\nAlthough ST-Transformers do not require predefined correlation\\r\\nrelationships, existing methods still learn correlations between\\r\\nnodes from the perspective of time steps, and experiments show that\\r\\nthe learned correlations are still limited. This issue is also present in\\r\\nSTGNNs that do not require predefined adjacency matrices. In the\\r\\nlatest research, some studies have recognized that time steps are\\r\\nnot an ideal perspective for modeling time series. Crossformer uses\\r\\nboth temporal and feature perspectives to model time series, while\\r\\nexperiments with ITransformer indicate that using only features as\\r\\ntokens to model individual time series yields better results.\\r\\nWe believe that viewing time series from both feature and tem\\x02poral perspectives reflects two completely different characteristics.\\r\\nWhile experiments with ITransformer have shown that modeling\\r\\nthe time series of a single node using only features as tokens yields\\r\\nbetter results, when learning the correlations between different\\r\\nnodes using node time series, the feature and temporal perspectives\\r\\ncan capture completely different node characteristics and yield two\\r\\ndistinct types of correlations. Integrating these two different types\\r\\nof correlations can more comprehensively reflect the real influ\\x02ences between nodes. Based on this, we propose a novel method\\r\\nsuitable for stock prediction, focusing on both the time series and\\r\\nthe correlations between time series, named DPA-STIFormer.\\r\\nSpecifically, DPA-STIFormer consists of multiple Double-Path\\r\\nAdaptive-correlation Inverted Encoders and a single Decoder Block\\r\\nwith Decomposed Fitting. The DPA-IEncoder includes an Inverted\\r\\nTemporal Block and a Double-Path Adaptive-correlation Block. The\\r\\nInverted Temporal Block models the time series of each node using\\r\\nfeatures as tokens. Considering that each feature has a different\\r\\nlevel of influence on the node, we introduce importance weights to\\r\\nensure our method can account for the importance of tokens when\\r\\ncalculating attention. The Double-Path Adaptive-correlation Block\\r\\nadaptively aggregates information from both the feature and tem\\x02poral paths and learns two different types of correlations through\\r\\nthe attention mechanism. We then propose a gating mechanism\\r\\nto effectively integrate information from the two paths. Following\\r\\nmultiple encoder blocks, we introduce a specialized decoder suitable\\r\\nfor stock prediction, which decomposes the prediction target into\\r\\nmean and deviation predictions, effectively handling both minor\\r\\nand drastic changes. In summary, the main contributions are as\\r\\nfollows:\\r\\n• We propose a novel Spatial-Temporal Transformer model,\\r\\nDPA-STIFormer, to address spatial-temporal prediction in\\r\\nstock markets. The model employs features as tokens and in\\x02troduces an importance weight mechanism to better encode\\r\\nnode characteristics.\\r\\n• We introduce DPABlock to model the correlation compre\\x02hensively from two perspectives. The Double Direction Self\\x02adaptation Fusion mechanism is proposed to separate dif\\x02ferent features from time series. We learn the correlations\\r\\nthrough a double path approach, and a gating mechanism is\\r\\nproposed to integrate features from different perspectives.\\r\\n• We conducted extensive experiments on 4 stock datasets,\\r\\nand the experimental results demonstrate that our method\\r\\ncan more effectively model the spatial-temporal correlations\\r\\nin time series.\\r\\n2 PRELIMINARY\\r\\nWe first define the concepts of Spatial-Temporal Forecasting\\r\\nProblem. The spatial-temporal forecasting problem, refers to the\\r\\ntask of using historical 𝑇 time steps data 𝑿 ∈ R\\r\\n𝑁 ×𝑇 ×𝐹\\r\\nand corre\\x02lation adjacency matrix 𝑨 ∈ R\\r\\n𝑁 ×𝑁 to predict future values for 𝑡\\r\\ntime steps 𝒀 ∈ R\\r\\n𝑁 ×𝑡\\r\\n.The adjacency matrix can be predefined or\\r\\nlearned by the model, as in our approach. For each sample, there\\r\\nare 𝑁 nodes, and each node has a time series 𝑋𝑖, where 𝑋𝑖 contains\\r\\n𝑇 time steps, and each time step has 𝐹 features. Additionally, the\\r\\ncorrelation adjacency matrix 𝐴 indicates the degree of correlation\\r\\nbetween the nodes, where 𝑎𝑖𝑗 represents the correlation degree be\\x02tween node 𝑖 and node 𝑗. The neighbors set of node 𝑖 is represented\\r\\nas 𝐾 = { 𝑗 | 𝑗 ≠ 𝑖 𝑎𝑛𝑑 𝑎𝑖𝑗 ≠ 0}.\\r\\n3 METHOD\\r\\nIn this section, we present our approach. The precise architecture\\r\\nof the DPA-STIFormer is illustrated in Fig. 1.\\r\\n3.1 Overall\\r\\nThe model is composed of multiple encoder blocks called Double\\x02Path Adaptive-correlation Inverted Encoder (DPA-IEncoder) and a\\r\\nsingle Decoder Block with Decomposed Fitting (DF-Decoder) for\\r\\nstock prediction. Each DPA-IEncoder consists of an Inverted Tem\\x02poral Block, a Double-Path Adaptive-correlation Block, and two\\r\\nfeedforward neural networks, all interconnected by residual con\\x02nections. The Inverted Temporal Encoder processes the time series\\r\\n𝑥𝑖 for an individual node and the Double-Path Adaptive Correlation\\r\\nEncoder concurrently aggregates information from both temporal\\r\\nand feature dimensions, adaptively learns the relationships among\\r\\nneighboring nodes, and integrates the encoding results through\\r\\na gating mechanism. Finally, the DF-Decoder demonstrates high\\r\\ncapacity in stock prediction with proposed decomposed fitting.\\r\\n3.2 Inverted Temporal Block\\r\\nIn many Transformer-based time series prediction approaches, fea\\x02tures at the same time step are treated analogously to words in nat\\x02ural language processing. However, these methods often yield sub\\x02optimal performance. This is due to the inherently low information\\x02to-noise ratio in time series data and the potential insignificance\\r\\nof individual time steps. Crucially, information is typically embed\\x02ded in the temporal evolution of features rather than in isolated\\r\\ntime points. Inspired by Itransformer[14] and crossformer[24] , we\\r\\nno longer consider features at the same time step as tokens, but\\r\\nrather the change of the same feature over time as a token. Thus,\\r\\nfor the time series 𝑥𝑖 ∈ R\\r\\n𝑇 ×𝐹\\r\\nin each node, We first Invert 𝑥𝑖 to get\\r\\n𝑥\\r\\n𝐼\\r\\n𝑖\\r\\n∈ R\\r\\n𝐹 ×𝑇\\r\\n, rendering the temporal sequence of each feature as a\\r\\ntoken.\\r\\n𝑥\\r\\n𝐼\\r\\n𝑖\\r\\n= 𝑖𝑛𝑣𝑒𝑟𝑡(𝑥𝑖), 𝑓 𝑜𝑟 𝑖 = 1, 2, ..., 𝑛 (1)\\r\\nwhere 𝑖𝑛𝑣𝑒𝑟𝑡(·) stands for matrix transpose. Furthermore, consider\\x02ing the diverse features contained within the same node, it is evident\\r\\nthat each feature exerts a different degree of influence on the node’s\\r\\nvariation, due to their distinct meanings. We introduce importance\\nDouble-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting KDD ’25, August 3-7, 2025, Toronto, ON, Canada\\r\\n(c)DF-Decoder\\r\\nADD&LN\\r\\nFeedForward\\r\\n(b)DPABlock\\r\\n（a)Inverted\\r\\nTemporal Block\\r\\nFeedForward\\r\\nADD&LN\\r\\nADD&LN\\r\\nADD&LN\\r\\nDPA-IEncoder DPA-IEncoder\\r\\n(a) Inverted Temporal Block （c)DF-Decoder\\r\\nIn\\r\\nv\\r\\nert Double Direction Self-adaptation Fusion Double-path gating mechanism\\r\\nQ\\r\\nK\\r\\n⨂ Softmax 𝒕𝒆\\r\\nQ\\r\\nK\\r\\n⨂ Softmax 𝒕 ⨀\\r\\nSum\\r\\n⨀Sum\\r\\n𝒕𝒆\\r\\n𝒕\\r\\nS\\r\\nelf-Atte\\r\\nntio\\r\\nn\\r\\nS\\r\\nelf-Atte\\r\\nntio\\r\\nn\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n𝒕\\r\\n𝒕\\r\\n\\r\\nM\\r\\nL\\r\\nP\\r\\n⨂\\r\\n⨂\\r\\n⨀\\r\\nt\\r\\nt\\r\\n⨀\\r\\nConcat σ ⨀\\r\\n(b) DPABlock (Double-Path Adaptive-correlation Block)\\r\\nIn\\r\\nv\\r\\nert\\r\\nC\\r\\no\\r\\nn\\r\\nc\\r\\nat\\r\\nImportance weight\\r\\nM\\r\\nL\\r\\nP\\r\\nS\\r\\nelf-Atte\\r\\nntio\\r\\nn\\r\\nM\\r\\nL\\r\\nP\\r\\nM\\r\\nL\\r\\nP\\r\\n𝑚𝑒\\r\\n𝑚\\r\\n⨁\\r\\nM\\r\\ne L\\r\\nP\\r\\n⨀\\r\\nSoftmax\\r\\nSum\\r\\nSpatio-Temporal Data\\r\\nSimple representation\\r\\nN\\r\\nT\\r\\nF\\r\\nSpatio-Temporal\\r\\nDataℝ××\\r\\nInverted\\r\\nSpatio-Temporal\\r\\nDataℝ××\\r\\nFigure 1: The overview of the proposed Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer.\\r\\nweights for each feature. Specifically, we pass 𝑥\\r\\n𝐼\\r\\n𝑖\\r\\nthrough a two\\x02layer fully-connected network to get their importance𝑤𝑖 and apply\\r\\nthe softmax function to ensure that Í\\r\\n𝑤𝑖 = 1. Then, splice the 𝑤𝑖\\r\\nto each token:\\r\\n𝑤𝑖 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (𝑊2 ∗ 𝜎(𝑊1 ∗ 𝑥\\r\\n𝐼\\r\\n𝑖\\r\\n+ 𝑏1) + 𝑏2) (2)\\r\\n𝑥ˆ\\r\\n𝐼\\r\\n𝑖\\r\\n=< 𝑤𝑖, 𝑥𝐼\\r\\n𝑖 > (3)\\r\\nwhere 𝑊1,𝑊2 represent learnable weight matrices, 𝑏1, 𝑏2 denote\\r\\nlearnable bias vectors, 𝜎 is the 𝑅𝑒𝐿𝑈 () activation function, and\\r\\n< · > indicates matrix concatenation. The set of inverted time\\r\\nseries with importance is denoted as 𝑋ˆ𝐼.\\r\\n𝑋ˆ\\r\\n𝐼 = [𝑥ˆ𝐼\\r\\n1\\r\\n, 𝑥ˆ\\r\\n𝐼\\r\\n2\\r\\n, ..., 𝑥ˆ\\r\\n𝐼\\r\\n𝑛\\r\\n] (4)\\r\\nThen, we model the time series of each node using the self-attention\\r\\nlayer as in Transformer[17].\\r\\nQ = 𝑊𝑇\\r\\n𝑄\\r\\n𝑋ˆ\\r\\n𝐼\\r\\n, K = 𝑊𝑇\\r\\n𝐾\\r\\n𝑋ˆ\\r\\n𝐼\\r\\n, V = 𝑊𝑇\\r\\n𝑉\\r\\n𝑋ˆ\\r\\n𝐼\\r\\n(5)\\r\\n𝑂\\r\\n𝐼 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (\\r\\nQKT\\r\\n√︁\\r\\n𝑑𝑘\\r\\n)V (6)\\r\\nwhere𝑊𝑄 ,𝑊𝐾,𝑊𝑉 are independent weights matrix that map 𝑋ˆ𝐼to\\r\\nthree different spaces, 𝑂\\r\\n𝐼\\r\\nrepresents the output of the self-attention\\r\\nmechanism. Through this self-attention mechanism, we aim to\\r\\nlearn the correlations between multiple features and develop better\\r\\nrepresentations of each node. Subsequently, layer normalization and\\r\\nresidual connections are applied to enhance the encoding process.\\r\\n𝑋ˆ\\r\\n𝐼\\r\\n𝑜𝑢𝑡 = 𝐿𝑁 (𝑂\\r\\n𝐼 + 𝑟𝑒𝑠𝑖𝑑𝑢𝑎𝑙(𝑋ˆ𝐼\\r\\n)) (7)\\r\\nwhere 𝐿𝑁 (·) stand for layer normalization, and 𝑟𝑒𝑠𝑖𝑑𝑢𝑎𝑙(·) is a\\r\\nsingle-layer fully connection network to ensure equal dimension.\\r\\nFinally, the feedforward (FFD) neural network is applied:\\r\\n𝑍\\r\\n𝐼 = 𝐿𝑁 (𝑋ˆ𝐼\\r\\n𝑜𝑢𝑡 + 𝐹 𝐹𝐷(𝑋ˆ\\r\\n𝐼\\r\\n𝑜𝑢𝑡)) (8)\\r\\n3.3 Double-Path Adaptive-correlation Block\\r\\nDouble-path Adaptive-correlation Block (DPABlock) is designed to\\r\\nadaptively model the correlations between nodes. It comprises three\\r\\nmain components: Double Direction Self-adaptation Fusion, N\\x02neighbor Adaptive Correlation Attention, and Double-Path Gating\\r\\nMechanism.\\r\\n3.3.1 Double Direction Self-adaptation Fusion. For spatial-temporal\\r\\nprediction problems, each node is often featured as a two-dimensional\\r\\ntime series 𝑥𝑖 ∈ R\\r\\n𝑇 ×𝐹\\r\\n. The computational cost can become pro\\x02hibitively large if all features are used to learn the correlations\\r\\nbetween nodes. A common approach is to use the features from\\r\\nthe nearest time step or the mean of each time step as the node\\r\\nfeatures. However, this approach has significant limitations. On\\r\\nthe one hand, simple averaging over time steps does not provide a\\r\\nrobust representation of the nodes. Different features hold varying\\r\\nlevels of importance at different time steps, and averaging may\\r\\ndilute these distinctions, rendering the features less meaningful.\\r\\nOn the other hand, many features of a time series are embedded\\r\\nin its successive changes, and aggregating multiple series into a\\r\\nsingle one may result in information loss compared to averaging\\r\\nover time steps.\\r\\nWe designed a double direction adaptive information fusion\\r\\nmethod as shown in Fig. 1. We first invert 𝑍\\r\\n𝐼\\r\\nagain to obtain 𝑍,\\r\\n𝑍 = [𝑧1, 𝑧2, ..., 𝑧𝑛] (9)\\r\\nThen, we respectively map 𝑍\\r\\n𝐼\\r\\nand 𝑍 to different spaces through\\r\\nfully connected layers\\r\\nQ𝐹 = 𝑊𝑇\\r\\n𝑄,𝐹𝑍, K𝐹 = 𝑊𝑇𝐾,𝐹𝑍\\r\\nQ\\r\\n𝐼\\r\\n𝐹\\r\\n= 𝑊\\r\\n𝐼,𝑇\\r\\n𝑄,𝐹𝑍\\r\\n𝐼\\r\\n, K\\r\\n𝐼\\r\\n𝐹\\r\\n= 𝑊\\r\\n𝐼,𝑇\\r\\n𝐾,𝐹𝑍\\r\\n𝐼\\r\\n(10)\\nKDD ’25, August 3-7, 2025, Toronto, ON, Canada Wenbo Yan and Ying Tan\\r\\nwhere Q𝐹 ,K𝐹 ,Q\\r\\n𝐼\\r\\n𝐹\\r\\n, and K𝐼\\r\\n𝐹\\r\\nare the Query and Key of 𝑍 and 𝑍\\r\\n𝐼\\r\\n, re\\x02spectively. They are all mapped to the same dimension.𝑊𝑇\\r\\n𝑄,𝐹 ,𝑊𝑇𝐾,𝐹 ,\\r\\n𝑊\\r\\n𝐼,𝑇\\r\\n𝑄,𝐹 and 𝑊\\r\\n𝐼,𝑇\\r\\n𝐾,𝐹 are the corresponding weight matrices. We learn\\r\\nthe weights of each feature at each time step by mutual querying,\\r\\nand we ensure that these weights sum to 1 in a certain direction\\r\\nusing the softmax function.\\r\\nW𝑡𝑒𝑚𝑝 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (Q𝐹 K\\r\\n𝐼\\r\\n𝐹\\r\\n)\\r\\nW𝑓 𝑒𝑎𝑡 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (Q𝐼\\r\\n𝐹 K𝐹 )\\r\\n(11)\\r\\n𝐻𝑡𝑒𝑚𝑝 = 𝑆𝑢𝑚(𝑊𝑡𝑒𝑚𝑝 · 𝑍)\\r\\n𝐻𝑓 𝑒𝑎𝑡 = 𝑆𝑢𝑚(𝑊𝑓 𝑒𝑎𝑡 · 𝑍\\r\\n𝐼\\r\\n)\\r\\n(12)\\r\\nwhere W𝑡𝑒𝑚𝑝 and W𝑓 𝑒𝑎𝑡 are weighting matrices used to summa\\x02rize information in the temporal and feature directions, 𝑆𝑢𝑚(·) is\\r\\nthe summation of the last dimension, 𝐻𝑡𝑒𝑚𝑝 ∈ R\\r\\n𝑁 ×𝑇\\r\\nand 𝐻𝑓 𝑒𝑎𝑡 ∈\\r\\nR\\r\\n𝑁 ×𝐹\\r\\nare node representations that adaptively fusion from feature\\r\\nand temporal perspectives.\\r\\n3.3.2 N-neighbor correlation attention. Node representation 𝐻𝑓 𝑒𝑎𝑡\\r\\nand𝐻𝑡𝑒𝑚𝑝 each capture different characteristics of the nodes. There\\x02fore, we adaptively learn the correlations of the nodes from both\\r\\nperspectives. For each node and its neighbors, a close neighbor has a\\r\\npositive effect on the node, whereas information from more distant\\r\\nneighbors tends to be ineffective or even negative as the correlation\\r\\ndiminishes. To address this, we propose an N-neighbor correlation\\r\\nattention mechanism. This mechanism learns correlations between\\r\\nnodes based on each node representation and preserves the top-n\\r\\nneighbors of each node through masking. We define the process\\r\\n𝑁𝑐𝑜𝑟𝑟𝐴𝑡𝑡𝑛 as follows:\\r\\nQ𝐺 = 𝑊𝑇\\r\\n𝑄,𝐺𝐻, K𝐺 = 𝑊𝑇𝐾,𝐺𝐻, V𝐺 = 𝑊𝑇𝑉 ,𝐺𝑍\\r\\n𝐼\\r\\n𝑁𝑐𝑜𝑟𝑟𝐴𝑡𝑡𝑛(𝐻, 𝑋) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (\\r\\n𝑡𝑜𝑝𝑁 (Q𝐺K𝑇\\r\\n𝐺\\r\\n)\\r\\n√︁\\r\\n𝑑𝑔\\r\\n)V𝐺\\r\\n(13)\\r\\nwhere𝑊𝑇\\r\\n𝐾,𝐺 ,𝑊𝑇𝑄,𝐺 ,𝑊𝑇𝑉 ,𝐺 are independent weights matrix, and 𝑡𝑜𝑝𝑁 (·)\\r\\nrepresents the retention of only the largest 𝑁 values in each row\\r\\nof the matrix, with 𝑁 typically set to 10% of the number of nodes.\\r\\nQuery Q𝐺 and Key K𝐺 are obtained by mapping node representa\\x02tion 𝐻 to different space, and Value V𝐺 is mapped from 𝑍\\r\\n𝐼\\r\\n. This\\r\\napproach is taken because the node representation captures only\\r\\none aspect of the node’s characteristics and is suitable for comput\\x02ing the N-neighbor attention matrix. However, relying solely on\\r\\nthis enriched aspect can lead to the loss of the original features of\\r\\nthe time series. Therefore, the Value should still be mapped from\\r\\nthe original time series to retain the complete information. We can\\r\\nget two aspects of encoding:\\r\\n𝑂𝑓 𝑒𝑎𝑡 = 𝑁𝑐𝑜𝑟𝑟𝐴𝑡𝑡𝑛(𝐻𝑓 𝑒𝑎𝑡, 𝑍𝐼)\\r\\n𝑂𝑡𝑒𝑚𝑝 = 𝑁𝑐𝑜𝑟𝑟𝐴𝑡𝑡𝑛(𝐻𝑡𝑒𝑚𝑝, 𝑍𝐼)\\r\\n(14)\\r\\nSpecial Note that because 𝐻𝑓𝑒𝑎𝑡, 𝐻𝑡 𝑒𝑚𝑝 are both from 𝑍\\r\\n𝐼\\r\\nand\\r\\nValue is also from 𝑍\\r\\n𝐼\\r\\n, we use the same V𝐺 = 𝑊𝑇\\r\\n𝑉 ,𝐺𝑍\\r\\n𝐼\\r\\nas Value to\\r\\nsave computational resources.\\r\\n3.3.3 Double-path gating mechanism. In N-neighbor correlation\\r\\nattention, we model spatial relationships from both temporal and\\r\\nfeature perspectives simultaneously, forming the integrated spatial\\x02temporal encodings𝑂𝑓 𝑒𝑎𝑡 and𝑂𝑡𝑒𝑚𝑝 . For different spatial-temporal\\r\\nsamples, the importance of the two paths may vary. Therefore, we\\r\\npropose a double-path gating (DPgate) mechanism to adaptively\\r\\ncontrol the transmission and fusion of information from both sides.\\r\\nOur mechanism is illustrated in Fig. 1. DPgate mainly consists of\\r\\ntwo types of gates: self-passing gates and mutual-passing gates.\\r\\nEach path passes through a self-passing gate, which determines\\r\\nwhich information from that path passes through and which does\\r\\nnot. Both paths simultaneously pass through a mutual-passing gate,\\r\\nwhere the proportion of information fusion is determined by the\\r\\ncombined information from both paths. The specific calculation\\r\\nprocess is as follows:\\r\\n𝑔𝑓 𝑒𝑎𝑡 = 𝑡𝑎𝑛ℎ(𝑊𝑇\\r\\n𝑠,𝑓 𝑒𝑎𝑡𝑂𝑓 𝑒𝑎𝑡 + 𝑏𝑠,𝑓 𝑒𝑎𝑡)\\r\\n𝑔𝑡𝑒𝑚𝑝 = 𝑡𝑎𝑛ℎ(𝑊𝑇\\r\\n𝑠,𝑡𝑒𝑚𝑝𝑂𝑡𝑒𝑚𝑝 + 𝑏𝑠,𝑡𝑒𝑚𝑝 )\\r\\n𝑚 = 𝜎(𝑊𝑇\\r\\n𝑚 · [𝑂𝑓 𝑒𝑎𝑡,𝑂𝑡𝑒𝑚𝑝 ])\\r\\n𝑜𝑓 𝑒𝑎𝑡 = 𝑔𝑓 𝑒𝑎𝑡 ∗ 𝑂𝑓 𝑒𝑎𝑡\\r\\n𝑜𝑡𝑒𝑚𝑝 = 𝑔𝑡𝑒𝑚𝑝 ∗ 𝑂𝑡𝑒𝑚𝑝\\r\\n𝑀 = 𝑜𝑓 𝑒𝑎𝑡 ∗ 𝑚 + 𝑜𝑡𝑒𝑚𝑝 ∗ (1 − 𝑚)\\r\\n(15)\\r\\nwhere 𝑊𝑠,𝑓 𝑒𝑎𝑡, 𝑊𝑠,𝑡𝑒𝑚𝑝 are weight matrices of self-passing gate on\\r\\neach path, 𝑊𝑇\\r\\n𝑚 is the weight matrix of the mutual-passing gate,\\r\\n𝑡𝑎𝑛ℎ(·) and 𝜎(·) are the Tanh and Sigmoid activation functions,\\r\\n𝑔𝑓 𝑒𝑎𝑡 and 𝑔𝑡𝑒𝑚𝑝 are the self-passing gate on each path, 𝑜𝑓 𝑒𝑎𝑡 and\\r\\n𝑜𝑡𝑒𝑚𝑝 are the information passed through the self-passing gates,\\r\\nand 𝑀 is the information fused by the mutual-passing gate. The\\r\\nself-passing gate adjusts the effective information of each path itself,\\r\\nwhile the mutual-passing gate combines the information of both\\r\\npaths to control the passing ratio of the two paths.\\r\\n3.4 Decoder and decomposed fitting for stock\\r\\nprediction\\r\\nAfter processing through several Encoder Blocks, the model compre\\x02hensively extracts and integrates temporal and spatial information,\\r\\nrepresented as 𝑀 ∈ R\\r\\n𝑁 ×𝐹 ×𝑑𝑔 . Given that we have inverted the\\r\\ninput, the encoding process is no longer sequential. Consequently,\\r\\nwe deviate from the traditional transformer decoding approach.\\r\\nInstead, we employ a Multilayer Perceptron (MLP) to map the en\\x02coded information directly to the prediction target. Initially, we\\r\\nintegrate information from various features:\\r\\n𝑀𝑤 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (𝑊𝑇\\r\\n1𝑑𝑀)\\r\\n𝑀𝑚 = 𝑆𝑢𝑚(𝑀𝑤 · 𝑀)\\r\\n(16)\\r\\nwhere𝑊1𝑑is an 𝑁 ×1 mapping matrix. By applying the 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (·)\\r\\nfunction and the𝑊1𝑑, , each feature is assigned a weight that reflects\\r\\nits importance. Then aggregates all features to form a new mixed\\r\\nencoding 𝑀𝑚 ∈ R\\r\\n𝑁 ×𝑑𝑔 .\\r\\nFor stock prediction problems, the mean of each stocks may be\\r\\nrelatively stable over a period. Each time step can be seen as a\\r\\ndeviation from this mean. We use MLP to fit both the mean and\\r\\ndeviation of each time series, and let the output be the sum of the\\nDouble-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting KDD ’25, August 3-7, 2025, Toronto, ON, Canada\\r\\nmean and deviation.\\r\\n𝑚𝑒𝑎𝑛 = 𝑊𝑇\\r\\n𝑚𝑒𝑎𝑛𝑀𝑚 + 𝑏𝑚𝑒𝑎𝑛\\r\\n𝑑𝑒𝑣 = 𝑡𝑎𝑛ℎ(𝑊𝑇\\r\\n𝑑𝑒𝑣𝑀𝑚 + 𝑏𝑑𝑒𝑣 )\\r\\n𝑦ˆ = 𝑚𝑒𝑎𝑛 + 𝑒\\r\\n𝑑𝑒𝑣\\r\\n(17)\\r\\nThe prediction target is decomposed into an insensitive mean pre\\x02diction and a sensitive deviation prediction. This approach enhances\\r\\nrobustness and improves prediction performance. When each time\\r\\nstep undergoes minor changes, the mean prediction remains rela\\x02tively stable, while the sensitive deviation prediction provides an\\r\\naccurate fit. Conversely, when each time step experiences signif\\x02icant changes, the mean prediction adjusts accordingly, whereas\\r\\nthe deviation prediction remains within a certain range, thereby\\r\\npreventing overfitting.\\r\\n3.4.1 Loss. To attain high performance in time series prediction\\r\\ntasks, we consider both the accuracy of the predictions and the\\r\\ncorrelation between predictions across time steps in the loss func\\x02tion. Initially, we utilize Mean Squared Error loss L𝑚𝑠𝑒 as the loss\\r\\nfunction to evaluate the accuracy of the predictions and employ\\r\\nthe Pearson correlation coefficient loss L𝑝𝑒𝑎𝑟𝑠𝑜𝑛 to gauge the cor\\x02relation of predictions within a time step.\\r\\nThe final form of the loss function is expressed as follows:\\r\\nL = 𝜆𝑚L𝑚𝑠𝑒 + L𝑝𝑒𝑎𝑟𝑠𝑜𝑛 (18)\\r\\nwhere 𝜆𝑚 denotes the weight of L𝑚𝑠𝑒 which is usually set to 0.1\\r\\nto ensure that the two loss functions are of the same order of\\r\\nmagnitude.\\r\\nDataset Samples Node Partition\\r\\nCSI500 3159 500 2677/239/243\\r\\nCSI1000 3159 1000 2677/239/243\\r\\nNASDAQ 1225 1026 738/253/234\\r\\nNYSE 1225 1737 738/253/234\\r\\nTable 1: The overall information for datasets\\r\\n4 EXPERIMENTS\\r\\nIn this section, we conduct experiments to demonstrate the effec\\x02tiveness of our method.\\r\\n4.1 Experimental Setup\\r\\n4.1.1 Datasets. We conducted experiments on four datasets: CSI500,\\r\\nCSI1000, NASDAQ, NYSE. Brief statistical information is listed in\\r\\nTable 1. Detailed information about the datasets can be found in\\r\\nthe appendix.\\r\\n4.1.2 Baseline. We choose LSTM-based neural networks: FC-LSTM[8],\\r\\nTPA-LSTM[16], spatial-temporal graph neural networks: ASTGCN\\r\\n[9], MSTGCN [10], MTGNN [19], STEMGNN [3], STGCN [22],\\r\\nDCRNN[12] and transformer-based networks: Itransformer[14],Crossformer[24]\\r\\nas baseline to compare the performance of the models in all direc\\x02tions.\\r\\n4.1.3 Metrics. We evaluate the performances of all baseline by\\r\\nnine metrics which are commonly used in stock prediction task\\r\\nincluding Information Coefficient (IC), Profits and Losses (PNL), An\\x02nual Return (A_RET), Volatility (A_VOL), Max Drawdown (MAXD),\\r\\nSharpe Ratio (SHARPE), Win Rate (WINR), Profit/Loss Ratio (PL).\\r\\n4.2 Main Results\\r\\nWe compare our proposed model DPA-STIFormer, with 10 baseline\\r\\nmodels, encompassing three common types of models in time series\\r\\nforecasting: RNNs, Transformers, and STGNNs. The comparison\\r\\nresults are presented in Table 2. Overall, our model achieves the\\r\\nbest results across all four datasets. Our model consistently attains\\r\\noptimal results on multiple metrics simultaneously, and its perfor\\x02mance improves with larger datasets. Compared to STGNNs such\\r\\nas ASTGCN, our model improves IC by an average of 30%, A_RET\\r\\nby an average of 20%, and WINR by more than 3% on average. The\\r\\nsignificant improvement is attributed to our model’s ability to com\\x02prehensively capture the correlations between node time series.\\r\\nMany spatial-temporal graph neural networks rely on predefined\\r\\ngraphs, or the relationships between nodes learned by these mod\\x02els are not sufficiently comprehensive. This also underscores the\\r\\neffectiveness of our DPABlock.\\r\\nCompared to RNNs, our model demonstrates superior perfor\\x02mance, particularly in terms of IC. On average, our model achieves\\r\\nmore than a 30% improvement in both PNL and A_RET. Additionally,\\r\\nSHARPE shows significant improvement in the CSI500, NASDAQ,\\r\\nand NYSE datasets, ranking second only to MTGNN in the CSI1000\\r\\ndataset. In comparison to Transformer-based models, our model\\r\\nalso outperforms significantly. Transformers models, exhibit much\\r\\nlower IC values than our model and STGNNs. Metrics such as WINR\\r\\nand A_RET further highlight that former-based models perform\\r\\npoorly in stock prediction without this crucial node correlation\\r\\ninformation. This also indicates the ineffectiveness of using time as\\r\\ntokens for modeling time series, aligning with the findings of Liu\\r\\net al. [14]. Our model, on the other hand, uses features as tokens to\\r\\nmodel time series and employs a double-path approach to compre\\x02hensively capture the correlations between nodes. Using a gating\\r\\nmechanism, it effectively integrates information from both node\\r\\ncorrelations and time series. The experimental results validate the\\r\\neffectiveness of our method.\\r\\n4.3 Ablation Study\\r\\nTo validate the effectiveness of the key components, we conduct\\r\\nan ablation study on the CSI500 dataset, comparing the changes in\\r\\nIC, A_RET, and SHARPE. Specifically, we tested the performance\\r\\nimpact of removing the following components: DPgate, temporal\\r\\npath, feature path, Inverted Temporal Block, and importance weight.\\r\\nFirstly, removing any path from the double-path structure or the\\r\\nfinal gating mechanism during data fusion results in approximately\\r\\na 15% decrease in IC, with varying degrees of decrease in A_RET\\r\\nand SHARPE. The most severe drop occurs when the gating mech\\x02anism is removed, resulting in about a 25% decrease. This indicates\\r\\nthat removing any path leads to biased learning of correlations. Re\\x02moving the gating mechanism and merging the information with\\r\\nequal weights leads to rough information fusion, causing adverse\\r\\neffects. From the changes in A_RET and SHARPE, it is clear that\\nKDD ’25, August 3-7, 2025, Toronto, ON, Canada Wenbo Yan and Ying Tan\\r\\nCSI500 CSI1000\\r\\nIC PNL A_RET A_VOL MAXD SHARPE WINR PL IC PNL A_RET A_VOL MAXD SHARPE WINR PL\\r\\nSTGNN\\r\\nASTGCN 0.1022 0.3758 0.3727 0.0872 0.1813 4.2740 0.6281 2.0069 0.0840 0.7716 0.7652 0.1019 0.1285 6.8903 0.6694 3.0884\\r\\nSTGCN 0.0349 0.0466 0.0462 0.0676 0.1693 0.6841 0.5537 1.1236 0.0404 0.0388 0.0385 0.0865 0.2850 0.4449 0.5000 0.9310\\r\\nMSTGCN 0.0732 0.0328 0.0325 0.0694 0.2246 0.4692 0.5793 1.9280 0.0919 0.8804 0.8732 0.1180 0.1333 7.3996 0.6901 3.2232\\r\\nMTGNN 0.1176 0.4652 0.4614 0.1006 0.1652 4.5845 0.6116 2.1265 0.0941 1.1608 1.1512 0.0756 0.0324 15.2259 0.7769 23.4614\\r\\nSTEMGNN 0.0845 0.2124 0.2107 0.0842 0.1223 2.5016 0.5702 1.5455 0.0176 0.0154 0.0153 0.0205 0.0141 0.7448 0.0455 1.6529\\r\\nDCRNN 0.0609 0.0899 0.0892 0.0839 0.2084 1.0630 0.5289 1.1794 0.0420 0.1080 0.1071 0.0850 0.2552 1.2591 0.4793 0.8216\\r\\nRNN\\r\\nTPA_LSTM 0.0052 -0.0959 -0.0951 0.0717 0.2364 -1.3272 0.4959 0.7986 0.0344 0.0413 0.0409 0.0811 0.2304 0.5048 0.4959 0.9201\\r\\nFC_LSTM 0.1193 0.5402 0.5357 0.1056 0.1428 5.0750 0.6033 2.3893 0.0482 1.0291 1.0206 0.1041 0.1053 9.8080 0.7190 5.4323\\r\\nFORMER\\r\\nCROSSFORMER 0.0134 0.1447 0.1435 0.0779 0.2596 1.8423 0.4463 0.7295 0.0365 0.3537 0.3508 0.0964 0.2412 3.6383 0.5909 1.7650\\r\\nITRANSFORMER 0.0225 -0.0517 -0.0513 0.1031 0.2853 -0.4976 0.5331 0.9226 0.0391 0.1947 0.1931 0.0945 0.2717 2.0267 0.5614 1.3757\\r\\nDPA-STIFormer 0.1312 0.8794 0.8721 0.1033 0.1164 8.4395 0.7273 4.2453 0.1404 1.2809 1.2703 0.1111 0.0791 12.4611 0.8058 9.2447\\r\\nNYSE NASDAQ\\r\\nIC PNL A_RET A_VOL MAXD SHARPE WINR PL IC PNL A_RET A_VOL MAXD SHARPE WINR PL\\r\\nSTGNN\\r\\nASTGCN 0.0400 0.0314 0.0322 0.3251 0.3936 0.9893 0.5171 0.8252 0.0323 0.2578 0.2644 0.1879 0.1369 1.4070 0.5128 1.2688\\r\\nSTGCN 0.0150 0.1009 0.1034 0.2707 0.1926 0.3821 0.5128 1.0667 0.0383 0.3145 0.3225 0.2408 0.1766 1.3396 0.5085 1.2675\\r\\nMSTGCN 0.0351 0.0767 0.0786 0.2887 0.3035 0.2723 0.5085 1.0489 0.0227 0.2588 0.2655 0.1776 0.0973 1.1524 0.4915 1.2845\\r\\nMTGNN -0.0035 -0.0291 -0.0299 0.1209 0.0604 -0.1000 0.4957 0.9831 0.0163 0.1375 0.1410 0.1537 0.1076 0.9179 0.5385 1.1803\\r\\nSTEMGNN 0.0254 -0.1557 -0.1597 0.2335 0.2691 -0.6839 0.4915 0.8772 0.0171 0.0273 0.0280 0.2240 0.2597 0.1249 0.4786 1.0224\\r\\nDCRNN 0.0016 0.0122 0.0125 0.1642 0.2281 0.0763 0.4701 1.0128 -0.0056 0.0253 0.0259 0.1394 0.1273 0.1858 0.5284 1.0325\\r\\nRNN\\r\\nTPA_LSTM 0.0260 0.1729 0.1774 0.2557 0.3244 0.6937 0.5385 1.2037 0.0270 -0.0899 -0.0922 0.2065 0.3087 -0.4465 0.4786 0.9253\\r\\nFC_LSTM -0.0012 0.1255 0.1287 0.3391 0.3694 0.3795 0.4872 1.0676 0.0083 0.2479 0.2542 0.2206 0.2393 1.2508 0.4829 1.2476\\r\\nFORMER\\r\\nCROSSFORMER 0.0097 0.0204 0.0209 0.2767 0.2766 0.0756 0.5128 1.0128 0.0231 0.0611 0.0627 0.2081 0.1436 0.3010 0.4786 1.0544\\r\\nITRANSFORMER 0.0011 -0.5296 -0.5431 0.1141 0.5510 -4.7592 0.4316 0.3288 -0.0097 -0.0931 -0.0954 0.1158 0.1185 -0.8241 0.5369 0.5229\\r\\nDPA-STIFormer 0.0453 0.2135 0.2190 0.2987 0.1999 1.8115 0.5427 1.1256 0.0416 0.3902 0.4002 0.3199 0.1462 1.4945 0.5470 1.2320\\r\\nTable 2: Comparison results on four datasets. ↓ indicates that the smaller the metric is better. The best result is in bold.\\r\\nIC A_RET SHARPE\\r\\nDPA-STIFormer 0.1312 0.8721 8.4395\\r\\nw/o DPgate 0.1170 0.4804 4.4671\\r\\nw/o temporal path 0.1135 0.6465 6.7517\\r\\nw/o feature path 0.1190 0.5154 6.1406\\r\\nw/o Inverted Temporal Block 0.1010 0.5819 5.6339\\r\\nw/o importance weigh 0.1100 0.6142 5.0505\\r\\nTable 3: Ablation Study\\r\\nthe information learned from the feature path has a greater impact\\r\\non the results.\\r\\nCompared to the model without the Inverted Temporal Block,\\r\\nremoving this block results in a significant decrease in IC and\\r\\nSHARPE. This indicates a decline in overall predictive performance\\r\\nand increased instability, as the absence of individual denoising\\r\\nand modeling of each time series negatively affects the modeling of\\r\\ncorrelations. For the model without importance weight, the results\\r\\nshow that including feature importance enhances overall predictive\\r\\ncapability and returns, enabling the model to better utilize different\\r\\nfeatures to model node characteristics. However, the decrease in\\r\\nSHARPE suggests that the addition of importance weight also leads\\r\\nto an increase in risk.\\r\\n4.4 Hyper-parameter Study\\r\\nWe conducted extensive parameter experiments on the CSI500\\r\\ndataset to examine the effects of Layer Number 𝑛𝑙, the number of\\r\\nattention heads 𝑛ℎ, and the dimensions of the feedforward neural\\r\\nnetwork 𝑑𝑚𝑜𝑑𝑒𝑙 . For 𝑛𝑙, we found that the model performs best\\r\\nwith 3 layers. When the number of layers is less than 3, the IC is\\r\\n2 3 4\\r\\nLayer Number\\r\\n0.110\\r\\n0.115\\r\\n0.120\\r\\n0.125\\r\\n0.130\\r\\nIC\\r\\nIC\\r\\nWinR\\r\\n2 4 8\\r\\nHead Number\\r\\n0.10\\r\\n0.11\\r\\n0.12\\r\\n0.13\\r\\nIC\\r\\nIC\\r\\nWinR\\r\\n128 256 512\\r\\nFeedForward Dim\\r\\n0.1050\\r\\n0.1075\\r\\n0.1100\\r\\n0.1125\\r\\n0.1150\\r\\n0.1175\\r\\n0.1200\\r\\nIC\\r\\nIC\\r\\nWinR\\r\\n0.66\\r\\n0.68\\r\\n0.70\\r\\n0.72\\r\\nWinR\\r\\n0.62\\r\\n0.64\\r\\n0.66\\r\\n0.68\\r\\n0.70\\r\\n0.72\\r\\nWinR\\r\\n0.61\\r\\n0.62\\r\\n0.63\\r\\n0.64\\r\\n0.65\\r\\n0.66\\r\\n0.67\\r\\n0.68\\r\\nWinR\\r\\nFigure 2: Hyper-parameter Study\\r\\nrelatively low, indicating that the model suffers from overfitting\\r\\ndue to insufficient parameters, leading to inadequate overall pre\\x02dictive capability. When the model has more than 3 layers, there\\r\\nis a significant decrease in WINR, likely because the data is insuf\\x02ficient to support training, resulting in underfitting. Since each\\r\\nadditional layer significantly increases the model parameters, only\\r\\n1 or 2 layers are needed for smaller datasets. Regarding 𝑛ℎ„ the\\r\\noptimal performance was observed with 4 heads. With fewer heads,\\r\\nthe model cannot effectively capture all relationships. Conversely,\\r\\nwith too many heads, the model tends to overfit, diminishing its\\r\\noverall predictive capability. Comparing different 𝑑𝑚𝑜𝑑𝑒𝑙 , we see\\r\\nthat smaller dimensions greatly impact model performance, because\\r\\neach token cannot be fully encoded and distinguished. However,\\r\\nwhen the 𝑑𝑚𝑜𝑑𝑒𝑙 is too large, the model’s parameter count increases\\r\\nrapidly, leading to gradual underfitting.\\r\\n4.5 Visualization\\r\\n4.5.1 Double-path Visualization. To deeply analyze the Double\\x02path adaptive correlation block, we visualized the attention com\\x02putation results of the final layer of the Double-path attention\\r\\nmechanism. It can be observed that the attention matrix obtained\\r\\nfrom the N-neighbor correlation attention is sparse, with each node\\nDouble-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting KDD ’25, August 3-7, 2025, Toronto, ON, Canada\\r\\nFigure 3: Visualization of Double-path Attention\\r\\nfocusing on its nearest N nodes. By comparing the two attention\\r\\nmatrices, we can find that the Feature-path and Temporal-path\\r\\nhave learned two different types of adjacency relationships. In the\\r\\nTemporal-path Attention, there are certain leading nodes that in\\x02fluence all other nodes. Conversely, in the Feature-path Attention,\\r\\nthe adjacency relationships between nodes are more complex, with\\r\\nthe N-neighbors of different nodes being more diversified.\\r\\n0.900 0.875 0.850 0.825 0.800 0.775 0.750 0.725\\r\\nValue\\r\\n0\\r\\n10\\r\\n20\\r\\n30\\r\\n40\\r\\n50\\r\\n60\\r\\n70\\r\\nFrequency\\r\\nShort Term Mean\\r\\n0.8 0.7 0.6 0.5 0.4\\r\\nValue\\r\\n0\\r\\n10\\r\\n20\\r\\n30\\r\\n40\\r\\n50\\r\\nFrequency\\r\\nShort Term Deviation\\r\\n0.8 0.7 0.6 0.5 0.4 0.3\\r\\nValue\\r\\n0\\r\\n100\\r\\n200\\r\\n300\\r\\n400\\r\\n500\\r\\nFrequency\\r\\nLong Term Mean\\r\\n0.950 0.925 0.900 0.875 0.850 0.825 0.800 0.775 0.750\\r\\nValue\\r\\n0\\r\\n100\\r\\n200\\r\\n300\\r\\n400\\r\\n500\\r\\n600\\r\\n700\\r\\nFrequency\\r\\nLong Term Deviation\\r\\nFigure 4: Visualization of Mean-deviation Prediction\\r\\n4.5.2 Mean-deviation Prediction Visualization. We visualize the\\r\\ndistributions of mean and deviation for the short and long term to\\r\\nbetter illustrate the advantages of our Mean-deviation Prediction.\\r\\nThe short-term refers to a period of continuous time-step prediction,\\r\\nand it can be seen that the distribution of the mean is denser while\\r\\nthe distribution of the deviation is sparser, which indicates that in\\r\\ncontinuous prediction, the mean of our prediction method varies\\r\\nslowly, fitting the variation of the time series through the deviation.\\r\\nLong-term refers to the prediction after an interval of several time\\r\\nsteps, where the range of the mean value has changed significantly\\r\\nwhile the deviation is still concentrated in a smaller range. This\\r\\nindicates that after an interval of time steps, Mean-deviation Pre\\x02diction relies on the mean value to fit the drastic change in the time\\r\\nseries and avoids too large a change in the deviation.\\r\\n5 RELATED WORK\\r\\n5.1 Spatial-Temporal Graph Neural Networks\\r\\nIn recent years, there has been a burgeoning interest in integrating\\r\\ntemporal and spatial information within time series data, leading to\\r\\nthe development of a series of models known as Spatial-Temporal\\r\\nGraph Neural Networks. STGCN [22] was the first to introduce\\r\\nGraph Convolutional Neural Networks (GCNs) [11] into time series\\r\\nforecasting. Subsequently, additional convolution-based models,\\r\\nsuch as Graph Wave Net [20], MT-GNN [19], StemGNN [3], H\\x02STGCN [4], and GSTNet [7], have been proposed. These models\\r\\nincorporate various gating mechanisms atop convolutions to en\\x02hance feature capture. Concurrently, other studies have focused\\r\\non more complex convolutional structures, such as ST-GDN [31]\\r\\nand ST-ResNet [23], which achieve improved performance through\\r\\nsophisticated architectural designs and mechanisms.\\r\\nMoreover, some works, including ARGCN [1], DCRNN [12],\\r\\nand TGCN [25], combine Recurrent Neural Networks (RNNs) with\\r\\nGraph Neural Networks (GNNs). Additionally, with the advent of\\r\\nTransformers, many models have integrated transformer architec\\x02tures or attention mechanisms into spatial-temporal modeling, as\\r\\nevidenced by ASTGCN [9], STGNN [18], and GMAN [26].\\r\\n5.2 Spatial-Temporal Transformer\\r\\nRecent advancements in spatial-temporal modeling have seen the in\\x02tegration of Transformers to capture complex dependencies across\\r\\ntime and space. Notable models such as the TFT [13] and the STTN\\r\\n[21] have demonstrated the efficacy of combining temporal and spa\\x02tial attention mechanisms. These models leverage the self-attention\\r\\nmechanism to effectively handle the intricacies of spatial-temporal\\r\\ndata, thereby improving forecasting accuracy and interpretability.\\r\\n6 CONCLUSION\\r\\nIn this paper, we proposes a novel DPA-STIFormer. The Inverted\\r\\nTemporal Block models the temporal sequences of each node by\\r\\ntreating features as tokens and introduces importance weights to\\r\\nenable the attention mechanism to consider the significance of fea\\x02tures. The Double-Path Adaptive-correlation Block is introduced to\\r\\nmodel the correlations between nodes. The Double Direction Self\\x02adaptation Fusion adaptively blends temporal and feature perspec\\x02tives from the node embeddings, modeling inter-node correlations\\r\\nfrom both paths simultaneously. Finally, the proposed Double-path\\r\\nGating Mechanism integrates the encodings from both paths. Ex\\x02tensive experiments on four real-world stock datasets, along with\\r\\nvisualization of the learned correlations, demonstrate the superior\\x02ity of our approach.\\r\\nReferences\\r\\n[1] Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive graph\\r\\nconvolutional recurrent network for traffic forecasting. Advances in neural\\r\\ninformation processing systems 33 (2020), 17804–17815.\\r\\n[2] George E. P. Box and Gwilym M. Jenkins. 1970. Time Series Analysis: Forecasting\\r\\nand Control. Holden-Day.\\r\\n[3] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang,\\r\\nYunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, et al. 2020. Spectral temporal graph\\r\\nneural network for multivariate time-series forecasting. Advances in neural\\r\\ninformation processing systems 33 (2020), 17766–17778.\\r\\n[4] Rui Dai, Shenkun Xu, Qian Gu, Chenguang Ji, and Kaikui Liu. 2020. Hybrid\\r\\nspatio-temporal graph convolutional network: Improving traffic prediction with\\nKDD ’25, August 3-7, 2025, Toronto, ON, Canada Wenbo Yan and Ying Tan\\r\\nnavigation data. In Proceedings of the 26th acm sigkdd international conference on\\r\\nknowledge discovery & data mining. 3074–3082.\\r\\n[5] Wei Dai, Yuan An, and Wen Long. 2022. Price change prediction of ultra high\\r\\nfrequency financial data based on temporal convolutional network. Procedia\\r\\nComputer Science 199 (2022), 1177–1183.\\r\\n[6] Robert F Engle. 1982. Autoregressive conditional heteroscedasticity with esti\\x02mates of the variance of United Kingdom inflation. Econometrica: Journal of the\\r\\neconometric society (1982), 987–1007.\\r\\n[7] Shen Fang, Qi Zhang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. 2019.\\r\\nGSTNet: Global Spatial-Temporal Network for Traffic Flow Prediction.. In IJCAI.\\r\\n2286–2293.\\r\\n[8] Alex Graves. 2013. Generating sequences with recurrent neural networks. In\\r\\narXiv preprint arXiv:1308.0850.\\r\\n[9] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.\\r\\nAttention based spatial-temporal graph convolutional networks for traffic flow\\r\\nforecasting. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.\\r\\n922–929.\\r\\n[10] Ziyu Jia, Youfang Lin, Jing Wang, Xiaojun Ning, Yuanlai He, Ronghao Zhou,\\r\\nYuhan Zhou, and H Lehman Li-wei. 2021. Multi-view spatial-temporal graph\\r\\nconvolutional networks with domain generalization for sleep stage classification.\\r\\nIEEE Transactions on Neural Systems and Rehabilitation Engineering 29 (2021),\\r\\n1977–1986.\\r\\n[11] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\\r\\nconvolutional networks. arXiv preprint arXiv:1609.02907 (2016).\\r\\n[12] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2017. Diffusion convolu\\x02tional recurrent neural network: Data-driven traffic forecasting. arXiv preprint\\r\\narXiv:1707.01926 (2017).\\r\\n[13] Bryan Lim, Sabri Eyuboglu Arik, Nicolai Loeff, and Tomas Pfister. 2021. Temporal\\r\\nFusion Transformers for Interpretable Multi-horizon Time Series Forecasting.\\r\\nInternational Journal of Forecasting 37, 4 (2021), 1748–1764.\\r\\n[14] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and\\r\\nMingsheng Long. 2023. itransformer: Inverted transformers are effective for time\\r\\nseries forecasting. arXiv preprint arXiv:2310.06625 (2023).\\r\\n[15] Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison\\r\\nCottrell. 2017. A dual-stage attention-based recurrent neural network for time\\r\\nseries prediction. arXiv preprint arXiv:1704.02971 (2017).\\r\\n[16] Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. 2019. Temporal pattern attention\\r\\nfor multivariate time series forecasting. Machine Learning 108 (2019), 1421–1441.\\r\\n[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\r\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\\r\\nyou need. Advances in neural information processing systems 30 (2017).\\r\\n[18] Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan\\r\\nJia, and Jian Yu. 2020. Traffic flow prediction via spatial temporal graph neural\\r\\nnetwork. In Proceedings of the web conference 2020. 1082–1092.\\r\\n[19] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi\\r\\nZhang. 2020. Connecting the dots: Multivariate time series forecasting with graph\\r\\nneural networks. In Proceedings of the 26th ACM SIGKDD international conference\\r\\non knowledge discovery & data mining. 753–763.\\r\\n[20] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.\\r\\nGraph wavenet for deep spatial-temporal graph modeling. arXiv preprint\\r\\narXiv:1906.00121 (2019).\\r\\n[21] Ke Xu, Yihong Li, Yun Li, Xiaohui Jiang, Jun Duan, Yi Wang, and Yue Lin. 2021.\\r\\nSpatio-Temporal Transformer Networks for Traffic Flow Forecasting. IEEE Trans\\x02actions on Intelligent Transportation Systems 22, 11 (2021), 7137–7147.\\r\\n[22] Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2017. Spatio-temporal graph con\\x02volutional networks: A deep learning framework for traffic forecasting. arXiv\\r\\npreprint arXiv:1709.04875 (2017).\\r\\n[23] Xiyue Zhang, Chao Huang, Yong Xu, Lianghao Xia, Peng Dai, Liefeng Bo, Junbo\\r\\nZhang, and Yu Zheng. 2021. Traffic flow forecasting with spatial-temporal graph\\r\\ndiffusion network. In Proceedings of the AAAI conference on artificial intelligence,\\r\\nVol. 35. 15008–15015.\\r\\n[24] Yunhao Zhang and Junchi Yan. 2022. Crossformer: Transformer utilizing cross\\x02dimension dependency for multivariate time series forecasting. In The Eleventh\\r\\nInternational Conference on Learning Representations.\\r\\n[25] Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and\\r\\nHaifeng Li. 2019. T-gcn: A temporal graph convolutional network for traffic\\r\\nprediction. IEEE transactions on intelligent transportation systems 21, 9 (2019),\\r\\n3848–3858.\\r\\n[26] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: A\\r\\ngraph multi-attention network for traffic prediction. In Proceedings of the AAAI\\r\\nconference on artificial intelligence, Vol. 34. 1234–1241.\\nDouble-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting KDD ’25, August 3-7, 2025, Toronto, ON, Canada\\r\\nA Dataset\\r\\n• CSI500: CSI500 is a stock dataset that contains the perfor\\x02mance of 500 small and medium-sized companies listed on\\r\\nthe Shanghai and Shenzhen stock exchanges. It contains\\r\\ndaily frequency data for 500 stocks, with a total time step of\\r\\n3159 and a feature number of 45.\\r\\n• CSI1000: CSI1000 contains daily frequency data for 1000\\r\\nstocks, with a total time step of 3159 and a feature number\\r\\nof 45.\\r\\n• NASDAQ: NASDAQ contains daily frequency data for 1026\\r\\nstocks in National Association of Securities Dealers Auto\\x02mated Quotations , with a total time step of 1225 and a feature\\r\\nnumber of 5.\\r\\n• NYSE: NYSE contains daily frequency data for 1737 stocks\\r\\nin New York Stock Exchange, with a total time step of 1225\\r\\nand a feature number of 5.\\r\\nA.1 Model Implementation\\r\\nWe set the time window to 30, meaning each moment looks back at\\r\\nthe past 30 time steps. The dimensions of the feedforward neural\\r\\nnetwork and all attention mechanisms are set to 256. The number\\r\\nof attention heads is set to 4. For the CSI500 and CSI1000 datasets,\\r\\n3 layers of DPA-IEncoder are used. For the NASDAQ and NYSE\\r\\ndatasets, which have less data, only 1 and 2 layers of DPA-IEncoder\\r\\nare used.\\r\\nB Experimental environment\\r\\nWe ran the experiment on a server containing 4 NVIDIA GeForce\\r\\nRTX 3090 and AMD EPYC 7282 16-Core Processor, repeating each\\r\\nexperiment five times and averaging the results.\\r\\nC Metrics\\r\\n• IC: the Pearson correlation between predicted ranking scores\\r\\nand real ranking scores, widely used to evaluate the perfor\\x02mance of stock ranking.\\r\\n𝐼𝐶 =\\r\\nCov(𝑌, 𝑌 ˆ )\\r\\n𝜎𝑌ˆ 𝜎𝑌\\r\\n• PNL: the aggregate profits and losses of trading strategies,\\r\\ni.e., the cumulative returns.\\r\\n𝑃𝑁 𝐿 =\\r\\n∑︁\\r\\n𝑁𝑑\\r\\n𝑛=1\\r\\n𝑟𝑛\\r\\n• AR: the annualized returns of trading strategies, denoting\\r\\nthe profitability of portfolios. In the experiments, the number\\r\\nof trading days in one year is assigned as 240.\\r\\n𝐴𝑅 =\\r\\n240\\r\\n𝑁𝑑\\r\\n∑︁\\r\\n𝑁𝑑\\r\\n𝑛=1\\r\\n𝑟𝑛 × 100%\\r\\n• VOL: the annualized volatility of portfolio returns, indicating\\r\\nthe stability of portfolios.\\r\\n𝑉𝑂𝐿 = 𝜎(𝑅) × √240 × 100%\\r\\n• MDD: a commonly used metric to measure risk control abil\\x02ity, which is the largest decline in portfolios from peak to\\r\\ntrough.\\r\\n𝑀𝐷𝐷 = 𝑟𝑚𝑎𝑥 − 𝑟𝑚𝑖𝑛 × 100%\\r\\n• Sharpe: a widely-used metric to measure the investment\\r\\nquality of portfolios, which is the ratio of the average and\\r\\nstandard deviation of portfolio returns, considering both of\\r\\nprofitability and investment risk.\\r\\n𝑆ℎ𝑎𝑟𝑝𝑒 =\\r\\n𝜇(𝑅)\\r\\n𝜎(𝑅)\\r\\n×\\r\\n√\\r\\n240\\r\\nwhere 𝑅 = (𝑟1, 𝑟2, ..., 𝑟(𝑁𝑑) is the portfolio returns, and the\\r\\nannualized values of SR are used in the experiments.\\r\\n• Calmar: another metric to measure the quality of portfolios,\\r\\nwhich is the ratio of average returns and Max Drawdown\\r\\n(MDD).\\r\\n𝐶𝑎𝑙𝑚𝑎𝑟 =\\r\\n𝜇(𝑅)\\r\\n𝑀𝐷𝐷\\r\\n• WinR: a widely-used metric to evaluate the accuracy of the\\r\\npredictions, which is the ratio of the number of positive\\r\\npurchases to the total number of purchases.\\r\\n𝑊 𝑖𝑛𝑅 =\\r\\n𝑁+\\r\\n𝑁𝑡𝑜𝑡𝑎𝑙\\r\\n× 100%\\r\\n• PL-ratio: the profit/loss ratio is the average profit on winning\\r\\ntrades divided by the average loss on losing trades over a\\r\\nspecified time period.\\r\\n𝑃𝐿 − 𝑟𝑎𝑡𝑖𝑜 =\\r\\nÍ\\r\\n𝑟+\\r\\n𝑁𝑤𝑖𝑛 \\x1e Í\\r\\n𝑟−\\r\\n𝑁𝑙𝑜𝑠𝑒\\r\\nC.1 Losses\\r\\nMSE Loss:\\r\\nL𝑚𝑠𝑒 =\\r\\n1\\r\\n𝑁\\r\\n∑︁\\r\\n𝑁\\r\\n𝑖=1\\r\\n(𝑦b𝑖 − 𝑦𝑖)\\r\\n2\\r\\nPearson Correlation Coefficient Loss:\\r\\nL𝑝𝑒𝑎𝑟𝑠𝑜𝑛 = −\\r\\nÍ𝑁\\r\\n𝑖=1\\r\\n(𝑦𝑖 − 𝑦) (𝑦b𝑖 − 𝑦b)\\r\\n√︃Í𝑁\\r\\n𝑖=1\\r\\n(𝑦𝑖 − 𝑦)\\r\\n2\\r\\n√︃Í𝑁\\r\\n𝑖=1\\r\\n(𝑦b𝑖 − 𝑦b)\\r\\n2'},\n",
       " {'name': '2308.11421v1.pdf',\n",
       "  'content': 'TurboViT: Generating Fast Vision Transformers via\\r\\nGenerative Architecture Search\\r\\nAlexander Wong1,2,3,∗, Saad Abbasi1,2,3,∗, Saeejith Nair1,2\\r\\n1University of Waterloo, Waterloo, Ontario, Canada\\r\\n2Waterloo Artificial Intelligence Institute, Waterloo, Ontario, Canada\\r\\n3DarwinAI, Waterloo, Ontario, Canada\\r\\n∗\\r\\nequal contributors\\r\\nAbstract\\r\\nVision transformers have shown unprecedented levels of performance in tackling\\r\\nvarious visual perception tasks in recent years. However, the architectural and\\r\\ncomputational complexity of such network architectures have made them chal\\x02lenging to deploy in real-world applications with high-throughput, low-memory\\r\\nrequirements. As such, there has been significant research recently on the design of\\r\\nefficient vision transformer architectures. In this study, we explore the generation\\r\\nof fast vision transformer architecture designs via generative architecture search\\r\\n(GAS) to achieve a strong balance between accuracy and architectural and compu\\x02tational efficiency. Through this generative architecture search process, we create\\r\\nTurboViT, a highly efficient hierarchical vision transformer architecture design\\r\\nthat is generated around mask unit attention and Q-pooling design patterns. The\\r\\nresulting TurboViT architecture design achieves significantly lower architectural\\r\\ncomputational complexity (> 2.47× smaller than FasterViT-0 while achieving\\r\\nsame accuracy) and computational complexity (> 3.4× fewer FLOPs and 0.9%\\r\\nhigher accuracy than MobileViT2-2.0) when compared to 10 other state-of-the-art\\r\\nefficient vision transformer network architecture designs within a similar range\\r\\nof accuracy on the ImageNet-1K dataset. Furthermore, TurboViT demonstrated\\r\\nstrong inference latency and throughput in both low-latency and batch process\\x02ing scenarios (> 3.21× lower latency and > 3.18× higher throughput compared\\r\\nto FasterViT-0 for low-latency scenario). These promising results demonstrate\\r\\nthe efficacy of leveraging generative architecture search for generating efficient\\r\\ntransformer architecture designs for high-throughput scenarios.\\r\\n1 Introduction\\r\\nVision transformers have shown unprecedented levels of performance in tackling various visual\\r\\nperception tasks in recent years [1–8]. However, the architectural and computational complexity of\\r\\nsuch network architectures have made them challenging to deploy in real-world applications with\\r\\nhigh-throughput, low-memory requirements. As such, there has been significant research recently on\\r\\nthe design of efficient vision transformer architectures [9–16]. For example, Cai et al. [13] introduced\\r\\na lightweight multi-scale attention comprised of only lightweight and hardware-efficient operations.\\r\\nVasu et al. [11] introduced a hybrid convolutional-transformer architecture design that leverages\\r\\nstructural reparameterization to reduce memory access costs. Hatamizadeh et al. [12] introduces a\\r\\nhierarchical attention approach that decomposes global self-attention into multi-level attention to\\r\\nreduce computational complexity. Another interesting approach is one proposed by Wu et al. [15],\\r\\nwhere they not only introduced convolutions to vision transformers, but also employed a data-adapted\\r\\nneural architecture search for discovering efficient vision transformer architecture designs. It is along\\r\\nthis later direction of network architecture search that we will explore for finding efficient vision\\r\\ntransformers, but accomplished instead via a generative approach.\\r\\narXiv:2308.11421v1 [cs.CV] 22 Aug 2023\\nFigure 1: Proposed TurboViT vision transformer architecture design, generated via generative archi\\x02tecture search (GAS) around global attention (GA), mask unit attention (MUA), and Q-pooling design\\r\\npatterns introduced in [7]. The values (D/H) within a ViT block indicates its hidden dimensionality\\r\\nand head count, while numbers below a block indicates the number of sequential blocks of that type.\\r\\nThe values (I/O) within a convolution block or a linear layer indicates its input dimensionality and\\r\\noutput dimensionality.\\r\\nIn this study, we explore the generation of fast vision transformer architecture designs via generative\\r\\narchitecture search (GAS) [17, 18] to achieve a strong balance between accuracy and architectural and\\r\\ncomputational efficiency. Through this generative architecture search process, we create TurboViT,\\r\\na highly efficient hierarchical vision transformer architecture design that is generated around mask\\r\\nunit attention and Q-pooling design patterns. The paper is organized as follows. Section 2 details the\\r\\ngenerative architecture search strategy used to create TurboViT as well as the resulting generated\\r\\nnetwork architecture design. Section 3 presents experiments and discussion comparing TurboViT\\r\\nwith state-of-the-art efficient vision transformer architecture designs across multiple performance\\r\\nmetrics.\\r\\n2 Methods\\r\\nGenerative Architecture Search. In this study, we leveraged generative synthesis [17] to conduct\\r\\ngenerative architecture search (GAS) to identify the architecture design of TurboViT. More specif\\x02ically, generative synthesis can be formulated as a constrained optimization problem, where the\\r\\ngoal is to identify the optimal generator G⋆(·) generating network architectures N that maximizes a\\r\\nuniversal performance function U (e.g., [19]) under a given set of operational constraints defined by\\r\\nan indicator function 1r(·),\\r\\nG = max\\r\\nG\\r\\nU(G(s)) subject to 1r(G(s)) = 1, ∀ ∈ S. (1)\\r\\nThis constrained optimization problem is solved in an iterative manner, with a full detailed description\\r\\nof the process presented in [17]. In this study, we impose the following design constraints via 1r(·) to\\r\\nidentify a vision transformer architecture design for TurboViT that achieves the desired computational\\r\\ncomplexity tailored for high-throughput scenarios:\\r\\n1. Leverage both global attention and mask unit attention design patterns as introduced in [7].\\r\\nThese design patterns have been shown to greatly streamline vision transformer architecture\\r\\ncomplexities by forgoing vision-specific components without sacrificing accuracy.\\r\\n2. Enforcing the use of Q-pooling design pattern at three locations for reducing architectural\\r\\nand computational complexity via spatial query reduction in a similar way as [7], thus\\r\\nresulting in a hierarchical architecture design.\\r\\n3. Enforcing a computational complexity constraint of 2.5 GFLOPs to ensure TurboViT has\\r\\nlow computational complexity (less than all state-of-the-art vision transformer architecture\\r\\ndesigns compared in this study) for high-throughput scenarios.\\r\\nArchitecture Design. Figure 1 demonstrates the TurboViT architecture design generated via genera\\x02tive architecture search. Overall, it can be observed that the architecture design is quite clean and\\r\\n2\\nTable 1: Top-1 accuracy, number of parameters, and number of FLOPs of TurboViT in comparison to\\r\\n10 state-of-the-art efficient vision transformer architecture designs with a similar range of accuracy\\r\\nfor image classification on the ImageNet-1K dataset. Best results are in bold.\\r\\nModel Top-1 Accuracy # Parameters FLOPs\\r\\n(%) (M) (G)\\r\\nMobileViTv2-2.0 [10] 81.2 18.5 7.5\\r\\nMViTv2-T [8] 82.3 24.0 5.0\\r\\nSwin-T [5] 81.3 29.0 4.5\\r\\nSwinV2-T [6] 81.8 28.3 4.4\\r\\nCvT-13-NAS [15] 82.2 18.0 4.1\\r\\nFastViT-SA24 [11] 82.6 20.6 3.8\\r\\nLITv2-S [20] 82.0 28.0 3.7\\r\\nFasterViT-0 [12] 82.1 31.4 3.3\\r\\nPiT-S [16] 81.9 23.5 2.9\\r\\nTwins-S [21] 81.7 24.1 2.8\\r\\nTurboViT 82.1 12.7 2.2\\r\\nstreamlined when compared to other state-of-the-art efficient vision transformer architecture designs\\r\\n(particularly more complex hybrid convolutional-transformer architecture designs), consisting largely\\r\\nof a sequence of ViT blocks with relatively low hidden dimensionalities as well as relatively low\\r\\nhead counts (especially when compared to ViT [1]), thus facilitating for greater architectural and\\r\\ncomputational efficiency.\\r\\nAs expected, it can be observed that the TurboViT architecture design consists of Q-pooling at three\\r\\ndifferent locations in the architecture design to allow for architectural and computational efficiencies\\r\\nthrough spatial reductions, with the majority of layers located after the second Q-pooling. It can\\r\\nalso be observed that earlier ViT blocks in the TurboViT architecture design leverage local attention\\r\\nvia mask unit attention while later ViT blocks leverage global attention, thus not harnessing global\\r\\nattention where it is less useful in exchange for significant gains in computational efficiency.\\r\\nA particularly interesting observation about the TurboViT architecture design is the presence of a\\r\\nhidden dimensionality condensation mechanism imposed at the beginning of the architecture design,\\r\\nwhere the hidden dimensionality is greatly reduced at the second ViT block to form a highly condensed\\r\\nembedding when compared to the first ViT block before the hidden dimensionality progressively\\r\\nincreases as we move down the architecture. Such a condensation mechanism appears to be effective\\r\\nat greatly reducing computational complexity while still achieving high representational capabilities\\r\\nin the overall architecture design.\\r\\n3 Results and Discussion\\r\\nThe efficacy of the proposed TurboViT architecture design is evaluated on ImageNet-1K dataset\\r\\nand is compared with 10 different state-of-the-art efficient vision transformer architecture designs\\r\\nwithin a similar range of accuracy for image classification (MobileViTv2-2.0 [10], MViTv2-T [8],\\r\\nSwin-T [5], SwinV2-T [6], CvT-13-NAS [15], FastViT-SA24 [11], LITv2-S [20], FasterViT-0 [12],\\r\\nPiT-S [16], and Twins-S [21]) across three metrics: 1) Top-1 accuracy, 2) architectural complexity\\r\\n(based on number of parameters), and 3) computational complexity (based on number of FLOPs).\\r\\nFurthermore, for TurboViT, FastViT-SA24 [11], and FasterViT-0 [12], we also compare inference\\r\\nlatency and throughput on an Nvidia RTX A6000 GPU across two difference scenarios.\\r\\nArchitectural Complexity. Table 1 shows a comparison between the proposed TurboViT architecture\\r\\ndesign with the 10 different state-of-the-art efficient vision transformer architecture designs. In terms\\r\\nof architectural complexity, TurboViT is significantly smaller than all other state-of-the-art efficient\\r\\nvision transformer architecture designs in this comparison. For example, TurboViT is >2.47× smaller\\r\\nthan FasterViT-0 while achieving same accuracy, and >1.45× smaller than MobileViT2-2.0 while\\r\\nat the same time achieving 0.9% higher accuracy. Even when compared against CvT-13-NAS, the\\r\\nsecond smallest vision transformer architecture design and also the only other design that is created\\r\\nusing network architecture search in this study, TurboViT is >1.41× smaller while achieving similar\\r\\naccuracy.\\r\\n3\\nTable 2: Results for inference latency and throughput of TurboViT for two different scenarios (low\\x02latency processing (batch size of 1) and batch processing (batch size of 32) for 224×224 sized\\r\\nRGB image input in comparison to 2 state-of-the-art efficient vision transformer architecture designs\\r\\n(FastViT-SA24 [11] and FasterViT-0 [12]). All evaluations were conducted on an Nvidia RTX A6000\\r\\nGPU for N = 1000 runs. Best results are in bold.\\r\\nModel Batch Size Latency Throughput\\r\\n(ms) (images/s)\\r\\nFastViT-SA24 [11] 1 6.3 157.9\\r\\nFasterViT-0 [12] 1 12.2 75.8\\r\\nTurboViT 1 3.8 241.3\\r\\nFastViT-SA24 [11] 32 28.9 1089.6\\r\\nFasterViT-0 [12] 32 19.0 1663.5\\r\\nTurboViT 32 18.8 1696.9\\r\\nComputational Complexity. In terms of computational complexity, TurboViT requires significantly\\r\\nfewer FLOPs than all other state-of-the-art efficient vision transformer architecture designs in this\\r\\ncomparison. For example, TurboViT requires >3.4× fewer FLOPs than MobileViT2-2.0 while at the\\r\\nsame time achieving 0.9% higher accuracy. When compared to the only other design that is created\\r\\nusing network architecture search in this study (CvT-13-NAS), TurboViT requires >1.86× fewer\\r\\nFLOPs. Even when compared against Twins-S, the vision transformer architecture design requiring\\r\\nthe second fewest FLOPs, TurboViT requires >1.27× fewer FLOPS and is >1.89× smaller while\\r\\nachieving 0.4% higher accuracy.\\r\\nAccuracy. In terms of accuracy, TurboViT achieves strong top-1 accuracy at significantly lower\\r\\narchitectural and computational complexity than all other state-of-the-art efficient vision transformer\\r\\narchitecture designs in this comparison. For example, TurboViT achieves 0.8% and 0.3% higher\\r\\naccuracy when compared to Swin-T and SwinV2-T, respectively, even though it requires >2.04× and\\r\\n2× fewer FLOPs, respectively. When compared to the only other design that is created using network\\r\\narchitecture search in this study (CvT-13-NAS), TurboViT achieves very similar top-1 accuracy (only\\r\\n0.1% lower) but at much lower architectural and computational complexity (>1.41× smaller and\\r\\n> 1.86× fewer FLOPs, respectively). Finally, TurboViT achieves a top-1 accuracy that is only 0.5%\\r\\nlower than the top-performing state-of-the-art efficient vision transformer architecture design in this\\r\\nstudy (FastViT-SA24). However, it is important to note that TurboViT is >1.62× smaller and requires\\r\\n>1.72× fewer FLOPs than FastViT-SA24.\\r\\nInference Latency and Throughput. In terms of inference latency and throughput, we evaluated two\\r\\ndifferent scenarios: 1) low-latency processing (using batch size of 1), and 2) batch processing (using\\r\\nbatch size of 32). Table 2 shows a comparison for inference latency and throughput on an Nvidia RTX\\r\\nA6000 GPU between TurboViT, FasterViT-0 [12], and FastViT-SA24 [11]. It can be observed that,\\r\\nfor the low-latency processing scenario, TurboViT significantly outperforms both FasterViT-0 [12]\\r\\nand FastViT-SA24 [11], with latency being >3.21× and >1.66× better, respectively. Furthermore,\\r\\nthe throughput of TurboViT for this scenario is >3.18× and >1.53× higher than FasterViT-0 [12]\\r\\nand FastViT-SA24 [11], respectively. It can be observed that, for the batch processing scenario,\\r\\nTurboViT significantly outperforms FastViT-SA24, with latency being >1.54× lower and throughput\\r\\nbeing >1.56× higher. Interestingly, both TurboViT and FasterViT-0 achieved comparable latency and\\r\\nthroughput in this scenario.\\r\\nThese results demonstrate that TurboViT strikes a good balance between between accuracy, architec\\x02tural complexity, computational complexity as well as demonstrating strong latency and throughput\\r\\nfor both low-latency and batch processing scenarios, making it well-suited for high-throughput\\r\\nuse cases. Furthermore, these promising results demonstrate the efficacy of leveraging generative\\r\\narchitecture search for generating efficient transformer architecture designs for high-throughput\\r\\nscenarios.\\r\\nReferences\\r\\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\r\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\\r\\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\\r\\n4\\n[2] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\\r\\nJégou. Training data-efficient image transformers and distillation through attention, 2021.\\r\\n[3] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit, 2022.\\r\\n[4] Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Global context vision\\r\\ntransformers, 2023.\\r\\n[5] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\\r\\ntransformer: Hierarchical vision transformer using shifted windows, 2021.\\r\\n[6] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,\\r\\nLi Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and resolution, 2022.\\r\\n[7] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal,\\r\\nArkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, and Christoph\\r\\nFeichtenhofer. Hiera: A hierarchical vision transformer without the bells-and-whistles, 2023.\\r\\n[8] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph\\r\\nFeichtenhofer. Multiscale vision transformers, 2021.\\r\\n[9] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly\\r\\nvision transformer, 2022.\\r\\n[10] Sachin Mehta and Mohammad Rastegari. Separable self-attention for mobile vision transformers, 2022.\\r\\n[11] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: A fast\\r\\nhybrid vision transformer using structural reparameterization, 2023.\\r\\n[12] Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, Jose M. Alvarez, Jan Kautz, and Pavlo\\r\\nMolchanov. Fastervit: Fast vision transformers with hierarchical attention, 2023.\\r\\n[13] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention\\r\\nfor on-device semantic segmentation, 2023.\\r\\n[14] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory\\r\\nefficient vision transformer with cascaded group attention, 2023.\\r\\n[15] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\\r\\nIntroducing convolutions to vision transformers, 2021.\\r\\n[16] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\\r\\nRethinking spatial dimensions of vision transformers, 2021.\\r\\n[17] Alexander Wong, Mohammad Javad Shafiee, Brendan Chwyl, and Francis Li. Ferminets: Learning\\r\\ngenerative machines to generate efficient neural networks via generative synthesis. arXiv preprint\\r\\narXiv:1809.05989, 2018.\\r\\n[18] Alexander Wong, Yifan Wu, Saad Abbasi, Saeejith Nair, Yuhao Chen, and Mohammad Javad Shafiee. Fast\\r\\ngraspnext: A fast self-attention neural network architecture for multi-task learning in computer vision tasks\\r\\nfor robotic grasping on the edge, 2023.\\r\\n[19] Alexander Wong. Netscore: Towards universal metrics for large-scale performance analysis of deep neural\\r\\nnetworks for practical on-device edge usage. arXiv preprint arXiv:1806.05512, 2018.\\r\\n[20] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hilo attention, 2023.\\r\\n[21] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua\\r\\nShen. Twins: Revisiting the design of spatial attention in vision transformers, 2021.\\r\\n5'},\n",
       " {'name': '1808.01560v5.pdf',\n",
       "  'content': 'Stock Price Correlation Coefficient\\r\\nPrediction with ARIMA-LSTM\\r\\nHybrid Model\\r\\nHyeong Kyu Choi, B.A Student\\r\\nDept. of Business Administration\\r\\nKorea University\\r\\nSeoul, Korea\\r\\nimhgchoi@korea.ac.kr\\r\\nAbstract\\r\\nPredicting the price correlation of two assets for future time periods is im\\x02portant in portfolio optimization. We apply LSTM recurrent neural networks\\r\\n(RNN) in predicting the stock price correlation coefficient of two individual\\r\\nstocks. RNN’s are competent in understanding temporal dependencies. The\\r\\nuse of LSTM cells further enhances its long term predictive properties. To en\\x02compass both linearity and nonlinearity in the model, we adopt the ARIMA\\r\\nmodel as well. The ARIMA model filters linear tendencies in the data and\\r\\npasses on the residual value to the LSTM model. The ARIMA-LSTM hybrid\\r\\nmodel is tested against other traditional predictive financial models such as\\r\\nthe full historical model, constant correlation model, single-index model and\\r\\nthe multi-group model. In our empirical study, the predictive ability of the\\r\\nARIMA-LSTM model turned out superior to all other financial models by a\\r\\nsignificant scale. Our work implies that it is worth considering the ARIMA\\x02LSTM model to forecast correlation coefficient for portfolio optimization.\\r\\nKeywords – Recurrent Neural Network, Long Short-Term Memory cell, ARIMA\\r\\nmodel, Stock Correlation Coefficient, Portfolio Optimization\\r\\ni\\r\\narXiv:1808.01560v5 [cs.CE] 1 Oct 2018\\nContents\\r\\n1 Introduction 1\\r\\n2 Various Financial Models for Correlation prediction 2\\r\\n2.1 Full Historical Model . . . . . . . . . . . . . . . . . . . . . . . 3\\r\\n2.2 Constant Correlation Model . . . . . . . . . . . . . . . . . . . 3\\r\\n2.3 Single-Index Model . . . . . . . . . . . . . . . . . . . . . . . . 4\\r\\n2.4 Multi-Group Model . . . . . . . . . . . . . . . . . . . . . . . . 4\\r\\n3 The ARIMA-LSTM Hybrid Model 5\\r\\n3.1 ARIMA model sector . . . . . . . . . . . . . . . . . . . . . . . 6\\r\\n3.2 LSTM model sector . . . . . . . . . . . . . . . . . . . . . . . . 7\\r\\n4 Research Methodology 10\\r\\n4.1 ARIMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\r\\n4.1.1 the Data . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\r\\n4.1.2 Model Fitting . . . . . . . . . . . . . . . . . . . . . . . 11\\r\\n4.1.3 the Algorithm . . . . . . . . . . . . . . . . . . . . . . . 13\\r\\n4.2 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\r\\n4.2.1 the Data . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\r\\n4.2.2 Model Training . . . . . . . . . . . . . . . . . . . . . . 13\\r\\n4.2.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 15\\r\\n4.2.4 the Algorithm . . . . . . . . . . . . . . . . . . . . . . . 17\\r\\n5 Results And Evaluation 17\\r\\n6 Conclusion 20\\r\\nAppendices 21\\r\\nA 150 S&P500 Stocks 21\\r\\nB LSTM Model Source Code 22\\r\\nii\\n1 Introduction\\r\\nWhen constructing and selecting a portfolio for investment, evaluation of\\r\\nits expected returns and risks is considered the bottom line. Markowitz has\\r\\nintroduced the Modern Portfolio Theory which proposes methods to quantify\\r\\nreturns and risks of a portfolio, in his paper ‘Portfolio Selection’ (1952) [13].\\r\\nWith the derived return and risk, we draw the efficient frontier, which is a\\r\\ncurve that connects all the combination of expected returns and risks that\\r\\nyield the highest return-risk ratio. Investors then select a portfolio on the\\r\\nefficient frontier, depending on their risk tolerance.\\r\\nHowever, there have been criticisms on Markowitz’s assumptions. One\\r\\nof them is that the correlation coefficient used in measuring risk is con\\x02stant and fixed. According to Francois Chesnay & Eric Jondeau’s empirical\\r\\nstudy on correlation coefficients, stock markets’ prices tend to have positive\\r\\ncorrelations during times of financial turbulence [6]. This implies that the\\r\\ncorrelation of any two assets may as well deviate from mean historical cor\\x02relation coefficients subject to financial conditions; thus, the correlation is\\r\\nnot stable. Frank Fabozzi, Francis Gupta and Harry Markowitz himself also\\r\\nbriefly discussed the shortcomings of the Modern Portfolio Theory in their\\r\\npaper, ‘The Legacy of Modern Portfolio Theory’ (2002) [7].\\r\\nAcknowledging such pitfalls of the full historical correlation coefficient\\r\\nevaluation measures, numerous models for correlation coefficient prediction\\r\\nhave been devised. One alternative is the Constant Correlation model, which\\r\\nsets all pairs of assets’ correlations equal to the mean of all the correlation\\r\\ncoefficients of the assets in the portfolio [3]. Some other forecast models\\r\\ninclude the Multi-Group model and the Single-Index model. We will cover\\r\\nthese models in our paper at part 2, ‘Various Financial Models for Corre\\x02lation Prediction’. Although there have been many financial and statistical\\r\\napproaches to estimate future correlation, few have implemented the neural\\r\\nnetwork to carry out the task. Neural networks are frequently used to pre\\x02dict future stock returns and have produced noteworthy results1\\r\\n. Given that\\r\\nstock correlation data can also be represented as time series data – deriving\\r\\nthe correlation coefficient dataset with a rolling time window – application of\\r\\nneural networks in forecasting future correlation coefficients can be expected\\r\\nto have successful results as well. Rather than circumventing by predicting\\r\\nindividual asset returns to compute the correlation coefficient, we cast pre\\x02dictions directly on the correlation coefficient value itself.\\r\\n1Y. Yoon, G. Swales (1991) [18]; A. N. Refenes et al. (1994) [1]; K. Kamijo, T. Tanigawa\\r\\n(1990) [11] ; M. Dixon et al. (2017) [12]\\r\\n1\\nIn this paper, we suggest a hybrid model of the ARIMA and the neu\\x02ral network to predict future correlation coefficients of stock pairs that are\\r\\nrandomly selected among the S&P500 corporations. The model adopts the\\r\\nRecurrent Neural Network with Long Short-Term Memory cells (for conve\\x02nience, the model using this cell will be called LSTM in the rest of our paper).\\r\\nTo better predict the time series trend, we also utilize the ARIMA model. In\\r\\nthe first phase, the ARIMA model catches the linear tendencies in the time\\r\\nseries data. Then, the LSTM model attempts to capture nonlinearity in the\\r\\nresidual values, which is the output of the former phase. This ARIMA and\\r\\nneural network hybrid model was discussed in Peter Zhang’s literature [19],\\r\\nand an empirical study was conducted by James Hansen and Ray Nelson\\r\\non a variety of time series data [10]. The model architecture used in these\\r\\nliteratures are different from what is demonstrated in our paper. We only\\r\\nfocus on the hybrid model’s versatile predictive potential to capture both\\r\\nlinearity and nonlinearity. Further model details will be elaborated at part\\r\\n3, ‘The ARIMA-LSTM Hybrid Model’.\\r\\nIn the final evaluation step, the ARIMA-LSTM hybrid model will be\\r\\ntested on two time periods which were not involved in the training step. The\\r\\nlayout and methodology of this research will be discussed in detail at part 4,\\r\\n‘Research Methodology’. The data will be explored as well in this section.\\r\\nThe performance of the model will then be compared with that of the full\\r\\nhistorical model as well as other frequently used predictive models that are\\r\\nintroduced in part 2. Finally, the results will be summarized and evaluated\\r\\nin part 5, ‘Results and Evaluation’.\\r\\n2 Various Financial Models for Correlation\\r\\nprediction\\r\\nThe impreciseness of the full historical model for correlation prediction\\r\\nhas largely been acknowledged [6, 7]. There have been numerous attempts to\\r\\ncomplement mispredictions. In this section, we discuss three other frequently\\r\\nused models, along with the full historical model; three of which cited in\\r\\nthe literature by Elton et al. (1978) [3] – Full Historical model, Constant\\r\\nCorrelation model, and the Single-Index model and the other, the Multi\\x02Group model, in another paper of Elton et al. (1977) [5].\\r\\n2\\n2.1 Full Historical Model\\r\\nThe Full Historical model is the simplest method to implement for the\\r\\nportfolio correlation estimation. This model adopts the past correlation value\\r\\nto forecast future correlation coefficient. That is, the correlation of two assets\\r\\nfor a certain future time span is expected to be equal to the correlation value\\r\\nof a given past period [3].\\r\\nρˆ\\r\\n(t)\\r\\nij = ρ\\r\\n(t−1)\\r\\nij (1)\\r\\ni, j : asset index in the correlation coefficient matrix\\r\\nHowever, this model has encountered criticisms on its relative inferior pre\\x02diction quality compared to other equivalent models\\r\\n2.2 Constant Correlation Model\\r\\nThe Constant Correlation model assumes that the full historical model\\r\\nencompasses only the information of the mean correlation coefficient [3]. Any\\r\\ndeviation from the mean correlation coefficient is considered a random noise;\\r\\nit is sufficient to estimate the correlation of each pair of assets to be the aver\\x02age correlation of all pairs of assets in a given portfolio. Therefore, applying\\r\\nthe Constant Correlation model, all assets in a single portfolio have the same\\r\\ncorrelation coefficient.\\r\\nρˆ\\r\\n(t)\\r\\nij =\\r\\nX\\r\\ni>j\\r\\nρ\\r\\n(t−1)\\r\\nij\\r\\nn(n − 1)/2\\r\\n(2)\\r\\ni, j : asset index in the correlation coefficient matrix\\r\\nn : number of assets in the portfolio\\r\\n3\\n2.3 Single-Index Model\\r\\nThe Single-Index model presumes that asset returns move in a systematic\\r\\nway with the ‘single-index’, that is, the market return [3]. To quantify the\\r\\nsystematic movement with respect to the market return, we need to specify\\r\\nthe market return itself. We call this specification the ‘market model’, which\\r\\nwas contrived by H. M. Markowitz [4], and furthered by Sharpe (1963) [17].\\r\\nThe ‘market model’ relates the return of asset i with the market return at\\r\\ntime t, which is represented in the following equation:\\r\\nRi,t = αi + βi Rm,t + \\x0fi,t\\r\\nRi,t : return of asset i at time t\\r\\nRm,t : return of the market at time t\\r\\nαi: risk adjusted excess return of asset i\\r\\nβi: sensitivity of asset i to the market\\r\\n\\x0fi,t: residual return; error term\\r\\nsuch that, E(\\x0fi) = 0 ; V ar(\\x0fi) = σ\\r\\n2\\r\\n\\x0fi\\r\\nHere, we use the beta(β) of asset i and j to estimate the correlation coef\\x02ficient. With the equation that,\\r\\nCov(Ri, Rj ) = ρijσiσj = βiβjσ\\r\\n2\\r\\nm\\r\\nσi/σj: standard deviation of asset i / j’s return\\r\\nσm : standard deviation of market return\\r\\nThe estimated correlation coefficient ˆρij would be,\\r\\nρˆ\\r\\n(t)\\r\\nij =\\r\\nβiβjσ\\r\\n2\\r\\nm\\r\\nσiσj\\r\\n(3)\\r\\n2.4 Multi-Group Model\\r\\nThe Multi-Group model [5] takes the asset’s industry sector into account.\\r\\nUnder the assumption that assets in the same industry sector generally per\\x02form similarly, the model sets each correlation coefficient of asset pairs iden\\x02tical to the mean correlation of the industry sector pair’s correlation value.\\r\\n4\\nIn other words, the Multi-Group model is a model that applies the Constant\\r\\nCorrelation model to each pair of business sectors. For instance, if company\\r\\nA and company B, each belongs to industry sector α and β, their correlation\\r\\ncoefficient would be the mean value of all the correlation coefficients of asset\\r\\npairs with the same industry sector combination (α, β).\\r\\nThe equation for the prediction is slightly different depending on whether\\r\\nthe two industry sectors α and β are identical or not. The equation is as\\r\\nfollows.\\r\\nρˆ\\r\\n(t)\\r\\nij =\\r\\n\\uf8f1\\r\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\r\\nXnα\\r\\ni∈α\\r\\nXnβ\\r\\nj∈β;i6=j\\r\\nρ\\r\\n(t−1)\\r\\nij\\r\\nnα(nβ − 1) , where α = β\\r\\nXnα\\r\\ni∈α\\r\\nXnβ\\r\\nj∈β;i6=j\\r\\nρ\\r\\n(t−1)\\r\\nij\\r\\nnαnβ\\r\\n, where α 6= β\\r\\n(4)\\r\\nα / β : industry sector notation\\r\\nnα / nβ : the number of assets in each industry sector\\r\\n3 The ARIMA-LSTM Hybrid Model\\r\\nTime series data is assumed to be composed of the linear portion and the\\r\\nnonlinear portion [19]. Thus, we can express as follows.\\r\\nxt = Lt + Nt + \\x0ft\\r\\nLt represents the linearity of data at time t, while Nt signifies nonlinearity.\\r\\nThe \\x0f value is the error term.\\r\\nThe Autoregressive Integrated Moving Average (ARIMA) model is one\\r\\nof the traditional statistical models for time series prediction. The model\\r\\nis known to perform decently on linear problems. On the other hand, the\\r\\nLong Short-Term Memory (LSTM) model can capture nonlinear trends in\\r\\nthe dataset. So, the two models are consecutively combined to encompass\\r\\nboth linear and nonlinear tendencies in the model. The former sector is the\\r\\nARIMA model, and the latter is the LSTM model.\\r\\n5\\n3.1 ARIMA model sector\\r\\nThe ARIMA model is fundamentally a linear regression model accommo\\x02dated to track linear tendencies in stationary time series data. The model\\r\\nis expressed as ARIMA(p,d,q). Parameters p, d, and q are integer values\\r\\nthat decide the structure of the time series model; parameter p, q each is\\r\\nthe order of the AR model and the MA model, and parameter d is the level\\r\\nof differencing applied to the data. The mathematical representation of the\\r\\nARMA model of order (p,q) is as follows.\\r\\nxˆt = c + φ1xt−1 + φ2xt−2 + · · · + φpxt−p\\r\\n−θ1εt−1 − θ2εt−2 − · · · − θqεt−q\\r\\n= c +\\r\\nX\\r\\np\\r\\nk=1\\r\\nφkxt−k −\\r\\nX\\r\\nq\\r\\nl=1\\r\\nθl εt−l\\r\\nTerm c is a constant; φk and θk are coefficient values of AR model variable\\r\\nxt−k, and MA model variable εt−l. εt−lis an error notation at period tl\\r\\n(εt−l = xt−l − xˆt−l). It is assumed that εt−l has zero mean with constant\\r\\nvariance, and satisfies the i.i.d condition.\\r\\nBox & Jenkins [9] introduced a standardized methodology to build an\\r\\nARIMA model. The methodology consists of three iterative steps. (1) Model\\r\\nidentification and model selection the type of model, the AR(p) or MA(q), or\\r\\nARMA(p,q), is determined. (2) Parameter estimation the model parameters\\r\\nare adjusted to optimize the model. (3) Model Checking residual analysis is\\r\\nperformed to better the model.\\r\\nIn the model identification and model selection step, the proper type of\\r\\nmodel among the AR and MA models is decided. To judge which model\\r\\nfits best, stationary time series data needs to be provided. Stationarity re\\x02quires that basic statistical properties such as the mean, variance, covariance\\r\\nor autocorrelation be constant over time periods. In cases of dealing with\\r\\nnon-stationary data, differencing is applied once or twice to achieve station\\x02arity differencing more than two times is not frequently implemented. After\\r\\nstationarity conditions are satisfied, the autocorrelation function (ACF) plot\\r\\nand the partial autocorrelation function (PACF) plot are examined to select\\r\\nthe model type.\\r\\nThe parameter estimation step involves an optimization process utilizing\\r\\nmathematical error metrics such as the Akaike Information Criterion (AIC),\\r\\nthe Bayesian Information Criterion (BIC), or the Hannan-Quinn Information\\r\\nCriterion (HQIC). In this paper, we resolve to use the AIC metric to estimate\\r\\nparameters.\\r\\n6\\nAIC = −2 ln(Lˆ) + 2k\\r\\nThe ln(Lˆ) notation is the value of the likelihood function, and k is the degree\\r\\nof freedom, that is, the number of parameters used. A model that has a small\\r\\nAIC value is generally considered a better model. There are different ways\\r\\nto compute the likelihood function, ln(Lˆ). We use the maximum likelihood\\r\\nestimator for the computation. This method tends to be slow, but produces\\r\\naccurate results. Lastly, in the model checking step, residual analysis is car\\x02ried out to finalize the ARIMA model. If residual analysis concludes that\\r\\nthe residual value does not suffice standards, the three steps are iterated un\\x02til an optimal ARIMA model is attained. Here, we use the residual that is\\r\\ncalculated from the ARIMA model as the input for the subsequent LSTM\\r\\nmodel. As the ARIMA model has identified the linear trend, the residual is\\r\\nassumed to encompass the non-linear features [19].\\r\\nxt − Lt = Nt + \\x0ft\\r\\nThe \\x0ft value would be the final error term of our model.\\r\\n3.2 LSTM model sector\\r\\nNeural Networks are known to perform well on nonlinear tasks. Because\\r\\nof its versatility due to large dimension of parameters, and the use of nonlin\\x02ear activation functions in each layer, the model can adapt to nonlinear trends\\r\\nin the data. But empirical studies on financial data show that the perfor\\x02mance of neural networks are rather mixed. For example, in D.M.Q. Nelson\\r\\net al.’s literature [2], the accuracy of an LSTM neural network for stock price\\r\\nprediction generally tops other non-neural-network models. However, there\\r\\nare overlapping portions in the accuracy range of each model, implying that\\r\\nthe model not always performs superior to others. This provides a ground\\r\\nfor our paper to use an ARIMA-LSTM hybrid model that encompasses both\\r\\nlinearity and nonlinearity, so as to produce a more sophisticated result com\\x02pared to pure LSTM neural network models.\\r\\nTo understand the LSTM model, the mechanism of Recurrent Neural\\r\\nNetworks (RNN) should first be discussed. The RNN is a type of sequential\\r\\nmodel that performs effectively on time series data. It takes a sequence of\\r\\nvectors of time series data as input X = [x1, x2, x3, · · · , xt] and outputs a\\r\\nvector value computed by the neural network structures in the model’s cell,\\r\\nsymbolized as A in Figure 1. Vector X is a time series data spanning t time\\r\\nperiods. The values in vector X is sequentially passed through cell A. At\\r\\n7\\neach time step, the cell outputs a value, which is concatenated with the next\\r\\ntime step data, and the cell state C. The output value and C serve as input\\r\\nfor the next time step. The process is repeated up to the last time step data.\\r\\nThen, the Backward Propagation Through Time (BPTT) process, where the\\r\\nweight matrices are updated, initiates. The BPTT process will not be further\\r\\nillustrated in this paper. For detailed illustration, refer to S. Hochreiter and\\r\\nJ. Schmidhuber’s literature on Long Short-Term Memory (1997) [16].\\r\\nThe A cell in Figure 1 can be substituted with various types of cells. In\\r\\nthis paper, we select the standard LSTM cell with forget gates, which was\\r\\nintroduced by F. Gers et al. (1999) [8]. The LSTM cell adopted in this pa\\x02per comprises four interactive neural networks, each representing the forget\\r\\ngate, input gate, input candidate gate, and the output gate. The forget gate\\r\\noutputs a vector whose element values are between 0 and 1. It serves as a\\r\\nforgetter that is multiplied to the cell state Ct−1 from the former time step\\r\\nto drop values that are not needed and keep those that are necessary for the\\r\\nprediction.\\r\\nft = σ( Wf · [ht−1, xt] + bf )\\r\\nThe σ function, also denoted with the same symbol in Figure 2, is the logistic\\r\\nfunction, often called the sigmoid. It serves as the activation function that\\r\\nenables nonlinear capabilities for the model.\\r\\nσ(X) = 1\\r\\n1 + e\\r\\n−X\\r\\nIn the next phase, the input gate and the input candidate gate operate\\r\\ntogether to render the new cell state Ct, which will be passed on to the next\\r\\ntime step as the renewed cell state. The input gate uses the sigmoid as the\\r\\nactivation function and the input candidate utilizes the hyperbolic tangent,\\r\\neach outputting it and C˜\\r\\nt\\r\\n. The it selects which feature in C˜ should be reflect\\r\\ned in to the new cell state Ct.\\r\\nit = σ( Wi· [ht−1, xt] + bi )\\r\\nC˜\\r\\nt = tanh( WC · [ht−1, xt\\r\\n] + bC )\\r\\nThe tanh function, denoted ‘tanh’ in Figure 2 as well, is the hyperbolic tan\\x02gent. Unlike the sigmoid, which renders value between 0 and 1, the hyperbolic\\r\\ntangent outputs value between -1 and 1.\\r\\ntanh(X) = e\\r\\nX − e−X\\r\\ne\\r\\nX + e−X\\r\\n8\\nFigure 1. Structure of Recurrent Neural Network\\r\\nFigure 2. Inner structure of a Long Short-Term Memory cell\\r\\nFinally, the output gate decides what values are to be selected, combining\\r\\not with the tanh-applied state Ct as output ht. The new cell state is a\\r\\ncombination of the forget-gate-applied former cell state Ct−1 and the new\\r\\ntanh-applied state Ct.\\r\\not = σ( Wo · [ht−1, xt] + bo )\\r\\nCt = ft· Ct−1 + it· C˜\\r\\nt\\r\\nht = ot· tanh(Ct)\\r\\nThe cell state Ct and output ht will be passed to the next time step, and\\r\\nwill go through a same process. Depending on the task, further activation\\r\\n9\\nfunctions such as the Softmax or Hyperbolic tangent can be applied to ht\\r\\n’s.\\r\\nIn our paper’s case, which is a regression task that has output with values\\r\\nbounded between -1 and 1, we apply the hyperbolic tangent function to\\r\\nthe output of the last element of data vector X. Figure 2 provides a visual\\r\\nillustration to aid understanding of the LSTM cell inner structure.\\r\\n4 Research Methodology\\r\\n4.1 ARIMA\\r\\n4.1.1 the Data\\r\\nIn this paper, we resolve to utilize the adjusted close’ price of the S&P500\\r\\nfirms 2. The price data from 2008-01-01 to 2017-12-31 of the S&P500 firms\\r\\nare downloaded3. The data has a small ratio of missing values. The ratio\\r\\nof missing price data of each asset is around 0.1%, except for one asset with\\r\\nticker ‘MMM’, which has a ratio around 1.1%. Although MMM’s ratio is not\\r\\nthat high, missing data imputation seems improbable because the missing\\r\\nvalues are found in consecutive days, creating great chasms in the time se\\x02ries. This may cause distortion when computing the correlation coefficient.\\r\\nSo we exclude MMM from our research. For other assets, we impute the\\r\\nmissing data at time t with the value of time t-1 for all assets. Then, we ran\\x02domly select 150 stocks from the fully imputed price dataset. The randomly\\r\\nselected 150 firms’ tickers are enlisted in ‘Appendices A’.\\r\\nUsing the fully imputed 150 set of price data, we compute the correlation\\r\\ncoefficient of each pair of assets with a 100-day time window. In order to add\\r\\ndiversity, we set five different starting values, 1st, 21st, 41st, 61st and 81st, and\\r\\neach apply a rolling 100-day window with a 100-day stride until the end of the\\r\\ndataset. This process renders 55875 sets of time series data (150C2 · 5), each\\r\\nwith 24 time steps. Finally, we generate the train, development, and test1&2\\r\\ndata set with the 55875 × 24 data. We split the data as follows by means to\\r\\nimplement the walk-forward optimization [15] in the model evaluation phase.\\r\\n· Train set : index 1 ˜ 21\\r\\n· Development set : index 2 ˜ 22\\r\\n· Test1 set : index 3 ˜ 23\\r\\n· Test2 set : index 4 ˜ 24\\r\\n2https://en.wikipedia.org/wiki/List of S%26P 500 companies(accessed 23 May, 2018)\\r\\n3We utilize the Quandl API to download stock price data\\r\\n(https://github.com/quandl/quandl-python)\\r\\n10\\nFigure 3. Data generation scheme\\r\\n4.1.2 Model Fitting\\r\\nBefore fitting an ARIMA model, the order of the model must be specified.\\r\\nThe ACF plot and the PACF plot aids the decision process. Most of the\\r\\ndatasets showed an oscillatory trend that seemed close to a white noise as\\r\\nshown in Table 1. Other notable trends includes an increasing/decreasing\\r\\ntrend, occasional big dips while steady correlation coefficient, and having\\r\\nmixed oscillatory-steady periods. Although the ACF/PACF plots indicate\\r\\nthat a great portion of the datasets are close to a white noise, several orders\\r\\n(p, d, q) = (1, 1, 0), (0, 1, 1), (1, 1, 1), (2, 1, 1), (2, 1, 0) seems applicable.\\r\\nWe fit the ARIMA model4 with these five orders and select the model with\\r\\nthe least AIC value, for each train/development/test1/test2 dataset’s data.\\r\\nThe method we use to compute the log likelihood function for the AIC metric\\r\\nis the maximum likelihood estimator.\\r\\nAfter fitting the ARIMA model, we generate predictions for each 21 time\\r\\nsteps to compute the residual value. Then, the last data point of each data\\r\\nwill serve as the target variable Y, and the rest as variable X (Figure 3). The\\r\\nnewly X/Y-split datasets will be the input values for the next LSTM model\\r\\nsector.\\r\\n4We utilize the pyramid module to fit ARIMA models\\r\\n(https://github.com/tgsmith61591/pyramid)\\r\\n11\\nTable 1. Notable trends and the ACF/PACF of 1-level-differenced data\\r\\n12\\n4.1.3 the Algorithm\\r\\nAlgorithm 1. ARIMA model fitting algorithm\\r\\n1: datasets = [train, dev, test1, test2]\\r\\n2: orders = [ (1,1,0), (0,1,1), (1,1,1), (2,1,1), (2,1,0) ]\\r\\n3: for all data in datasets do\\r\\n4: X = empty list\\r\\n5: Y = empty list\\r\\n6: for all time-series T in data do\\r\\n7: models = empty list\\r\\n8: for all order in orders do\\r\\n9: Morder = fit ARIMA(T, order)\\r\\n10: add Morder to models\\r\\n11: with least-AIC model Mfit in models\\r\\n12: residual = X - predict(T, Mfit)\\r\\n13: add residual[0:20] to X\\r\\n14: add residual[20] to Y\\r\\n15: save X, Y\\r\\n4.2 LSTM\\r\\n4.2.1 the Data\\r\\nWe use the residual values, derived from the ARIMA model, of the 150\\r\\nrandomly selected S&P500 stocks as input for the LSTM model. The datasets\\r\\ninclude the train X/Y, development X/Y, test1 X/Y, and test2 X/Y. Each\\r\\nX dataset has 55875 lines with 20 time steps, with a corresponding Y dataset\\r\\nfor each time series (Figure 3). The data points are generally around 0, as\\r\\nthe input is a residual dataset (Figure 4).\\r\\n4.2.2 Model Training\\r\\nThe architecture of the model for our task is an RNN neural network that\\r\\nemploys 25 LSTM units5. The last outputs of 25 LSTM units is merged into\\r\\na single value with a fully connected layer. Then, the value will be passed\\r\\nthrough a doubled-hyperbolic tangent activation function to output a single\\r\\nfinal prediction. The doubled-hyperbolic tangent is simply the hyperbolic\\r\\n5We utilize the keras module to train the LSTM model\\r\\n(https://github.com/keras-team/keras)\\r\\n13\\nFigure 4. Data Point Distribution\\r\\ntangent function scaled by a factor of 2. Figure 5 shows a simplified archi\\x02tecture of the model.\\r\\nWhen training the model, it is crucial to keep an eye on overfitting.\\r\\nOverfitting occurs when the model fits excessively on dataset while train\\x02ing. Hence, the predictive performance on the train dataset will be high, but\\r\\nwill be poor on other newly introduced data. To monitor this problem, a\\r\\nseparate set of development dataset is used. We train the LSTM model with\\r\\nthe train dataset until the predictive performances on the train dataset and\\r\\ndevelopment dataset become similar to each other.\\r\\nThe dropout method is one of the widely used methods to prevent over\\x02fitting. It prevents the neurons to develop interdependency, which causes\\r\\noverfitting. This is executed by simply turning off neurons in the network\\r\\nduring training with probability p. Then, in the testing phase, dropout is\\r\\ndisabled and each weight values are multiplied by p, to scale down the output\\r\\nvalue into a desired boundary. Moreover, dropout has the effect of training\\r\\nmultiple neural networks and averaging the outputs [14].\\r\\nOther than dropout, we considered more regularization to prevent over\\x02fitting. There are mainly two types of regularization methods: Lasso regu\\x02larization (L1) and Ridge regularization (L2). These regularizers keep the\\r\\nweight values of each network in the LSTM model from becoming too large.\\r\\nBig parameter values of each layers may cause the network to focus severely\\r\\non few features, which may result in overfitting. A general expression of the\\r\\nerror function with regularization is as follows.\\r\\n14\\nXn\\r\\ni=1\\r\\n{Yi − (W · Xi + b)}\\r\\n2 + λW\\r\\nX\\r\\nk\\r\\ni=1\\r\\nk\\r\\nX0\\r\\nj=1\\r\\nW2\\r\\nij + λb\\r\\nX\\r\\nl\\r\\ni=1\\r\\nl\\r\\nX0\\r\\nj=1\\r\\nb\\r\\n2\\r\\nij\\r\\nParameters λW and λb determine the intensity of regularization of the cost\\r\\nfunction. If the lambda values are too high, the model will be under-trained.\\r\\nOn the other hand, if they are too low, regularization affect will be mini\\x02mal. In our model, after trial and error, it turned out that not applying any\\r\\nregularization performs better. We tried more complex architectures with\\r\\nregularization, but for all architectures, models with no regularization had\\r\\nsuperior outputs.\\r\\nAnother problem to pay attention to when training a neural network\\r\\nmodel is the vanishing/exploding gradient. This is particularly emphatic for\\r\\nRNN’s. Due to a deep propagation through time, the gradients far away\\r\\nfrom the output layer tend to be very small or large, preventing the model\\r\\nfrom training properly. The remedy for this problem is the LSTM cell itself.\\r\\nThe LSTM is capable of connecting large time intervals without loss of in\\x02formation [16].\\r\\nOther miscellaneous details about the training process includes the use\\r\\nof mini-batch of size 500, the ADAM optimization function et cetera. For\\r\\ndetail, refer to the LSTM section source codes in ‘Appendices B’.\\r\\n4.2.3 Evaluation\\r\\nThe walk-forward optimization method [15] is used as the evaluation\\r\\nmethod. The walk-forward optimization requires that a model be fitted for\\r\\neach rolling time intervals. Then, for each time interval, the newly trained\\r\\nmodel is tested on the next time step. This ensures the robustness of the\\r\\nmodel fitting strategy. However, this process is computationally expensive.\\r\\nIn addition, our paper’s motive is to fit parameters of a model that gener\\x02alizes well on various assets as well as on different time periods. Thus, it is\\r\\nneedless to train multiple models to approve of the model-fitting strategy.\\r\\nRather than training a new model for each rolling train-set window, we re\\x02solve to train a single model with the first window and apply it to three time\\r\\nintervals the development set and the test1/test2 set.\\r\\nWe selected our optimal model with the Mean Squared Error (MSE)\\r\\nmetric. That is, the cost function of our model was the MSE. For further\\r\\nevaluation, the Mean Absolute Error (MAE) and Root Mean Squared Er\\x02ror(RMSE) was also investigated.\\r\\n15\\nFigure 5. LSTM model sector architecture\\r\\nMSE = 1\\r\\nn\\r\\nPn\\r\\ni=1(yi − yˆi)\\r\\n2\\r\\nMAE = 1\\r\\nn\\r\\nPn\\r\\ni=1 |yi − yˆi\\r\\n|\\r\\nThe selected optimal model is then tested on two recent time periods. We\\r\\nuse two separate datasets to test the model because the development set is\\r\\ndeemed to be involved in the learning process as well.\\r\\nIf the model’s correlation coefficient prediction on two time periods turn\\r\\nout decent as well, we then test our model against former financial predictive\\r\\nmodels. The MSE and MAE values are computed for the four financial\\r\\nmodels as well. For the constant correlation model and the multi-group\\r\\nmodel, we regarded the 150 assets we selected randomly to be our portfolio\\r\\nconstituents.\\r\\n16\\n4.2.4 the Algorithm\\r\\nAlgorithm 2. LSTM model training algorithm\\r\\n1: read [train X/Y, dev X/Y, test1 X/Y, test2 X/Y]\\r\\n2: define model\\r\\n3: add LSTM(units = 25)\\r\\n4: add Dense(shape=(25,1), activation=‘double-tanh’)\\r\\n5: Repeat\\r\\n6: Forward propagate model with train X\\r\\n7: Backward propagate model with train Y\\r\\n8: Update model parameters\\r\\n9: train MSE, train MAE = model(train X, train Y)\\r\\n10: dev MSE, dev MAE = model(dev X, dev Y)\\r\\n11: if train MSE, dev MSE converged\\r\\n12: end Repeat\\r\\n13: test1 MSE, test1 MAE = model(test1 X, test1 Y)\\r\\n14: test2 MSE, test2 MAE = model(test2 X, test2 Y)\\r\\n5 Results And Evaluation\\r\\nAfter around 200 epochs, the train dataset’s MSE value and development\\r\\ndataset’s MSE value started to converge (Figure 6). The MAE learning curve\\r\\nshowed a similar trend as well. Among the models, we selected the 247th\\r\\nepoch’s model. The epoch was decided based on both the overfitting met\\x02ric and the performance metric. The overfitting metric was represented with\\r\\nthe normalized value of the MSE difference between the train & development\\r\\ndataset. And the performance metric was represented with the normalized\\r\\nvalue of the MSE sum of the train & development datset. Then, the sum of\\r\\nthe two normalized value was calculated to find the epoch that had the least\\r\\nvalue. The mathematical representation of the criterion is as follows.\\r\\ncriterion =\\r\\ndiffMSE − mean(diffMSE)\\r\\nstdev(diffMSE)\\r\\n+\\r\\nsumMSE − mean(sumMSE)\\r\\nstdev(sumMSE)\\r\\nWith the selected ARIMA-LSTM hybrid model, the MSE, RMSE and\\r\\nMAE values of the prediction were calculated. The MSE value on the de\\x02velopment, test1, and test2 dataset were 0.1786, 0.1889, 0.2154 each. The\\r\\nvalues have small variations, which means the model has been generalized\\r\\nadequately.\\r\\n17\\nThen, the metric values were compared with that of other financial mod\\x02els. Among the financial models, the Constant Correlation model performed\\r\\nthe best on our 150 S&P500 stocks’ dataset, just as what the empirical study\\r\\nof E. J. Elton et al. has shown [3]. However, its performance was nowhere\\r\\nnear the ARIMA-LSTM hybrid model’s predictive capacity. The ARIMA\\x02LSTM’s MSE value was nearly two thirds of that of other equivalent models.\\r\\nThe MAE metric also showed clear outperformance. Table 2 demonstrates\\r\\nall metrics’ values for every dataset, for each model. The least value of each\\r\\nmetric was boldfaced. Here, we can easily notice the how all the metric val\\x02ues of the ARIMA-LSTM model are in boldface.\\r\\nFor further investigation, we tested our final model on different assets\\r\\nin the S&P500 firms. Excluding the 150 assets we have already selected to\\r\\ntrain our model, we randomly selected 10 assets and generated datasets with\\r\\nidentical structures as the ones used in the model training and testing. This\\r\\ngenerates 180 lines of data. We then pass the data into our ARIMA-LSTM\\r\\nhybrid model and evaluate the predictions with the MSE, RMSE and MAE\\r\\nmetrics. We iterate this process 10 times to check for model stability. The\\r\\noutput of 10 iterations are demonstrated in Table 3.\\r\\nThe MSE values of 10 iterations range from 0.1447 to 0.2353. Although\\r\\nthere is some variation in the results compared to the Test1 & 2, this may\\r\\nbe due to a relatively small sample size, and the outstanding performance of\\r\\nthe model makes it negligible. Therefore, we may carefully affirm that our\\r\\nARIMA-LSTM model is robust.\\r\\nFigure 6. Learning curves of the ARIMA-LSTM model training process\\r\\n18\\nDevelopment dataset Test1 dataset Test2 dataset\\r\\nMSE RMSE MAE MSE RMSE MAE MSE RMSE MAE\\r\\nARIMA-LSTM .1786 .4226 .3420 .1889 .4346 .3502 .2154 .4641 .3735\\r\\nFull Historical .4597 .6780 .5449 .5005 .7075 .5741 .4458 .6677 .5345\\r\\nConstant Correlation .2954 .5435 .4423 .2639 .5137 .4436 .2903 .5388 .4576\\r\\nSingle-Index .4035 .6352 .5165 .3517 .5930 .4920 .3860 .6213 .5009\\r\\nMulti-Group .3079 .5549 .4515 .2910 .5394 .4555 .2874 .5361 .4480\\r\\nTable 2. ARIMA-LSTM model performance results and its comparison\\r\\nIter. Tickers MSE RMSE MAE\\r\\n1\\r\\nPRGO, MRO, ADP, HCP, FITB,\\r\\nPEG, SYMC, EOG, MDT, NI .2025 .4500 .3732\\r\\n2\\r\\nSTI, COP, MCD, AON, JBHT,\\r\\nDISH, GS, LRCX, CTXS, LEG .1517 .3895 .3331\\r\\n3\\r\\nTJX, EMN, JCI, C, BIIB,\\r\\nHOG, PX, PH, XEC, JEC .1680 .4099 .3476\\r\\n4\\r\\nROP, AZO, URI, TROW, CMCSA,\\r\\nSLB, VZ, MAC, ADS, MCK .1966 .4434 .3605\\r\\n5\\r\\nRL, CVX, SRE, PFE, PCG,\\r\\nUTX, NTRS, INCY, COP, HRL .2353 .4851 .3951\\r\\n6\\r\\nFE, STI, EA, AAL, XOM,\\r\\nJNJ, COL, APC, MCD, VFC .2175 .4664 .3709\\r\\n7\\r\\nBBY, AXP, CAG, TGT, EMR,\\r\\nMNST, HSY, MCK, INCY, WBA .1447 .3804 .3094\\r\\n8\\r\\nBXP, HST, NI, ESS, GILD,\\r\\nTSN, T, MSFT, LEG, COST .1997 .4469 .3518\\r\\n9\\r\\nCVX, FE, WMT, IDXX, GOOGL,\\r\\nPKI, EQIX, DISH, FTI, HST .1785 .4225 .3331\\r\\n10 NKE, VAR, DVN, VRSN, PFG,\\r\\nHAS, UNP, EQT, FE, AIV .2168 .4656 .3742\\r\\nTable 3. ARIMA-LSTM testing results on different asset combinations\\r\\n19\\n6 Conclusion\\r\\nThe purpose of our empirical study was to propose a model that per\\x02forms superior to extant financial correlation coefficient predictive models.\\r\\nWe adopted the ARIMA-LSTM hybrid model in an attempt to first filter\\r\\nout linearity in the ARIMA modeling step, then predict nonlinear tenden\\x02cies in the LSTM recurrent neural network. The testing results showed that\\r\\nthe ARIMA-LSTM hybrid model performs far superior to other equivalent\\r\\nfinancial models. Model performance was validated on both different time\\r\\nperiods and on different combinations of assets with various metrics such as\\r\\nthe MSE, RMSE, and the MAE. The values nearly halved that of the Con\\x02stant Correlation model, which, in our experiment, turned out to perform\\r\\nbest among the four financial models. Judging from such outperformance,\\r\\nwe may presume that the ARIMA-LSTM hybrid model has sufficient predic\\x02tive potential. Thus, the ARIMA-LSTM model as a correlation coefficient\\r\\npredictor for portfolio optimization would be considerable. With a better\\r\\npredictor, the portfolio is optimized more precisely, thereby enhancing re\\x02turns in investments.\\r\\nHowever, our experiment did not cover time periods before the year 2008.\\r\\nSo our model may be susceptible to specific financial conditions that were\\r\\nnot present in the years between 2008 and 2017. But financial anomalies and\\r\\nnoises are always prevalent. It is impossible to embrace all probable specific\\r\\ntendencies into the model. Hence, further research into dealing with financial\\r\\nblack swans is called for.\\r\\n20\\nAppendices\\r\\nA 150 S&P500 Stocks\\r\\n* This is the list of tickers of the 150 randomly selected S&P500 stocks.\\r\\nCELG PXD WAT LH AMGN AOS\\r\\nEFX CRM NEM JNPR LB CTAS\\r\\nMAT MDLZ VLO APH ADM MLM\\r\\nBK NOV BDX RRC IVZ ED\\r\\nSBUX GRMN CI ZION COO TIF\\r\\nRHT FDX LLL GLW GPN IPGP\\r\\nGPC HPQ ADI AMG MTB YUM\\r\\nSYK KMX AME AAP DAL A\\r\\nMON BRK BMY KMB JPM CCI\\r\\nAET DLTR MGM FL HD CLX\\r\\nOKE UPS WMB IFF CMS ARNC\\r\\nVIAB MMC REG ES ITW NDAQ\\r\\nAIZ VRTX CTL QCOM MSI NKTR\\r\\nAMAT BWA ESRX TXT EXR VNO\\r\\nBBT WDC UAL PVH NOC PCAR\\r\\nNSC UAA FFIV PHM LUV HUM\\r\\nSPG SJM ABT CMG ALK ULTA\\r\\nTMK TAP SCG CAT TMO AES\\r\\nMRK RMD MKC WU CAN HIG\\r\\nTEL DE ATVI O UNM VMC\\r\\nETFC CMA NRG RHI RE FMC\\r\\nMU CB LNT GE CBS ALGN\\r\\nSNA LLY LEN MAA OMC F\\r\\nAPA CDNS SLG HP XLNX SHW\\r\\nAFL STT PAYX AIG FOX MA\\r\\n21\\nB LSTM Model Source Code\\r\\n* This source code is a simplified version; unnecessary portions were con\\x02tracted or omitted. For original and other relevant source codes, visit\\r\\n‘https://github.com/imhgchoi/Corr Prediction ARIMA LSTM Hybrid’.\\r\\nimport pandas as pd\\r\\nimport numpy as np\\r\\nimport os\\r\\nfrom keras.models import Sequential, load_model\\r\\nfrom keras.layers import Dense, LSTM, Activation\\r\\nfrom keras import backend as K\\r\\nfrom keras.utils.generic_utils import get_custom_objects\\r\\nfrom keras.callbacks import ModelCheckpoint\\r\\nfrom keras.regularizers import l1_l2\\r\\n# Train - Dev - Test Generation\\r\\ntrain_X= pd.read_csv(’~/train_dev_test/after_arima/train_X.csv’)\\r\\ndev_X = pd.read_csv(’~/train_dev_test/after_arima/dev_X.csv’)\\r\\ntest1_X = pd.read_csv(’~/train_dev_test/after_arima/test1_X.csv’)\\r\\ntest2_X = pd.read_csv(’~/train_dev_test/after_arima/test2_X.csv’)\\r\\ntrain_Y = pd.read_csv(’~/train_dev_test/after_arima/train_Y.csv’)\\r\\ndev_Y = pd.read_csv(’~/train_dev_test/after_arima/dev_Y.csv’)\\r\\ntest1_Y = pd.read_csv(’~/train_dev_test/after_arima/test1_Y.csv’)\\r\\ntest2_Y = pd.read_csv(’~/train_dev_test/after_arima/test2_Y.csv’)\\r\\n# data sampling\\r\\nSTEP = 20\\r\\n_train_X = np.asarray(train_X).reshape((int(1117500/STEP), 20, 1))\\r\\n_dev_X = np.asarray(dev_X).reshape((int(1117500/STEP), 20, 1))\\r\\n_test1_X = np.asarray(test1_X).reshape((int(1117500/STEP), 20, 1))\\r\\n_test2_X = np.asarray(test2_X).reshape((int(1117500/STEP), 20, 1))\\r\\n_train_Y = np.asarray(train_Y).reshape(int(1117500/STEP), 1)\\r\\n_dev_Y = np.asarray(dev_Y).reshape(int(1117500/STEP), 1)\\r\\n_test1_Y = np.asarray(test1_Y).reshape(int(1117500/STEP), 1)\\r\\n_test2_Y = np.asarray(test2_Y).reshape(int(1117500/STEP), 1)\\r\\n22\\n#define custom activation\\r\\nclass Double_Tanh(Activation):\\r\\ndef __init__(self, activation, **kwargs):\\r\\nsuper(Double_Tanh, self).__init__(activation, **kwargs)\\r\\nself.__name__ = ’double_tanh’\\r\\ndef double_tanh(x):\\r\\nreturn (K.tanh(x) * 2)\\r\\nget_custom_objects().update({’double_tanh’:Double_Tanh(double_tanh)})\\r\\n# Model Generation\\r\\nmodel = Sequential()\\r\\nmodel.add(LSTM(25, input_shape=(20,1)))\\r\\nmodel.add(Dense(1))\\r\\nmodel.add(Activation(double_tanh))\\r\\nmodel.compile(loss=’mean_squared_error’, optimizer=’adam’,\\r\\nmetrics=[’mse’, ’mae’])\\r\\n# Fitting the Model\\r\\nmodel_scores = {}\\r\\nepoch_num=1\\r\\nfor _ in range(50):\\r\\n# train the model\\r\\ndir = ’~/models/hybrid_LSTM’\\r\\nfile_list = os.listdir(dir)\\r\\nif len(file_list) != 0 :\\r\\nepoch_num = len(file_list) + 1\\r\\nrecent_model_name = ’epoch’+str(epoch_num-1)+’.h5’\\r\\nfilepath = ’~/models/hybrid_LSTM/’+recent_model_name\\r\\nmodel = load_model(filepath)\\r\\nfilepath = ’~/models/hybrid_LSTM/epoch’+str(epoch_num)+’.h5’\\r\\ncheckpoint = ModelCheckpoint(filepath, monitor=’loss’,\\r\\nverbose=1, save_best_only=False, mode=’min’)\\r\\ncallbacks_list = [checkpoint]\\r\\nif len(callbacks_list) == 0:\\r\\nmodel.fit(_train_X, _train_Y, epochs=1, batch_size=500,\\r\\nshuffle=True)\\r\\nelse:\\r\\nmodel.fit(_train_X, _train_Y, epochs=1, batch_size=500,\\r\\nshuffle=True, callbacks=callbacks_list)\\r\\n23\\n# test the model\\r\\nscore_train = model.evaluate(_train_X, _train_Y)\\r\\nscore_dev = model.evaluate(_dev_X, _dev_Y)\\r\\n# get former score data\\r\\ndf = pd.read_csv(’~/models/hybrid_LSTM.csv’)\\r\\ntrain_mse = list(df[’TRAIN_MSE’])\\r\\ndev_mse = list(df[’DEV_MSE’])\\r\\ntrain_mae = list(df[’TRAIN_MAE’])\\r\\ndev_mae = list(df[’DEV_MAE’])\\r\\n# append new data\\r\\ntrain_mse.append(score_train[1])\\r\\ndev_mse.append(score_dev[1])\\r\\ntrain_mae.append(score_train[2])\\r\\ndev_mae.append(score_dev[2])\\r\\n# organize newly created score dataset\\r\\nmodel_scores[’TRAIN_MSE’] = train_mse\\r\\nmodel_scores[’DEV_MSE’] = dev_mse\\r\\nmodel_scores[’TRAIN_MAE’] = train_mae\\r\\nmodel_scores[’DEV_MAE’] = dev_mae\\r\\n# save newly created score dataset\\r\\nmodel_scores_df = pd.DataFrame(model_scores)\\r\\nmodel_scores_df.to_csv(’~/models/hybrid_LSTM.csv’)\\r\\nAcknowledgement\\r\\nWe thank developers of the ‘Quandl’ API, ‘Pyramid-arima’ module, and\\r\\n‘Keras’ module, who provided open source codes that alleviated the burden\\r\\nof our research.\\r\\nWe also thank an anonymous commenter with a pseudo-name ’Moosefly’,\\r\\nwho discovered a crucial error in the ARIMA modeling section source code.\\r\\n24\\nReferences\\r\\n[1] G. Francis A. N. Refenes, A. Zapranis. Stock performance modeling\\r\\nusing neural networks: A comparative study with regression models.\\r\\nNeural Networks, 7(2):375–388, 1994.\\r\\n[2] R.A. de Oliveira D.M.Q. Nelson, A.C.M. Pereira. Stock markets price\\r\\nmovement prediction with lstm neural networks. In 2017 International\\r\\nJoint Conference on Neural Networks (IJCNN), pages 1419–1426, 2017.\\r\\n[3] T. J. Urich E. J. Elton, M. J. Gruber. Are betas best? Journal of\\r\\nFinance, 33:1375–1384, 1978.\\r\\n[4] M. J. Gruber E.J. Elton. Modern portfolio theory, 1950 to date. Journal\\r\\nof banking And Finance, 21:1743–1759, 1997.\\r\\n[5] M. W. Padberg E.J. Elton, M. J. Gruber. Simple rules for optimal\\r\\nportfolio selection: The multi group case. Journal of Financial and\\r\\nQuantitative Analysis, 12(3):329–349, 1977.\\r\\n[6] E. Jondeau F. Chesnay. Does correlation between stock returns really in\\x02crease during turbulent periods? Economic Notes, Vol.30, no.1-2001:53–\\r\\n80, 2001.\\r\\n[7] H. M. Markowitz F. J. Fabozzi, F. Gupta. The legacy of modern portfolio\\r\\ntheory. The Journal of Investing, Vol. 11, No. 3:7–22, 2002 Fall.\\r\\n[8] F. Cummins F.A. Gers, J. Schmidhuber. Learning to forget: Continual\\r\\nprediction with lstm. Technical Report, IDSIA-01-99, 1999.\\r\\n[9] G. Jenkins G.E.P. Box. Time series analysis, forecasting and control.\\r\\nHolden-Day, San Francisco, CA, 1970.\\r\\n[10] R. D. Nelson J.V. Hansen. Time-series analysis with neural networks and\\r\\narima-neural network hybrids. Journal of Experimental And Theoretical\\r\\nArtificial Intelligence, 15:3:315–330, 2003.\\r\\n[11] T. Tanigawa K. Kamijo. Stock price pattern recognition - a recurrent\\r\\nneural network approach. 1990 IJCNN International Joint Conference\\r\\non Neural Networks, vol.1, San Diego, CA, USA:215–221, 1990.\\r\\n[12] J. H. Bang M. Dixon, D. Klabjan. Classification-based financial markets\\r\\nprediction using deep neural networks. Algorithmic Finance, 6(3-4):67–\\r\\n77, 2017.\\r\\n25\\n[13] H. M. Markowitz. Portfolio selection. The Journal of Finance, Vol.7,\\r\\nNo.1:77–91, Mar. 1952.\\r\\n[14] A. Krizhvsky I. Sutskever R. Salakhutdinov N. Srivastave, G. Hinton.\\r\\nDropout: A simple way to prevent neural networks from overfitting.\\r\\nJournal of Machine Learning Research, 15:1929–1958, 2014.\\r\\n[15] P. Grzegorzewski P. ladyzynski, K. Zbikowski. Stock trading with ran\\x02dom forests, trend detection tests and force index volume indicators.\\r\\nArtificial Intelligence and Soft Computing, Lecture Notes in Computer\\r\\nScience, 7895:pp.441–452, 2013.\\r\\n[16] J. Schmidhuber S. Hochreiter. Long short-term memory. Neural Com\\x02putation, 9(8):1735–1780, 1997.\\r\\n[17] W.F. Sharpe. A simplified model for portfolio analysis. Management\\r\\nScience, 13:277–293, 1963.\\r\\n[18] G. Swales Y. Yoon. Predicting stock price performance: A neural net\\x02work approach. System Sciences, Proceedings of the Twenty-Fourth An\\x02nual Hawaii International Conference on, Vol. 4, IEEE:156–162, 1991.\\r\\n[19] G.P. Zhang. Time series forecasting using a hybrid arima and neural\\r\\nnetwork model. Neurocomputing, 50:159–175, 2003.\\r\\n[20] F. Zhao. Forecast correlation coefficient matrix of stock returns in port\\x02folio analysis, 2013.\\r\\n26'},\n",
       " {'name': '2304.05206v1.pdf',\n",
       "  'content': 'The Capacity and Robustness Trade-off: Revisiting the Channel\\r\\nIndependent Strategy for Multivariate Time Series Forecasting\\r\\nLu Han, Han-Jia Ye, De-Chuan Zhan\\r\\nState Key Laboratory for Novel Software Technology, Nanjing University\\r\\n{hanlu,yehj}@lamda.nju.edu.cn,zhandc@nju.edu.cn\\r\\nABSTRACT\\r\\nMultivariate time series data comprises various channels of vari\\x02ables. The multivariate forecasting models need to capture the\\r\\nrelationship between the channels to accurately predict future val\\x02ues. However, recently, there has been an emergence of methods\\r\\nthat employ the Channel Independent (CI) strategy. These methods\\r\\nview multivariate time series data as separate univariate time series\\r\\nand disregard the correlation between channels. Surprisingly, our\\r\\nempirical results have shown that models trained with the CI strat\\x02egy outperform those trained with the Channel Dependent (CD)\\r\\nstrategy, usually by a significant margin. Nevertheless, the reasons\\r\\nbehind this phenomenon have not yet been thoroughly explored\\r\\nin the literature. This paper provides comprehensive empirical and\\r\\ntheoretical analyses of the characteristics of multivariate time series\\r\\ndatasets and the CI/CD strategy. Our results conclude that the CD\\r\\napproach has higher capacity but often lacks robustness to accu\\x02rately predict distributionally drifted time series. In contrast, the CI\\r\\napproach trades capacity for robust prediction. Practical measures\\r\\ninspired by these analyses are proposed to address the capacity\\r\\nand robustness dilemma, including a modified CD method called\\r\\nPredict Residuals with Regularization (PRReg) that can surpass the\\r\\nCI strategy. We hope our findings can raise awareness among re\\x02searchers about the characteristics of multivariate time series and\\r\\ninspire the construction of better forecasting models.\\r\\nPVLDB Reference Format:\\r\\nLu Han, Han-Jia Ye, De-Chuan Zhan. The Capacity and Robustness\\r\\nTrade-off: Revisiting the Channel Independent Strategy for Multivariate\\r\\nTime Series Forecasting. PVLDB, 14(1): XXX-XXX, 2020.\\r\\ndoi:XX.XX/XXX.XX W PVLDB Artifact Availability:\\r\\nThe source code, data, and/or other artifacts have been made available at\\r\\nhttps://github.com/hanlu-nju/channel_independent_MTSF.\\r\\n1 INTRODUCTION\\r\\nTime series forecasting is a critical area of research that finds ap\\x02plications in both industry and academia. Multivariate time series\\r\\nare common and comprise multiple channels of variates that are\\r\\nusually correlated, with examples ranging from stock market prices\\r\\nand traffic flows to solar power plant outputs and temperatures\\r\\nacross various cities [20]. With the powerful representation capabil\\x02ity of deep models, channel correlation can be implicitly learned or\\r\\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\\r\\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\\r\\nthis license. For any use beyond those covered by this license, obtain permission by\\r\\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\\r\\nlicensed to the VLDB Endowment.\\r\\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\\r\\ndoi:XX.XX/XXX.XX\\r\\nexplicitly modeled by performing forecasting tasks [6, 7, 22, 39, 40].\\r\\nTwo widely used methods for time series forecasting are recur\\x02rent neural networks (RNNs) and convolutional neural networks\\r\\n(CNNs). RNNs model successive time points based on the Markov\\r\\nassumption [5, 16, 32], while CNNs extract variation information\\r\\nalong the temporal dimension using techniques such as tempo\\x02ral convolutional networks (TCNs) [2, 12]. However, due to the\\r\\nMarkov assumption in RNN and the local reception property in\\r\\nTCN, both of the two models are unable to capture the long-term\\r\\ndependencies in sequential data. Recently, Transformers with at\\x02tention mechanisms have gained increasing popularity in other\\r\\nfields like natural language processing [8], speech recognition [9],\\r\\nand even computer vision [10]. Researchers have also explored the\\r\\npotential of Transformer models in long-term multivariate time\\r\\nseries forecasting (MTSF) tasks [24, 38, 43, 44].\\r\\nDespite the significant progress made by Transformer-based\\r\\nmethods in forecasting long-term future values, a recent paper\\r\\nquestions the effectiveness of Transformer [42]. The authors have\\r\\ndemonstrated that a simple linear model can outperform all state\\x02of-the-art Transformer-based methods. However, it is important to\\r\\nnote that the linear model used by the authors employs a channel\\x02independent training strategy that is different from previous works.\\r\\nInstead of considering all the channels as a whole, the authors train\\r\\na univariate forecast model that is shared across all the channels.\\r\\nThis training strategy is closely related to the global [33] or cross\\x02learning [34] approach when there is a set of related univariate\\r\\ntime series. Global methods assume that all the time series in the\\r\\nset come from the same process and fit a single univariate fore\\x02casting function [31]. Despite the heterogeneity of real-world time\\r\\nseries, global methods have demonstrated unexpectedly good per\\x02formance [13, 21, 29]. [27] attribute the improvement to the relief of\\r\\noverfit by larger number of training examples. Multivariate time se\\x02ries can be viewed as a collection of multiple interdependent series\\r\\nthat are synchronized in time. However, it is necessary to consider\\r\\nall channels of the variables in order to fully capture the character\\x02istics of the object at each time step. Moreover, disregarding the\\r\\ncorrelation between channels can result in incomplete modeling.\\r\\nTherefore, the effectiveness of a channel-dependent strategy in\\r\\nimproving the modeling of multivariate time series remains to be\\r\\nthoroughly investigated, along with the underlying reasons for its\\r\\nsuccess.\\r\\nThis paper conducts a comprehensive investigation of the two\\r\\ntraining strategies that have emerged in recent works on multi\\x02variate time series forecasting. The first strategy is the Channel\\x02Dependent (CD) approach, which predicts future values by taking\\r\\ninto account the historical data of all the channels [24, 38, 43, 44].\\r\\nThe second strategy is the Channel-Independent (CI) approach,\\r\\nwhich treats multivariate time series as separate univariate time\\r\\narXiv:2304.05206v1 [cs.LG] 11 Apr 2023\\nseries and constructs a multivariate forecaster using univariate fore\\x02casting functions [28, 42]. With this strategy, the predicted value\\r\\nof a particular channel depends solely on its own historical values,\\r\\nwhile the other channels are ignored. Intuitively, since an object\\r\\ncannot be fully described by considering only one of its features,\\r\\nthe CI is supposed to perform poorly. We test the two strategies\\r\\nwith various kinds of machine learning algorithms on 9 commonly\\r\\nused long-term forecast datasets. Interestingly, our results indicate\\r\\nthat, regardless of the algorithm used, the CI strategy consistently\\r\\noutperforms the CD strategy, often by a substantial margin.\\r\\nTo explore the reasons behind this, we examined the linear model\\r\\nas an illustrative example, both theoretically and empirically. First,\\r\\nwe observed the distribution drift in the real-world multivariate\\r\\ntime series. Specifically, we found that the autocorrelation functions\\r\\n(ACFs) of each channel, which are relevant to the linear model,\\r\\nexhibit substantial differences between training and testing phases.\\r\\nNext, we demonstrated that the linear model using CI strategy relies\\r\\nsolely on the mean of ACFs across all channels, while CD strategy\\r\\nrelies on each ACF separately. Given that the mean ACF drifts less\\r\\nthan most of the channel ACFs, this leads to CI strategy achieving\\r\\nsuperior performance. Our analysis led us to the conclusion that\\r\\nCI and CD exhibit different trade-offs in terms of capacity and\\r\\nrobustness. Specifically, CI has lower capacity but better robustness,\\r\\nwhereas CD is the opposite.\\r\\nThrough our analyses, we give some practice to improve the per\\x02formance of existing algorithms. First, we propose an new objective\\r\\ncalled Predict Residuals with Regularization (PRReg). This objec\\x02tive is designed to address the non-robustness of the CD strategy\\r\\nand has demonstrated superior performance compared to both the\\r\\noriginal CD and CI strategies in the majority of cases. Furthermore,\\r\\nwe have identified several factors that may influence algorithm\\r\\nperformance. By taking these factors into consideration and imple\\x02menting the PRReg objective, it may be possible to further enhance\\r\\nalgorithm performance.\\r\\nWe conclude our contribution as follows:\\r\\n• We present the Channel Dependent (CD) and Channel Inde\\x02pendent (CI) training strategies for multivariate time series\\r\\nforecasting, and find that CI outperformed CD by a signifi\\x02cant margin, despite ignoring channel correlation.\\r\\n• Through theoretical and empirical analysis on linear model,\\r\\nwe identified that CI has high capacity and low robustness,\\r\\nwhile CD has low capacity and high robustness. In real-world\\r\\nnon-stationary time series forecasting, robustness is more\\r\\nimportant, which explains CI’s superior performance in most\\r\\ncases.\\r\\n• We presented practical strategies for improving forecasting\\r\\nmodel performance, including the use of the Predict Residu\\x02als with Regularization (PRReg) objective and other factors\\r\\nthat can influence CD and CI performance.\\r\\n2 PRELIMINARIES\\r\\nIn this section, we introduce the concepts of Multivariate Time\\r\\nSeries Forecasting (MTSF), Channel Dependent (CD) Strategy, and\\r\\nChannel Independent (CI) Strategy.\\r\\n2.1 Multivariate Time Series Forecasting\\r\\nMTSF deals with time series data that contain multiple variables,\\r\\nor channels, at each time step. Given historical values 𝑿 ∈ R\\r\\n𝐿×𝐶\\r\\nwhere 𝐿 represents the length of the look-back window, and 𝐶 is\\r\\nthe number of channels. the goal of MTSF is to predict the future\\r\\nvalues 𝒀 ∈ R\\r\\n𝐻×𝐶, where 𝐻 > 0 is the forecast horizon.\\r\\n2.2 Channel Dependent (CD) Strategy\\r\\nThe CD strategy involves building a model that forecasts the fu\\x02ture values of each channel by considering all the history of all\\r\\nthe channels. Most of the multivariate forecaster employ this strat\\x02egy [33, 38, 43, 44]. To be specified, the objective of CD model is\\r\\nthe minimize the forecasting risk R:\\r\\nmin\\r\\n𝑓\\r\\nR (𝑓 ) = min\\r\\n𝑓\\r\\nE(𝑿,𝒀)ℓ(𝑓 (𝑿), 𝒀 ). (1)\\r\\nℓ is the regression loss. We apply the commonly used L-2 (MSE)\\r\\nloss unless specified otherwise [11, 38, 42–44]. To minimize the ex\\x02pectation objective (eq. (1)), the model 𝑓 is trained by the empirical\\r\\nloss on the training set {(𝑿\\r\\n(𝑖)\\r\\n, 𝒀\\r\\n(𝑖)\\r\\n)}𝑁\\r\\n𝑖=1\\r\\n. This is referred to as the\\r\\nChannel Dependent (CD) loss:\\r\\nmin\\r\\n𝑓\\r\\n1\\r\\n𝑁\\r\\n∑︁\\r\\n𝑁\\r\\n𝑖=1\\r\\nℓ(𝑓 (𝑿\\r\\n(𝑖)\\r\\n), 𝒀\\r\\n(𝑖)\\r\\n). (2)\\r\\nHere 𝑁 is the number of time series used for training.\\r\\n2.3 Channel Independent (CI) Strategy\\r\\nAlternatively, multivariate time series can also be viewed as a\\r\\nset of multiple time series, i.e., the given look-back window 𝑿 =\\r\\n[𝒙1, 𝒙2, . . . , 𝒙𝐶] and the target 𝒀 = [𝒚1,𝒚2, . . . ,𝒚𝐶], where 𝒙𝑐 ∈\\r\\n𝑅\\r\\n𝐿\\r\\n,𝒚𝑐 ∈ 𝑅\\r\\n𝐻 , 1 ≤ 𝑐 ≤ 𝐶 is history and future values of the univari\\x02ate time seires for 𝑐-th channel. In this case, a forecast model can\\r\\nbe learned by the following Channel Independent (CI) loss:\\r\\nmin\\r\\n𝑓\\r\\n1\\r\\n𝑁𝐶\\r\\n∑︁\\r\\n𝑁\\r\\n𝑖=1\\r\\n∑︁\\r\\n𝐶\\r\\n𝑐=1\\r\\nℓ(𝑓 (𝒙\\r\\n(𝑖)\\r\\n𝑐\\r\\n),𝒚\\r\\n(𝑖)\\r\\n𝑐\\r\\n). (3)\\r\\nThe CI loss is the mean of the losses of all channels, with each\\r\\nchannel’s loss being minimized independently.\\r\\nIn fig. 1, we illustrate the difference between the Channel Depen\\x02dent (CD) and Channel Independent (CI) strategies for multivariate\\r\\ntime series forecasting. CD takes all the channels of a time series as\\r\\ninput and aims to capture the relationships between them, while CI\\r\\nhandles each channel independently. It is natural to assume that CD\\r\\nwould outperform CI, but in the next section, we demonstrate that\\r\\nthe opposite is true across different benchmarks and algorithms,\\r\\nincluding both non-deep and deep methods.\\r\\n3 EMPIRICAL COMPARISON OF CD AND CI\\r\\nThe previous section introduced two strategies – CD and CI – for\\r\\nsolving multivariate time series forecasting tasks. While CD con\\x02siders all channels, one might assume that models trained with\\r\\nCD would outperform CI by a significant margin. Surprisingly, the\\r\\nopposite is true: CI outperforms CD in most cases. In this sec\\x02tion, we present empirical comparisons of CD and CI across diverse\\r\\ndatasets on various methods, including recent Transformer-based\\r\\n2\\n𝑓𝑓 forecast\\r\\n𝑓𝑓\\r\\nforecast\\r\\n𝑓𝑓\\r\\n𝑓𝑓\\r\\n𝑓𝑓\\r\\n(a) Channel Dependent (CD) Strategy (b) Channel Independent (CI) Strategy\\r\\nFigure 1: Comparison of two training strategies for Multivariate Time Series Forecasting (MTSF) tasks. The left shows the\\r\\nChannel Dependent (CD) strategy where all the channels are taken as input and forecasted future values depend on the history\\r\\nof all the channels. The right shows the Channel Independent (CI) strategy, which treats the multivariate series as multiple\\r\\nunivariate series and trains a unified model on these series. The prediction of each channel depends solely on its own historical\\r\\nvalues, and the relationship between different channels is ignored.\\r\\nmethods. Additionally, we provide both theoretical and empirical\\r\\nanalyses to explain the reasons behind these results.\\r\\n3.1 Experiment Setup\\r\\nDatasets. We conduct extensive experiments on nine widely\\x02used, real-world datasets that cover five mainstream time series\\r\\nforecasting applications, namely energy, traffic, economics, weather,\\r\\nand disease. The datasets include:\\r\\n• ETT (Electricity Transformer Temperature) [43]\\r\\n1\\r\\ncom\\x02prises two hourly-level datasets (ETTh) and two 15-minute\\x02level datasets (ETTm). Each dataset contains seven oil and\\r\\nload features of electricity transformers from July 2016 to\\r\\nJuly 2018.\\r\\n• Traffic2 describes the road occupancy rates. It contains the\\r\\nhourly data recorded by the sensors of San Francisco free\\x02ways from 2015 to 2016.\\r\\n• Electricity3collects the hourly electricity consumption of\\r\\n321 clients from 2012 to 2014.\\r\\n• Exchange-Rate [20]\\r\\n4\\r\\ncollects the daily exchange rates of 8\\r\\ncountries from 1990 to 2016.\\r\\n• Weather5\\r\\nincludes 21 indicators of weather, such as air\\r\\ntemperature, and humidity. Its data is recorded every 10\\r\\nmin for 2020 in Germany.\\r\\n• ILI6 describes the ratio of patients seen with influenza-like\\r\\nillness and the number of patients. It includes weekly data\\r\\nfrom the Centers for Disease Control and Prevention of the\\r\\nUnited States from 2002 to 2021.\\r\\nWe also summarize the datasets in table 1.\\r\\nEvaluation metrics. In line with previous research [38, 42–44], we\\r\\ncompare the performance of different methods using two primary\\r\\nevaluation metrics: Mean Squared Error (MSE) and Mean Absolute\\r\\nError (MAE).\\r\\nCompared methods. Our analysis includes a range of methods,\\r\\nincluding non-deep models, Transformer-based models, and other\\r\\ndeep learning models. Specifically:\\r\\n1https://github.com/zhouhaoyi/ETDataset\\r\\n2http://pems.dot.ca.gov\\r\\n3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\r\\n4https://github.com/laiguokun/multivariate-time-series-data\\r\\n5https://www.bgc-jena.mpg.de/wetter/\\r\\n6https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\\r\\nTable 1: Statistics of nine multivariate time series datasets.\\r\\nDataset(s) channels Timesteps Granularity\\r\\nETTh1&ETTh2 7 17,420 1hour\\r\\nETTm1&ETTm2 7 69,680 5min\\r\\nTraffic 862 17,544 1hour\\r\\nElectricity 321 26,304 1hour\\r\\nExchange-Rate 8 7,588 1day\\r\\nWeather 21 52,696 10min\\r\\nILI 7 966 1week\\r\\n• Non-deep methods. We select two popular non-deep mod\\x02els in recent time – Linear [42] and GBRT [11]. Following\\r\\nthe practice in [42], we use the auto-gradient framework [30]\\r\\nto implement the Linear model and optimize it using gra\\x02dient descent, even though it is non-deep. The results are\\r\\nreproduced by their codes in the public repository 7. For\\r\\nGBRT, we integrate the XGBoost [4] implementation in the\\r\\nrepository 8. Linear is a representative linear model and\\r\\nGBRT is a non-linear model.\\r\\n• Deep methods. Transformers are popular and enjoy rapid\\r\\ndevelopment in long-term multivariate forecast tasks. We in\\x02clude two recent Transformer-based methods:Informer [43]\\r\\nand traditional Transformer [36]. Codes are taken from the\\r\\nInformer repository 9. For generality, we also include the\\r\\nCNN-based method TCN [2], RNN-based method DeepAR [33]\\r\\nand a simple two-layer MLP model with ReLU activation.\\r\\nOther details. For each experiment, we set the length of the look\\x02back window to 36 for ILI and 96 for other datasets. These values\\r\\nfollow the setup in [43] and differ from the values in [42]. When\\r\\nusing the CD strategy for Linear and GBRT, we flatten the input as\\r\\nthe feature for these models. Specifically, for a look-back window\\r\\nwith 𝐿 time steps and 𝐶 channels, the input feature has a dimen\\x02sionality of 𝐿𝐶. However, this approach may result in high input\\r\\ndimensionality when dealing with datasets with many channels,\\r\\nsuch as the Traffic dataset, which has 862 channels. This can lead\\r\\nto computational and storage issues for dense methods like Linear.\\r\\nTherefore, we report only those results that are feasible for one\\r\\nRTX 3090 GPU.\\r\\n7https://github.com/cure-lab/LTSF-Linear\\r\\n8https://github.com/Daniela-Shereen/GBRT-for-TSF\\r\\n9https://github.com/zhouhaoyi/Informer2020\\r\\n3\\n3.2 Main Results\\r\\nThe study’s results are presented in tables 2 and 3 where the algo\\x02rithms’ performance is measured by MAE and MSE, respectively. A\\r\\nviolin plot in fig. 2 illustrates the performance distribution across\\r\\nthe seven algorithms. The study reveals several noteworthy results.\\r\\nCI outperforms CD in the majority of cases. (1) CI significantly\\r\\nenhances the performance of almost all algorithms, with an average\\r\\nimprovement of at least 20%. OOn complex and dense algorithms\\r\\nlike MLP, Transformer, and Informer, the improvement exceeds\\r\\n30%. Simple methods like linear experience less improvement. In\\r\\nmost cases, replacing the CD strategy with CI yields significant im\\x02provement (>10%). On all algorithms, the improvement is observed\\r\\nin more than half of the cases. Only 3 cases show a significant drop\\r\\n(<-10%) in MAE and 9 in MSE, while the number of significant im\\x02provements is 92 in MAE and 95 in MSE. (2) On most benchmarks,\\r\\nCI improves performance consistently. This is apparent in the left\\r\\nseven datasets, where the improvement is consistent. On ETTh2, CI\\r\\nimproves performance by at least 30%, while Weather and ILI show\\r\\nless improvement. Nonetheless, CI remains superior, as evidenced\\r\\nby the performance distribution in fig. 2.\\r\\nCI strategy narrows the performance difference. Figure 2 re\\x02veals that the CI strategy has not only a lower error mean but\\r\\nalso a smaller variance than the CD strategy. This indicates that\\r\\nwhen using the CI strategy, the model performance does not differ\\r\\nsignificantly. With the exception of Weather, methods with the CI\\r\\nstrategy achieve the best results on the other datasets. But the best\\r\\nmethods vary. For instance, on Electricity, GBRT and Transformer\\r\\nyield the best results. MLP achieves the best outcome on the 96\\r\\nhorizon of ETTh2. While most state-of-the-art (SOTA) results are\\r\\nachieved using linear, other methods are not too far behind.\\r\\nConclusion. This section demonstrates that changing the CD strat\\x02egy to the CI strategy can significantly enhance the performance of\\r\\nmultivariate forecasting methods. Hence, the superiority of some\\r\\nrecent methods is mainly due to the training strategy rather than\\r\\nthe algorithm’s design [42]. For a fair comparison, the training\\r\\nstrategy and algorithm should be decoupled. The subsequent sec\\x02tion explores why the CI strategy outperforms and elucidates the\\r\\ntrade-off between capacity and robustness.\\r\\nE l ectri ci ty\\r\\nE T h 1E T h 2\\r\\nE T m1E T m2\\r\\nE xch an ge_\\r\\nRate\\r\\nTraf i c\\r\\nWeath er\\r\\nI LI\\r\\n0. 0\\r\\n0. 5\\r\\n1 . 0\\r\\n1 . 5\\r\\n2. 0\\r\\nM\\r\\nA\\r\\nE\\r\\nCD\\r\\nCI\\r\\nE l ectri ci ty\\r\\nE T h 1E T h 2\\r\\nE T m1E T m2\\r\\nE xch an ge_\\r\\nRate\\r\\nTraf i c\\r\\nWeath er\\r\\nI LI\\r\\n\\x04 \\x03 \\x01\\r\\n\\x04 \\x02 \\x01\\r\\n0. 0\\r\\n1 . 1\\r\\n5\\r\\n6\\r\\n7\\r\\n8\\r\\nM\\r\\nS\\r\\nE\\r\\nCD\\r\\nCI\\r\\nFigure 2: The performance distribution of 7 models utilizing\\r\\nthe CI and CD strategy. Values come from table 2 and table 3.\\r\\nThe prediction length is 24 for ILI dataset and 48 for the oth\\x02ers. In most cases, CI has a lower error mean and a smaller\\r\\nvariance than CD strategy. It means that CI performs bet\\x02ter than CD. Also, when using CI strategy, the model perfor\\x02mance does not differ very much.\\r\\n4 ANALYSIS\\r\\nThis section aims to provide an in-depth analysis of why CI is su\\x02perior for multivariate forecasting tasks in most cases, using the\\r\\nLinear [42] model as an example. It is closely related to the AutoRe\\x02gression (AR) in statistics [3]. We begin by examining the presence\\r\\nof distribution shift in real-world datasets. Subsequently, we eval\\x02uate the Linear model with CI and CD strategies, demonstrating\\r\\nhow the drifted statistics impact its performance. Our analysis high\\x02lights the fact that CI reduces the statistics gap between the training\\r\\nand test data. Finally, we decompose the risk to demonstrate that\\r\\nCI trades capacity for robustness, which translates to improved\\r\\nperformance on many real-world non-stationary time series.\\r\\n4.1 Distribution Drift\\r\\nReal-world datasets are characterized by time series with changing\\r\\nvalues over time, often accompanied by changes in the underlying\\r\\ndistribution, referred to as non-stationarity [1, 18]. In this section,\\r\\nwe investigate the AutoCorrelation Function (ACF), which is com\\x02monly used in time series analysis:\\r\\nDefinition 4.1 (AutoCorrelation Function (ACF) [25]). The au\\x02tocorrelation function of a stochastic process, {𝑋 (𝑡)}, is defined\\r\\nas:\\r\\n𝜌 (𝑡1, 𝑡2) =\\r\\n𝛾 (𝑡1, 𝑡2)\\r\\n√︁\\r\\n𝜎\\r\\n2\\r\\n(𝑡1) 𝜎\\r\\n2\\r\\n(𝑡2)\\r\\nwhere 𝛾 (𝑡1, 𝑡2) = Cov [𝑋 (𝑡1), 𝑋 (𝑡2)] is the covariance function,\\r\\nand 𝜎\\r\\n2\\r\\n(𝑡) = 𝛾 (𝑡, 𝑡) is the variance at time 𝑡.\\r\\nIf the process is stationary, then ACF is only a function of the\\r\\ntime difference 𝜏 = 𝑡2 − 𝑡1, i.e.:\\r\\n𝜌 (𝜏) =\\r\\n𝛾 (𝜏)\\r\\n𝛾 (0)\\r\\n, (4)\\r\\nwhere 𝛾 (𝜏) = Cov [𝑋 (𝑡), 𝑋 (𝑡 + 𝜏)].\\r\\nIn this paper, we temporarily assume that the stochastic process\\r\\nof each channel is stationary. But we will show that our analysis\\r\\nresults still holds in real-world data. To estimate the AutoCorrela\\x02tion Function (ACF) of a given time series 𝒙 ∈ R\\r\\n𝑇\\r\\n, we employ the\\r\\nmethod from [19], which is a commonly used practice. Specifically,\\r\\nwe use the following equation to estimate the ACF:\\r\\n𝜌ˆ(𝜏) =\\r\\n𝛾ˆ(𝜏)\\r\\n𝛾ˆ(0)\\r\\n(5)\\r\\nwhere 𝛾ˆ(𝜏) =\\r\\n1\\r\\n𝑇−𝑘\\r\\nÍ𝑇−𝜏\\r\\n𝑡=1\\r\\n(𝒙𝑡 − 𝒙¯) (𝒙𝑡+𝜏 − 𝒙¯) is the estimated co\\x02variance function, 𝑥¯ =\\r\\n1\\r\\n𝑇\\r\\nÍ𝑇\\r\\n𝑖=1\\r\\n𝒙𝑡is the mean value of 𝒙.\\r\\nWe display the ACF of the train series and test series on each\\r\\ndataset in fig. 3, following the train-test split used in previous\\r\\nworks [38, 43, 44]. From fig. 3, it is evident that distribution drift is\\r\\npresent in each dataset, owing to various reasons. For instance, in\\r\\n(MCIL, ETTh2) and (146, Electricity), the anomaly in the training\\r\\nseries leads to distribution drift. In (OT, ILI) and (wv (m/s), Weather),\\r\\nvariation in the trend is the main cause. The rest of two figures can\\r\\nnot be concluded by simple reasons. Nevertheless, distribution drift\\r\\nis a prevalent phenomenon in real-world time series datasets.\\r\\nThe observed distribution drift has a profound impact on the\\r\\nperformance of machine learning models, as the fundamental as\\x02sumption of these models is that the training and test data are drawn\\r\\n4\\nTable 2: MAE on nine multivariate time series datasets across various forecasting models. CD means taking the Channel De\\x02pendent strategy where the algorithm takes all the channels in the look-back window as input. CI means the algorithm takes\\r\\neach channel as an individual univariate series and trains a shared model. For each benchmark, we mark the best model results\\r\\nin bold. We also display the improvement percentage by CI relative to CD. The significant improvement (> 10%) and signif\\x02icant drop (< −10%) are marked by bold red and bold green respectively. The last column displays the number of significant\\r\\nimprovement/total cases, significant drop/total cases and average improvement (%) respectively.\\r\\nDataset Electricity ETTh1 ETTh2 ETTm1 ETTm2 Exchange_Rate Traffic Weather ILI Mean\\r\\nHorizon 48 96 48 96 48 96 48 96 48 96 48 96 48 96 48 96 24 36\\r\\nLinear (CD) 0.488 0.493 0.426 0.497 0.645 0.961 0.427 0.441 0.277 0.372 0.246 0.366 - - 0.219 0.247 0.955 0.945 11/16\\r\\nLinear (CI) 0.275 0.279 0.374 0.398 0.302 0.373 0.382 0.377 0.251 0.285 0.164 0.218 0.428 0.397 0.229 0.261 1.163 1.135 2/16\\r\\nImprove (%) +43.57 +43.39 +12.26 +19.87 +53.28 +61.15 +10.49 +14.35 +9.37 +23.29 +33.29 +40.52 - - -4.81 -5.36 -21.75 -20.09 +19.55\\r\\nGBRT (CD) - - 0.499 0.560 0.732 0.936 0.424 0.477 0.404 0.517 0.719 0.912 - - 0.236 0.268 1.597 1.554 12/14\\r\\nGBRT (CI) 0.249 0.256 0.385 0.415 0.454 0.614 0.360 0.380 0.297 0.355 0.336 0.401 0.282 0.286 0.190 0.232 1.459 1.501 0/14\\r\\nImprove (%) - - +22.87 +25.84 +37.92 +34.38 +15.00 +20.41 +26.49 +31.49 +53.19 +56.05 - - +19.49 +13.61 +8.62 +3.44 +26.34\\r\\nMLP (CD) 0.385 0.398 0.523 0.625 1.028 1.543 0.480 0.511 0.439 0.410 0.617 0.676 26.834 26.054 0.218 0.251 1.161 1.254 12/18\\r\\nMLP (CI) 0.287 0.289 0.395 0.422 0.319 0.365 0.453 0.483 0.266 0.294 0.265 0.255 0.406 0.388 0.230 0.261 1.358 1.369 1/18\\r\\nImprove (%) +25.43 +27.43 +24.45 +32.52 +68.93 +76.36 +5.66 +5.38 +39.39 +28.09 +57.07 +62.33 +98.49 +98.51 -5.67 -4.00 -16.89 -9.14 +34.13\\r\\nDeepAR (CD) 0.401 0.378 0.668 0.763 0.938 1.042 0.594 0.612 0.505 0.662 0.795 0.874 0.361 0.386 0.395 0.456 1.593 1.570 13/18\\r\\nDeepAR (CI) 0.330 0.342 0.587 0.594 0.541 0.597 0.511 0.520 0.304 0.354 0.623 0.660 0.370 0.410 0.240 0.287 1.449 1.454 0/18\\r\\nImprove (%) +17.72 +9.51 +12.14 +22.17 +42.28 +42.66 +14.03 +15.09 +39.76 +46.54 +21.65 +24.55 -2.46 -6.08 +39.29 +37.09 +9.00 +7.38 +21.80\\r\\nTCN (CD) 0.423 0.440 0.647 0.746 0.985 0.985 0.803 0.712 0.769 0.841 0.971 0.955 0.627 0.637 0.427 0.399 1.600 1.482 12/18\\r\\nTCN (CI) 0.322 0.349 0.405 0.471 0.441 0.585 0.555 0.502 0.358 0.386 0.929 0.971 0.441 0.469 0.388 0.411 1.837 1.593 1/18\\r\\nImprove (%) +23.75 +20.69 +37.42 +36.92 +55.20 +40.65 +30.83 +29.45 +53.44 +54.16 +4.29 -1.69 +29.63 +26.37 +9.26 -2.89 -14.81 -7.49 +23.62\\r\\nInformer (CD) 0.424 0.424 0.766 0.959 0.906 1.386 0.477 0.568 0.428 0.478 0.717 0.769 0.403 0.416 0.402 0.371 1.565 1.590 15/18\\r\\nInformer (CI) 0.285 0.285 0.509 0.655 0.372 0.427 0.408 0.447 0.264 0.350 0.308 0.312 0.337 0.297 0.228 0.343 1.486 1.552 0/18\\r\\nImprove (%) +32.90 +32.90 +33.56 +31.77 +58.89 +69.23 +14.54 +21.29 +38.41 +26.74 +56.99 +59.48 +16.31 +28.58 +43.27 +7.31 +5.08 +2.40 +32.20\\r\\nTransformer (CD) 0.352 0.357 0.734 0.774 0.829 1.111 0.458 0.533 0.404 0.547 0.571 0.769 0.364 0.359 0.343 0.452 1.508 1.555 17/18\\r\\nTransformer (CI) 0.281 0.255 0.565 0.501 0.347 0.461 0.407 0.466 0.254 0.321 0.227 0.312 0.303 0.273 0.232 0.287 1.348 1.525 0/18\\r\\nImprove (%) +20.06 +28.66 +23.03 +35.36 +58.18 +58.51 +11.30 +12.62 +37.15 +41.27 +60.25 +59.48 +16.70 +23.78 +32.57 +36.49 +10.62 +1.88 +31.55\\r\\nTable 3: MSE on nine multivariate time series datasets across various forecasting models.\\r\\nDataset Electricity ETTh1 ETTh2 ETTm1 ETTm2 Exchange_Rate Traffic Weather ILI Mean\\r\\nHorizon 48 96 48 96 48 96 48 96 48 96 48 96 48 96 48 96 24 36\\r\\nLinear (CD) 0.442 0.444 0.402 0.514 0.711 1.520 0.404 0.433 0.161 0.269 0.119 0.274 - - 0.142 0.165 2.343 2.436 11/16\\r\\nLinear (CI) 0.195 0.196 0.345 0.386 0.226 0.319 0.354 0.351 0.147 0.189 0.051 0.088 0.703 0.651 0.169 0.202 2.847 2.857 4/16\\r\\nImprove (%) +55.88 +55.91 +14.17 +24.94 +68.16 +79.04 +12.20 +18.85 +8.45 +29.73 +56.95 +67.84 - - -19.15 -22.16 -21.52 -17.29 +25.75\\r\\nGBRT (CD) - - 0.497 0.592 1.039 1.633 0.428 0.500 0.370 0.606 0.919 1.387 - - 0.539 0.475 5.128 4.845 12/14\\r\\nGBRT (CI) 0.165 0.171 0.365 0.414 0.636 1.167 0.341 0.367 0.236 0.318 0.270 0.335 0.532 0.550 0.146 0.185 5.186 4.983 0/14\\r\\nImprove (%) - - +26.63 +29.99 +38.77 +28.57 +20.43 +26.63 +36.13 +47.50 +70.58 +75.87 - - +72.96 +61.01 -1.13 -2.85 +37.93\\r\\nMLP (CD) 0.293 0.305 0.517 0.695 1.664 3.651 0.453 0.507 0.323 0.303 0.590 0.802 1257.104 1118.137 0.140 0.167 2.959 3.494 12/18\\r\\nMLP (CI) 0.199 0.199 0.360 0.408 0.254 0.321 0.457 0.513 0.157 0.197 0.172 0.118 0.666 0.639 0.169 0.202 3.618 3.840 3/18\\r\\nImprove (%) +32.31 +34.57 +30.40 +41.36 +84.74 +91.22 -0.73 -1.14 +51.33 +34.85 +70.87 +85.28 +99.95 +99.94 -21.35 -21.37 -22.28 -9.88 +37.78\\r\\nDeepAR (CD) 0.316 0.293 0.755 0.918 1.326 1.609 0.736 0.735 0.444 0.747 0.912 1.093 0.644 0.691 0.380 0.473 5.593 5.418 14/18\\r\\nDeepAR (CI) 0.231 0.247 0.723 0.724 0.601 0.714 0.616 0.566 0.200 0.268 0.824 0.878 0.641 0.708 0.173 0.221 4.590 4.501 0/18\\r\\nImprove (%) +26.73 +15.65 +4.30 +21.08 +54.71 +55.63 +16.26 +22.92 +54.91 +64.16 +9.67 +19.65 +0.55 -2.48 +54.38 +53.22 +17.94 +16.92 +28.12\\r\\nTCN (CD) 0.359 0.383 0.735 0.890 1.453 1.539 1.095 0.834 0.858 1.114 1.453 1.334 1.088 1.095 0.377 0.348 5.224 4.775 13/18\\r\\nTCN (CI) 0.258 0.290 0.401 0.507 0.404 0.663 0.614 0.534 0.251 0.313 1.488 1.562 0.784 0.835 0.290 0.339 6.671 5.142 2/18\\r\\nImprove (%) +28.28 +24.47 +45.40 +42.99 +72.17 +56.94 +43.93 +35.90 +70.77 +71.87 -2.41 -17.11 +27.98 +23.73 +23.05 +2.56 -27.70 -7.68 +28.62\\r\\nInformer (CD) 0.326 0.349 0.689 0.959 1.270 3.137 0.517 0.632 0.310 0.370 0.790 0.894 0.715 0.736 0.322 0.301 5.377 5.288 16/18\\r\\nInformer (CI) 0.208 0.183 0.560 0.532 0.311 0.382 0.366 0.426 0.156 0.262 0.169 0.190 0.601 0.549 0.162 0.260 4.980 5.254 0/18\\r\\nImprove (%) +36.07 +47.47 +18.67 +44.58 +75.49 +87.83 +29.29 +32.61 +49.70 +29.10 +78.64 +78.69 +15.95 +25.43 +49.74 +13.41 +7.38 +0.65 +40.04\\r\\nTransformer (CD) 0.250 0.257 0.861 0.966 1.031 1.868 0.458 0.554 0.281 0.520 0.511 0.659 0.645 0.650 0.251 0.423 5.309 5.406 17/18\\r\\nTransformer (CI) 0.185 0.163 0.655 0.533 0.274 0.466 0.379 0.496 0.148 0.237 0.101 0.137 0.558 0.526 0.168 0.225 4.307 5.033 0/18\\r\\nImprove (%) +26.10 +36.59 +23.85 +44.84 +73.43 +75.07 +17.32 +10.43 +47.27 +54.40 +80.27 +79.30 +13.43 +19.13 +33.14 +46.87 +18.88 +6.89 +39.29\\r\\nfrom identical and independent distributions (i.i.d.) [26]. This dis\\x02crepancy undermines the accuracy of these models in predicting\\r\\nunseen data. For instance, we extend the autoregressive (AR) model\\r\\nto long-term forecasting tasks and demonstrate the adverse effects\\r\\nof distribution drift on the model’s performance:\\r\\nProposition 4.2 (Yule-Walker equation [35, 37] extended). Assum\\x02ing a long-term AR model on time series 𝒙 with look-back window\\r\\n(order) 𝐿 and horizon 𝐻 is defined as:\\r\\n(𝒙𝑡+𝐻−1, . . . , 𝒙𝑡)\\r\\n𝑇 = 𝑾 (𝒙𝑡−1, . . . , 𝒙𝑡−𝐿)⊤ (6)\\r\\nwhere 𝑾 ∈ R\\r\\n𝐻×𝐿\\r\\nis the coefficients of the model. Then the best\\r\\nestimation 𝑾∗can be computed by extended version of Yule-Walker\\r\\nequation [35, 37]:\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n𝜌 (1) 𝜌 (2) · · · 𝜌 (𝐻)\\r\\n𝜌 (2) 𝜌 (3) · · · 𝜌 (𝐻 + 1)\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n𝜌 (𝐿) 𝜌 (𝐿 + 1) · · · 𝜌 (𝐻 + 𝐿 − 1)\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n=\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n𝜌 (0) 𝜌 (−1) · · · 𝜌 (−𝐿 + 1)\\r\\n𝜌 (1) 𝜌 (0) · · · 𝜌 (−𝐿 + 2)\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n𝜌 (𝐿 − 1) 𝜌 (𝐿 − 2) · · · 𝜌 (0)\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n𝑾∗\\r\\n(7)\\r\\nwhere 𝜌 (𝜏) = 𝜌 (−𝜏) is the autocorrelation of time delay 𝜏.\\r\\nProposition 4.2 establishes that the performance of an autore\\x02gressive (AR) model is closely linked to the autocorrelation function\\r\\n5\\n2016-07\\r\\n2016-102017-012017-042017-072017-102018-012018-042018-07\\r\\ndate\\r\\n2\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nSeries\\r\\ntrain\\r\\nval\\r\\ntest\\r\\n0 10 20 30\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTrain ACF\\r\\n0 10 20 30\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTest ACF\\r\\n(a) MUFL, ETTh2\\r\\n19921996200020042008\\r\\ndate\\r\\n2\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\nSeries\\r\\ntrain\\r\\nval\\r\\ntest\\r\\n0 5 10 15 20 25 30\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTrain ACF\\r\\n0 5 10 15 20 25 30\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTest ACF\\r\\n(b) 3, Exchange-Rate\\r\\n2002200420062008201020122014201620182020\\r\\ndate\\r\\n2\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\nSeries\\r\\ntrain\\r\\nval\\r\\ntest\\r\\n0 5 10 15 20\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTrain ACF\\r\\n0 5 10 15 20\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTest ACF\\r\\n(c) OT, ILI\\r\\n2020-012020-032020-052020-072020-09\\r\\n2020-11\\r\\n2021-01\\r\\ndate\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\nSeries\\r\\ntrain\\r\\nval\\r\\ntest\\r\\n0 10 20 30 40\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTrain ACF\\r\\n0 10 20 30 40\\r\\n0.2\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTest ACF\\r\\n(d) wv (m/s), Weather\\r\\n2016-092017-012017-052017-092018-012018-052018-092019-012019-05\\r\\ndate\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\nSeries\\r\\ntrain\\r\\nval\\r\\ntest\\r\\n0 10 20 30\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTrain ACF\\r\\n0 10 20 30\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTest ACF\\r\\n(e) 146, Electricity\\r\\n2016-072016-102017-012017-042017-072017-102018-012018-042018-07\\r\\ndate\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n7\\r\\nSeries\\r\\ntrain\\r\\nval\\r\\ntest\\r\\n0 10 20 30\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTrain ACF\\r\\n0 10 20 30\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\nTest ACF\\r\\n(f) 607, Traffic\\r\\nFigure 3: The ACF of train series and test series. Captions of each subfigure represent the tuple (channel, dataset). For each\\r\\nsubfigure, the leftmost plot displays the series split, with the training series in black, validation in blue, and test in purple. The\\r\\nmiddle and right display the ACF of train and test series respectively. The middle and right plots show the ACF of the training\\r\\nand test series, respectively. The results reveal a significant discrepancy in the statistics between the training and test series.\\r\\n(ACF). Specifically, when there is a significant disparity between\\r\\nthe ACF of the training and testing data, the resulting difference in\\r\\nthe estimated values of 𝑾∗ on the two datasets can be substantial.\\r\\nThis can result in high error rates when applying the trained model\\r\\nto the test data.\\r\\nRegrettably, real-world multivariate datasets often exhibit large\\r\\ndisparities in ACF across channels. These distribution drifts in\\r\\ncertain channels can significantly impact the performance of the\\r\\ntrained model. However, we demonstrate in the next section that\\r\\nwhile the AR model using the CD strategy is sensitive to such drifts,\\r\\nthe model using the CI strategy is more robust to them.\\r\\n4.2 CI Alleviates Distribution Drift\\r\\nIn the previous section, we highlighted the presence of distribu\\x02tion drift (as measured by ACF) in real-world datasets. We also\\r\\npresented theoretical insights into how such drift can impact the\\r\\nperformance of linear autoregressive (AR) models in univariate\\r\\nscenarios. n this section, we extend our analysis to multivariate\\r\\ntasks and demonstrate that the CI strategy can alleviate distribution\\r\\ndrift in each channel, while the CD strategy is vulnerable to such drift.\\r\\nCoefficients of CI and CD. To facilitate our analysis, we reshape\\r\\nthe set of series data. Specifically, given a set of data {(𝑿\\r\\n(𝑖)\\r\\n, 𝒀\\r\\n(𝑖)\\r\\n)}𝑁\\r\\n𝑖=1\\r\\n,\\r\\nwe rearrange the series of each channel to its unique matrix 𝑨(𝑐) ∈\\r\\nR\\r\\n𝑁 ×𝐿\\r\\nand 𝑩\\r\\n(𝑐) ∈ R𝑁 ×𝐻 , i.e., 𝑨\\r\\n(𝑐)\\r\\n𝑖,𝑙 = 𝑿\\r\\n(𝑖)\\r\\n𝑙,𝑐 , 𝑩\\r\\n(𝑐)\\r\\n𝑖,ℎ = 𝒀\\r\\n(𝑖)\\r\\nℎ,𝑐 . Basic on\\r\\nthis representation, we can express the objectives of Linear (CD)\\r\\nand Linear (CI) as follows:\\r\\nDefinition 4.3 (Objective of Linear (CD) and Linear (CI)). Assum\\x02ing the series of each channel is centered, the ordinary least square\\r\\nobjective of Linear (CD) can be defined as:\\r\\nL𝑐𝑑 = ∥𝑨𝑐𝑑𝑾𝑐𝑑 − 𝑩𝑐𝑑 ∥\\r\\n2\\r\\n𝐹\\r\\n(8)\\r\\nwhere 𝑨𝑐𝑑 =\\r\\nh\\r\\n𝑨(1)𝑨(2). . . 𝑨(𝐶)\\r\\ni\\r\\n∈ R\\r\\n𝑁 ×𝐿𝐶 is the vertical con\\x02catenation of 𝑨(1)\\r\\n, 𝑨(2), . . . , 𝑨(𝐶), 𝑩𝑐𝑑 the same. 𝑾𝑐𝑑 ∈ R\\r\\n𝐿𝐶×𝐻𝐶\\r\\nis the coefficient.\\r\\nSimiarly, the objective of Linear (CI) can be defined as:\\r\\nL𝑐𝑖 = ∥𝑨𝑐𝑖𝑾𝑐𝑖 − 𝑩𝑐𝑖 ∥\\r\\n2\\r\\n𝐹\\r\\n(9)\\r\\nwhere 𝑨𝑐𝑖 =\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n𝑨(1)\\r\\n𝑨(2)\\r\\n.\\r\\n.\\r\\n.\\r\\n𝑨(𝐶)\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n∈ R\\r\\n𝑁𝐶×𝐿\\r\\nis the horizontal concatenation of\\r\\n𝑨(1), 𝑨(2), . . . , 𝑨(𝐶), 𝑩𝑐𝑖 the same. 𝑾𝑐𝑑 ∈ R\\r\\n𝐿×𝐻 is the coefficient.\\r\\nFrom definition 4.3, we can see that the primary distinction\\r\\nbetween CD and CI strategy on Linear model is the way data are\\r\\nstacked. By solving the two objectives, the CD and CI coefficient of\\r\\ncan be estimated according to the following proposition:\\r\\nProposition 4.4 (Yule-Walker equation of Linear (CD) and Linear\\r\\n(CI)). Define the (auto-/cross-)correlation matrix:\\r\\n𝑹𝑐1,𝑐2 =\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n𝜌𝑐1,𝑐2(0) 𝜌𝑐1,𝑐2(−1) · · · 𝜌𝑐1,𝑐2(−𝐿 + 1)\\r\\n𝜌𝑐1,𝑐2(1) 𝜌𝑐1,𝑐2(0) · · · 𝜌𝑐1,𝑐2(−𝐿 + 2)\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n𝜌𝑐1,𝑐2(𝐿 − 1) 𝜌𝑐1,𝑐2(𝐿 − 2) · · · 𝜌𝑐1,𝑐2(0)\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n∈ R\\r\\n𝐿×𝐿\\r\\n.\\r\\n𝑹\\r\\n′\\r\\n𝑐1,𝑐2\\r\\n=\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n𝜌 (1) 𝜌 (2) · · · 𝜌 (𝐻)\\r\\n𝜌 (2) 𝜌 (3) · · · 𝜌 (𝐻 + 1)\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n𝜌 (𝐿) 𝜌 (𝐿 + 1) · · · 𝜌 (𝐻 + 𝐿 − 1)\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n∈ R\\r\\n𝐿×𝐻\\r\\nwhere 𝜌𝑐1,𝑐2(𝜏) is the auto-/cross-correlation at time delay 𝜏 when\\r\\n𝑐1 = 𝑐2 / 𝑐1 ≠ 𝑐2.\\r\\nAssuming the series of each channel has the same variance, then\\r\\nthe Yule-Walker equation of Linear (CD) is:\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n𝑅\\r\\n′\\r\\n1,1\\r\\n𝑅\\r\\n′\\r\\n1,2\\r\\n. . . 𝑅′\\r\\n1,𝐶\\r\\n𝑅\\r\\n′\\r\\n2,1\\r\\n𝑅\\r\\n′\\r\\n2,2\\r\\n. . . 𝑅′\\r\\n2,𝐶\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n𝑅\\r\\n′\\r\\n𝐶,1\\r\\n𝑅\\r\\n′\\r\\n𝐶,2\\r\\n. . . 𝑅′\\r\\n𝐶,𝐶\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n=\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n𝑅1,1 𝑅1,2 . . . 𝑅1,𝐶\\r\\n𝑅2,1 𝑅2,2 . . . 𝑅2,𝐶\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n𝑅𝐶,1 𝑅𝐶,2 . . . 𝑅𝐶,𝐶\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n𝑾∗\\r\\n𝑐𝑑\\r\\n(10)\\r\\nand the Yule-Walker equation of Linear (CI) is:\\r\\n∑︁\\r\\n𝐶\\r\\n𝑐=1\\r\\n𝑅\\r\\n′\\r\\n𝑐,𝑐 = (\\r\\n∑︁\\r\\n𝐶\\r\\n𝑐=1\\r\\n𝑅𝑐,𝑐 )𝑾∗\\r\\n𝑐𝑖 (11)\\r\\n6\\nProof. Taking the derivative of eq. (8), we get the ordinary least\\r\\nsquare equation:\\r\\n𝑨\\r\\n⊤\\r\\n𝑐𝑑𝑨𝑐𝑑𝑾𝑐𝑑 = 𝑨\\r\\n⊤\\r\\n𝑐𝑑𝑩𝑐𝑑 (12)\\r\\n𝑨\\r\\n⊤\\r\\n𝑐𝑑𝑨𝑐𝑑 =\\r\\nh\\r\\n𝑨\\r\\n(1)𝑨(2)\\r\\n. . . 𝑨\\r\\n(𝐶)\\r\\ni⊤ h\\r\\n𝑨\\r\\n(1)𝑨(2)\\r\\n. . . 𝑨\\r\\n(𝐶)\\r\\ni\\r\\n=\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n(𝑨(1))\\r\\n⊤(𝑨(1)\\r\\n) (𝑨(1))\\r\\n⊤(𝑨(2)\\r\\n) . . . (𝑨(1))\\r\\n⊤(𝑨(𝐶)\\r\\n)\\r\\n(𝑨(2))\\r\\n⊤(𝑨(2)\\r\\n) (𝑨(2))\\r\\n⊤(𝑨(2)\\r\\n) . . . (𝑨(2))\\r\\n⊤(𝑨(𝐶)\\r\\n)\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n(𝑨(𝐶))\\r\\n⊤(𝑨(𝐶)\\r\\n) (𝑨(𝐶))\\r\\n⊤(𝑨(𝐶)\\r\\n) . . . (𝑨(1))\\r\\n⊤(𝑨(𝐶)\\r\\n)\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n(13)\\r\\nis in a form of outer product. Each (𝑨(𝑐1))\\r\\n⊤(𝑨(𝑐2)\\r\\n) is a estimation\\r\\nof co-variance matrix [25]. Since the variances of each channel\\r\\nseries are assumed to be the same. So by dividing on both side of\\r\\neq. (12) by the variance, we can get eq. (10).\\r\\nSimiarly, 𝑨⊤\\r\\n𝑐𝑑𝑨𝑐𝑑 =\\r\\nh\\r\\n𝑨(1)𝑨(2). . . 𝑨(𝐶)\\r\\ni h𝑨(1)𝑨(2)\\r\\n. . . 𝑨(𝐶)\\r\\ni⊤\\r\\nis in the form of inner product. By the same process, we can get\\r\\neq. (11). □\\r\\nBy analyzing the difference between eq. (10) and eq. (11), we\\r\\ncan draw an important conclusion– coefficients of CD is de\\x02termined by the (auto-/cross-)correlation function of each\\r\\nchannel, while the coefficients of the CI strategy are deter\\x02mined solely by the summation (or mean) of the ACF of all\\r\\nchannels.\\r\\nTakeaways\\r\\nThe optimal coefficients of the Linear model using the CD\\r\\nstrategy are determined by the ACF of all channels, while\\r\\nthe optimal coefficients of the model using the CI strategy\\r\\nare only determined by the sum of the ACF across all\\r\\nchannels.\\r\\nCI strategy leads to less distribution drift. It is noteworthy that\\r\\nthe summation operation used in the CI strategy mitigates the dis\\x02tribution gap between the training and test series. To demonstrate\\r\\nthis, we examine the differences in the values of the ACF between\\r\\nthe training and test portions. Specifically, we denote the ACF of\\r\\nthe training portion in channel 𝑐 as 𝜌\\r\\n(𝑡𝑟)\\r\\n𝑐 and the corresponding\\r\\ntest ACF as 𝜌\\r\\n(𝑡𝑒)\\r\\n𝑐\\r\\n. Then, we calculate the ACF difference in channel\\r\\n𝑐 as follows:\\r\\nDiff𝑐 =\\r\\n∑︁\\r\\n𝑇\\r\\n𝑡=0\\r\\n(𝜌\\r\\n(𝑡𝑟)\\r\\n𝑐\\r\\n(𝑡) − 𝜌\\r\\n(𝑡𝑒)\\r\\n𝑐\\r\\n(𝑡))2.\\r\\nWhen employing the CD strategy, the linear model is susceptible\\r\\nto the distribution drift of each channel. However, the CI strategy\\r\\nensures that the linear model is solely determined by the sum of\\r\\nACF over all channels. Therefore, we only need to evaluate the\\r\\nchanges in the sum of ACF when using the CI strategy. Hence, we\\r\\ncalculate the difference in the ACF summation between the training\\r\\nand test sets using the following equation, referred to as the sum\\r\\ndiff:\\r\\nDiffsum =\\r\\n∑︁\\r\\n𝑇\\r\\n𝑡=0\\r\\n(\\r\\n1\\r\\n𝐶\\r\\n∑︁\\r\\n𝐶\\r\\n𝑐=1\\r\\n𝜌\\r\\n(𝑡𝑟)\\r\\n𝑐\\r\\n(𝑡) − 1\\r\\n𝐶\\r\\n∑︁\\r\\n𝐶\\r\\n𝑐=1\\r\\n𝜌\\r\\n(𝑡𝑒)\\r\\n𝑐\\r\\n(𝑡))2.\\r\\nConsidering the scale, we compute the mean instead of the sum.\\r\\nBut we still name it sum diff. Sum diff can be regarded as the ACF\\r\\ndifference when using CI strategy.\\r\\nWe present the ACF difference (Diff 𝑐) using bar plots, sorted in\\r\\ndescending order, and indicate the sum diff (Diff sum) with a hori\\x02zontal line. Our results are summarized in fig. 4.Several observations\\r\\ncan be made from the figure: (1) Most real-world datasets ex\\x02hibit channels with significant ACF differences between the\\r\\ntraining and test data, indicating severe distribution drift in\\r\\nthe time series of these channels. In ETT datasets (a)-(d), for\\r\\ninstance, the largest ACF differences range between 5 and 8, which\\r\\nis substantial given that ACF values typically fall within [0, 1].\\r\\nETTh1 and ETTm1 show relatively uniform ACF differences, while\\r\\nETTh2 and ETTm2 feature two channels with particularly large\\r\\nACF differences compared to the rest. Other datasets exhibit simi\\x02lar patterns, with Exchange (e) displaying two channels with ACF\\r\\ndifferences that greatly exceed those of other channels, and ILI\\r\\nfeaturing a largest difference that is more than twice that of the rest.\\r\\nIn datasets with many channels, such as weather (g), electricity\\r\\n(h), and traffic (i), the largest difference can be up to 15, 32, and\\r\\n24, respectively, which is much greater than that of most other\\r\\nchannels. Furthermore, we observe a rapid decay of the ACF differ\\x02ence as the channel index increases. (2) The sum diff is typically\\r\\nsmaller than the ACF difference of most channels, suggest\\x02ing that the distribution drift with CI strategy is less severe\\r\\nthan with CD strategy. Across the 9 benchmarks, 7 datasets have\\r\\na sum diff that is smaller than that of more than 50% of the channels,\\r\\nindicating that the distribution drift, as measured by the ACF dif\\x02ference, is smaller than that of most channels when using CI. Even\\r\\nin the two exceptions, ETTm2 (d) and Exchange-Rate (e), the sum\\r\\ndiff is still much smaller than the head channels. On ETTm2 (d), for\\r\\nexample, the sum diff is 0.4678, significantly smaller than the head\\r\\nvalues, which are nearly 5. The sum diff is also not much larger\\r\\nthan channels 4-7. Similar observations hold for Exchange-Rate (e),\\r\\nwhere the sum diff is only lower than two channels, but its value\\r\\nof 0.1044 is much smaller than 0.75. On the remaining 7 datasets,\\r\\nnot only is the sum diff smaller than that of most channels, but\\r\\nit is also much smaller in value. For instance, on ETTh2 (b), the\\r\\nsum diff of 0.3713 is 20 times smaller than that of channel 1, while\\r\\non Electricity (h) and Traffic (i), it is 500 and 8000 times smaller,\\r\\nrespectively.\\r\\nTakeaways\\r\\nThe sum of ACF differences between training and test data\\r\\nexhibits less variation than the ACF differences of most\\r\\nindividual channels. This means employing the CI strategy\\r\\nresults in reduced distribution drift.\\r\\n4.3 Capacity and Robustness\\r\\nAlthough the CI strategy can reduce the distribution gap between\\r\\ntraining and test data, we cannot conclude that it leads to better\\r\\ngeneralization performance. This is primarily due to the fact that\\r\\nthe hypothesis spaces under CI and CD strategies are not the same,\\r\\nwhere 𝑾𝑐𝑖 ∈ R\\r\\n𝐿×𝐻 and 𝑾𝑐𝑑 ∈ R𝐿𝐶×𝐻𝐶. To analyze the risks asso\\x02ciated with these strategies, we follow the risk analysis framework\\r\\n7\\n1 2 3 4 5 6 7\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 2. 6302\\r\\n<57. 1 4% (4/7) ch an n el s\\r\\n(a) ETTh1\\r\\n1 2 3 4 5 6 7\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n7\\r\\n8\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 0. 371 3\\r\\n<57. 1 4% (4/7) ch an n el s\\r\\n(b) ETTh2\\r\\n1 2 3 4 5 6 7\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 2. 5908\\r\\n<57. 1 4% (4/7) ch an n el s\\r\\n(c) ETTm1\\r\\n1 2 3 4 5 6 7\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 0. 4678\\r\\n<42. 86% (3/7) ch an n el s\\r\\n(d) ETTm2\\r\\n1 2 3 4 5 6 7\\r\\n0. 0\\r\\n0. 1\\r\\n0. 2\\r\\n0. 3\\r\\n0. 4\\r\\n0. 5\\r\\n0. 6\\r\\n0. 7\\r\\n0. 8\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 0. 1 044\\r\\n<25. 00% (2/8) ch an n el s\\r\\n(e) Exchange-Rate\\r\\n1 2 3 4 5 6 7\\r\\n0. 0\\r\\n0. 5\\r\\n1 . 0\\r\\n1 . 5\\r\\n2. 0\\r\\n2. 5\\r\\n3. 0\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 0. 51 48\\r\\n<57. 1 4% (4/7) ch an n el s\\r\\n(f) ILI\\r\\n5 1 0 1 5 20\\r\\n0. 0\\r\\n0. 2\\r\\n1 5. 2\\r\\n1 5. 4\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 0. 0350\\r\\n<52. 38% (1 1 /21 ) ch an n el s\\r\\n(g) Weather\\r\\n50 1 00 1 50 200 250 300\\r\\n0. 0\\r\\n0. 5\\r\\n1 . 0\\r\\n32. 5\\r\\n33. 0\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 0. 0661\\r\\n<64. 49% (207/321 ) ch an n el s\\r\\n(h) Electricity\\r\\n1 00 200 300 400 500 600 700 800\\r\\n0\\r\\n5\\r\\n1 0\\r\\n1 5\\r\\n20\\r\\nA\\r\\nC\\r\\nF\\r\\nDi f\\r\\nf\\r\\nCh an n el I n dex\\r\\nSum Di ff: 0. 0031\\r\\n<97. 45% (840/862) ch an n el s\\r\\n(i) Traffic\\r\\nFigure 4: The difference of ACF between training data and test data. The ACF difference for each channel is depicted in bar\\r\\ncharts, arranged in descending order. The sum diff, which represents the overall ACF difference under the CI strategy, is\\r\\nshown as a horizontal line. the sum diff is smaller than the ACF difference of each channel, indicating that the CI strategy can\\r\\neffectively mitigate distribution drift.\\r\\nin machine learning proposed by Mohri et al. [26] and decompose\\r\\nthe risk according to the following equation:\\r\\nR (𝑾ˆ ) =\\r\\n\\x12\\r\\nR (𝑾ˆ ) − inf\\r\\n𝑾 ∈W\\r\\nR (𝑾)\\r\\n\\x13\\r\\n| {z }\\r\\nnon-robustness\\r\\n+ inf\\r\\n𝑾 ∈W\\r\\nR (𝑾)\\r\\n| {z }\\r\\nincapacity\\r\\n. (14)\\r\\nwhere R (·) is the risk defined by eq. (1) and 𝑾ˆ denotes the model\\r\\nobtained by minimizing the empirical CD loss as in eq. (2), or the\\r\\nempirical CI loss as in eq. (3). We use the notation W to refer to the\\r\\nhypothesis space. In this paper, we consider two types of hypothesis\\r\\nspace: the hypothesis space of CI, denoted by W𝑐𝑖 = R\\r\\n𝐿×𝐻 , and\\r\\nthe hypothesis space of CD, denoted by W𝑐𝑑 = R\\r\\n𝐿𝐶×𝐻𝐶.\\r\\nUnlike the traditional terminology [26], we interpret the first\\r\\nterm as the (non-)robustness of a model, which is the risk gap\\r\\nbetween the model trained on the training set and the optimum\\r\\nmodel on the test data distribution. It measures the ability of the\\r\\nmodel to handle unseen data and achieve nearly optimal perfor\\x02mance. A lower value of this term indicates a more robust model.\\r\\nThe (in)capacity measures how well the optimum model fits the\\r\\ndata, with a lower value indicating a better ability to fit the data.\\r\\nSimple algorithms like linear regression usually have low capacity\\r\\n(high incapacity), while complex algorithms like neural networks\\r\\nhave high capacity. In addition to the choice of algorithm, different\\r\\ntraining strategies such as CI and CD also affect the robustness\\r\\nand capacity of the obtained model. In the following sections, we\\r\\nprovide empirical results to further illustrate this concept.\\r\\nIt is not possible to calculate the risk R directly as access to the\\r\\nunderlying data distribution is unavailable. In this paper, we opt to\\r\\napproximate it using the empirical risk on the test data. For ease of\\r\\nreference, we represent the training and test set utilizing the CI strat\\x02egy as (𝐴\\r\\n(𝑡𝑟)\\r\\n𝑐𝑖, 𝐵(𝑡𝑟)𝑐𝑖) and (𝐴\\r\\n(𝑡𝑒)\\r\\n𝑐𝑖, 𝐵(𝑡𝑒)𝑐𝑖), while the training\\r\\nand test set using the CD strategy is denoted as (𝐴\\r\\n(𝑡𝑟)\\r\\n𝑐𝑑, 𝐵(𝑡𝑟)𝑐𝑑)\\r\\nand (𝐴\\r\\n(𝑡𝑒)\\r\\n𝑐𝑑, 𝐵(𝑡𝑒)𝑐𝑑). We compute the subsequent statistics to\\r\\ndemonstrate the performance of CI and CD on the benchmarks:\\r\\n(1) Train Error (Incapacity). The training error L\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\nis com\\x02puted as the following:\\r\\nL\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n= ∥𝑨\\r\\n(𝑡𝑟)\\r\\n𝑖 𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑩\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n∥\\r\\n2\\r\\n𝐹\\r\\n, 𝑖 ∈ {𝑐𝑖, 𝑐𝑑} (15)\\r\\nwhere\\r\\n𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n= arg min\\r\\n𝑾\\r\\n∥𝑨\\r\\n(𝑡𝑟)\\r\\n𝑖 𝑾 − 𝑩\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n∥\\r\\n2\\r\\n𝐹\\r\\nis the optimum parameter for the training data. Train Error\\r\\nis also a measure of capacity but empirically computed on\\r\\nthe training set.\\r\\n(2) Test Error (Incapacity). The test error L\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\nis computed\\r\\nas the following:\\r\\nL\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n= ∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n− 𝑩\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n∥\\r\\n2\\r\\n𝐹\\r\\n, 𝑖 ∈ {𝑐𝑖, 𝑐𝑑} (16)\\r\\nwhere:\\r\\n𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n= arg min\\r\\n𝑾\\r\\n∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖 𝑾 − 𝑩\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n∥\\r\\n2\\r\\n𝐹\\r\\nis the optimum parameter for the test data. Test loss describes\\r\\nthe best error a linear model can achieve on the test data. It\\r\\nis an approximation of inf𝑾 ∈W R (𝑾) in eq. (14).\\r\\n(3) Gen Error (R (𝑾ˆ )). The generalization error L\\r\\n(𝑔𝑒𝑛)\\r\\n𝑖\\r\\nis com\\x02puted as:\\r\\nL\\r\\n(𝑔𝑒𝑛)\\r\\n𝑖\\r\\n= ∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖 𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑩\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n∥\\r\\n2\\r\\n𝐹\\r\\n. 𝑖 ∈ {𝑐𝑖, 𝑐𝑑}\\r\\nIt is the performance measure on the benchmarks.\\r\\n(4) W Diff (Non-Robustness). It is an approximation of non\\x02robustness in eq. (14). Its value is computed as:\\r\\nDiff𝑊𝑖= ∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n(𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n) ∥2\\r\\n𝐹\\r\\n. (17)\\r\\n8\\neq. (17) is inspired by ordinal least square in fixed design\\r\\nsettings [26], where the estimation error is computed as the\\r\\nMahalanobis distance between 𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\nand 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n. eq. (17) is\\r\\nan extension of Mahalanobis distance, since:\\r\\nDiff𝑊𝑖= ∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n(𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n) ∥2\\r\\n𝐹\\r\\n= tr( (𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n)\\r\\n⊤(𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n)\\r\\n⊤𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n(𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n))\\r\\n= tr( (𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n)\\r\\n⊤Σˆ\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n(𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n))\\r\\n. (18)\\r\\ntris the trace operation for a matrix and Σˆ\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n= (𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n)\\r\\n⊤𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\nis the unnormalized sample covariance matrix. When 𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\nand 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\nbecome vectors, eq. (18) falls back to the Maha\\x02lanobis distance parameterized by Mahalanobis matrix Σˆ\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n.\\r\\nIn this sense, the W diff can also be considered as a measure\\r\\nof distribution drift, since it is a distance measure between\\r\\n𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\nand 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n, and 𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\nand 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\nare derived by the\\r\\nACF of train and test data.\\r\\nAnother interpretation of Diff𝑊𝑖takes it as a lower bound\\r\\nfor the estimation error:\\r\\nDiff𝑊𝑖= ∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n(𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n) ∥2\\r\\n𝐹\\r\\n≤ ∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖 𝑾\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n− 𝑩\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n∥\\r\\n2\\r\\n𝐹\\r\\n− ∥𝑨\\r\\n(𝑡𝑒)\\r\\n𝑖 𝑾\\r\\n(𝑡𝑟)\\r\\n𝑖\\r\\n− 𝑩\\r\\n(𝑡𝑒)\\r\\n𝑖\\r\\n∥\\r\\n2\\r\\n𝐹\\r\\n= L\\r\\n(𝑔𝑒𝑛)\\r\\n𝑖\\r\\n− L(𝑡𝑒)\\r\\n𝑖\\r\\n, 𝑖 ∈ {𝑐𝑖, 𝑐𝑑}\\r\\nSimilar to the risk decomposition (eq. (14)), we can also decom\\x02pose the gen loss by the following equation:\\r\\nL\\r\\n(𝑔𝑒𝑛)\\r\\n𝑖\\r\\n= (L(𝑔𝑒𝑛)\\r\\n𝑖\\r\\n− L(𝑡𝑒)\\r\\n𝑖\\r\\n)\\r\\n| {z }\\r\\n≈Diff𝑊𝑖\\r\\n+L(𝑡𝑒)\\r\\n𝑖\\r\\n. 𝑖 ∈ {𝑐𝑖, 𝑐𝑑} (19)\\r\\nWe illustrate the above statistics on the 9 datasets in fig. 5 and\\r\\ndraw the following conclusions. (1) CD models exhibit lower\\r\\ntrain/test loss as compared to CI models, indicating that CD\\r\\nstrategy trains a model with higher capacity. On all of the 9\\r\\nmodels, the train/test error is always lower than CI. When the num\\x02ber of channels is large, CD model may have 0 errors. This trend\\r\\nis consistent across all 9 models, with the CD model registering\\r\\nzero errors for larger channel numbers. This outcome is anticipated\\r\\ndue to the fact that the hypothesis space of CI is a subset of CD,\\r\\nthereby enabling the construction of a CD linear model by replicat\\x02ing elements of a CI linear model. As per the definition, the best\\r\\nCD model will inevitably have a lower error rate than CI. (2) CD\\r\\nmodels have a significantly larger W diff than CI models, in\\x02dicating that CI is much more robust than CD. This trend is\\r\\napparent across all 9 datasets, with CD models having W diff values\\r\\nusually over 10 times the value of CI. The phenomenon is especially\\r\\nconspicuous in datasets with multiple channels such as Electricity\\r\\n(h) and Traffic (i). The distribution drift between train and test data\\r\\nis responsible for this trend. In the previous section, we have shown\\r\\nthat the ACF coefficients of CD and CI models are determined by\\r\\nthe ACF of all channels and the sum of ACF across all channels,\\r\\nwith the latter exhibiting a lower difference than the former. This\\r\\ndifference contributes to the gap between optimal models on train\\r\\nand test data being different for CD and CI strategies. CI strategy\\r\\nleads to a lighter distribution gap, resulting in a smaller W Diff\\r\\nvalue. (3) W Diff values are more significant than Test Error in most\\r\\ncases, leading to CI models having lower Gen Error than CD models.\\r\\ni.e., Robustness is more crucial than capacity. For the 4 ETT\\r\\nbenchmarks, there is not much difference in test error between CI\\r\\nand CD models, but W diff values are significantly distinct. Hence,\\r\\nCI models perform better than CD models in terms of gen loss. On\\r\\ndatasets (e), (f), (h), (i), the test loss varies considerably, but the W\\r\\ndiff values differ significantly more than the test loss, leading to\\r\\nCI models performing better than CD models. The only exception\\r\\nto this trend is the weather benchmark (g), where test error holds\\r\\ngreater significance than W diff values. Consequently, CD strategy\\r\\nperforms better in this case. Figure 6 summarizes these findings.\\r\\nFrom analyses in this section, we draw an overall conclusion:\\r\\nSection Conclusion\\r\\nThe Channel Dependent (CD) strategy has high capacity\\r\\nbut low robustness. The Channel Dependent (CI) strat\\x02egy has low capacity but high robustness. In numerous\\r\\nreal-world non-stationary time series with distribution\\r\\ndrifts, robustness is a more crucial factor than capacity in\\r\\nforecasting tasks. Consequently, CI strategy often delivers\\r\\nbetter performance.\\r\\nTo provide readers with a clear understanding, we demonstrate\\r\\nthe differences in prediction outcomes between the CD and CI\\r\\nstrategies using a visual representation in fig. 7. These examples\\r\\nwere selected as they are indicative of the findings obtained across\\r\\nthe experiments. From these four figures, We observe that the CD\\r\\napproach produces sharp predictions, while the CI approach gener\\x02ates smoother predictions. This discrepancy can be attributed to\\r\\nthe sum over effect, which we have analysed in detail in the pre\\x02vious subsection. Unfortunately, the non-robust and sharp nature\\r\\nof CD predictions make them unsuitable for accurately predict\\x02ing real-world non-stationary time-series. Figure 7.(a) displays a\\r\\nscenario where both strategies capture the correct trend and sea\\x02sonal component, but the CI approach more closely aligns with\\r\\nthe ground-truth compared to the CD approach. (b) shows that the\\r\\nCD approach may predict incorrect trends, while the CI approach\\r\\nis less prone to making such errors. When faced with anomalous\\r\\ntime-series like (c), the CI approach is more robust and produces\\r\\nless oscillation. Nonetheless, there are instances where the CD ap\\x02proach outperforms the CI approach, as shown in fig. 7.(d), where\\r\\nthe CD model’s high capacity can be advantageous for capturing\\r\\ncomplex but predictable patterns. Intuitively, real-world time series\\r\\nthat exhibit regular patterns and smooth changes are predictable.\\r\\nConversely, drastically oscillating time series, such as the one de\\x02picted in fig. 7.(c), are anomalies and therefore unpredictable. In\\r\\nsuch cases, conservative and robust predictions are preferable. As a\\r\\nresult, the CI strategy yields superior results on average compared\\r\\nto the CD strategy.\\r\\n5 PRACTICAL GUIDES\\r\\nThe previous section’s analyses have revealed that the CD strategy\\r\\nexhibits high capacity but low robustness, making it unsuitable for\\r\\nhandling real-world non-stationary time series where distribution\\r\\ndrifts are significant. Conversely, the CI approach displays higher\\r\\nrobustness, resulting in superior performance compared to the CD\\r\\n9\\n0. 346\\r\\n0. 448\\r\\n0. 01 2\\r\\n0. 460\\r\\n0. 291\\r\\n0. 359\\r\\n0. 21 0\\r\\n0. 569\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 0\\r\\n0. 1\\r\\n0. 2\\r\\n0. 3\\r\\n0. 4\\r\\n0. 5\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(a) ETTh1\\r\\n0. 392\\r\\n0. 225\\r\\n0. 045\\r\\n0. 270\\r\\n0. 241\\r\\n0. 1 73\\r\\n0. 884\\r\\n1 . 057\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 0\\r\\n0. 2\\r\\n0. 4\\r\\n0. 6\\r\\n0. 8\\r\\n1 . 0\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(b) ETTh2\\r\\n0. 332\\r\\n0. 377\\r\\n0. 027\\r\\n0. 404\\r\\n0. 255\\r\\n0. 31 9\\r\\n0. 1 59\\r\\n0. 478\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 0\\r\\n0. 1\\r\\n0. 2\\r\\n0. 3\\r\\n0. 4\\r\\n0. 5\\r\\n0. 6\\r\\n0. 7\\r\\n0. 8\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(c) ETTm1\\r\\n0. 229\\r\\n0. 1 52\\r\\n0. 01 7\\r\\n0. 1 70\\r\\n0. 1 83\\r\\n0. 1 26\\r\\n0. 067\\r\\n0. 1 94\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 00\\r\\n0. 02\\r\\n0. 04\\r\\n0. 06\\r\\n0. 08\\r\\n0. 1 0\\r\\n0. 1 2\\r\\n0. 1 4\\r\\n0. 1 6\\r\\n0. 1 8\\r\\n0. 20\\r\\n0. 22\\r\\n0. 24\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(d) ETTm2\\r\\n0. 1 1 6\\r\\n0. 073\\r\\n0. 007\\r\\n0. 080\\r\\n0. 082\\r\\n0. 01 2\\r\\n0. 229\\r\\n0. 241\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 00\\r\\n0. 05\\r\\n0. 1 0\\r\\n0. 1 5\\r\\n0. 20\\r\\n0. 25\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(e) Exchange-Rate\\r\\n0. 594\\r\\n2. 339\\r\\n0. 579\\r\\n2. 91 7\\r\\n0. 1 48 0. 000\\r\\n5. 095 5. 095\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0\\r\\n2\\r\\n4\\r\\n6\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(f) ILI\\r\\n0. 491 0\\r\\n0. 1 786\\r\\n0. 01 60\\r\\n0. 1 946\\r\\n0. 3381\\r\\n0. 091 3\\r\\n0. 0830\\r\\n0. 1 742\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 0\\r\\n0. 1\\r\\n0. 2\\r\\n0. 3\\r\\n0. 4\\r\\n0. 5\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(g) Weather\\r\\n0. 21 62 0. 21 55\\r\\n0. 0004\\r\\n0. 21 59\\r\\n0. 01 07\\r\\n0. 0000\\r\\n2. 3938 2. 3938\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 00\\r\\n0. 05\\r\\n0. 1 0\\r\\n0. 1 5\\r\\n0. 20\\r\\n0. 25\\r\\n2. 30\\r\\n2. 35\\r\\n2. 40\\r\\n2. 45\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(h) Electricity\\r\\n0. 472\\r\\n0. 746\\r\\n0. 01 9\\r\\n0. 765\\r\\n0. 000 0. 000\\r\\n1 . 368 1 . 368\\r\\nTrai n E rror Test E rror W Di ff Gen E rror\\r\\n0. 0\\r\\n0. 2\\r\\n0. 4\\r\\n0. 6\\r\\n0. 8\\r\\n1 . 0\\r\\n1 . 2\\r\\n1 . 4\\r\\nV\\r\\nal u\\r\\ne\\r\\nCI\\r\\nCD\\r\\n(i) Traffic\\r\\nFigure 5: The train error, test error, W diff and gen error when using CI and CD strategy on the 9 datasets. Train/test error\\r\\nmeasures model capacity on train/test data. W diff measures the difference between the optimal model on train and test data.\\r\\nIt reveals the robustness of a model. Gen error measures the risk of an algorithm. Although CD can achieve lower optimal\\r\\nerror, it is much less robust to the distribution drift than CI. Consequently, in most cases, CI outperforms CD.\\r\\n𝑊𝑊𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡)\\r\\n𝑊𝑊𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡) 𝑊𝑊𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡)\\r\\n𝑊𝑊𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡)\\r\\nBayes Optimal Model\\r\\n𝑊𝑊𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡)\\r\\nOptimal CI Model\\r\\n𝑊𝑊 on Train/Test data\\r\\n𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡)\\r\\n𝑊𝑊𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡)\\r\\n𝑊𝑊𝑐𝑐𝑐𝑐\\r\\n(𝑡𝑡𝑡𝑡)\\r\\nOptimal CD Model\\r\\non Train/Test data\\r\\nEquipotential Line\\r\\nof Test Error\\r\\nFigure 6: Illustration of the conclusions drawn from fig. 5.\\r\\nCD have better optimal model on test data but larger dis\\x02tance between the optimal train and test model. CI is the con\\x02trary. In most cases, model difference matters more than the\\r\\noptimal model. Thus CI often achieves better performance.\\r\\nstrategy. These findings provide valuable guidance for designing or\\r\\nimproving existing multivariate forecasting models. Specifically, we\\r\\nrecommend increasing the robustness of CD models and increasing\\r\\nthe capacity of CI models.\\r\\nIn this section, we propose a simple modified CD objective that\\r\\ncan help models surpass the CI strategy. Additionally, we discuss\\r\\nseveral factors that may influence the performance of CD or CI\\r\\nmodels. By considering these factors, we can further optimize and\\r\\ntailor the models for specific use cases.\\r\\n5.1 Predict Residuals with Regularization\\r\\nOur analysis of fig. 7 in the previous section has led us to conclude\\r\\nthat the primary disadvantage of CD models is their tendency to\\r\\ngenerate \"sharp\" and non-robust predictions that often diverge\\r\\nfrom the actual trend. To address this issue, we propose a simple\\r\\nmethod to improve the performance of CD models called Predict\\r\\nResiduals with Regularization (PRReg), inspired by measures taken\\r\\nin N-BEATS [29] and NLinear [42]. The core idea of PRReg is to\\r\\nensure that the prediction remains close to the nearest known\\r\\nhistory and that the forecasted series remains smooth. To achieve\\r\\nthis objective, we reformulate the CD objective into the following\\r\\nform:\\r\\nmin\\r\\n𝑓\\r\\n1\\r\\n𝑁\\r\\n∑︁\\r\\n𝑁\\r\\n𝑖=1\\r\\nℓ(𝑓 (𝑿\\r\\n(𝑖) − 𝑁(𝑖)\\r\\n) + 𝑁\\r\\n(𝑖)\\r\\n, 𝒀\\r\\n(𝑖)\\r\\n) + 𝜆Ω(𝑓 ). (20)\\r\\nwhere 𝑁\\r\\n(𝑖) = 𝑿\\r\\n(𝑖)\\r\\n:,𝐿 is the last values of each channel in 𝑿\\r\\n(𝑖)\\r\\n. With\\r\\nthis objective, the goal of 𝑓 is changed from accurately predicting\\r\\nfuture values to the variety from the nearest history. The regulariza\\x02tion term Ω serves a dual purpose: to restrict the predictions within\\r\\na reasonable distance from 𝑁\\r\\n(𝑖)\\r\\nand to encourage smoothness in the\\r\\npredicted values. In our study, we adopted 𝐿2 regularization, which\\r\\nwas implemented as weight decay in PyTorch [30]. Our proposed\\r\\nobjective is applicable to various forecasting models, and its effec\\x02tiveness is illustrated in fig. 8 for Linear [42] and Transformer [36]\\r\\nmodels. Table 4 presents the results, where we compare the PRReg\\r\\nstrategy with CD and CI. We observe that PRReg outperforms both\\r\\nCD and CI in most cases when the regularization strength 𝜆 is\\r\\nchosen appropriately. A too-small 𝜆 fails to provide the required\\r\\nrobustness since PRReg is fundamentally a CD strategy, while an\\r\\nexcessively large regularization strength causes underfitting with\\r\\nsufficient capacity. Thus, choosing a suitable value of 𝜆 results in\\r\\nan optimal trade-off between capacity and robustness and leads to\\r\\nthe best possible results.\\r\\nThe PRReg objective offers several benefits. Firstly, it is model\\r\\nagnostic, which implies that it can be used with various multivari\\x02ate forecasting models. Secondly, it is a modified version of the\\r\\nCD strategy that incorporates the correlations between different\\r\\nchannels. Lastly, it outperforms the CI strategy. It is intuitive that\\r\\n10\\n0 20 40 60 80 1 00 1 20 1 40 1 60 1 80\\r\\n\\x05 \\x03 \\x01 \\x04\\r\\n\\x05 \\x03 \\x01 \\x02\\r\\n\\x05 \\x02 \\x01 \\x04\\r\\n0. 0\\r\\n0. 5\\r\\n1 . 0\\r\\n1 . 5\\r\\n2. 0\\r\\nV\\r\\nal u\\r\\ne\\r\\nTime\\r\\nH i story\\r\\nFu tu re\\r\\nCD P redi cti on\\r\\nCI P redi cti on\\r\\n(a)\\r\\n0 20 40 60 80 1 00 1 20 1 40 1 60 1 80 200\\r\\n\\x06 \\x04 \\x01 \\x02\\r\\n\\x06 \\x03 \\x01 \\x05\\r\\n\\x06 \\x03 \\x01 \\x02\\r\\n\\x06 \\x02 \\x01 \\x05\\r\\n0. 0\\r\\n0. 5\\r\\nV\\r\\nal u\\r\\ne\\r\\nTime\\r\\nH i story\\r\\nFu tu re\\r\\nCD P redi cti on\\r\\nCI P redi cti on\\r\\n(b)\\r\\n0 20 40 60 80 1 00 1 20 1 40 1 60 1 80 200\\r\\n\\x08 \\x07 \\x01 \\x02\\r\\n\\x08 \\x06 \\x01 \\x07\\r\\n\\x08 \\x06 \\x01 \\x02\\r\\n\\x08 \\x05 \\x01 \\x07\\r\\n\\x08 \\x05 \\x01 \\x02\\r\\n\\x08 \\x04 \\x01 \\x07\\r\\n\\x08 \\x04 \\x01 \\x02\\r\\n\\x08 \\x03 \\x01 \\x07\\r\\n\\x08 \\x03 \\x01 \\x02\\r\\nV\\r\\nal u\\r\\ne\\r\\nTime\\r\\nH i story\\r\\nFu tu re\\r\\nCD P redi cti on\\r\\nCI P redi cti on\\r\\n(c)\\r\\n0 20 40 60 80 1 00 1 20 1 40 1 60 1 80 200\\r\\n\\x06 \\x04 \\x01 \\x02\\r\\n\\x06 \\x03 \\x01 \\x05\\r\\n\\x06 \\x03 \\x01 \\x02\\r\\n\\x06 \\x02 \\x01 \\x05\\r\\n0. 0\\r\\n0. 5\\r\\n1 . 0\\r\\n1 . 5\\r\\nV\\r\\nal u\\r\\ne\\r\\nTime\\r\\nH i story\\r\\nFu tu re\\r\\nCD P redi cti on\\r\\nCI P redi cti on\\r\\n(d)\\r\\nFigure 7: We select four representative examples to demonstrate the differences between CD and CI strategies. The experiment\\r\\nis conducted on ETTh2 with Linear model. (a) When both of them capture the correct trend and seasonal component. CD tends\\r\\nto generate “sharp” predictions, while the CI produces smoother ones. (b) The CD could predict wrong trends, while CI is less\\r\\nlikely to do so. (c) When faced with abnormal series, CI are more robust with less oscillation. (d) The high capacity of CD\\r\\nmodels may be beneficial for capturing complex but predictable patterns. CI is unable to capture them.\\r\\n𝑓𝑓\\r\\nRegularization\\r\\nΩ(𝑓𝑓)\\r\\nNearest History\\r\\nSmooth\\r\\nResidual\\r\\nRobust \\r\\nPrediction\\r\\nFigure 8: The idea of Predict Residuals with Regularization\\r\\n(PRReg). The input series is subtracted by the last value.\\r\\nThen the predictor is regularized to predict smoothed resid\\x02ual. Robust prediction is made by adding back the nearest\\r\\nhistory.\\r\\nwe should not treat each channel independently since they rep\\x02resent the features of the same object. However, the CD strategy\\r\\noften exhibits inferior performance due to its lack of robustness.\\r\\nThe PRReg objective successfully resolves this issue, striking a bal\\x02ance between capacity and robustness, thereby achieving superior\\r\\nresults.\\r\\n5.2 Some Other Factors\\r\\nWe list some of the factors that may influence the performance\\r\\nof CD or CI models by altering their capacity and robustness. It\\r\\nis important to note that capacity and robustness are intertwined\\r\\nin the model selection process, and increasing one often requires\\r\\ndecreasing the other. Thus, these factors can impact CD and CI\\r\\nstrategies in various ways. We list some factors only for inspiration.\\r\\nLow rank layer. The low rank assumption is widely used in ro\\x02bust learning [23, 41]. Following the approach proposed in [17],\\r\\nwe replace the linear output projection of each attention layer\\r\\nwith a low rank linear layer. Specifically, if the weight of the orig\\x02inal linear layer is 𝑊 ∈ R\\r\\n𝑚×𝑛\\r\\n, we replace it with 𝑀1𝑀2, where\\r\\n𝑀1 ∈ R\\r\\n𝑚×𝑟\\r\\nand 𝑀2 ∈ R\\r\\n𝑟×𝑛\\r\\n. We varied the rank reduction rate\\r\\nin 2, 4, 8, 16, 32, 64, 128, 256, 512, which means that if the rate is 2,\\r\\n𝑟 = ⌊\\r\\nmin𝑚,𝑛\\r\\n2\\r\\n⌋. Figure 9 presents the results of our experiments. As\\r\\nthe rank reduction rate increases, the error initially drops and then\\r\\nrises. Low rank regularization reduces the capacity and increases\\r\\nthe robustness of a model. Thus, an appropriate choice of the rank\\r\\ncan help a CD model perform better.\\r\\nRobust loss. The Mean Absolute Error (MAE), also known as the L\\x021 loss, has been demonstrated to be resilient to noisy labels [14, 15].\\r\\n1 x 2x 4x 8x 1 6x 32x 64x 1 28x 256x 51 2x\\r\\n0. 2\\r\\n0. 4\\r\\n0. 6\\r\\n0. 8\\r\\n1 . 0\\r\\nM\\r\\nA\\r\\nE\\r\\nRan k Redu cti on\\r\\nE l ectri ci ty\\r\\nE TTh 1\\r\\nE TTh 2\\r\\nE TTm1\\r\\nE TTm2\\r\\nE xch an ge_Rate\\r\\nTraffi c\\r\\nWeath er\\r\\nFigure 9: The MAE error of Transformer (CD) model with\\r\\nlow rank linear layer on different datasets. We vary the\\r\\nrank reduction rate from 1x to 512x, gradually reducing the\\r\\nrank. The errors drop and rise with increasing reduction\\r\\nrate. Thus, suitable rank regularization helps.\\r\\nHence, by applying the L-2 loss to a model trained using the CD\\r\\nstrategy, its robustness can be improved. To avoid ambiguity, we\\r\\nwill refer to the loss used during training as L-2/L-1, and the evalu\\x02ation metric as Mean Squared Error (MSE)/MAE. The outcomes of\\r\\napplying L-1 and L-2 losses to the Transformer (CD) are presented\\r\\nin Table 5. We observe that L-1 loss enhances the robustness of the\\r\\nmodel, resulting in more accurate predictions.\\r\\nLength of look-back windows. The length of the look-back win\\x02dow determines the amount of memory that a forecasting model\\r\\ncan utilize. In the context of multiple series forecasting, increasing\\r\\nthe memory capacity can improve the performance of the global\\r\\nmodel [27]. Our study demonstrates that the window length also\\r\\naffects the performance of CD and CI strategies in different ways.\\r\\nWe varied the sequence length of the input look-back window from\\r\\n48 to 432, while keeping the horizon fixed at 48. We selected Linear\\r\\nand Transformer models as representative methods to illustrate\\r\\nthis phenomenon. The results of comparing the performance of\\r\\nthese two models with CD and CI strategies on some datasets are\\r\\npresented in fig. 10. A longer length of the look-back window pro\\x02vides more information about the historical data but also increases\\r\\nthe capacity of the model. For CD models, when the history is not\\r\\ntoo short, increasing the length often leads to worse performance.\\r\\nOn the other hand, CI always benefits from longer window lengths.\\r\\n11\\nTable 4: Comparison among forecasters trained with PRReg (varying 𝜆), CD and CI strategies. The base forecasters are Linear\\r\\nand Transformer. Performance is measured by MSE. The best results of each setting (row) are marked bold. PRReg is able to\\r\\nsurpass CD and CI if the 𝜆 is selected properly. Meaning that it produces a suitable balance between capacity and robustness.\\r\\nModel CD CI PRReg (ours)\\r\\n𝜆 = 10−6 𝜆 = 10−5 𝜆 = 10−4 𝜆 = 10−3 𝜆 = 10−2 𝜆 = 10−1 𝜆 = 1\\r\\nElectricity Linear - - - - - - - - -\\r\\nTransformer 0.250 0.185 0.218 0.219 0.227 0.269 0.378 0.742 1.527\\r\\nETTh1 Linear 0.402 0.345 0.346 0.345 0.344 0.342 0.355 0.426 0.737\\r\\nTransformer 0.861 0.655 0.624 0.624 0.623 0.625 0.539 0.744 1.164\\r\\nETTh2 Linear 0.711 0.226 0.335 0.329 0.296 0.248 0.239 0.259 0.298\\r\\nTransformer 1.031 0.274 0.319 0.319 0.323 0.373 0.424 0.273 0.332\\r\\nETTm1 Linear 0.404 0.354 0.315 0.315 0.314 0.311 0.318 0.378 0.668\\r\\nTransformer 0.458 0.379 0.374 0.374 0.375 0.349 0.370 0.536 1.148\\r\\nETTm2 Linear 0.161 0.147 0.142 0.142 0.140 0.136 0.141 0.163 0.195\\r\\nTransformer 0.281 0.148 0.171 0.164 0.160 0.144 0.149 0.182 0.211\\r\\nExchange_Rate Linear 0.119 0.051 0.050 0.050 0.049 0.046 0.043 0.042 0.042\\r\\nTransformer 0.511 0.101 0.098 0.098 0.092 0.074 0.056 0.044 0.044\\r\\nWeather Linear 0.142 0.169 0.133 0.133 0.133 0.132 0.131 0.141 0.169\\r\\nTransformer 0.251 0.168 0.189 0.191 0.195 0.180 0.189 0.297 0.193\\r\\nILI Linear 2.343 2.847 2.599 2.599 2.597 2.581 2.467 2.299 2.693\\r\\nTransformer 5.309 4.307 3.258 3.258 3.257 3.254 3.310 3.848 4.793\\r\\nTable 5: Performance of Transformer (CD) with L-1 and L-2\\r\\nloss. When using the L-1 loss which is more robust, Trans\\x02former (CD) forecast more accurately.\\r\\nMetric MAE MSE\\r\\nLoss L-2 L-1 L-2 L-1\\r\\nElectricity 0.352 0.347 0.250 0.253\\r\\nETTh1 0.734 0.604 0.861 0.669\\r\\nETTh2 0.829 0.536 1.031 0.463\\r\\nETTm1 0.458 0.421 0.458 0.403\\r\\nETTm2 0.404 0.328 0.281 0.210\\r\\nExchange_Rate 0.571 0.451 0.511 0.316\\r\\nTraffic 0.364 0.347 0.645 0.668\\r\\nWeather 0.343 0.274 0.251 0.229\\r\\nILI 1.508 1.373 5.309 4.457\\r\\n48 96 1 44 1 92 240 288 336 384 432\\r\\n0. 295\\r\\n0. 300\\r\\n0. 305\\r\\n0. 31 0\\r\\n0. 31 5\\r\\n0. 320\\r\\n0. 325\\r\\nLi n ear (CI )\\r\\nLi n ear (CD)\\r\\nSteps\\r\\nM\\r\\nA\\r\\nE\\r\\n0. 5\\r\\n0. 6\\r\\n0. 7\\r\\n0. 8\\r\\n0. 9\\r\\n1 . 0\\r\\n1 . 1\\r\\n1 . 2\\r\\nETTh 2\\r\\nM\\r\\nA\\r\\nE\\r\\n48 96 1 44 1 92 240 288 336 384 432\\r\\n0. 1 95\\r\\n0. 200\\r\\n0. 205\\r\\n0. 21 0\\r\\n0. 21 5\\r\\n0. 220\\r\\n0. 225\\r\\n0. 230 Li n ear (CI )\\r\\nLi n ear (CD)\\r\\nSteps\\r\\nM\\r\\nA\\r\\nE\\r\\n0. 1 90\\r\\n0. 1 95\\r\\n0. 200\\r\\n0. 205\\r\\n0. 21 0\\r\\n0. 21 5\\r\\n0. 220\\r\\nWeath er\\r\\nM\\r\\nA\\r\\nE\\r\\n48 96 1 44 1 92 240 288 336 384 432\\r\\n0. 2496\\r\\n0. 2592\\r\\n0. 2688\\r\\n0. 2784\\r\\n0. 2880\\r\\n0. 2976 Tran sform er (U F)\\r\\nTran sform er (MF)\\r\\nSteps\\r\\nM\\r\\nA\\r\\nE\\r\\n0. 350\\r\\n0. 355\\r\\n0. 360\\r\\n0. 365\\r\\n0. 370\\r\\n0. 375\\r\\n0. 380\\r\\nETTm1\\r\\nM\\r\\nA\\r\\nE\\r\\n48 96 1 44 1 92 240 288 336 384 432\\r\\n0. 34\\r\\n0. 36\\r\\n0. 38\\r\\n0. 40\\r\\n0. 42\\r\\n0. 44\\r\\n0. 46\\r\\n0. 48\\r\\n0. 50\\r\\n0. 52\\r\\n0. 54\\r\\nTran sform er (U F)\\r\\nTran sform er (MF)\\r\\nSteps\\r\\nM\\r\\nA\\r\\nE\\r\\n0. 42\\r\\n0. 44\\r\\n0. 46\\r\\n0. 48\\r\\n0. 50\\r\\n0. 52\\r\\n0. 54\\r\\n0. 56\\r\\nEl ectri city\\r\\nM\\r\\nA\\r\\nE\\r\\nFigure 10: MAE performance of Linear and Transformer\\r\\nwith CI and CD strategy on certain datasets. The X-axis rep\\x02resents the length of look-back window. The capacity of\\r\\nboth CI and CD model are increased when we input a longer\\r\\nwindow. We can see that a longer window may do harm to\\r\\nthe performance of CD models, while CI can benefit from it.\\r\\n6 DISCUSSION ABOUT LIMITATIONS\\r\\nIt is important to stress that the conclusions drawn in this paper are\\r\\nclosely tied to the characteristics of the datasets employed. While\\r\\nthe Channel Independent (CI) training approach generally outper\\x02forms the Channel Dependent (CD) strategy, there are exceptions.\\r\\nFor instance, as indicated in Table 2 for the ILI dataset, CD performs\\r\\nbetter on average. Nonetheless, the analysis of CI and CD provides\\r\\nvaluable insights into the peculiarities of real-world time series and\\r\\nhow different strategies can leverage them.\\r\\nAnalyses of this paper may also be limited to numerical channels.\\r\\nNevertheless, it is possible to handle numerical and non-numerical\\r\\nfeatures separately. We defer an analysis of strategies on more\\r\\ngeneral types of time-series data to future research.\\r\\n7 CONCLUSION\\r\\nRecent years have seen the emergence of several methods for long\\x02term Multivariate Time Series Forecasting (MTSF), with some adopt\\x02ing the channel independent (CI) strategy to achieve good perfor\\x02mance. By the analyses of this paper, we show the performance\\r\\nboost generated by these methods is often not due to their design,\\r\\nbut rather to the training strategy. Despite lower model capacity,\\r\\nthe CI strategy exhibits higher robustness, making it better suited\\r\\nfor non-stationary time series in practice. We hope that this article\\r\\nwill alert the researchers the characteristics of MTSF benchmarks\\r\\nand inspire researchers to better deal with multivariate time series\\r\\nforecasting problems.\\r\\nACKNOWLEDGMENTS\\r\\nThis research was supported by NSFC (61773198, 62006112,61921006),\\r\\nCollaborative Innovation Center of Novel Software Technology and\\r\\nIndustrialization, NSF of Jiangsu Province (BK20200313).\\r\\n12\\nREFERENCES\\r\\n[1] OD Anderson. 1976. Time-Series. 2nd edn.\\r\\n[2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An Empirical Evaluation of\\r\\nGeneric Convolutional and Recurrent Networks for Sequence Modeling. CoRR\\r\\nabs/1803.01271 (2018).\\r\\n[3] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. 2015.\\r\\nTime series analysis: forecasting and control. John Wiley & Sons.\\r\\n[4] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.\\r\\nIn SIGKDD. 785–794.\\r\\n[5] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio.\\r\\n2014. On the Properties of Neural Machine Translation: Encoder-Decoder Ap\\x02proaches. In SSST@EMNLP, Dekai Wu, Marine Carpuat, Xavier Carreras, and\\r\\nEva Maria Vecchi (Eds.). Association for Computational Linguistics, 103–111.\\r\\n[6] Razvan-Gabriel Cirstea, Darius-Valer Micu, Gabriel-Marcel Muresan, Chenjuan\\r\\nGuo, and Bin Yang. 2018. Correlated time series forecasting using multi-task\\r\\ndeep neural networks. In ICKM. 1527–1530.\\r\\n[7] Yue Cui, Kai Zheng, Dingshan Cui, Jiandong Xie, Liwei Deng, Feiteng Huang, and\\r\\nXiaofang Zhou. 2021. METRO: A Generic Graph Neural Network Framework for\\r\\nMultivariate Time Series Forecasting. Proc. VLDB Endow. 15, 2 (2021), 224–236.\\r\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\r\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\r\\nProceedings of the 2019 Conference of the North American Chapter of the Associa\\x02tion for Computational Linguistics: Human Language Technologies (NAACL-HLT),\\r\\nMinneapolis, MN, USA, June 2-7, 2019. 4171–4186.\\r\\n[9] Linhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-Transformer: A No\\x02Recurrence Sequence-to-Sequence Model for Speech Recognition. In ICASSP.\\r\\nIEEE, 5884–5888.\\r\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi\\x02aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\\r\\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\\r\\nWorth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR.\\r\\n[11] Shereen Elsayed, Daniela Thyssens, Ahmed Rashed, Lars Schmidt-Thieme, and\\r\\nHadi Samer Jomaa. 2021. Do We Really Need Deep Learning Models for Time\\r\\nSeries Forecasting? CoRR abs/2101.02118 (2021).\\r\\n[12] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. 2019. Unsuper\\x02vised Scalable Representation Learning for Multivariate Time Series. In NeurIPS,\\r\\nHanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc,\\r\\nEmily B. Fox, and Roman Garnett (Eds.). 4652–4663.\\r\\n[13] Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram,\\r\\nDavid Salinas, Valentin Flunkert, and Tim Januschowski. 2019. Probabilistic\\r\\nforecasting with spline quantile function RNNs. In AISTATS. PMLR, 1901–1910.\\r\\n[14] Aritra Ghosh, Himanshu Kumar, and P. S. Sastry. 2017. Robust Loss Functions\\r\\nunder Label Noise for Deep Neural Networks. In AAAI, Satinder Singh and Shaul\\r\\nMarkovitch (Eds.). 1919–1925.\\r\\n[15] Aritra Ghosh, Naresh Manwani, and P. S. Sastry. 2015. Making risk minimization\\r\\ntolerant to label noise. Neurocomputing 160 (2015), 93–107.\\r\\n[16] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural\\r\\ncomputation 9, 8 (1997), 1735–1780.\\r\\n[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\r\\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large\\r\\nLanguage Models. In ICLR.\\r\\n[18] Rob J Hyndman and George Athanasopoulos. 2018. Forecasting: principles and\\r\\npractice. OTexts.\\r\\n[19] Gwilym M Jenkins and MB Priestley. 1957. The spectral analysis of time-series.\\r\\nJournal of the Royal Statistical Society: Series B (Methodological) 19, 1 (1957), 1–12.\\r\\n[20] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling\\r\\nlong-and short-term temporal patterns with deep neural networks. In SIGIR.\\r\\n95–104.\\r\\n[21] Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. 2017. Time-series\\r\\nextreme event forecasting with neural networks at uber. In ICML, Vol. 34. 1–5.\\r\\n[22] Bryan Lim and Stefan Zohren. 2020. Time Series Forecasting With Deep Learning:\\r\\nA Survey. CoRR abs/2004.13408 (2020).\\r\\n[23] Guangcan Liu, Zhouchen Lin, and Yong Yu. 2010. Robust Subspace Segmenta\\x02tion by Low-Rank Representation. In ICML, Johannes Fürnkranz and Thorsten\\r\\nJoachims (Eds.). Omnipress, 663–670.\\r\\n[24] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and\\r\\nSchahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for\\r\\nLong-Range Time Series Modeling and Forecasting. In ICLR.\\r\\n[25] Henrik Madsen. 2007. Time series analysis. Chapman and Hall/CRC.\\r\\n[26] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations\\r\\nof machine learning. MIT press.\\r\\n[27] Pablo Montero-Manso and Rob J Hyndman. 2021. Principles and algorithms for\\r\\nforecasting groups of time series: Locality and globality. International Journal of\\r\\nForecasting 37, 4 (2021), 1632–1653.\\r\\n[28] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.\\r\\nA Time Series is Worth 64 Words: Long-term Forecasting with Transformers.\\r\\nCoRR abs/2211.14730 (2022).\\r\\n[29] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N\\x02BEATS: Neural basis expansion analysis for interpretable time series forecasting.\\r\\nIn ICLR.\\r\\n[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\r\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des\\x02maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\\r\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\r\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\r\\nLibrary. In NeurIPS. 8024–8035.\\r\\n[31] Stephan Rabanser, Tim Januschowski, Valentin Flunkert, David Salinas, and Jan\\r\\nGasthaus. 2020. The Effectiveness of Discretization in Forecasting: An Empirical\\r\\nStudy on Neural Time Series Models. CoRR abs/2005.10111 (2020).\\r\\n[32] Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella,\\r\\nYuyang Wang, and Tim Januschowski. 2018. Deep State Space Models for Time\\r\\nSeries Forecasting. In NeurIPS, Samy Bengio, Hanna M. Wallach, Hugo Larochelle,\\r\\nKristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 7796–7805.\\r\\n[33] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\r\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter\\x02national Journal of Forecasting 36, 3 (2020), 1181–1191.\\r\\n[34] Slawek Smyl. 2020. A hybrid method of exponential smoothing and recurrent\\r\\nneural networks for time series forecasting. International Journal of Forecasting\\r\\n36, 1 (2020), 75–85.\\r\\n[35] George Udny Yule. 1927. On a method of investigating periodicities in disturbed\\r\\nseries, with special reference to Wolfer’s sunspot numbers. Philosophical Trans\\x02actions of the Royal Society of London Series A 226 (1927), 267–298.\\r\\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\r\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\\r\\nyou need. NeurIPS 30 (2017).\\r\\n[37] Gilbert Thomas Walker. 1931. On periodicity in series of related terms. Proceedings\\r\\nof the Royal Society of London. Series A, Containing Papers of a Mathematical and\\r\\nPhysical Character 131, 818 (1931), 518–532.\\r\\n[38] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De\\x02composition transformers with auto-correlation for long-term series forecasting.\\r\\nIn NeurIPS. 101–112.\\r\\n[39] Xinle Wu, Dalin Zhang, Chenjuan Guo, Chaoyang He, Bin Yang, and Christian S.\\r\\nJensen. 2021. AutoCTS: Automated Correlated Time Series Forecasting. Proc.\\r\\nVLDB Endow. 15, 4 (2021), 971–983.\\r\\n[40] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi\\r\\nZhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with\\r\\nGraph Neural Networks. In KDD ’20: The 26th ACM SIGKDD Conference on\\r\\nKnowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020,\\r\\nRajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM, 753–763.\\r\\n[41] Xiaoming Yuan and Junfeng Yang. 2009. Sparse and low-rank matrix decomposi\\x02tion via alternating direction methods. preprint 12, 2 (2009).\\r\\n[42] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2022. Are Transformers\\r\\nEffective for Time Series Forecasting? CoRR abs/2205.13504 (2022).\\r\\n[43] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\r\\nand Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long\\r\\nSequence Time-Series Forecasting. In AAAI, Vol. 35. 11106–11115.\\r\\n[44] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.\\r\\nFEDformer: Frequency enhanced decomposed transformer for long-term series\\r\\nforecasting. In ICML.\\r\\n13'},\n",
       " {'name': '2401.16777v2.pdf',\n",
       "  'content': 'IN-Flow: Instance Normalization Flow for Non-stationary\\r\\nTime Series Forecasting\\r\\nWei Fan\\r\\nweifan.oxford@gmail.com\\r\\nUniversity of Oxford\\r\\nOxford, UK\\r\\nShun Zheng\\r\\nshun.zheng@microsoft.com\\r\\nMicrosoft Research Asia\\r\\nBeijing, China\\r\\nPengyang Wang∗\\r\\npywang@um.edu.mo\\r\\nUniversity of Macau\\r\\nMacau SAR, China\\r\\nRui Xie\\r\\nrui.xie@ucf.edu\\r\\nUniversity of Central Florida\\r\\nOrlando, US\\r\\nKun Yi\\r\\nkunyi.cn@gmail.com\\r\\nState Information Center\\r\\nBeijing, China\\r\\nQi Zhang\\r\\nzhangqi_cs@tongji.edu.cn\\r\\nTongji University\\r\\nShanghai, China\\r\\nJiang Bian\\r\\njiang.bian@microsoft.com\\r\\nMicrosoft Research Asia\\r\\nBeijing, China\\r\\nYanjie Fu\\r\\nyanjie.fu@asu.edu\\r\\nArizona State University\\r\\nTempe, US\\r\\nAbstract\\r\\nDue to the non-stationarity of time series, the distribution shift\\r\\nproblem largely hinders the performance of time series forecasting.\\r\\nExisting solutions either rely on using certain statistics to specify\\r\\nthe shift, or developing specific mechanisms for certain network\\r\\narchitectures. However, the former would fail for the unknown shift\\r\\nbeyond simple statistics, while the latter has limited compatibility\\r\\non different forecasting models. To overcome these problems, we\\r\\nfirst propose a decoupled formulation for time series forecasting,\\r\\nwith no reliance on fixed statistics and no restriction on forecasting\\r\\narchitectures. This formulation regards the removing-shift proce\\x02dure as a special transformation between a raw distribution and\\r\\na desired target distribution and separates it from the forecasting.\\r\\nSuch a formulation is further formalized into a bi-level optimization\\r\\nproblem, to enable the joint learning of the transformation (outer\\r\\nloop) and forecasting (inner loop). Moreover, the special require\\x02ments of expressiveness and bi-direction for the transformation\\r\\nmotivate us to propose instance normalization flow (IN-Flow), a\\r\\nnovel invertible network for time series transformation. Different\\r\\nfrom the classic “normalizing flow” models, IN-Flow does not aim\\r\\nfor normalizing input to the prior distribution (e.g., Gaussian distri\\x02bution) for generation, but creatively transforms time series distri\\x02bution by stacking normalization layers and flow-based invertible\\r\\nnetworks, which is thus named “normalization” flow. Finally, we\\r\\nhave conducted extensive experiments on both synthetic data and\\r\\nreal-world data, which demonstrate the superiority of our method.\\r\\n∗Corresponding Author.\\r\\nPermission to make digital or hard copies of all or part of this work for personal or\\r\\nclassroom use is granted without fee provided that copies are not made or distributed\\r\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\r\\non the first page. Copyrights for components of this work owned by others than the\\r\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\r\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\r\\nand/or a fee. Request permissions from permissions@acm.org.\\r\\nKDD ’25, Toronto, ON, Canada.\\r\\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\r\\nACM ISBN 979-8-4007-1245-6/25/08\\r\\nhttps://doi.org/10.1145/3690624.3709260\\r\\nCCS Concepts\\r\\n• Computing methodologies → Neural networks.\\r\\nKeywords\\r\\nTime Series Forecasting; Distribution Shift; Normalizing Flows\\r\\nACM Reference Format:\\r\\nWei Fan, Shun Zheng, Pengyang Wang, Rui Xie, Kun Yi, Qi Zhang, Jiang\\r\\nBian, and Yanjie Fu. 2025. IN-Flow: Instance Normalization Flow for Non\\x02stationary Time Series Forecasting. In Proceedings of the 31st ACM SIGKDD\\r\\nConference on Knowledge Discovery and Data Mining V.1 (KDD ’25), August\\r\\n3–7, 2025, Toronto, ON, Canada. ACM, New York, NY, USA, 12 pages. https:\\r\\n//doi.org/10.1145/3690624.3709260\\r\\n1 Introduction\\r\\nWith wide applications in many real-world scenarios, such as elec\\x02tricity consumption planning [2, 14], traffic flow analysis [35], and\\r\\nweather condition estimation [1, 19], time series forecasting has re\\x02ceived broad research attention for decades, with rapidly-evolving\\r\\nlearning paradigm from traditional statistical approaches [20, 21, 51–\\r\\n53] to modern deep learning based methods [8, 9, 39, 45, 46, 55, 57,\\r\\n58, 62], resulting in progressively yet significantly improved time\\r\\nseries forecasting performance.\\r\\nIt is noteworthy that most of these methods, especially deep\\r\\nlearning-based forecasting models, inherently follow a stationarity\\r\\nassumption: the temporal dependencies between historical obser\\x02vations and future predictions are stationary such that they can be\\r\\neffectively captured and generalized to future time points. How\\x02ever, this stationarity assumption overlooks the non-stationarity\\r\\nof real-world time series data, which can be characterized by con\\x02tinuously shifted joint distribution over time [24]. As well-known\\r\\nas the distribution shift problem, it can substantially hinder the\\r\\nperformance of modern deep forecasting models [15, 24, 33] due\\r\\nto the resulting discrepancies of underlying distributions between\\r\\nthe training and test data. Since time series data are usually col\\x02lected at a high frequency over a long duration, such non-stationary\\r\\nsequences with millions of timesteps might even lead to a worse\\r\\nsituation for forecasting [16, 54].\\r\\narXiv:2401.16777v2 [cs.LG] 6 Feb 2025\\nKDD ’25, August 3–7, 2025, Toronto, ON, Canada. Wei Fan et al.\\r\\nNevertheless, reviewing all the achievements in non-stationary\\r\\ntime series forecasting, existing studies always explicitly consid\\x02ering the distribution shift issue in time series forecasting either\\r\\nrelied on certain statistics, such as rescaling time series with global\\r\\nminimum and maximum [38] and specifying non-stationary in\\x02formation as the mean, standard deviation, and learnable/sliced\\r\\nstatistics [15, 24, 34], or focused on developing specific mecha\\x02nisms for certain network architectures to relieve the shift [12, 33].\\r\\nThe former methods conduct normalizations towards data as pre\\x02processing or in-processing steps, which would probably fail for\\r\\nunknown distribution shifts beyond a simple change of statistics.\\r\\nIn contrast, the latter architecture-specific methods have limited\\r\\ncompatibility and could be unadaptable for advanced forecasting\\r\\narchitectures in the future.\\r\\nTo provide a common understanding of distribution shift and\\r\\nnon-strationary forecasting and effectively address its interfering\\r\\neffects on forecasting models, in this paper, we develop a systematic\\r\\napproach following two crucial principles: (1) making no assump\\x02tion that distribution shifts can be quantified by certain statistics;\\r\\n(2) making the method agnostic to the architectures of forecasting\\r\\nmodels. To achieve these ends, we develop a decoupled formulation\\r\\nfor time series forecasting: regarding the procedure of removing dis\\x02tribution shift as a special distribution transformation, we separate\\r\\nthe functionalities of removing shift and performing forecasting\\r\\ninto a transformation module and a forecasting module respectively,\\r\\nwhere the first module is responsible for the transformation be\\x02tween the raw data distribution and the desired target distribution\\r\\nwithout shifts, and the second module holds the potential to obtain\\r\\nmore accurate forecasting on \"cleaned\" data samples. Then, we\\r\\nnaturally formalize such a decoupled formulation into a bi-level\\r\\noptimization problem [7, 18]: in order to enable the joint learning\\r\\nof the two modules, we iteratively improve the forecasting ability\\r\\nby minimizing an inner objective (training errors) and optimize the\\r\\ntransformation ability by minimizing an outer object (validation\\r\\nerrors) that corresponds to the generalization on the shifted data.\\r\\nThis optimization procedure stimulates the transformation module\\r\\nto reduce shifts in a data-driven manner, leading to more stable and\\r\\nstationary time series forecasting.\\r\\nThough the formulation easily accommodates any forecasting\\r\\narchitectures, another critical challenge lies in the design of the\\r\\ntransformation module. Some special requirements motivate us to\\r\\nintroduce certain inductive biases. First, this module should trans\\x02form distributions effectively (expressiveness) to relieve the shift.\\r\\nSecond, the module is required to transform to and fro (bi-direction)\\r\\nbetween the raw distribution and the target distribution for the\\r\\ninput/output of forecasting models. Such two considerations in\\x02spire us to leverage the key idea of normalizing flows [10, 11, 26],\\r\\nwhich can reversibly transform data distributions as one kind of\\r\\ninvertible neural networks [3]. To this end, we propose instance\\r\\nnormalization flow (IN-Flow), a novel invertible network for time\\r\\nseries distribution transformation. In the domain of time series\\r\\nforecasting, the format of multi-variate sequences motivates us to\\r\\nintegrate instance normalization [22, 24, 49] together with coupling\\r\\nlayers [10, 11, 26]. Different from traditional normalizing flows tar\\x02geting for the transformation to a prior simple distribution, IN-Flow\\r\\ntargets for the transformation to a desired distribution with shifted\\r\\ninformation largely removed but forecasting-related information\\r\\npreserved, thus facilitating the final forecasting. In summary, our\\r\\ncontributions can be summarized as follows:\\r\\n• We present a decoupled formulation for time series forecasting\\r\\nthat separates the functionalities of removing shifts and perform\\x02ing forecasting into a distribution transformation module and a\\r\\nforecasting module, which doesn’t rely on fixed statistics, and\\r\\nhas no restriction on forecasting architectures.\\r\\n• We formalize such a formulation into a bi-level optimization prob\\x02lem to enable the joint learning of forecasting and the transfor\\x02mation in a data-driven manner.\\r\\n• We propose a novel invertible neural network, IN-Flow, to ful\\x02fill the transformation for a desired distribution with shifted\\r\\ninformation removed but forecasting-related preserved.\\r\\n• We conduct extensive experiments on both synthetic data and on\\r\\nreal-world data, which demonstrate the consistent superiority of\\r\\nIN-Flow with regard to existing methods in non-stationary time\\r\\nseries forecasting.\\r\\n2 Related Work\\r\\n2.1 Deep Learning for Time Series Forecasting\\r\\nTime series forecasting has been a longstanding research topic. At\\r\\nan early stage, researchers have proposed statistical modeling ap\\x02proaches, including exponential smoothing [20] and auto-regressive\\r\\nmoving averages (ARMA) [52]. With the great successes of deep\\r\\nlearning, many deep learning models have been developed for time\\r\\nseries forecasting. In recent years, deep learning-based methods\\r\\nhave gained prominence in time series forecasting due to their\\r\\nability to capture nonlinear and complex correlations [29]. These\\r\\nmethods have employed various network architectures to learn\\r\\ntemporal dependencies, such as recurrent neural network [28, 46],\\r\\ntemporal convolution networks [4, 31], etc. One branch of methods\\r\\nhas applied pure fully-connected neural networks for forecasting.\\r\\nFor example, N-BEATS and DEPTS [17, 39] build residual layers\\r\\non stacked connected layers for forecasting. N-HiTS [5] integrates\\r\\nmulti-rate input sampling and hierarchical interpolation with MLPs\\r\\nto enhance univariate forecasting. DLinear [61] introduces a simple\\r\\napproach using a single-layer linear model to capture temporal\\r\\nrelationships between input and output time series data. FreTS [59]\\r\\nutilizes frequency-domain MLP layers for time series forecasting;\\r\\nFilternet [56] adopts the frequency filters with the linear layers for\\r\\nthe forecasting. Another branch of methods are built upon Trans\\x02former [50] for the time series forecasting task. Many research have\\r\\nimproved vanilla transformer in attention computation, memory\\r\\nconsumption, etc, to enhance forecasting [55, 62, 63]. To capture\\r\\nintricate dependencies and long-range interactions. PatchTST [37]\\r\\nsegments time series into patches as input tokens to the Trans\\x02former and maintaining channel independence. iTransformer [32]\\r\\ninverts the Transformer’s structure by treating independent series\\r\\nas variate tokens to capture multivariate correlations.\\r\\n3 Preliminary\\r\\n3.1 Non-stationary Time Series Forecasting\\r\\nTime series forecasting suffers from the non-stationarity and the\\r\\ndistribution shift issue considering distributions of real-world se\\x02ries change over time [2]. To overcome this problem, some specific\\nIN-Flow: Instance Normalization Flow for Non-stationary Time Series Forecasting KDD ’25, August 3–7, 2025, Toronto, ON, Canada.\\r\\nmechanisms have been designed for certain network architectures\\r\\nto relieve the distribution shift: for example, [12] proposed Adap\\x02tive RNNs to handle the shift problem on top of recurrent neu\\x02ral networks (RNNs); Nonstationary Transformers [33] have pro\\x02posed series stationarization and de-stationary attention to enhance\\r\\ntransformer-based forecasting models. These methods, however,\\r\\nare network-specific architectures that have limited compatibility\\r\\nwith future advanced forecasting methods. Another kind of method\\r\\nfor the non-stationarity problem is to utilize the normalization\\r\\ntechniques. Usually, normalization is model-agnostic and can be\\r\\ncoupled with different backbones. For example, the pioneer work,\\r\\nAdaptive Norm [38] rescale time series through global minimum\\r\\nand maximum. DAIN [42] use adaptive z-score normalization by\\r\\nglobal average and std. for the task of time series classification.\\r\\nRecently, RevIN [24] proposes to use instance normalization to\\r\\nreduce time series distribution shift for deep learning time series\\r\\nforecasting methods. Dish-TS [15] considers both the inter-space\\r\\nshift and intra-space shift and proposes coefficient networks to\\r\\ncapture learnable statistics for normalization. SAN [34] considers\\r\\nthe sliced statistics of time series and proposes the corresponding\\r\\nadaptive sliced normalization. However, these normalization tech\\x02niques rely on statistics calculation, which fall short in overcoming\\r\\nunknown shifts beyond the statistics.\\r\\n3.2 Time Series Forecasting\\r\\nLet 𝒔 = [𝒔1;𝒔2; · · · ;𝒔𝑇 ] ∈ R\\r\\n𝑇 ×𝐷 be regularly sampled multi-variate\\r\\ntime series with 𝑇 timestamps and 𝐷 variates, where 𝒔𝑡 ∈ R\\r\\n𝐷 de\\x02notes the multi-variate values at timestamp 𝑡. Besides, we use 𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n∈\\r\\nR\\r\\n𝐿×𝐷 to denote a length-𝐿 segment of 𝒔 ending at timestamp 𝑡 (ex\\x02clusive), namely 𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n= 𝒔𝑡−𝐿:𝑡 = [𝒔𝑡−𝐿;𝒔𝑡−𝐿+1; · · · ;𝒔𝑡−1]. Similarly,\\r\\nwe represent a length-𝐻 segment of 𝒔 starting from timestamp 𝑡\\r\\n(inclusive) as 𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n, so we have 𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n= 𝒔𝑡:𝑡+𝐻 = [𝒔𝑡;𝒔𝑡+1; · · · ;𝒔𝑡+𝐻 −1].\\r\\nThe classic time series forecasting formulation is to project histori\\x02cal observations 𝒙\\r\\n𝐿\\r\\n𝑡\\r\\ninto their subsequent future values 𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n. Specif\\x02ically, a typical forecasting model 𝑓𝜃\\r\\n: R\\r\\n𝐿×𝐷 → R𝐻 ×𝐷 produces\\r\\nforecasts by 𝒚ˆ\\r\\n𝐻\\r\\n𝑡\\r\\n= 𝑓𝜃(𝒙\\r\\n𝐻\\r\\n𝑡\\r\\n) where 𝒚ˆ\\r\\n𝐻\\r\\n𝑡\\r\\nstands for the forecasting\\r\\nresult, 𝜃 encapsulates the model parameters, and 𝐻 and 𝐿 are usu\\x02ally referred to as the lengths of horizon (lookahead) windows and\\r\\nlookback windows, respectively.\\r\\n3.3 Bi-level Optimization\\r\\nBi-level optimization problems are one kind of optimization prob\\x02lems in which a set of variables in an objective function are con\\x02strained by the optimal solution of another optimization problem\\r\\n[7]. Specifically, given two functions 𝐽𝑖𝑛 and 𝐽𝑜𝑢𝑡 as the inner ob\\x02jectives and outer objectives, we consider two set of variables\\r\\n𝜃 ∈ R\\r\\n𝑚, 𝜙 ∈ R𝑛\\r\\nas the inner variables and outer variables, re\\x02spectively; the bi-level problem can be given by min\\r\\n𝜙,𝜃𝜙\\r\\n𝐽𝑜𝑢𝑡 (𝜃𝜙, 𝜙)\\r\\nsuch that 𝜃𝜙 ∈ arg min\\r\\n𝜃\\r\\n𝐽𝑖𝑛 (𝜃, 𝜙). Bi-level problems have involved in\\r\\nmany applications, including hyperparmeter optimization, multi\\x02task, and meta-learning [7, 18, 44, 48].\\r\\n3.4 Normalzing Flows\\r\\nNormalzing flows [40], as one common kind of invertible neu\\x02ral networks, can conduct transformation between distributions.\\r\\nGiven a high-dimensional random variable x ∈ X, flow-based\\r\\nmodels [10, 11, 26, 41] transform densities 𝑝X into certain target\\r\\ndistribution 𝑝Z (e.g., an isotropic Gaussian) on a latent variable\\r\\nz ∈ Z. The transformation is a bijection 𝑔 : X → Z (with inverse\\r\\n𝑔\\r\\n−1\\r\\n). The probability density can be written by the change of vari\\x02able formula as: 𝑝X (x) = 𝑝Z (z)\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\ndet \\x10\\r\\n𝜕𝑔(x)\\r\\n𝜕x\\r\\n\\x11\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\nwhere 𝜕𝑔(x)/𝜕x is\\r\\nthe Jacobian of 𝑔 at x. The transformation 𝑔 is composed of a se\\x02quence of differentiable invertible (or bijective) mapping functions\\r\\n𝑔 = 𝑔1 ◦ 𝑔2 ◦ · · · ◦ 𝑔𝑃 that map x\\r\\n𝑔1 ←→ h1𝑔2 ←→ h2 · · ·𝑔𝑃 ←→ z re\\x02versibly. Usually, flow-based models are trained via maximum likeli\\x02hood on training data. Thus, the probability density function of the\\r\\nflows given a data point is log 𝑝X (x) = log 𝑝Z (z) +log\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\ndet \\x10\\r\\n𝜕z\\r\\n𝜕x\\r\\n\\x11\\x0c\\r\\n\\x0c\\r\\n\\x0c =\\r\\nlog 𝑝Z (z) + Í𝑃\\r\\n𝑖=1\\r\\nlog |det (𝜕h𝑖/𝜕h𝑖−1)|, where h0 ≜ x and h𝑃 ≜ z;\\r\\nlog |det (𝜕h𝑖/𝜕h𝑖−1)| is called log-determinant of (𝜕h𝑖/𝜕h𝑖−1). The\\r\\nkey idea of designing transformation in normalizing flows is to\\r\\nmake the computation of log-determinant simple and easy [27, 40].\\r\\n4 Methodology\\r\\nIn this section, we elaborate on our principled approach that makes\\r\\nno assumption to specify distribution shifts and accommodates\\r\\nall kinds of forecasting models. First, we introduce a decoupled\\r\\nformulation in Section 4.1; then, we formalize a bi-level optimization\\r\\nproblem for the decoupled formulation in Section 4.2, and present\\r\\nour specifically designed IN-Flow model in Section 4.3. Besides,\\r\\nFigure 1 demonstrates an overview of our framework.\\r\\n4.1 The Novel Decoupled Formulation\\r\\nIt is noteworthy that the aforementioned formulation (𝒚ˆ\\r\\n𝐻\\r\\n𝑡\\r\\n= 𝑓𝜃(𝒙\\r\\n𝐻\\r\\n𝑡\\r\\n)),\\r\\nembracing a large body of forecasting models [36, 39, 55, 60, 62],\\r\\nessentially follows a stationary assumption: the conditional data\\r\\nprobability 𝑃 (𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n) does not change over time. However, in prac\\x02tice, some recent studies [12, 24, 33] have well recognized the wide\\r\\nexistence of non-stationary properties in real-world time series.\\r\\nBesides, we further note that those distribution shift patterns can\\r\\nbe far more sophisticated than simple statistical measures, such as\\r\\nmean and standard deviation used in [24]. Thus arbitrarily speci\\x02fying non-stationary properties has the risk of removing certain\\r\\ninformation crucial to forecasting and is limited in tackling more\\r\\ndiverse distribution shifts.\\r\\nTo provide a generic approach to address the problem of distri\\x02bution shift in time-series forecasting, we regard the procedure of\\r\\nremoving non-stationary information as a special distribution trans\\x02formation (transformation module) and separate its functionality\\r\\nfrom the forecasting module. Accordingly, we develop a decoupled\\r\\nformulation as:\\r\\n𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n) = 𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\n) · 𝑝(𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\n) · 𝑝(𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n) (1)\\r\\nwhere 𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\nand 𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\ndenote the transformed variables of 𝒙\\r\\n𝐿\\r\\n𝑡\\r\\nand 𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n,\\r\\nrespectively. We assume that 𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\nand 𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\ncome from certain desired\\r\\ndistributions with shifted information removed but forecasting\\x02related information preserved. In this way, we decouple the mod\\x02eling of conditional data distribution 𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n) into three parts:\\r\\n(i) 𝑝(𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n), performing a transformation on raw input series to\\nKDD ’25, August 3–7, 2025, Toronto, ON, Canada. Wei Fan et al.\\r\\n(#!\\r\\n\"|#!#)\\r\\n(!\\r\\n\",(&)\\r\\n(!\\r\\n\",(()\\r\\n(!\\r\\n\",())\\r\\nTransformed Space\\r\\n!\\r\\n#,(&)\\r\\n!\\r\\n#,(()\\r\\n!\\r\\n#,())\\r\\nRaw Data Space\\r\\n(!\\r\\n#)\\r\\nTransformation\\r\\nModule \\r\\n+ *\\r\\nForecasting\\r\\nModule\\r\\nInput Series\\r\\n,!\\r\\n\",(&)\\r\\n,!\\r\\n\",(()\\r\\n,!\\r\\n\",())\\r\\n( -!\\r\\n\"|#!\")\\r\\nRaw Data Space\\r\\n! \" !\\r\\n#$\\r\\nBi-level Optimization\\r\\n \\r\\n(#!\\r\\n#|!#)\\r\\n(!\\r\\n#,(&)\\r\\n(!\\r\\n#,(()\\r\\n(!\\r\\n#,())\\r\\nTransformed Space\\r\\nTransformed Series Transformed Output Forecasts\\r\\n!\"# = ! − ∇ℒ !,!,$%&!\\'\\r\\n!\"# = ! − ∇ℒ !\"#,!,(&)\\r\\niteration loop  iteration loop  + \\r\\nDecoupled Formulation\\r\\n!\"*\"# = !\"* − ∇ℒ !\"*,!\"*,$%&!\\'\\r\\n!\"*\"# = !\"* − ∇ℒ !\"*\"#,!\"*,(&)\\r\\nFigure 1: Framework overview. The upper part is the proposed decoupled formulation (Section 4.1): it separates the time series\\r\\nforecasting into the transformation module (𝑔𝜙 ) and the forecasting module (𝑓𝜃); the input series with the raw data space is\\r\\nconverted into the transformed space by 𝑔𝜙 for the forecasting conducted by the forecasting module 𝑓𝜃; then the predicted\\r\\nresults will be recovered by the inverse of 𝑔𝜙 to recovered to the raw data space. The lower part is the bi-level optimization\\r\\n(Section 4.2) for the decoupled formulation of time series forecasting.\\r\\nremove non-stationary (shifted) information; (ii) 𝑝(𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\n), con\\x02ducting forecasting on the transformed space; (iii) 𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\n), per\\x02forming a reverse transformation to recover certain non-stationary\\r\\npatterns and produce final forecasts.\\r\\nThis decoupled formulation lays the foundation for respective pa\\x02rameterization and optimization of transformation and forecasting\\r\\nmodules. Then in addition to the forecasting function 𝑓𝜃, we intro\\x02duce another two functions to be responsible for the bi-directional\\r\\ndistribution transformations in the transformation module. To be\\r\\nspecific, 𝑔𝜙𝐿 : 𝒙\\r\\n𝐿\\r\\n𝑡 → 𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\n, parameterized by 𝜙\\r\\n𝐿\\r\\n, stands for the trans\\x02formation from a raw data distribution X𝐿\\r\\nto a desired distribution\\r\\nX˜ 𝐿 without non-stationary information, and 𝑔\\r\\n−1\\r\\n𝜙𝐻\\r\\n: 𝒙\\r\\n𝐻\\r\\n𝑡 → 𝒙˜\\r\\n𝐻\\r\\n𝑡\\r\\npa\\x02rameterized by 𝜙\\r\\n𝐻 denotes the reverse transformation from X˜ 𝐻 to\\r\\nX𝐻 . Accordingly, we emit a forecast 𝒚ˆ\\r\\n𝐻\\r\\n𝑡\\r\\ngiven an input 𝒙\\r\\n𝐿\\r\\n𝑡\\r\\nas:\\r\\n𝒚ˆ\\r\\n𝐻\\r\\n𝑡 = 𝑔\\r\\n−1\\r\\n𝜙𝐻\\r\\n\\x10\\r\\n𝑓𝜃\\r\\n\\x10\\r\\n𝑔𝜙𝐿\\r\\n\\x10\\r\\n𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n\\x11 \\x11 \\x11 (2)\\r\\n4.2 Bi-level Optimization for the Decoupled\\r\\nTransformation and Forecasting\\r\\nGiven the decoupled formulation, we further consider a learning\\r\\nprocedure to fulfill the needs of 1) removing distribution shifts that\\r\\nhinder the forecasting performance and 2) performing accurate\\r\\nforecasting based on samples from a transformed stationary dis\\x02tribution. Motivated by these two-sided objectives, we naturally\\r\\ndevelop a bi-level optimization problem [7]:\\r\\n𝜙∗ = arg min\\r\\n𝜙\\r\\nL (𝜃∗ (𝜙), 𝜙, T𝑣𝑎𝑙)\\r\\ns. t. 𝜃∗ (𝜙) = arg min\\r\\n𝜃\\r\\nL (𝜃, 𝜙, T𝑡𝑟𝑎𝑖𝑛)\\r\\n(3)\\r\\nwhere 𝜙 encapsulates 𝜙\\r\\n𝐻 and 𝜙𝐿\\r\\n, T𝑡𝑟𝑎𝑖𝑛 and T𝑣𝑎𝑙 denote the sets\\r\\nof time steps in training and validation data, respectively, and\\r\\nL (𝜃, 𝜙, T ) =\\r\\nÍ\\r\\n𝑡 ∈ T ℓ(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n, 𝑔−1\\r\\n𝜙𝐻\\r\\n(𝑓𝜃(𝑔𝜙𝐿 (𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n)))) is the summation of\\r\\nlosses, specified by ℓ(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n,𝒚ˆ\\r\\n𝐻\\r\\n𝑡\\r\\n), over all time steps in T. The inner\\r\\nloop optimization indicates that given any transformation model\\r\\nparameterized by 𝜙, the solution 𝜃∗ (𝜙) minimizes the forecasting\\r\\nlosses on the training data, which ensures the effective learning of\\r\\nthe forecasting module on the transformed space. The outer loop\\r\\noptimization aims to identify the transformation solution 𝜙∗ that\\r\\ncontributes to the best generalization performance on the valida\\x02tion data, which aligns with our motivation to remove uncertain\\r\\nshifted information hindering forecasting.\\r\\nMoreover, a standard approach to solve the bi-level optimiza\\x02tion problem of Equation (3) is to introduce hyper gradients [6].\\r\\nNevertheless, the calculation of hyper gradients involves the compu\\x02tation of 𝜕L (𝜃∗ (𝜙),𝜙,T𝑣𝑎𝑙 )\\r\\n𝜕𝜃∗ (𝜙)\\r\\n𝜕𝜃∗ (𝜙)\\r\\n𝜕𝜙\\r\\n𝑇\\r\\n, which can be prohibitive when\\r\\nperforming exact inner optimization. Therefore, we adopt the first\\x02order approximation, as did in [30], to cut off the gradient depen\\x02dency introduced by 𝜕𝜃∗ (𝜙)\\r\\n𝜕𝜙 and alternate between the following\\r\\ntwo gradient updates:\\r\\n𝜃𝑖+1 = 𝜃𝑖 − 𝛼\\r\\n𝜕L (𝜃𝑖, 𝜙𝑖, T𝑡𝑟𝑎𝑖𝑛)\\r\\n𝜕𝜃𝑖\\r\\n,\\r\\n𝜙𝑖+1 = 𝜙𝑖 −𝛾\\r\\n𝜕L (𝜃𝑖+1, 𝜙𝑖, T𝑣𝑎𝑙)\\r\\n𝜕𝜙𝑖\\r\\n.\\r\\n(4)\\r\\nwhere 𝑖 means the 𝑖-th iteration; 𝛼 and 𝛾 are the learning rate for\\r\\nthe forecasting module and the transformation module respectively.\\r\\nIn this way, we actually turn the original bi-level optimization into\\r\\nalternate optimization [6]. While in practice, we find that applying\\r\\nsuch an approximation to our case brings very fast computation\\r\\nwith negligible performance compromise.\\nIN-Flow: Instance Normalization Flow for Non-stationary Time Series Forecasting KDD ’25, August 3–7, 2025, Toronto, ON, Canada.\\r\\nBatchNorm\\r\\nCoupling \\r\\nLayer\\r\\nSplit\\r\\nPermute\\r\\n× \\r\\n(a) RealNVP\\r\\nInstanceNorm\\r\\nInstanceNorm\\r\\n(optional)\\r\\nCoupling \\r\\nLayer\\r\\nSplit\\r\\nPermute\\r\\n× \\r\\n(b) IN-Flow\\r\\nFigure 2: The specific architecture of RealNVP [11] and our\\r\\nIN-Flow. We discuss the difference of pre- and post-norm in\\r\\nSection 5.3.1 and take the pre-norm version as IN-Flow.\\r\\n4.3 Instance Normalizaiton Flow\\r\\nThough the decoupled formulation with bi-level optimization can\\r\\naccommodate any 𝑓𝜃, another critical challenge is the design of the\\r\\ntransformation module and optimization for 𝜙. First, the module\\r\\nshould have adequate ability to transform data distributions effec\\x02tively (expressiveness). Second, the module is responsible for the\\r\\nbi-directional transformation between X (X𝐿, X𝐻 ) and X˜(X˜ 𝐿, X˜ 𝐻 ).\\r\\nSuch considerations inspire us to leverage the key idea of normal\\x02izing flows [27], one kind of invertible networks [3] that transform\\r\\ndata distributions.\\r\\nExisting normalizing flows conduct reversible transformation on\\r\\none raw data distribution (see Section 3.4). In contrast, we notice our\\r\\nformulation is composed of two transformations, 𝑔𝜙𝐿 : 𝒙\\r\\n𝐿\\r\\n𝑡 → 𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\nand 𝑔\\r\\n−1\\r\\n𝜙𝐻\\r\\n: 𝒙˜\\r\\n𝐻\\r\\n𝑡 → 𝒙\\r\\n𝐻\\r\\n𝑡\\r\\n, which actually operate in two variable spaces\\r\\n(𝑔𝜙𝐿 on X𝐿 ∈ R\\r\\n𝐿×𝐷 and 𝑔𝜙𝐻 on X𝐻 ∈ R𝐻 ×𝐷 ), due to the difference\\r\\nof lookback length (𝐿) and horizon length (𝐻). Nonetheless, these\\r\\ntwo distributions, 𝑝X𝐿 (𝒙\\r\\n𝐿\\r\\n) and 𝑝X𝐻 (𝒙\\r\\n𝐻 ), undoubtedly share many\\r\\ncommon parts in distribution because their samples are generated\\r\\nby segmenting the same time series (𝒔). Therefore, to fulfill bi\\x02directional transformations on two variable spaces and capture\\r\\nthe common patterns between them, we develop a novel invertible\\r\\nneural architecture, namely instance normalization flow (IN-Flow),\\r\\nthat instantiates 𝑔𝜙𝐿 and 𝑔𝜙𝐻 as a single network and works on\\r\\nvariable length of time series, also referred to as 𝑔𝜙 for brevity.\\r\\nFlow-based models [10, 11, 26, 41] usually adopt batch normal\\x02ization in their architectures for a stable training. However, a batch\\r\\nin time series forecasting may consist of samples of various distribu\\x02tions due to distribution shift [24]. In such cases, batch normaliza\\x02tion could meet with instability in calculating batch statistics and\\r\\ninfluence the transformation [47]. Moreover, we notice instance nor\\x02malization has recently been effective in computer vision [22, 49]\\r\\nand can be an alternative to get over the instability [13]. And in the\\r\\ndomain of time series, the sequence format can make distribution\\r\\nshift relieved with the application of instance normalization [24].\\r\\nInspired by them, our IN-Flow includes parametric instance nor\\x02malization layer which is written given 𝑑-th variate by:\\r\\n𝒉\\r\\n′𝐿,(𝑑)\\r\\n𝑡\\r\\n=\\r\\n\\x10\\r\\n𝒉\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n− 𝜇\\r\\n(𝑑)\\r\\n𝑡\\r\\n\\x11 \\x10𝜎\\r\\n2(𝑑)\\r\\n𝑡\\r\\n+ 𝜖\\r\\n\\x11−\\r\\n1\\r\\n2\\r\\nexp(𝛾\\r\\n(𝑑)\\r\\n𝑡\\r\\n) + 𝛽\\r\\n(𝑑)\\r\\n𝑡\\r\\n(5)\\r\\nwhere 𝒉\\r\\n′𝐿,(𝑑)\\r\\n𝑡\\r\\nare the corresponding mappings of 𝒉\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n∈ R\\r\\n𝐿×1\\r\\n.\\r\\nFor brevity, supposing the current layer is the first layer of\\r\\n𝑔𝜙 , we have 𝒉\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n≜ 𝒙\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n, we have 𝒉\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n≜ 𝒙\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n, 𝜇\\r\\n(𝑑)\\r\\n𝑡\\r\\n=\\r\\n1\\r\\n𝐿\\r\\nÍ\\r\\n𝒙\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n=\\r\\n1\\r\\n𝐿\\r\\nÍ𝑡−1\\r\\n𝑡−𝐿\\r\\n𝑠\\r\\n(𝑑)\\r\\n𝑡\\r\\nas the instance mean, and we have\\r\\n𝜎\\r\\n2(𝑑)\\r\\n𝑡\\r\\n=\\r\\n1\\r\\n𝐿\\r\\nÍ𝑡−1\\r\\n𝑡−𝐿\\r\\n\\x10\\r\\n𝑠\\r\\n(𝑑)\\r\\n𝑡\\r\\n− 𝜇\\r\\n(𝑑)\\r\\n𝑡\\r\\n\\x112\\r\\nas square of instance standard de\\x02viation, where 𝑠\\r\\n(𝑑)\\r\\n𝑡\\r\\nas the value of 𝑑-th variate at timestamp 𝑡.\\r\\nAnd 𝛾\\r\\n(𝑑)\\r\\n𝑡\\r\\n, 𝛽\\r\\n(𝑑)\\r\\n𝑡\\r\\nare affine parameters for transformation in nor\\x02malization layers. If it’s in the middle layer of IN-Flow, we have\\r\\n𝜇\\r\\n(𝑑)\\r\\n𝑡\\r\\n=\\r\\n1\\r\\n𝐿\\r\\nÍ\\r\\n𝒉\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\nand 𝜎\\r\\n2(𝑑)\\r\\n𝑡\\r\\n=\\r\\n1\\r\\n𝐿\\r\\nÍ𝑡−1\\r\\n𝑡−𝐿\\r\\n\\x10\\r\\nℎ\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\n− 𝜇\\r\\n(𝑑)\\r\\n𝑡\\r\\n\\x112\\r\\n. While\\r\\nEquation (5) is the forward transformation of 𝑝(𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n), since it is\\r\\nan invertible network, we can also write the inverse transformation\\r\\nof the above operation corresponding for 𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\n) in IN-Flow.\\r\\nSpecifically, in the inverse transformation, the instance normaliza\\x02tion given the 𝑑-th variate is by:\\r\\n𝒉\\r\\n𝐻,(𝑑)\\r\\n𝑡\\r\\n=\\r\\n\\x10\\r\\n𝒉\\r\\n′𝐻,(𝑑)\\r\\n𝑡\\r\\n− 𝛽\\r\\n(𝑑)\\r\\n𝑡\\r\\n\\x11\\r\\nexp(−𝛾\\r\\n(𝑑)\\r\\n𝑡\\r\\n)\\r\\n\\x10\\r\\n𝜎\\r\\n2(𝑑)\\r\\n𝑡\\r\\n+ 𝜖\\r\\n\\x11 1\\r\\n2\\r\\n+ 𝜇\\r\\n(𝑑)\\r\\n𝑡\\r\\n(6)\\r\\nwhere 𝒉\\r\\n𝐻,(𝑑)\\r\\n𝑡\\r\\nare the corresponding forecasts of 𝒉\\r\\n𝐿,(𝑑)\\r\\n𝑡\\r\\nin the trans\\x02formed space; 𝒉\\r\\n𝐻,(𝑑)\\r\\n𝑡\\r\\nare inverse transformed results of current\\r\\ninstance normalization layer; other parameters can be correspond\\x02ing for those in Equation (5).\\r\\nMoreover, to make the transformation more expressive, we fol\\x02low previous models to include one common reversible design of\\r\\nflows, affine coupling layers [10, 11] as part of IN-Flow. In imple\\x02mentation, we let the coupling layers work on the feature (variate)\\r\\ndimension by:\\r\\n(\\r\\n𝒉\\r\\n′𝐿,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n= 𝒉\\r\\n𝐿,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n𝒉\\r\\n′𝐿,(𝑑𝑐 :𝐷)\\r\\n𝑡\\r\\n= 𝒉\\r\\n𝐿,(𝑑𝑐 :𝐷)\\r\\n𝑡\\r\\n⊙ 𝑠\\r\\n\\x10\\r\\n𝒉\\r\\n𝐿,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n\\x11\\r\\n+ 𝑡\\r\\n\\x10\\r\\n𝒉\\r\\n𝐿,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n\\x11\\r\\n(7)\\r\\nwhere usually we let𝑑𝑐 = ⌈\\r\\n𝐷\\r\\n2\\r\\n⌉ stand for the split position; 𝒉\\r\\n𝐿,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n∈\\r\\nR\\r\\n𝐿×𝑑𝑐 denotes data composed of first𝑑𝑐 variates of 𝒉𝐿\\r\\n𝑡\\r\\nand 𝒉\\r\\n𝐿,(𝑑𝑐 :𝐷)\\r\\n𝑡\\r\\n∈\\r\\nR\\r\\n𝐿× (𝐷−𝑑𝑐 ) denotes the left 𝐷 − 𝑑𝑐 variates; 𝑠 and 𝑡 are scale and\\r\\ntranslation functions mapping from R\\r\\n𝑑𝑐\\r\\nto R\\r\\n𝐷−𝑑𝑐\\r\\n; 𝒉\\r\\n′𝐿\\r\\n𝑡\\r\\nis the pro\\x02cessed output. Accordingly, the inverse transformation of the above\\r\\ncoupling layers for 𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\n) in IN-Flow can be written by:\\r\\n(\\r\\n𝒉\\r\\n𝐻,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n= 𝒉\\r\\n′𝐻,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n𝒉\\r\\n𝐻,(𝑑𝑐 :𝐷)\\r\\n𝑡\\r\\n=\\r\\n\\x10\\r\\n𝒉\\r\\n′𝐻,(𝑑𝑐 :𝐷)\\r\\n𝑡\\r\\n− 𝑡\\r\\n\\x10\\r\\n𝒉\\r\\n′𝐻,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n\\x11 \\x11 /𝑠\\x10\\r\\n𝒉\\r\\n′𝐻,(1:𝑑𝑐 )\\r\\n𝑡\\r\\n\\x11\\r\\n(8)\\r\\nwhere 𝒉\\r\\n′𝐻\\r\\n𝑡\\r\\nare the forecasting results of 𝒉\\r\\n′𝐿\\r\\n𝑡\\r\\nin the hidden space;\\r\\n𝒉\\r\\n𝐻\\r\\n𝑡\\r\\nis the processed results of 𝒉\\r\\n′𝐻\\r\\n𝑡\\r\\nby the affine coupling layers.\\r\\nEspecially, if the coupling layers are the final layers of IN-Flow, we\\r\\ncan have 𝒚ˆ\\r\\n𝐻\\r\\n𝑡\\r\\n= 𝒉\\r\\n𝐻\\r\\n𝑡\\r\\n. In stacking IN-Flow layers, we find that the pre\\x02norm (IN-Flow) can achieve better performance than post-norm\\r\\n(IN-Flow-T in Section 5.3.1). Thus we take the pre-norm variant as\\r\\nthe final design of IN-Flow.\\r\\nOverall, the main architecture of IN-Flow is by stacking instance\\r\\nnormalization and coupling layers iteratively, similar to other classic\\r\\nnormalizing flows such as RealNVP [11], as shown in Figure 2. We\\r\\nalso adopt Split and Permute operations [10, 11, 26] to assist for\\r\\nstacking layers. Note that both Equation (5) and Equation (7) belong\\r\\nto the transformation of 𝑝(𝒙˜\\r\\n𝐿\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n). We can also accordingly write\\r\\nthe inverse transformation of above layers for 𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒚˜\\r\\n𝐻\\r\\n𝑡\\r\\n) in IN-Flow.\\nKDD ’25, August 3–7, 2025, Toronto, ON, Canada. Wei Fan et al.\\r\\nN-BEATS +RevIN +IN-Flow\\r\\n3.0\\r\\n3.5\\r\\n4.0\\r\\n1e7 Mean Squared Error\\r\\niTransformer +NonST +IN-Flow\\r\\n2.8\\r\\n3.0\\r\\n3.2\\r\\n1e7 Mean Squared Error\\r\\n(a) Comparison of forecasting on synthetic data-1\\r\\nN-BEATS +RevIN +IN-Flow\\r\\n2.2\\r\\n2.4\\r\\n2.6\\r\\n1e7 Mean Squared Error\\r\\niTransformer +NonST +IN-Flow\\r\\n2.00\\r\\n2.25\\r\\n2.50\\r\\n2.75\\r\\n1e7 Mean Squared Error\\r\\n(b) Comparison of forecasting on synthetic data-2\\r\\nFigure 3: Evaluation of forecasting on the synthetic data. NonST means non-stationary transformer.\\r\\n5 Experiments\\r\\nOur empirical studies in this section aim to answer the following\\r\\nquestions: 1) With some solutions existing, why should we further\\r\\nstudy distribution shifts and non-stationary time series forecast\\x02ing? 2) How much benefit can our framework gain for time series\\r\\nforecasting compared with existing state-of-the-art models in real\\x02world settings? 3) Does each part of IN-Flow designs (including\\r\\noptimization) count for good forecasting performance?\\r\\nBaselines. We mainly consider several kinds of state-of-the-art\\r\\nmethods for non-stationary time series forecasting. For the model\\x02agnostic normalization technique, we adopt RevIN [24], Dish-TS [15],\\r\\nand SAN [34]. For the backbones, we adopt the recently well\\x02performed forecasting methods, i.e., iTransformer [32], PatchTST [37],\\r\\nAutoformer [55], and N-BEATS [39]. For the model-specific mech\\x02anisms, we adopt Non-tationary Transformer [33] and we take\\r\\nvanilla Transformer [50], Informer [62] and Autoformer [55] as the\\r\\nbackbones More baseline details are in Appendix A.\\r\\nEvaluation Details. We evaluate time series forecasting perfor\\x02mance on the classic mean squared error (MSE) and mean absolute\\r\\nerror (MAE). To directly reflect distribution shifts in time series, all\\r\\nthe evaluations are conducted on original data without data scaling\\r\\nor normalization following settings of previous works [15]. The\\r\\nreported metrics on real-world data are scaled for readability. More\\r\\nevaluation details are included in Appendix C.2.\\r\\n5.1 Evaluation on Synthetic Data\\r\\nTo intuitively illustrate the importance of modeling distribution\\r\\nshifts in forecasting and the superiority of IN-Flow, we generate\\r\\nsynthetic time series data with distributions shifted. We first group\\r\\neach neighbored𝜏 points as a segment and let every segment follows\\r\\none distribution. Supposing to synthesize one series {𝑠1, · · · , 𝑠𝑇 } of\\r\\nlength 𝑇 , this series can be regarded to have 𝑈 = ⌈𝑇 /𝜏⌉ segments,\\r\\nwhere the 𝑢-th segment follows its distribution P𝑢. We can thus\\r\\ncontrol the degree of time series distribution shift through adjusting\\r\\n𝜏 and P𝑢. Under such settings, both 𝑝(𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n) and 𝑝(𝒚\\r\\n𝐻\\r\\n𝑡\\r\\n|𝒙\\r\\n𝐿\\r\\n𝑡\\r\\n) keep\\r\\nshifted over time if lookback length 𝐿 > 𝜏 and lookahead length\\r\\n𝐻 > 𝜏. Then for each P𝑢, we simply make a regular cosine wave\\r\\nsignal to model simple periodic patterns in time series. Specifically,\\r\\nwe let 𝑠𝑡 = 𝐴𝑢 cos(2𝜋\\r\\n1\\r\\n𝑇𝑢\\r\\n𝑡 + 𝐵𝑢) +𝐶𝑢 if time index 𝑡 belongs to 𝑢-th\\r\\nsegment, where 𝐴𝑢,𝑇𝑢, 𝐵𝑢,𝐶𝑢 indicate amplitude, period, phase\\r\\nand level parameters for the 𝑢-th segment.\\r\\nFollowing the above rules, we make three synthetic datasets,\\r\\ncalled Synthetic-1/2 by varying different 𝜏 and P𝑢. Due to space\\r\\nlimit, we include more details about synthetic datasets in Appendix\\r\\nB. Figure 3 shows two different comparisons of different backbones\\r\\non Synthetic-1 and Synthetic-2 datasets. We notice that in our\\r\\nsimulation settings, both two state-of-the-art techniques RevIN\\r\\nand non-stationary Transformers would fail significantly, causing\\r\\na more than 10% performance decrease on iTransformer and N\\x02BEATS respectively. These results demonstrate their limitations in\\r\\novercoming distribution shifts in forecasting when meeting some\\r\\nseverely shifted time series, while IN-Flow can still improve the\\r\\nforecasting ability of backbones by a large margin which shows\\r\\nour superiority in non-stationary forecasting.\\r\\n5.2 Evaluation on Real-world Data\\r\\nApart from simulation experiments, we further perform experi\\x02ments on real-world datasets. We adopt six existing shifted datasets\\r\\nfollowing previous forecasting literature: Electricity transformer\\r\\ntemperature datasets include time series of oil temperature and\\r\\npower load collected from electricity transformers in China. ETTm2\\r\\ndataset is recorded at a fifteen-minute frequency ranging from July\\r\\n2016 to July 2018. In contrast, ETTh1 dataset dataset is recorded\\r\\nevery hour. Electricity dataset contains the hourly electricity con\\x02sumption of 321 customers from 2012 to 2014. Weather dataset\\r\\ncontains meteorological measurements of 21 weather indicators,\\r\\nwhich are collected 10-minutely in 2020. CAISO dataset includes\\r\\nhourly electricity load series in different zones of California rang\\x02ing from 2017 to 2020. NordPool dataset includes hourly energy\\r\\nproduction volume series of several European countries in 2020.\\r\\nFor a stable evaluation, we perform z-score normalization for\\r\\neach backbone model and inverse the forecasting results to report\\r\\nmetrics. We rerun all the models under four different seeds to report\\r\\nthe average MSE/MAE on the testset of each dataset. For data split,\\r\\nwe follow [62] and split data into train/validation/test set by the\\r\\nratio 6:2:2 towards ETTh1 dataset ETTm2 dataset. We also adopt the\\r\\nratio 6:2:2 to split the data for CAISO dataset and NordPool dataset.\\r\\nFor Weather dataset and Electricity dataset, we follow [55] to split\\r\\ndata by the ratio of 7:1:2 for the train/validation/test set. To cover\\r\\nshort/long-term forecasting, we set the length of the lookback and\\r\\nhorizon window from 48 to 336. We train the forecasting models\\r\\nusing L2 loss and Adam [25] on a single NVIDIA A100 40GB GPU.\\r\\n5.2.1 Main Results. Table 1 demonstrates the overall performance\\r\\ncomparison of the basic models and their IN-Flow-equipped ver\\x02sions on four representative time series forecasting models. Based\\r\\non the results, we can observe that IN-Flow consistently improves\\r\\nbackbone models under different settings across the datasets. The\\r\\naverage improvements of IN-Flow on the backbone models come to\\r\\n28.3% on PatchTST, 21.3% on iTransformer, 28.5 on Autoformer, and\\r\\n12.1% on N-BEATS across different datasets, which can show the\\nIN-Flow: Instance Normalization Flow for Non-stationary Time Series Forecasting KDD ’25, August 3–7, 2025, Toronto, ON, Canada.\\r\\nTable 1: Performance comparison of time series forecasting on backbone models and IN-Flow. The length of look\\x02back/horizon windows is prolonged from 48 to 336 to cover short and long forecasting settings.\\r\\nMethod PatchTST +IN-Flow iTransformer +IN-Flow Autoformer +IN-Flow N-BEATS +IN-Flow\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1\\r\\n48 1.128 1.865 0.936 1.635 1.051 1.717 0.811 1.244 1.237 2.010 1.215 1.937 0.959 1.637 0.951 1.624\\r\\n96 1.320 2.109 1.025 1.782 1.188 1.933 0.896 1.542 1.710 2.347 1.428 2.141 1.072 1.789 1.027 1.768\\r\\n168 2.001 2.679 1.077 1.888 1.917 2.624 0.901 1.925 2.139 2.739 1.303 2.095 1.223 1.989 1.139 1.914\\r\\n336 1.784 2.613 1.121 2.021 1.734 2.568 1.107 1.189 2.444 2.965 1.502 2.305 1.222 2.083 1.146 2.044\\r\\nETTm2\\r\\n48 1.116 2.204 0.925 1.908 1.094 2.168 0.958 1.972 1.477 2.469 1.459 2.432 1.329 2.308 1.229 2.182\\r\\n96 1.503 2.699 0.992 1.965 1.470 2.178 0.996 1.973 1.962 2.901 1.869 2.748 1.628 2.604 1.570 2.496\\r\\n168 1.708 2.783 1.213 2.202 1.352 2.472 1.248 2.275 1.898 2.919 1.834 2.831 2.148 3.021 1.752 2.681\\r\\n336 2.664 3.465 1.564 2.482 2.124 2.713 1.678 2.532 2.271 3.235 2.113 3.003 2.376 3.339 1.928 2.847\\r\\nElectricity\\r\\n48 0.632 0.366 0.554 0.342 0.513 0.319 0.421 0.312 0.686 0.398 0.592 0.381 0.698 0.431 0.594 0.422\\r\\n96 0.882 0.698 0.589 0.341 0.710 0.558 0.479 0.313 0.936 0.763 0.669 0.397 0.912 0.751 0.872 0.723\\r\\n168 1.678 0.541 0.882 0.389 1.341 0.413 0.790 0.329 1.989 0.576 0.943 0.437 2.031 0.582 1.521 0.503\\r\\n336 2.482 0.814 2.104 0.768 2.012 0.654 1.678 0.594 2.934 0.898 2.521 0.813 2.835 0.872 2.671 0.852\\r\\nWeather\\r\\n48 1.003 3.291 0.413 1.710 1.001 3.288 0.447 1.897 1.297 3.217 0.402 1.844 0.981 3.226 0.432 1.698\\r\\n96 0.335 1.886 0.279 1.541 0.334 1.886 0.301 1.629 0.658 3.327 0.285 1.663 0.246 1.642 0.223 1.328\\r\\n168 0.338 2.138 0.207 1.421 0.337 2.138 0.263 1.581 0.398 2.260 0.253 1.708 0.192 1.392 0.182 1.217\\r\\n336 0.385 2.375 0.192 1.475 0.384 2.373 0.228 1.535 0.570 2.996 0.255 1.712 0.194 1.520 0.179 1.311\\r\\nCAISO\\r\\n48 0.986 0.499 0.641 0.383 0.906 0.425 0.627 0.377 1.789 0.740 0.823 0.467 1.099 0.684 0.514 0.341\\r\\n96 0.898 0.451 0.828 0.422 0.833 0.424 0.799 0.393 2.428 0.826 1.289 0.576 0.886 0.440 0.878 0.428\\r\\n168 1.369 0.577 1.161 0.500 1.297 0.503 1.071 0.411 2.832 0.907 1.344 0.575 1.189 0.504 1.150 0.498\\r\\n336 2.541 0.852 1.636 0.604 2.124 0.712 1.524 0.456 3.148 0.960 1.809 0.679 1.612 0.607 1.518 0.602\\r\\nNordPool\\r\\n48 1.304 0.664 1.156 0.636 1.331 0.800 1.233 0.769 1.632 0.765 1.478 0.732 1.192 0.639 1.150 0.632\\r\\n96 1.771 0.799 1.687 0.777 1.851 0.936 1.796 0.823 2.187 0.889 2.071 0.852 1.644 0.764 1.596 0.756\\r\\n168 2.513 0.986 2.009 0.865 2.597 1.035 2.584 0.908 2.596 0.992 2.430 0.957 2.044 0.867 1.918 0.836\\r\\n336 2.946 1.040 2.010 0.887 3.022 0.976 2.138 0.941 2.993 1.236 2.079 0.887 2.557 0.953 2.098 0.884\\r\\neffectiveness of our methods. Moreover, in certain situations, the\\r\\nimprovements are even higher. Notably, with the lookback/horizon\\r\\nlength as 168 on the Weather dataset, IN-Flow improves iTrans\\x02former by 32.5% (0.710 → 0.479) and improves PatchTST by 33.2%\\r\\n(0.882 → 0.589) on MSE. Another case on CAISO dataset is that the\\r\\nMSE of Autoformer with IN-Flow is reduced from 1.789 to 0.823,\\r\\nwhich achieves a very large (54.0%) improvement. The overall supe\\x02rior performance shows the great potential of IN-Flow in enhancing\\r\\ndeep forecasting models on prediction.\\r\\n5.2.2 Comparison with Other Non-stationary Forecasting Techniques.\\r\\nIn this section, we further compare our performance with more\\r\\nrecent normalization techniques, Dish-TS [15] and SAN [34] that\\r\\nhandle distribution shifts in time series forecasting. Table 2 has\\r\\nshown the performance comparison in time series forecasting tak\\x02ing the PatchTST [37] as the backbone. From the results, we can\\r\\nobserve that the existing Dish-TS can only improve the backbone\\r\\nin some shifted datasets. In some situations, Dish-TS might lead to\\r\\nworse performances, while SAN can perform better than it. Never\\x02theless, our IN-Flow can usually achieve the best performance. A\\r\\npotential explanation is that IN-Flow transforms data with multiple\\r\\ncoupling and normalization layers with much expressiveness.\\r\\nBesides, we also include the performance of IN-Flow in Table\\r\\n3 compared with the SOTA architecture-specific method, Non\\x02stationary Transformers [33], in which we consider three transformer\\x02based models as backbones to compose Transformer, Informer, and\\r\\nAutoformer. We can easily observe that IN-Flow achieves a sig\\x02nificant improvement over Non-stationary Transformers. The su\\x02periority signifies that even though IN-Flow is a model-agnostic\\r\\nmethod, it can have great adaptation for transformer-based models\\r\\nand even beat architecture-specific methods with tailor-designed\\r\\nmechanisms de-stationary attention in [33], which reveals the great\\r\\npotential of its compatibility to deep forecasting models.\\r\\n5.3 Additional Experiments\\r\\n5.3.1 Ablation Studies. In order to verify the effectiveness of our\\r\\ndesigns, we conduct adequate ablation studies by adjusting com\\x02ponents of IN-Flow and considering some other transformation\\r\\nmethods. Specifically, we consider four different variants for ab\\x02lation tests: RealNVP [11] adopts the coupling layers [10] and\\r\\nthe batch normalization [23]. In implementation, we mainly follow\\r\\nthe reproduced implementation1and abandon some complicated\\r\\nmodules such as multi-scale architectures [11, 26]. This method is\\r\\nshown in Figure 8(a) of Appendix. RealNVP-c [11] is a variant\\r\\nof RealNVP which only adopts the coupling layers. We use this\\r\\nmethod as comparison to verify the effectiveness of batch normal\\x02ization [23] for transformation in forecasting. The method is shown\\r\\nin Figure 8(b) in Appendix. IN-Flow-J is a variant of IN-Flow (as\\r\\nshown in Figure 8(d) in Appendix), which jointly optimizes the\\r\\n1https://github.com/kamenbliznashki/normalizing_flows\\nKDD ’25, August 3–7, 2025, Toronto, ON, Canada. Wei Fan et al.\\r\\nTable 2: Performance comparison of Dish-TS, SAN, and IN\\x02Flow on six datasets under three seeds when taking the\\r\\nPatchTST as the backbone model.\\r\\nMethod PatchTST +Dish-TS +SAN +IN-Flow\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE ETTh1\\r\\n48 1.128 1.865 0.975 1.661 0.971 1.668 0.936 1.635\\r\\n96 1.320 2.109 1.100 1.933 1.093 1.839 1.025 1.782\\r\\n168 2.001 2.679 1.229 2.024 1.110 1.950 1.077 1.888\\r\\n336 1.784 2.613 1.352 2.172 1.251 2.137 1.121 2.021\\r\\nWeather\\r\\n48 1.003 3.291 0.852 2.288 0.466 1.753 0.413 1.710\\r\\n96 0.335 1.886 0.314 1.856 0.309 1.777 0.279 1.541\\r\\n168 0.338 2.138 0.219 1.975 0.217 1.622 0.207 1.421\\r\\n336 0.385 2.375 0.256 2.612 0.239 1.907 0.192 1.475\\r\\nCAISO\\r\\n48 0.986 0.499 0.838 0.456 0.748 0.425 0.641 0.383\\r\\n96 1.098 0.451 1.216 0.541 0.901 0.446 0.828 0.422\\r\\n168 1.369 0.577 1.396 0.558 1.216 0.527 1.161 0.500\\r\\n336 2.541 0.852 1.925 0.708 1.831 0.683 1.636 0.604\\r\\nNordPool\\r\\n48 1.304 0.664 1.293 0.654 1.200 0.646 1.156 0.636\\r\\n96 1.971 0.892 1.895 0.924 1.812 0.800 1.687 0.777\\r\\n168 2.513 0.986 2.321 0.928 2.047 0.885 2.009 0.865\\r\\n336 2.946 1.040 2.707 1.010 2.014 0.889 2.010 0.887\\r\\nTable 3: Performance comparison with non-stationary trans\\x02formers, where Transformer, Informer, and Autoformer are\\r\\ncoupled into non-stationary versions to compare with IN\\x02Flow-coupled versions.\\r\\nDatasets ETTm2 Weather CAISO\\r\\nLength 48 336 48 336 48 336\\r\\nNS-Transformer 1.677 2.983 0.551 0.323 0.850 2.282\\r\\nINF-Transformer 1.352 2.053 0.360 0.210 0.757 1.548\\r\\nImprovement 19.4% 31.2% 34.6% 35.0% 10.9% 32.1%\\r\\nNS-Informer 2.094 2.710 0.566 0.289 0.861 1.805\\r\\nINF-Informer 1.487 2.096 0.400 0.249 0.768 1.739\\r\\nImprovement 32.2% 22.7% 29.3% 13.8% 3.4% 3.7%\\r\\nNS-Autoformer 1.553 2.117 0.580 0.375 0.844 1.923\\r\\nINF-Autoformer 1.427 2.068 0.421 0.261 0.794 1.535\\r\\nImprovement 8.1% 2.3% 27.7% 30.4% 5.9% 20.2%\\r\\ntransformation module and the forecasting module on the training\\r\\ndata without bi-level optimization in the formulation. This method\\r\\nis to verify the effectiveness of bi-level optimization in IN-Flow. IN\\x02Flow-T (post-norm) is another variant of IN-Flow, which changes\\r\\nthe relative position of coupling layers and instance normalization\\r\\nand puts the instance normalization to the tail of flows, as shown in\\r\\nFigure 8(c) in Appendix. This method is to verify the architecture\\r\\ndesign of IN-Flow.\\r\\nWe conduct extensive ablation studies considering above four\\r\\nkinds of variants and taking N-BEATS and Autoformer as the back\\x02bone models respectively. We include the experimental ablation\\r\\nresults on N-BEATS in Table 4. Based on the results, we can notice\\r\\nthat RealNVP (with batch normalization) can always perform the\\r\\nworst in the comparison. This signifies the batch normalization is\\r\\nnot proper for the transformation of time series. However, with\\r\\npure coupling layers (RealNVP-c), it can perform well in some cases.\\r\\nTable 4: Ablation studies taking N-BEATS as the backbone.\\r\\nMethod RealNVP RealNVP-c IN-Flow-J IN-Flow-T IN-Flow\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n48 1.039 4.973 0.533 2.665 0.495 1.840 0.460 1.731 0.432 1.698\\r\\n96 1.033 5.354 0.225 1.698 0.234 1.365 0.251 1.376 0.223 1.328\\r\\n168 0.771 4.724 0.183 1.494 0.186 1.303 0.191 1.243 0.182 1.217\\r\\n336 0.604 4.647 0.190 1.579 0.183 1.416 0.193 1.382 0.179 1.311\\r\\nNordPool\\r\\n48 3.137 1.147 1.166 0.643 1.167 0.643 1.188 0.639 1.150 0.632\\r\\n96 2.541 1.029 1.602 0.758 1.620 0.759 1.597 0.757 1.596 0.756\\r\\n168 2.008 0.910 2.006 0.870 1.980 0.845 1.972 0.849 1.918 0.836\\r\\n336 2.100 0.921 2.283 0.927 2.152 0.898 2.082 0.880 2.098 0.884\\r\\n0 20 40 60\\r\\nEpoch\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\nMSE\\r\\n1e6 Training Loss Curve\\r\\nN-BEATS\\r\\n+IN-Flow\\r\\n(a) Training Loss\\r\\n0 20 40 60\\r\\nEpoch\\r\\n2\\r\\n4\\r\\n6\\r\\nMSE\\r\\n1e5 Validation Loss Curve\\r\\nN-BEATS\\r\\n+IN-Flow\\r\\n(b) Validation Loss\\r\\nFigure 4: Learning loss curves with and without IN-Flow.\\r\\nBoth of these two observations motivate us to remove batch nor\\x02malization for transformation and further propose the instance\\r\\nnormalization instead. We can easily observe that, IN-Flow and its\\r\\nvariants can always beat RealNVP series methods, which demon\\x02strates the superiority of the instance normalization. Moreover,\\r\\nwe notice that the joint optimization can sometime work well, es\\x02pecially in CAISO dataset. However, the overall performance of\\r\\nbi-level optimization becomes more stable and competitive. Thus,\\r\\nwe finally adopt the bi-level optimization in our proposed methods.\\r\\n5.3.2 Training Analysis. To further study the learning process of\\r\\nIN-Flow, we visualize the learning loss curves of N-BEATS and IN\\x02Flow. Figure 5(a) and Figure 5(d) demonstrate the training loss and\\r\\nvalidation loss curve on CAISO dataset respectively. We notice that\\r\\nwith IN-Flow the training loss of N-BEATS has faster convergence\\r\\nwhich signifies the transformation module improves the learning\\r\\nof the forecasting module. After about 50 epochs, though the two\\r\\ntraining loss curves come close, the validation loss of IN-Flow is far\\r\\nlower than pure N-BEATS, which is attributed to the transformation\\r\\nmodule that relieves distribution shifts for fewer validation errors\\r\\nand better generalization in forecasting.\\r\\n5.3.3 Case Visualization. We also include visualization case study\\r\\ntowards forecasting. Figure 5 has shown a test sample predicted\\r\\nby RevIN, Dish-TS, SAN, and IN-Flow in the ETTm2 dataset when\\r\\ntaking PatchTST as the backbone. Based on the results, we can find\\r\\nthat all four methods can capture certain seasonal patterns. How\\x02ever, the amplitudes of the seasonal patterns are usually correctly\\r\\nlearned. More importantly, we notice that in both RevIN, Dish-TS\\r\\nand SAN, the trend patterns are not correctly modeled, leading\\r\\nto a larger deviation towards the ground truth. This signifies that\\r\\nexisting normalization techniques might abandon some useful infor\\x02mation and thus have some worse cases. In contrast, our IN-Flow\\r\\nhas adequate transformation ability for finishing forecasting by\\r\\nlearning both trend and seasonality accurately.\\nIN-Flow: Instance Normalization Flow for Non-stationary Time Series Forecasting KDD ’25, August 3–7, 2025, Toronto, ON, Canada.\\r\\n0 100 200 300\\r\\n15\\r\\n20\\r\\n25\\r\\n30\\r\\n35\\r\\n40 predict\\r\\nlabel\\r\\n(a) RevIN\\r\\n0 100 200 300\\r\\n15\\r\\n20\\r\\n25\\r\\n30\\r\\n35\\r\\n40 predict\\r\\nlabel\\r\\n(b) Dish-TS\\r\\n0 100 200 300\\r\\n15\\r\\n20\\r\\n25\\r\\n30\\r\\n35\\r\\n40 predict\\r\\nlabel\\r\\n(c) SAN\\r\\n0 100 200 300\\r\\n15\\r\\n20\\r\\n25\\r\\n30\\r\\n35\\r\\n40 predict\\r\\nlabel\\r\\n(d) IN-Flow\\r\\nFigure 5: Visualization of long-term 336 steps forecasting results of a test sample in ETTm2 dataset.\\r\\n0 1000 2000 3000 4000 5000\\r\\n0.0000\\r\\n0.0001\\r\\n0.0002\\r\\n0.0003\\r\\n0.0004\\r\\n0.0005\\r\\n0.0006 train distribution\\r\\ntest distribution\\r\\n(a) Raw distribution\\r\\n4 2 0 2 4 6\\r\\n0.00\\r\\n0.05\\r\\n0.10\\r\\n0.15\\r\\n0.20\\r\\n0.25\\r\\n0.30\\r\\n0.35 train distribution\\r\\ntest distribution\\r\\n(b) IN-Flow transformed distribution\\r\\nFigure 6: The distribution visualization on NordPool dataset.\\r\\n5.3.4 Distribution Visualization. In order to intuitively show the\\r\\neffectiveness of our IN-Flow, we have visualized the distribution\\r\\nof train and test sets before and after the transformation. Since the\\r\\noriginal time series is not i.i.d. and the window splitting decides\\r\\nthe distribution, we visualize the distribution of the mean value of\\r\\nwindows. Figure 6 shows the comparison of the distribution of train\\r\\nand test sets on the NordPool dataset. We can easily observe that\\r\\nin the original data, the train and test sets have obvious differences.\\r\\nAfter being transformed by IN-Flow, the distribution is much less\\r\\ndistinguishable, showing the shift alleviation function of IN-Flow.\\r\\n5.3.5 Statistic Measurements. We also include the statistical mea\\x02surements of the distribution difference of train and test distribution\\r\\nbefore or after the transformation of IN-Flow. We select Wasser\\x02stein distance as our measurements. In order to intuitively show the\\r\\neffectiveness of our IN-Flow, we have visualized the distribution\\r\\nof train and test sets before and after the transformation. Since the\\r\\noriginal time series is not i.i.d. and the window splitting decides\\r\\nthe distribution, we visualize the distribution of the mean value\\r\\nof windows of the data. The distribution difference between the\\r\\ntrain and test sets is shown in Table 5. From the results, we can\\r\\nobserve that IN-Flow can largely reduce the distribution difference\\r\\nbetween train and test distribution. This provides large potential\\r\\nfor the non-stationary forecasting.\\r\\nTable 5: Wassertein distance of train and test set.\\r\\nDataset Synthetic-2 ETTh1 Electricity\\r\\nTrain/Test Raw WD 0.2213 0.3270 0.3337\\r\\nWD (with IN-Flow) 0.0783 0.0702 0.0547\\r\\nDataset Weather CAISO NordPool\\r\\nTrain/Test Raw WD 0.1751 0.0944 0.1170\\r\\nWD (with IN-Flow) 0.0775 0.0857 0.0471\\r\\n6 Conclusion Remarks\\r\\nIn this paper, we propose a principled approach to address the time\\r\\nseries distribution shift and contribute to non-stationary time series\\r\\nforecasting. Our core contribution lies in three aspects: a decou\\x02pled formulation to separate the removing-shift procedure from the\\r\\nforecasting and regard it as a special distribution transformation, a\\r\\nbi-level optimization problem to formalize the decoupled formula\\x02tion and enable the joint learning of transformation and forecasting,\\r\\nand a novel invertible network, IN-Flow for time series distribution\\r\\ntransformation. Moreover, distribution shift is a well-known and\\r\\ncrucial topic but is still rarely studied in time series forecasting. We\\r\\nhope this principled approach, including the decoupled formulation,\\r\\nbi-level optimization, and IN-Flow can facilitate more research on\\r\\ndistribution shift in time series and non-stationary forecasting.\\r\\n7 Acknowledgments\\r\\nMost of this work is done when the first author was a research\\r\\nassistant in State Key Laboratory of Internet of Things for Smart\\r\\nCity at the University of Macau. This research is funded by the\\r\\nScience and Technology Development Fund (FDCT), Macau SAR\\r\\n(file no. 0123/2023/RIA2, 001/2024/SKL).\\r\\nReferences\\r\\n[1] Kumar Abhishek, MP Singh, Saswata Ghosh, and Abhishek Anand. 2012. Weather\\r\\nforecasting model using artificial neural network. Procedia Technology 4 (2012),\\r\\n311–318.\\r\\n[2] Diyar Akay and Mehmet Atak. 2007. Grey prediction with rolling mechanism\\r\\nfor electricity demand forecasting of Turkey. energy 32, 9 (2007), 1670–1675.\\r\\n[3] Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pelle\\x02grini, Ralf S Klessen, Lena Maier-Hein, Carsten Rother, and Ullrich Köthe. 2018.\\r\\nAnalyzing inverse problems with invertible neural networks. arXiv preprint\\r\\narXiv:1808.04730 (2018).\\r\\n[4] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An Empirical Evaluation of\\r\\nGeneric Convolutional and Recurrent Networks for Sequence Modeling. CoRR\\r\\nabs/1803.01271 (2018).\\nKDD ’25, August 3–7, 2025, Toronto, ON, Canada. Wei Fan et al.\\r\\n[5] Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergen\\x02thaler, and Artur Dubrawski. 2022. N-HiTS: Neural Hierarchical Interpolation\\r\\nfor Time Series Forecasting. CoRR abs/2201.12886 (2022).\\r\\n[6] Can Chen, Xi Chen, Chen Ma, Zixuan Liu, and Xue Liu. 2022. Gradient-based\\r\\nbi-level optimization for deep learning: A survey. arXiv preprint arXiv:2207.11719\\r\\n(2022).\\r\\n[7] Benoît Colson, Patrice Marcotte, and Gilles Savard. 2007. An overview of bilevel\\r\\noptimization. Annals of operations research 153, 1 (2007), 235–256.\\r\\n[8] Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor W\\r\\nTsang. 2024. Disentangling Structured Components: Towards Adaptive, Inter\\x02pretable and Scalable Time Series Forecasting. IEEE Transactions on Knowledge\\r\\nand Data Engineering (2024).\\r\\n[9] Jinliang Deng, Feiyang Ye, Du Yin, Xuan Song, Ivor Tsang, and Hui Xiong. 2024.\\r\\nParsimony or Capability? Decomposition Delivers Both in Long-term Time\\r\\nSeries Forecasting. In The Thirty-eighth Annual Conference on Neural Information\\r\\nProcessing Systems. https://openreview.net/forum?id=wiEHZSV15I\\r\\n[10] Laurent Dinh, David Krueger, and Yoshua Bengio. 2014. Nice: Non-linear inde\\x02pendent components estimation. arXiv preprint arXiv:1410.8516 (2014).\\r\\n[11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2016. Density estimation\\r\\nusing real nvp. arXiv preprint arXiv:1605.08803 (2016).\\r\\n[12] Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and\\r\\nChongjun Wang. 2021. Adarnn: Adaptive learning and forecasting of time\\r\\nseries. In Proceedings of the 30th ACM International Conference on Information\\r\\nand Knowledge Management. 402–411.\\r\\n[13] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. 2016. A learned\\r\\nrepresentation for artistic style. arXiv preprint arXiv:1610.07629 (2016).\\r\\n[14] Wei Fan, Yanjie Fu, Shun Zheng, Jiang Bian, Yuanchun Zhou, and Hui Xiong. 2024.\\r\\nDewp: Deep expansion learning for wind power forecasting. ACM Transactions\\r\\non Knowledge Discovery from Data 18, 3 (2024), 1–21.\\r\\n[15] Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou, and\\r\\nYanjie Fu. 2023. Dish-ts: a general paradigm for alleviating distribution shift\\r\\nin time series forecasting. In Proceedings of the AAAI Conference on Artificial\\r\\nIntelligence, Vol. 37. 7522–7529.\\r\\n[16] Wei Fan, Kun Yi, Hangting Ye, Zhiyuan Ning, Qi Zhang, and Ning An. 2024. Deep\\r\\nfrequency derivative learning for non-stationary time series forecasting. arXiv\\r\\npreprint arXiv:2407.00502 (2024).\\r\\n[17] Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan\\r\\nLiu. 2022. DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting.\\r\\nIn International Conference on Learning Representations.\\r\\n[18] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano\\r\\nPontil. 2018. Bilevel programming for hyperparameter optimization and meta\\x02learning. In International Conference on Machine Learning. PMLR, 1568–1577.\\r\\n[19] Jindong Han, Hao Liu, Hengshu Zhu, Hui Xiong, and Dejing Dou. 2021. Joint\\r\\nAir Quality and Weather Prediction Based on Multi-Adversarial Spatiotemporal\\r\\nNetworks. In Proceedings of the 35th AAAI Conference on Artificial Intelligence.\\r\\n[20] Charles C Holt. 1957. Forecasting trends and seasonal by exponentially weighted\\r\\nmoving averages. ONR Memorandum 52, 2 (1957).\\r\\n[21] Charles C Holt. 2004. Forecasting seasonals and trends by exponentially weighted\\r\\nmoving averages. International Journal of Forecasting 20, 1 (2004), 5–10.\\r\\n[22] Xun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time\\r\\nwith adaptive instance normalization. In Proceedings of the IEEE international\\r\\nconference on computer vision. 1501–1510.\\r\\n[23] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep\\r\\nnetwork training by reducing internal covariate shift. In International conference\\r\\non machine learning. PMLR, 448–456.\\r\\n[24] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and\\r\\nJaegul Choo. 2022. Reversible instance normalization for accurate time-series\\r\\nforecasting against distribution shift. In International Conference on Learning\\r\\nRepresentations.\\r\\n[25] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti\\x02mization. arXiv preprint arXiv:1412.6980 (2014).\\r\\n[26] Durk P Kingma and Prafulla Dhariwal. 2018. Glow: Generative flow with in\\x02vertible 1x1 convolutions. Advances in neural information processing systems 31\\r\\n(2018).\\r\\n[27] Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. 2020. Normalizing flows:\\r\\nAn introduction and review of current methods. IEEE transactions on pattern\\r\\nanalysis and machine intelligence 43, 11 (2020), 3964–3979.\\r\\n[28] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling\\r\\nLong- and Short-Term Temporal Patterns with Deep Neural Networks. In SIGIR.\\r\\n95–104.\\r\\n[29] Bryan Lim and Stefan Zohren. 2021. Time-series forecasting with deep learning:\\r\\na survey. Philosophical Transactions of the Royal Society A: Mathematical, Physical\\r\\nand Engineering Sciences 379, 2194 (feb 2021), 20200209. doi:10.1098/rsta.2020.0209\\r\\n[30] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS: Differentiable\\r\\nArchitecture Search. In International Conference on Learning Representations.\\r\\n[31] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and\\r\\nQiang Xu. 2022. SCINet: time series modeling and forecasting with sample\\r\\nconvolution and interaction. Advances in Neural Information Processing Systems\\r\\n35 (2022), 5816–5828.\\r\\n[32] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and\\r\\nMingsheng Long. 2023. iTransformer: Inverted Transformers Are Effective for\\r\\nTime Series Forecasting. CoRR abs/2310.06625 (2023).\\r\\n[33] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary\\r\\nTransformers: Exploring the Stationarity in Time Series Forecasting. In Advances\\r\\nin Neural Information Processing Systems.\\r\\n[34] Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, and\\r\\nEnhong Chen. 2023. Adaptive Normalization for Non-stationary Time Series\\r\\nForecasting: A Temporal Slice Perspective. In Thirty-seventh Conference on Neural\\r\\nInformation Processing Systems.\\r\\n[35] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. 2014.\\r\\nTraffic flow prediction with big data: a deep learning approach. IEEE Transactions\\r\\non Intelligent Transportation Systems 16, 2 (2014), 865–873.\\r\\n[36] LIU Minhao, Ailing Zeng, Muxi Chen, Zhijian Xu, LAI Qiuxia, Lingna Ma, and\\r\\nQiang Xu. 2022. SCINet: Time Series Modeling and Forecasting with Sample\\r\\nConvolution and Interaction. In Advances in Neural Information Processing Sys\\x02tems.\\r\\n[37] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.\\r\\nA Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In\\r\\nInternational Conference on Learning Representations.\\r\\n[38] Eduardo Ogasawara, Leonardo C Martinez, Daniel De Oliveira, Geraldo Zimbrão,\\r\\nGisele L Pappa, and Marta Mattoso. 2010. Adaptive normalization: A novel data\\r\\nnormalization approach for non-stationary time series. In The 2010 International\\r\\nJoint Conference on Neural Networks (IJCNN). IEEE, 1–8.\\r\\n[39] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2020. N\\x02BEATS: Neural basis expansion analysis for interpretable time series forecasting.\\r\\nIn International Conference on Learning Representations.\\r\\n[40] George Papamakarios, Eric T Nalisnick, Danilo Jimenez Rezende, Shakir Mo\\x02hamed, and Balaji Lakshminarayanan. 2021. Normalizing Flows for Probabilistic\\r\\nModeling and Inference. J. Mach. Learn. Res. 22, 57 (2021), 1–64.\\r\\n[41] George Papamakarios, Theo Pavlakou, and Iain Murray. 2017. Masked autore\\x02gressive flow for density estimation. Advances in neural information processing\\r\\nsystems 30 (2017).\\r\\n[42] Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and\\r\\nAlexandros Iosifidis. 2019. Deep adaptive input normalization for time series\\r\\nforecasting. IEEE transactions on neural networks and learning systems 31, 9 (2019),\\r\\n3760–3765.\\r\\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\r\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\\r\\nPytorch: An imperative style, high-performance deep learning library. Advances\\r\\nin neural information processing systems 32 (2019).\\r\\n[44] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. 2019.\\r\\nMeta-learning with implicit gradients. Advances in neural information processing\\r\\nsystems 32 (2019).\\r\\n[45] Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella,\\r\\nYuyang Wang, and Tim Januschowski. 2018. Deep state space models for time\\r\\nseries forecasting. Advances in neural information processing systems 31 (2018),\\r\\n7785–7794.\\r\\n[46] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\r\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter\\x02national Journal of Forecasting 36, 3 (2020), 1181–1191.\\r\\n[47] Sheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer.\\r\\n2020. Powernorm: Rethinking batch normalization in transformers. In Interna\\x02tional Conference on Machine Learning. PMLR, 8741–8751.\\r\\n[48] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. 2017. A review on bilevel\\r\\noptimization: from classical to evolutionary approaches and applications. IEEE\\r\\nTransactions on Evolutionary Computation 22, 2 (2017), 276–295.\\r\\n[49] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2016. Instance normaliza\\x02tion: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022\\r\\n(2016).\\r\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\r\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\\r\\nyou need. In Advances in neural information processing systems. 5998–6008.\\r\\n[51] Peter Whittle. 1951. Hypothesis testing in time series analysis. Almqvist & Wiksells\\r\\nboktr.\\r\\n[52] Peter Whittle. 1963. Prediction and regulation by linear least-square methods.\\r\\nEnglish Universities Press.\\r\\n[53] Peter R Winters. 1960. Forecasting sales by exponentially weighted moving\\r\\naverages. Management science 6, 3 (1960), 324–342.\\r\\n[54] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022.\\r\\nDeeptime: Deep time-index meta-learning for non-stationary time-series fore\\x02casting. arXiv preprint arXiv:2207.06046 (2022).\\r\\n[55] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De\\x02composition transformers with auto-correlation for long-term series forecasting.\\r\\nAdvances in Neural Information Processing Systems 34 (2021), 22419–22430.\\r\\n[56] Kun Yi, Jingru Fei, Qi Zhang, Hui He, Shufeng Hao, Defu Lian, and Wei Fan.\\r\\n2024. FilterNet: Harnessing Frequency Filters for Time Series Forecasting. In\\nIN-Flow: Instance Normalization Flow for Non-stationary Time Series Forecasting KDD ’25, August 3–7, 2025, Toronto, ON, Canada.\\r\\nThe Thirty-eighth Annual Conference on Neural Information Processing Systems.\\r\\nhttps://openreview.net/forum?id=ugL2D9idAD\\r\\n[57] Kun Yi, Qi Zhang, Longbing Cao, Shoujin Wang, Guodong Long, Liang Hu, Hui\\r\\nHe, Zhendong Niu, Wei Fan, and Hui Xiong. 2023. A Survey on Deep Learning\\r\\nbased Time Series Analysis with Frequency Transformation. CoRR abs/2302.02173\\r\\n(2023).\\r\\n[58] Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Long\\x02bing Cao, and Zhendong Niu. 2024. FourierGNN: Rethinking multivariate time\\r\\nseries forecasting from a pure graph perspective. Advances in Neural Information\\r\\nProcessing Systems 36 (2024).\\r\\n[59] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An,\\r\\nDefu Lian, Longbing Cao, and Zhendong Niu. 2023. Frequency-domain MLPs\\r\\nare More Effective Learners in Time Series Forecasting. In NeurIPS.\\r\\n[60] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2022. Are Transformers\\r\\nEffective for Time Series Forecasting? arXiv preprint arXiv:2205.13504 (2022).\\r\\n[61] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers\\r\\nEffective for Time Series Forecasting?. In AAAI. AAAI Press, 11121–11128.\\r\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\r\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se\\x02quence time-series forecasting. In Proceedings of the AAAI Conference on Artificial\\r\\nIntelligence, Vol. 35. 11106–11115.\\r\\n[63] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.\\r\\nFEDformer: Frequency enhanced decomposed transformer for long-term series\\r\\nforecasting. arXiv preprint arXiv:2201.12740 (2022).\\r\\nA More Baseline Details\\r\\nAs aforementioned, our decoupled formulation with IN-Flow is\\r\\nmodel-agnostic such that it can be integrated with any deep time\\r\\nseries forecasting models. To verify the effectiveness, we consider\\r\\nseveral state-of-the-art models as backbones. Mainly we reproduce\\r\\nthe model by following their open source GitHub links. The detailed\\r\\nimplementation details are as follows:\\r\\n• iTransformer [32] inverts the structure of Transformer\\r\\nwithout modifying any existing modules by encoding indi\\x02vidual series into variate tokens. These tokens are utilized\\r\\nby the attention mechanism to capture multivariate correla\\x02tions and FFNs are adopted for each variate token to learn\\r\\nnonlinear representations. The official implementation is\\r\\navailable at Github2. We follow the default parameters in\\r\\nthe original paper in our experiments.\\r\\n• PatchTST [37] divides time series data into subseries-level\\r\\npatches to extract local semantic information and adopts a\\r\\nchannel-independence strategy where each channel shares\\r\\nthe same embedding and Transformer weights across all the\\r\\nseries. The official implementation is available at Github3.\\r\\nWe follow the default parameters in the original paper.\\r\\n• Autoformer. [55] considers the auto-correlation series and\\r\\nuses auto-correlation to replace the self-attention. We use\\r\\nthe official open-source code of Autoformer4[55]. For hyper\\x02parameter, we follow the settings of the original paper includ\\x02ing number of heads, moving average of Auto-correlation\\r\\nlayer, dimension of Auto-Correlation layer, dimension of\\r\\nfeed-forward layer, etc. In addition, we only adopt the posi\\x02tional embeddings and remove time embeddings.\\r\\n• N-BEATS. We take the notable reproduced codes of N\\x02BEATS5\\r\\n. Note that N-BEATS is an univariate time series\\r\\nforecasting model. To align with the input and output of\\r\\nmultivariate settings (like Autoformer), we transform the\\r\\nmultivariate lookback windows from dimension 𝐵 ×𝐿 ×𝐷 to\\r\\n2https://github.com/thuml/iTransformer\\r\\n3https://github.com/yuqinie98/PatchTST\\r\\n4https://github.com/thuml/Autoformer\\r\\n5https://github.com/ElementAI/N-BEATS\\r\\n(𝐵×𝐿)×𝐷, where 𝐵 is batch size, 𝐿 is the lookback length and\\r\\n𝐷 is number of series. The horizon windows adopt the same\\r\\nstrategy. In the mean time, for multivatiate forecasting on\\r\\ndifferent datasets, we set the batch size 𝐵 equal to [1024/𝐷]\\r\\nto avoid very large batch size that influences training.\\r\\nTo overcome the distribution shift in time series forecasting, we take\\r\\nstate-of-the-art model-agnostic normalization techniques, RevIN\\r\\n[24], Dish-TS [15] and SAN [34] as the main baselines. We adopt\\r\\nthe official implementation6 of RevIN, the official implementation7\\r\\nof Dish-TS, the official implementation8 of SAN. We run our models\\r\\nin three different seeds and report the average performances.\\r\\nWe also consider another recent method that’s designed for the\\r\\nnon-stationarity of transformer-based forecasting models, namely\\r\\nNonstationary Transformers [33] for comparison. To couple with\\r\\nNonstationary Transformers, we take three model as backbones,\\r\\nincluding vanilla Transformer [50], Informer [62] and Autoformer\\r\\n[55]. We adopt the official implementation9 of Nonstationary Trans\\x02formers to set the parameters of nonstationary layers, including\\r\\nnonstationary hidden layers as 2 and hidden size as 128. Specifically,\\r\\n• Nonstationary Transformer. Based on the implementa\\x02tion of vanilla Transformer [50], Nonstationary Transformer\\r\\napplies series stationarization layer and revise the raw self\\r\\nattention into the de-stationary self attention [33]. Other hy\\x02perparameters are set the same as the vanilla Transformer.\\r\\n• Nonstationary Informer. Based on the original implemen\\x02tation10 of Informer [62], Nonstationary Informer also adds\\r\\nseries stationarization layer and changes the PropSparse\\r\\nattention [62] into the de-stationary PropSparse attention\\r\\nversion [33]. Other hyperparameters are the same as the\\r\\nInformer.\\r\\n• Nonstationary Autoformer. Since Autoformer replaces\\r\\nclassic self attention layers with auto-correlation layers [55],\\r\\nNonstationary Autoformer also adds series stationarization\\r\\nlayers and adopts the revised de-stationary auto-correlation\\r\\nlayers [33] for learning. And other hyperparameters are the\\r\\nsame as the Autoformer.\\r\\nNotably, when we integrate the backbones with our IN-Flow,\\r\\nwe set all the other hyperparameters the same as the backbones\\r\\nunder the same experimental settings (including the random seeds)\\r\\nin order to conduct a fair comparison.\\r\\nB More Details on Synthetic Data\\r\\nWe make two synthetic datasets, namely Synthetic-1 and Synthetic\\x022, We mainly follow the simple rules mentioned in Section 5.1, and\\r\\naccordingly evaluate the performance on the synthetic datasets.\\r\\nNow we illustrate the specific details of the choice of P𝑢 and\\r\\n𝜏 for the three datasets. Specifically, in the 𝑢-th segment, 𝑠𝑡 =\\r\\n𝐴𝑢 cos(2𝜋\\r\\n1\\r\\n𝑇𝑢\\r\\n𝑡 +𝐵𝑢) +𝐶𝑢 where the distribution P𝑢 of 𝑢-th segment\\r\\nis controlled by the amplitude parameters𝐴𝑢, the period parameters\\r\\n𝑇𝑢, the phase parameters 𝐵𝑢 and the level parameters 𝐶𝑢. In order\\r\\nto make time series always shifted, we randomly sample the values\\r\\n6https://github.com/ts-kim/RevIN\\r\\n7https://github.com/weifantt/Dish-TS\\r\\n8https://github.com/icantnamemyself/SAN\\r\\n9https://github.com/thuml/Nonstationary_Transformers/\\r\\n10https://github.com/zhouhaoyi/Informer2020\\nKDD ’25, August 3–7, 2025, Toronto, ON, Canada. Wei Fan et al.\\r\\n0 1000 2000 3000 4000 5000\\r\\n−2500\\r\\n0\\r\\n2500\\r\\n5000\\r\\nSynthetic-1\\r\\nFigure 7: An example series of Synthetic-1 dataset.\\r\\nBatchNorm\\r\\nCoupling \\r\\nLayer\\r\\nSplit\\r\\nPermute\\r\\n× \\r\\n(a) RealNVP\\r\\nCoupling \\r\\nLayer\\r\\nSplit\\r\\nPermute\\r\\n× \\r\\n(b) RealNVP-c\\r\\nInstanceNorm\\r\\n(optional)\\r\\nCoupling \\r\\nLayer\\r\\nSplit\\r\\nPermute\\r\\nInstanceNorm\\r\\n× \\r\\n(c) IN-Flow-T (post-norm)\\r\\nInstanceNorm\\r\\nInstanceNorm\\r\\n(optional)\\r\\nCoupling \\r\\nLayer\\r\\nSplit\\r\\nPermute\\r\\n× \\r\\n(d) IN-Flow\\r\\nFigure 8: Four kinds of variants in ablation studies, where 𝐾 stands for the number of blocks in flows; “split” and “permute”\\r\\nstand for two basic operations for coupling layers to assure reversibility [10, 11].\\r\\nof 𝐴𝑢,𝑇𝑢, 𝐵𝑢,𝐶𝑢 to make every segment shifted constantly. We\\r\\nlet the amplitude 𝐴𝑢 randomly sampled from (−1000, 1000), the\\r\\nperiod 𝑇𝑢 randomly sampled from (0, 100), the phase 𝐵𝑢 randomly\\r\\nsampled from (0, 100) and the level 𝐶𝑢 randomly sampled from\\r\\n(−⌈𝑡/100⌉ ∗ 50, −⌈𝑡/100⌉ ∗ 100), where 𝑡 is the timestamp. Then, we\\r\\ncan control the distribution changing frequency through adjusting\\r\\n𝜏. We set 𝑇 = 10, 000 to synthetic 10,000 points for each series, in\\r\\norder to make enough data for training and evaluation. We take\\r\\nthe first 6000 points for training, and 2000 points for validation\\r\\nand another 2000 points for test. We make Synthetic-1 dataset by\\r\\nsetting 𝜏 = 24 and we make the Synthetic-2 dataset by setting\\r\\n𝜏 = 12 in order to model different distribution shift situations. We\\r\\nlet each dataset include 5 distinct series, each of which is generated\\r\\nby setting different seeds. Figure 7 demonstrates an example series\\r\\nfrom the Synthetic-1 dataset respectively, where we show the first\\r\\nfive thousand points for visualization.\\r\\nC More Details of Evaluation on Real-world\\r\\nData\\r\\nC.1 More Implementation Details\\r\\nFor a stable evaluation, we rerun all the models under four differ\\x02ent seeds and report the average MSE/MAE on the testset of each\\r\\ndataset. In the main experiments of time series forecasting, we let\\r\\nthe lookback window and the horizon window have the same length,\\r\\nwhere we gradually prolong the length as {48, 96, 168, 336} to ac\\x02commodate short-term/long-term forecasting settings. We train all\\r\\nthe models using L2 loss and Adam [25] optimizer with learning\\r\\nrate of [1e-4, 1e-3]. The batchsize is set as 1024 for N-BEATS and\\r\\n128 for others. For every model, we use early stopping with the pa\\x02tience as five steps. For IN-Flow, we traverse the hyperparameters\\r\\nin the validation set: with the number of blocks in coupling layers\\r\\nas {2, 8, 16}, hidden size as 128. The learning rate for the transfor\\x02mation module is set as 1e-4. All the experiments are implemented\\r\\nwith PyTorch [43] on single NVIDIA A100 40GB GPU.\\r\\nC.2 More Evaluation Metric Details\\r\\nTo directly reflects the shift in time series, all experiments and\\r\\nevaluations are conducted on original data without any data nor\\x02malization or data scaling, which is different from experimental\\r\\nsettings of [55, 62] which use z-score normalization to process data\\r\\nbefore training and evaluation.\\r\\nIn evaluation, we evaluate the time series forecasting perfor\\x02mances on the mean squared error (MSE) and mean absolute error\\r\\n(MAE). Note that our experiments and evaluations are on original\\r\\ndata; thus the reported metrics on real-world data are scaled for\\r\\nreadability. Specifically, for MSE, the report metrics are scaled by\\r\\n1𝑒\\r\\n−1 on ETTh1 dataset and ETTm2 dataset, by 1𝑒−4 on Weather\\r\\nand Electricity dataset, by 1𝑒\\r\\n−6 on CAISO dataset and by 1𝑒−6 on\\r\\nNordPool dataset. For MAE, the reported metrics are not scaled\\r\\nfor ETT series datasets. In contrast, the reported MAE is scaled by\\r\\n1𝑒\\r\\n−1 on Weather dataset, by 1𝑒−3 on CAISO dataset and by 1𝑒−2\\r\\non NordPool dataset.'},\n",
       " {'name': '1606.08415v5.pdf',\n",
       "  'content': 'GAUSSIAN ERROR LINEAR UNITS (GELUS)\\r\\nDan Hendrycks∗\\r\\nUniversity of California, Berkeley\\r\\nhendrycks@berkeley.edu\\r\\nKevin Gimpel\\r\\nToyota Technological Institute at Chicago\\r\\nkgimpel@ttic.edu\\r\\nABSTRACT\\r\\nWe propose the Gaussian Error Linear Unit (GELU), a high-performing neural\\r\\nnetwork activation function. The GELU activation function is xΦ(x), where Φ(x)\\r\\nthe standard Gaussian cumulative distribution function. The GELU nonlinearity\\r\\nweights inputs by their value, rather than gates inputs by their sign as in ReLUs\\r\\n(x1x>0). We perform an empirical evaluation of the GELU nonlinearity against\\r\\nthe ReLU and ELU activations and find performance improvements across all\\r\\nconsidered computer vision, natural language processing, and speech tasks.\\r\\n1 INTRODUCTION\\r\\nEarly artificial neurons utilized binary threshold units (Hopfield, 1982; McCulloch & Pitts, 1943).\\r\\nThese hard binary decisions are smoothed with sigmoid activations, enabling a neuron to have a “fir\\x02ing rate” interpretation and to train with backpropagation. But as networks became deeper, training\\r\\nwith sigmoid activations proved less effective than the non-smooth, less-probabilistic ReLU (Nair &\\r\\nHinton, 2010) which makes hard gating decisions based upon an input’s sign. Despite having less of\\r\\na statistical motivation, the ReLU remains a competitive engineering solution which often enables\\r\\nfaster and better convergence than sigmoids. Building on the successes of ReLUs, a recent modifi\\x02cation called ELUs (Clevert et al., 2016) allows a ReLU-like nonlinearity to output negative values\\r\\nwhich sometimes increases training speed. In all, the activation choice has remained a necessary\\r\\narchitecture decision for neural networks lest the network be a deep linear classifier.\\r\\nDeep nonlinear classifiers can fit their data so well that network designers are often faced with the\\r\\nchoice of including stochastic regularizer like adding noise to hidden layers or applying dropout (Sri\\x02vastava et al., 2014), and this choice remains separate from the activation function. Some stochastic\\r\\nregularizers can make the network behave like an ensemble of networks, a pseudoensemble (Bach\\x02man et al., 2014), and can lead to marked accuracy increases. For example, the stochastic regular\\x02izer dropout creates a pseudoensemble by randomly altering some activation decisions through zero\\r\\nmultiplication. Nonlinearities and dropout thus determine a neuron’s output together, yet the two\\r\\ninnovations have remained distinct. More, neither subsumed the other because popular stochastic\\r\\nregularizers act irrespectively of the input and nonlinearities are aided by such regularizers.\\r\\nIn this work, we introduce a new nonlinearity, the Gaussian Error Linear Unit (GELU). It relates\\r\\nto stochastic regularizers in that it is the expectation of a modification to Adaptive Dropout (Ba &\\r\\nFrey, 2013). This suggests a more probabilistic view of a neuron’s output. We find that this novel\\r\\nnonlinearity matches or exceeds models with ReLUs or ELUs across tasks from computer vision,\\r\\nnatural language processing, and automatic speech recognition.\\r\\n2 GELU FORMULATION\\r\\nWe motivate our activation function by combining properties from dropout, zoneout, and ReLUs.\\r\\nFirst note that a ReLU and dropout both yield a neuron’s output with the ReLU deterministi\\x02cally multiplying the input by zero or one and dropout stochastically multiplying by zero. Also,\\r\\na new RNN regularizer called zoneout stochastically multiplies inputs by one (Krueger et al.,\\r\\n2016). We merge this functionality by multiplying the input by zero or one, but the values of\\r\\nthis zero-one mask are stochastically determined while also dependent upon the input. Specif\\x02ically, we can multiply the neuron input x by m ∼ Bernoulli(Φ(x)), where Φ(x) = P(X ≤\\r\\n∗Work done while the author was at TTIC. Code available at github.com/hendrycks/GELUs\\r\\n1\\r\\narXiv:1606.08415v5 [cs.LG] 6 Jun 2023\\nx), X ∼ N (0, 1) is the cumulative distribution function of the standard normal distribution.\\r\\nWe choose this distribution since neuron inputs tend to follow a normal distribution, especially\\r\\nwith Batch Normalization. In this setting, inputs have a higher probability of being “dropped”\\r\\nas x decreases, so the transformation applied to x is stochastic yet depends upon the input.\\r\\n4 3 2 1 0 1 2 3\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3 GELU\\r\\nReLU\\r\\nELU\\r\\nFigure 1: The GELU (µ = 0, σ = 1), ReLU, and ELU\\r\\n(α = 1).\\r\\nMasking inputs in this fashion re\\x02tains non-determinism but maintains\\r\\ndependency upon the input value. A\\r\\nstochastically chosen mask amounts to\\r\\na stochastic zero or identity transforma\\x02tion of the input. This is much like\\r\\nAdaptive Dropout (Ba & Frey, 2013),\\r\\nbut adaptive dropout is used in tandem\\r\\nwith nonlinearities and uses a logistic\\r\\nnot standard normal distribution. We\\r\\nfound that it is possible to train com\\x02petitive MNIST and TIMIT networks\\r\\nsolely with this stochastic regularizer,\\r\\nall without using any nonlinearity.\\r\\nWe often want a deterministic decision\\r\\nfrom a neural network, and this gives\\r\\nrise to our new nonlinearity. The non\\x02linearity is the expected transformation\\r\\nof the stochastic regularizer on an input x, which is Φ(x) × Ix + (1 − Φ(x)) × 0x = xΦ(x).\\r\\nLoosely, this expression states that we scale x by how much greater it is than other inputs. Since the\\r\\ncumulative distribution function of a Gaussian is often computed with the error function, we define\\r\\nthe Gaussian Error Linear Unit (GELU) as\\r\\nGELU(x) = xP(X ≤ x) = xΦ(x) = x ·\\r\\n1\\r\\n2\\r\\nh\\r\\n1 + erf(x/√\\r\\n2)i.\\r\\nWe can approximate the GELU with\\r\\n0.5x(1 + tanh[p2/π(x + 0.044715x\\r\\n3\\r\\n)])\\r\\nor\\r\\nxσ(1.702x),\\r\\nif greater feedforward speed is worth the cost of exactness.\\r\\nWe could use different CDFs. For example we could use Logistic Distribution CDF σ(x) to get\\r\\nwhat we call the Sigmoid Linear Unit (SiLU) xσ(x). We could use the CDF of N (µ, σ2) and have\\r\\nµ and σ be learnable hyperparameters, but throughout this work we simply let µ = 0 and σ = 1.\\r\\nConsequently, we do not introduce any new hyperparameters in the following experiments. In the\\r\\nnext section, we show that the GELU exceeds ReLUs and ELUs across numerous tasks.\\r\\n3 GELU EXPERIMENTS\\r\\nWe evaluate the GELU, ELU, and ReLU on MNIST classification (grayscale images with 10 classes,\\r\\n60k training examples and 10k test examples), MNIST autoencoding, Tweet part-of-speech tagging\\r\\n(1000 training, 327 validation, and 500 testing tweets), TIMIT frame recognition (3696 training,\\r\\n1152 validation, and 192 test audio sentences), and CIFAR-10/100 classification (color images with\\r\\n10/100 classes, 50k training and 10k test examples). We do not evaluate nonlinearities like the\\r\\nLReLU because of its similarity to ReLUs (see Maas et al. (2013) for a description of LReLUs).\\r\\n3.1 MNIST CLASSIFICATION\\r\\nLet us verify that this nonlinearity competes with previous activation functions by replicating an\\r\\nexperiment from Clevert et al. (2016). To this end, we train a fully connected neural network with\\r\\nGELUs (µ = 0, σ = 1), ReLUs, and ELUs (α = 1). Each 8-layer, 128 neuron wide neural\\r\\nnetwork is trained for 50 epochs with a batch size of 128. This experiment differs from those of\\r\\n2\\n0 10 20 30 40 50\\r\\nEpoch\\r\\n0.00\\r\\n0.02\\r\\n0.04\\r\\n0.06\\r\\n0.08\\r\\n0.10\\r\\n0.12\\r\\n0.14\\r\\nLog Loss (no dropout)\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\n0 10 20 30 40 50\\r\\nEpoch\\r\\n0.1\\r\\n0.2\\r\\n0.3\\r\\n0.4\\r\\n0.5\\r\\nLog Loss (dropout keep rate = 0.5)\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\nFigure 2: MNIST Classification Results. Left are the loss curves without dropout, and right are\\r\\ncurves with a dropout rate of 0.5. Each curve is the the median of five runs. Training set log losses\\r\\nare the darker, lower curves, and the fainter, upper curves are the validation set log loss curves.\\r\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\r\\nNoise Strength\\r\\n0.2\\r\\n0.3\\r\\n0.4\\r\\n0.5\\r\\n0.6\\r\\n0.7\\r\\n0.8\\r\\n0.9\\r\\n1.0\\r\\nTest Set Accuracy\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\r\\nNoise Strength\\r\\n0\\r\\n5\\r\\n10\\r\\n15\\r\\n20\\r\\n25\\r\\nTest Set Log Loss\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\nFigure 3: MNIST Robustness Results. Using different nonlinearities, we record the test set accuracy\\r\\ndecline and log loss increase as inputs are noised. The MNIST classifier trained without dropout\\r\\nreceived inputs with uniform noise Unif[−a, a] added to each example at different levels a, where\\r\\na = 3 is the greatest noise strength. Here GELUs display robustness matching or exceeding ELUs\\r\\nand ReLUs.\\r\\nClevert et al. in that we use the Adam optimizer (Kingma & Ba, 2015) rather than stochastic gra\\x02dient descent without momentum, and we also show how well nonlinearities cope with dropout.\\r\\nWeights are initialized with unit norm rows, as this has positive impact on each nonlinearity’s per\\x02formance (Hendrycks & Gimpel, 2016; Mishkin & Matas, 2016; Saxe et al., 2014). Note that we\\r\\ntune over the learning rates {10−3, 10−4, 10−5} with 5k validation examples from the training set\\r\\nand take the median results for five runs. Using these classifiers, we demonstrate in Figure 3 that\\r\\nclassifiers using a GELU can be more robust to noised inputs. Figure 2 shows that the GELU tends\\r\\nto have the lowest median training log loss with and without dropout. Consequently, although the\\r\\nGELU is inspired by a different stochastic process, it comports well with dropout.\\r\\n3.2 MNIST AUTOENCODER\\r\\nWe now consider a self-supervised setting and train a deep autoencoder on MNIST (Desjardins\\r\\net al., 2015). To accomplish this, we use a network with layers of width 1000, 500, 250, 30, 250,\\r\\n500, 1000, in that order. We again use the Adam optimizer and a batch size of 64. Our loss is\\r\\nthe mean squared loss. We vary the learning rate from 10−3to 10−4. We also tried a learning\\r\\nrate of 0.01 but ELUs diverged, and GELUs and RELUs converged poorly. The results in Figure 4\\r\\nindicate the GELU accommodates different learning rates and significantly outperforms the other\\r\\nnonlinearities.\\r\\n3\\n0 50 100 150 200 250\\r\\nEpoch\\r\\n0.004\\r\\n0.006\\r\\n0.008\\r\\n0.010\\r\\n0.012\\r\\n0.014\\r\\n0.016\\r\\nReconstruction Error (lr = 1e-3)\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\n0 50 100 150 200 250\\r\\nEpoch\\r\\n0.004\\r\\n0.006\\r\\n0.008\\r\\n0.010\\r\\n0.012\\r\\n0.014\\r\\n0.016\\r\\nReconstruction Error (lr = 1e-4)\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\nFigure 4: MNIST Autoencoding Results. Each curve is the median of three runs. Left are loss\\r\\ncurves for a learning rate of 10−3, and the right figure is for a 10−4learning rate. Light, thin curves\\r\\ncorrespond to test set log losses.\\r\\n0 5 10 15 20 25 30\\r\\nEpoch\\r\\n1.3\\r\\n1.4\\r\\n1.5\\r\\n1.6\\r\\n1.7\\r\\n1.8\\r\\nLog Loss\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\nFigure 5: TIMIT Frame Classification. Learning curves show training set convergence, and the\\r\\nlighter curves show the validation set convergence.\\r\\n3.3 TWITTER POS TAGGING\\r\\nMany datasets in natural language processing are relatively small, so it is important that an activation\\r\\ngeneralize well from few examples. To meet this challenge we compare the nonlinearities on POS\\x02annotated tweets (Gimpel et al., 2011; Owoputi et al., 2013) which contain 25 tags. The tweet\\r\\ntagger is simply a two-layer network with pretrained word vectors trained on a corpus of 56 million\\r\\ntweets (Owoputi et al., 2013). The input is the concatenation of the vector of the word to be tagged\\r\\nand those of its left and right neighboring words. Each layer has 256 neurons, a dropout keep\\r\\nprobability of 0.8, and the network is optimized with Adam while tuning over the learning rates\\r\\n{10−3, 10−4, 10−5}. We train each network five times per learning rate, and the median test set\\r\\nerror is 12.57% for the GELU, 12.67% for the ReLU, and 12.91% for the ELU.\\r\\n3.4 TIMIT FRAME CLASSIFICATION\\r\\nOur next challenge is phone recognition with the TIMIT dataset which has recordings of 680\\r\\nspeakers in a noiseless environment. The system is a five-layer, 2048-neuron wide classifier as\\r\\nin (Mohamed et al., 2012) with 39 output phone labels and a dropout rate of 0.5 as in (Srivas\\x02tava, 2013). This network takes as input 11 frames and must predict the phone of the center\\r\\n4\\n0 25 50 75 100 125 150 175 200\\r\\nEpoch\\r\\n0\\r\\n2\\r\\n4\\r\\n6\\r\\n8\\r\\n10\\r\\nClassification Error (%)\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\nFigure 6: CIFAR-10 Results. Each curve is the median of three runs. Learning curves show training\\r\\nset error rates, and the lighter curves show the test set error rates.\\r\\nframe using 26 MFCC, energy, and derivative features per frame. We tune over the learning rates\\r\\n{10−3, 10−4, 10−5} and optimize with Adam. After five runs per setting, we obtain the median\\r\\ncurves in Figure 5, and median test error chosen at the lowest validation error is 29.3% for the\\r\\nGELU, 29.5% for the ReLU, and 29.6% for the ELU.\\r\\n3.5 CIFAR-10/100 CLASSIFICATION\\r\\nNext, we demonstrate that for more intricate architectures the GELU nonlinearity again outperforms\\r\\nother nonlinearities. We evaluate this activation function using CIFAR-10 and CIFAR-100 datasets\\r\\n(Krizhevsky, 2009) on shallow and deep convolutional neural networks, respectively.\\r\\nOur shallower convolutional neural network is a 9-layer network with the architecture and training\\r\\nprocedure from Salimans & Kingma (2016) while using batch normalization to speed up training.\\r\\nThe architecture is described in appendix A and recently obtained state of the art on CIFAR-10\\r\\nwithout data augmentation. No data augmentation was used to train this network. We tune over\\r\\nthe learning initial rates {10−3, 10−4, 10−5} with 5k validation examples then train on the whole\\r\\ntraining set again based upon the learning rate from cross validation. The network is optimized with\\r\\nAdam for 200 epochs, and at the 100th epoch the learning rate linearly decays to zero. Results are\\r\\nshown in Figure 6, and each curve is a median of three runs. Ultimately, the GELU obtains a median\\r\\nerror rate of 7.89%, the ReLU obtains 8.16%, and the ELU obtains 8.41%.\\r\\nNext we consider a wide residual network on CIFAR-100 with 40 layers and a widening factor of 4\\r\\n(Zagoruyko & Komodakis, 2016). We train for 50 epochs with the learning rate schedule described\\r\\nin (Loshchilov & Hutter, 2016) (T0 = 50, η = 0.1) with Nesterov momentum, and with a dropout\\r\\nkeep probability of 0.7. Some have noted that ELUs have an exploding gradient with residual\\r\\nnetworks (Shah et al., 2016), and this is alleviated with batch normalization at the end of a residual\\r\\nblock. Consequently, we use a Conv-Activation-Conv-Activation-BatchNorm block architecture\\r\\nto be charitable to ELUs. Over three runs we obtain the median convergence curves in Figure 7.\\r\\nMeanwhile, the GELU achieves a median error of 20.74%, the ReLU obtains 21.77% (without our\\r\\nchanges described above, the original 40-4 WideResNet with a ReLU obtains 22.89% (Zagoruyko\\r\\n& Komodakis, 2016)), and the ELU obtains 22.98%.\\r\\n4 DISCUSSION\\r\\nAcross several experiments, the GELU outperformed previous nonlinearities, but it bears semblance\\r\\nto the ReLU and ELU in other respects. For example, as σ → 0 and if µ = 0, the GELU becomes\\r\\na ReLU. More, the ReLU and GELU are equal asymptotically. In fact, the GELU can be viewed\\r\\nas a way to smooth a ReLU. To see this, recall that ReLU = max(x, 0) = x1(x > 0) (where\\r\\n5\\n0 10 20 30 40 50\\r\\nEpoch\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\n2.5\\r\\n3.0\\r\\nLog Loss\\r\\nGELU\\r\\nELU\\r\\nReLU\\r\\nFigure 7: CIFAR-100 Wide Residual Network Results. Learning curves show training set conver\\x02gence with dropout on, and the lighter curves show the test set convergence with dropout off.\\r\\n1 is the indicator function), while the GELU is xΦ(x) if µ = 0, σ = 1. Then the CDF is a\\r\\nsmooth approximation to the binary function the ReLU uses, like how the sigmoid smoothed binary\\r\\nthreshold activations. Unlike the ReLU, the GELU and ELU can be both negative and positive. In\\r\\nfact, if we used the cumulative distribution function of the standard Cauchy distribution, then the\\r\\nELU (when α = 1/π) is asymptotically equal to xP(C ≤ x), C ∼ Cauchy(0, 1) for negative\\r\\nvalues and for positive values is xP(C ≤ x) if we shift the line down by 1/π. These are some\\r\\nfundamental relations to previous nonlinearities.\\r\\nHowever, the GELU has several notable differences. This non-convex, non-monotonic function is\\r\\nnot linear in the positive domain and exhibits curvature at all points. Meanwhile ReLUs and ELUs,\\r\\nwhich are convex and monotonic activations, are linear in the positive domain and thereby can lack\\r\\ncurvature. As such, increased curvature and non-monotonicity may allow GELUs to more easily\\r\\napproximate complicated functions than can ReLUs or ELUs. Also, since ReLU(x) = x1(x > 0)\\r\\nand GELU(x) = xΦ(x) if µ = 0, σ = 1, we can see that the ReLU gates the input depending\\r\\nupon its sign, while the GELU weights its input depending upon how much greater it is than other\\r\\ninputs. In addition and significantly, the GELU has a probabilistic interpretation given that it is the\\r\\nexpectation of a stochastic regularizer.\\r\\nWe also have two practical tips for using the GELU. First we advise using an optimizer with mo\\x02mentum when training with a GELU, as is standard for deep neural networks. Second, using a\\r\\nclose approximation to the cumulative distribution function of a Gaussian distribution is impor\\x02tant. A sigmoid function σ(x) = 1/(1 + e\\r\\n−x\\r\\n) is an approximation of a cumulative distribu\\x02tion function of a normal distribution. However, we found that a Sigmoid Linear Unit (SiLU)\\r\\nxσ(x) performs worse than GELUs but usually better than ReLUs and ELUs, so our SiLU is\\r\\nalso a reasonable nonlinearity choice. Instead of using a xσ(x) to approximate Φ(x), we used\\r\\n0.5x(1 + tanh[p\\r\\n2/π(x + 0.044715x\\r\\n3\\r\\n)]) (Choudhury, 2014)1 or xσ(1.702x). Both are sufficiently\\r\\nfast, easy-to-implement approximations, and we used the former in every experiment in this paper.\\r\\n5 CONCLUSION\\r\\nFor the numerous datasets evaluated in this paper, the GELU exceeded the accuracy of the ELU and\\r\\nReLU consistently, making it a viable alternative to previous nonlinearities.\\r\\n1Thank you to Dmytro Mishkin for bringing an approximation like this to our attention.\\r\\n6\\nACKNOWLEDGMENT\\r\\nWe would like to thank NVIDIA Corporation for donating several TITAN X GPUs used in this\\r\\nresearch.\\r\\nREFERENCES\\r\\nJimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Neural Infor\\x02mation Processing Systems, 2013.\\r\\nPhilip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Neural\\r\\nInformation Processing Systems, 2014.\\r\\nAmit Choudhury. A simple approximation to the area under standard normal curve. In Mathematics\\r\\nand Statistics, 2014.\\r\\nDjork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network ´\\r\\nlearning by exponential linear units (ELUs). In International Conference on Learning Represen\\x02tations, 2016.\\r\\nGuillaume Desjardins, Karen Simonyan, Razvan Pascanu, and Koray Kavukcuoglu. Natural neural\\r\\nnetworks. In arXiv, 2015.\\r\\nKevin Gimpel, Nathan Schneider, Brendan O′Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein,\\r\\nMichael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. Part-of-Speech Tagging\\r\\nfor Twitter: Annotation, Features, and Experiments. Association for Computational Linguistics\\r\\n(ACL), 2011.\\r\\nDan Hendrycks and Kevin Gimpel. Adjusting for dropout variance in batch normalization and\\r\\nweight initialization. In arXiv, 2016.\\r\\nJohn Hopfield. Neural networks and physical systems with emergent collective computational abil\\x02ities. In Proceedings of the National Academy of Sciences of the USA, 1982.\\r\\nDiederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. International\\r\\nConference for Learning Representations, 2015.\\r\\nAlex Krizhevsky. Learning Multiple Layers of Features from Tiny Images, 2009.\\r\\nDavid Krueger, Tegan Maharaj, Janos Kram ´ ar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary ´\\r\\nKe1, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, and Chris Pal. Zoneout:\\r\\nRegularizing RNNs by randomly preserving hidden activations. In Neural Information Processing\\r\\nSystems, 2016.\\r\\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with restarts. arXiv, 2016.\\r\\nAndrew L. Maas, Awni Y. Hannun, , and Andrew Y. Ng. Rectifier nonlinearities improve neural\\r\\nnetwork acoustic models. In International Conference on Machine Learning, 2013.\\r\\nWarren S. McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity.\\r\\nIn Bulletin of Mathematical Biophysics, 1943.\\r\\nDmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning\\r\\nRepresentations, 2016.\\r\\nAbdelrahman Mohamed, George E. Dahl, and Geoffrey E. Hinton. Acoustic modeling using deep\\r\\nbelief networks. In IEEE Transactions on Audio, Speech, and Language Processing, 2012.\\r\\nVinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.\\r\\nIn International Conference on Machine Learning, 2010.\\r\\nOlutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A.\\r\\nSmith. Improved part-of-speech tagging for online conversational text with word clusters. In\\r\\nNorth American Chapter of the Association for Computational Linguistics (NAACL), 2013.\\r\\n7\\nTim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to\\r\\naccelerate training of deep neural networks. In Neural Information Processing Systems, 2016.\\r\\nAndrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy\\x02namics of learning in deep linear neural networks. In International Conference on Learning\\r\\nRepresentations, 2014.\\r\\nAnish Shah, Sameer Shinde, Eashan Kadam, Hena Shah, and Sandip Shingade. Deep residual\\r\\nnetworks with exponential linear unit. In Vision Net, 2016.\\r\\nNitish Srivastava. Improving neural networks with dropout. In University of Toronto, 2013.\\r\\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\\r\\nDropout: A simple way to prevent neural networks from overfitting. In Journal of Machine\\r\\nLearning Research, 2014.\\r\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Confer\\x02ence, 2016.\\r\\n8\\nA NEURAL NETWORK ARCHITECTURE FOR CIFAR-10 EXPERIMENTS\\r\\nTable 1: Neural network architecture for CIFAR-10.\\r\\nLayer Type # channels x, y dimension\\r\\nraw RGB input 3 32\\r\\nZCA whitening 3 32\\r\\nGaussian noise σ = 0.15 3 32\\r\\n3 × 3 conv with activation 96 32\\r\\n3 × 3 conv with activation 96 32\\r\\n3 × 3 conv with activation 96 32\\r\\n2 × 2 max pool, stride 2 96 16\\r\\ndropout with p = 0.5 96 16\\r\\n3 × 3 conv with activation 192 16\\r\\n3 × 3 conv with activation 192 16\\r\\n3 × 3 conv with activation 192 16\\r\\n2 × 2 max pool, stride 2 192 8\\r\\ndropout with p = 0.5 192 8\\r\\n3 × 3 conv with activation 192 6\\r\\n1 × 1 conv with activation 192 6\\r\\n1 × 1 conv with activation 192 6\\r\\nglobal average pool 192 1\\r\\nsoftmax output 10 1\\r\\nB HISTORY OF THE GELU AND SILU\\r\\nThis paper arose from DH’s first research internship as an undergraduate in June 2016. The start of\\r\\nthe week after, this paper was put on arXiv, in which we discuss smoother ReLU activation functions\\r\\n(x × P(X ≤ x)) and their relation to stochastic regularizers. In 2016, we submitted the paper to\\r\\nICLR and made the paper and code publicly available. In the paper, we introduced and coined the\\r\\nSigmoid Linear Unit (SiLU) as x · σ(x).\\r\\nIn the first half of 2017, Elfwing et al. published a paper that proposed the same activation function\\r\\nas SiLU, x · σ(x), which they called “SIL.” At the end of 2017, over a year after this paper was first\\r\\nreleased, Quoc Le and others from Google Brain put out a paper proposing x · σ(x) without citing\\r\\neither the Elfwing et al. paper or this work. Upon learning this, we contacted both parties. Elfwing\\r\\nquickly updated their work to call the activation the “SiLU” instead of “SIL” to recognize that we\\r\\noriginally introduced the activation.\\r\\nUnlike Elfwing et al., the Google Brain researchers continued calling the activation “swish.” How\\x02ever, there was no novelty. The first author of the “swish” paper stated their oversight in public,\\r\\nsaying, “As has been pointed out, we missed prior works that proposed the same activation function.\\r\\nThe fault lies entirely with me for not conducting a thorough enough literature search.” To subdue\\r\\ncriticism, an update to the paper was released a week later. Rather than give credit to this work for\\r\\nthe SiLU, the update only cited this work for the GELU so that the “swish” appeared more novel. In\\r\\nthe updated paper, a learnable hyperparameter β was introduced, and the swish was changed from\\r\\nx · σ(x) to x · σ(β · x). This staked all of the idea’s novelty on an added learnable hyperparameter β.\\r\\nDespite the addition of the hyperparameter beta, nearly all of the community still used the original\\r\\n“swish” function without β (i.e., with β = 1). Since this paper was from Google Brain, the Tensor\\x02flow implementation ended up being called “swish,” and the default setting removed β, rendering it\\r\\nidentical to the SiLU. The practice of adding an unused hyperparameter allowed claiming novelty\\r\\nwhile effectively receiving credit for an idea that originated elsewhere. Future papers with the same\\r\\nsenior authors persistently referred to the “swish” function even when not using β, making it identi\\x02cal to the SiLU, originally proposed in this work. This resulted in the “swish” paper inappropriately\\r\\ngaining credit for the idea.\\r\\n9\\nThings changed as the GELU began to be used in BERT and GPT, becoming the default activation\\r\\nfor state-of-the-art Transformers. Now it is substantially more commonly used than the SiLU.\\r\\nSeparately, a reddit post “Google has a credit assignment problem in research” became popular\\r\\nand focused on how they refer to the SiLU as the swish. As an example, they mentioned “Smooth\\r\\nAdversarial Training” as an example of poor credit assignment. In the “Smooth Adversarial Train\\x02ing” paper, which came from the senior author of the swish, the term “swish” was used instead\\r\\nof “SiLU.” To reduce blowback from the post, the authors updated the paper and replaced “swish”\\r\\nwith the “SiLU,” recognizing this paper as the original source of the idea. After this post, popular\\r\\nlibraries such as Tensorflow and PyTorch also began to rename the function to “SiLU” instead of\\r\\n“swish.” For close observers, this issue has been largely settled, and we are grateful for the proper\\r\\nrecognition that has largely come to pass.\\r\\n10'},\n",
       " {'name': '2210.15925v1.pdf',\n",
       "  'content': 'arXiv:2210.15925v1 [q-fin.ST] 28 Oct 2022\\r\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 1\\r\\nIncorporating Interactive Facts for Stock\\r\\nSelection via Neural Recursive ODEs\\r\\nQiang Gao, Xinzhu Zhou, Kunpeng Zhang, Li Huang, Siyuan Liu, and Fan Zhou\\r\\nAbstract—Stock selection attempts to rank a list of stocks for optimizing investment decision making, aiming at minimizing investment\\r\\nrisks while maximizing profit returns. Recently, researchers have developed various (recurrent) neural network-based methods to\\r\\ntackle this problem. Without exceptions, they primarily leverage historical market volatility to enhance the selection performance.\\r\\nHowever, these approaches greatly rely on discrete sampled market observations, which either fail to consider the uncertainty of stock\\r\\nfluctuations or predict continuous stock dynamics in the future. Besides, some studies have considered the explicit stock\\r\\ninterdependence derived from multiple domains (e.g., industry and shareholder). Nevertheless, the implicit cross-dependencies among\\r\\ndifferent domains are under-explored. To address such limitations, we present a novel stock selection solution – StockODE, a latent\\r\\nvariable model with Gaussian prior. Specifically, we devise a Movement Trend Correlation module to expose the time-varying\\r\\nrelationships regarding stock movements. We design Neural Recursive Ordinary Differential Equation Networks (NRODEs) to capture\\r\\nthe temporal evolution of stock volatility in a continuous dynamic manner. Moreover, we build a hierarchical hypergraph to incorporate\\r\\nthe domain-aware dependencies among the stocks. Experiments conducted on two real-world stock market datasets demonstrate that\\r\\nStockODE significantly outperforms several baselines, such as up to 18.57% average improvement regarding Sharpe Ratio.\\r\\nIndex Terms—stock selection, stock movement learning, ordinary differential equations, Bayesian learning, hypergraph learning\\r\\n✦\\r\\n1 INTRODUCTION\\r\\nThe continual growth of global market capitalization has\\r\\nspawned the prosperity of quantitative investment, which\\r\\nprovides numerous investors and traders unprecedented\\r\\nopportunities to meticulously select valuable stocks for\\r\\nmaximizing profit returns. The majority of ground-breaking\\r\\nresearch focuses on stock prediction tasks that can be for\\x02mulated as binary classification or regression tasks to fore\\x02cast future stock movements (e.g. stock prices) [1], [2]. In\\r\\ncontrast, this study investigates a newly emerging but more\\r\\ncomplex task, i.e., stock selection, which can be framed as a\\r\\nlearning to rank problem. Specifically, stock selection mainly\\r\\nattempts to optimize the target of investment regarding the\\r\\nprofit returns, which not only considers the future trends of\\r\\nmassive stock candidates but also needs to select the stocks\\r\\nwith maximum profitable returns. Consequently, tackling\\r\\nthe stock selection problem is a challenging and non-trivial\\r\\ntask owing to several uncertainties such as high volatility\\r\\nand historical dependence on the stock market.\\r\\nAs one of the common clues, learning historical stock\\r\\ntransactions can help reveal the stock’s future evolution,\\r\\nLi Huang is the corresponding author.\\r\\n• Q. Gao, X. Zhou, and L. Huang are with the School of Computing and Ar\\x02tificial Intelligence, Southwestern University of Finance and Economics,\\r\\nChengdu, China, 611130.\\r\\nE-mail: qianggao@swufe.edu.cn, 222020204197@smail.swufe.edu.cn,\\r\\nlihuang@swufe.edu.cn\\r\\n• K. Zhang is with the University of Maryland, College Park, USA.\\r\\nE-mail: kpzhang@umd.edu\\r\\n• S. Liu is with the Pennsylvania State University, PA 16802 USA.\\r\\nE-mail: siyuan@psu.edu.\\r\\n• F. Zhou is with the School of Information and Software Engineering,\\r\\nUniversity of Electronic Science and Technology of China, China.\\r\\nE-mail: fan.zhou@uestc.edu.cn\\r\\nManuscript received XX XX, 2022; revised XX XX, 2022.\\r\\nwhich in turn enables us to determine which stock is the\\r\\nmost profitable investment. The motivation is that earlier\\r\\nstudies have demonstrated that stock movements, to some\\r\\nextent, are predictable according to literature in behavioral\\r\\nfinance and behavioral economics [3], [4]. Conventional\\r\\napproaches usually adopt time-series models to capture\\r\\nthe historical influences and changes of diverse time-series\\r\\nmarket signals, such as open price, close price, trading\\r\\nvolume, and others. For instance, researchers have used\\r\\nARIMA [5] or GARCH [6] to study the stock volatility\\r\\nbased on a given indicator, e.g., daily close price. Moreover,\\r\\nnumerous endeavors develop various machine learning al\\x02gorithms, e.g., SVM [7], Random Forest [8], HMM [9], to\\r\\nexplore the stock fluctuations from historical transaction\\r\\ndata. However, those approaches fail to take into account\\r\\nthe inherent non-stationary fluctuations of the stock market\\r\\nand are not usually compatible with stock trends in real\\r\\nfinancial scenarios, which could ultimately fail in choosing\\r\\nmore profitable stocks [10].\\r\\nRecently, the widely used deep learning approaches\\r\\nsuch as recurrent neural networks have become a natural\\r\\nchoice for capturing the evolution of historical stock transac\\x02tions [1], [11]. In addition, The Efficient Market Hypothesis\\r\\n(EMH) indicates that the stock movements could be affected\\r\\nby relevant information from multiple sources [12]. For\\r\\ninstance, textual data sources such as financial news and\\r\\npublic reviews are now being used to expose their influence\\r\\nin understanding the stock trends [13], [14], [15]. Publicly\\r\\navailable company information, as another source has been\\r\\nanalyzed to investigate the internal dependencies in the\\r\\nstock market. It has become a substantial and valuable clue\\r\\nto promote stock movement modeling in addition to solely\\r\\nexploring the historical stock moving patterns [4], [16].\\r\\nRecently, deep graph learning-based studies have demon-\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 2\\r\\n\\x03\\x01\\x01 \\x05\\x01\\x01 \\x07\\x01\\x01 \\t\\x01\\x01 \\x02\\x01\\x01\\x01\\r\\n\\x06\\x13\\x07\\n\\x0e\\x10\\r\\n\\x07\\x18\\x14\\r\\n\\x01\\x05\\r\\n\\x01\\x06\\r\\n\\x01\\x07\\r\\n\\x01\\x08\\r\\n\\x01\\t\\r\\n\\x01\\n\\r\\n\\x13\\x0b\\x15\\x16\\x13\\x10\\x13\\x07\\x15\\x0e\\x11\\r\\n\\x01\\x07\\x02\\x06\\x13\\x07\\x10\\x14\\x12\\x11\\x13\\x15\\x07\\x15\\x0e\\x11\\x10\\x05\\x0b\\x13\\x17\\x0e\\t\\x0b\\x14\\r\\n\\r\\x17\\x15\\r\\r\\n\\x12\\x12\\x18\\x16\\r\\n\\x15\\x0e\\x0f\\x0f\\x10\\r\\n\\x03\\x01\\x01 \\x05\\x01\\x01 \\x07\\x01\\x01\\r\\n\\x06\\x13\\x07\\n\\x0e\\x10\\r\\n\\x07\\x18\\x14\\r\\n\\x01\\x04\\x01\\r\\n\\x01\\x04\\x06\\r\\n\\x01\\x05\\x01\\r\\n\\x01\\x05\\x06\\r\\n\\x01\\x06\\x01\\r\\n\\x01\\x06\\x06\\r\\n\\x01\\x07\\x01\\r\\n\\x01\\x07\\x06\\r\\n\\x01\\x08\\x01\\r\\n\\x13\\x0b\\x15\\x16\\x13\\x10\\x13\\x07\\x15\\x0e\\x11\\r\\n\\x01\\x08\\x02\\x03\\x11\\x0f\\x12\\x16\\x15\\x0b\\x13\\x04\\x07\\x10\\x16\\x0c\\x07\\t\\x15\\x16\\x13\\x0e\\x10\\r\\r\\n\\x0b\\x0b\\x15\\x11\\r\\n\\x13\\x0f\\x0c\\r\\r\\n\\x14\\x12\\x0c\\x11\\r\\nFig. 1. An example of stock’s movement regarding return ratio. The\\r\\nchanges in return ratios for various stock market tickers are shown by\\r\\nthe easily distinct colored curves.\\r\\nstrated that modeling the historical stock movements along\\r\\nwith taking into account the stock dependencies can signif\\x02icantly help predict near-term stock trends, which further\\r\\nenables us to select or recommend profitable stocks for\\r\\ninvestment [2], [10]. We position our study in the second\\r\\ndirection, as it focuses more on the implied and interactive\\r\\ninformation behind the stocks.\\r\\nAlthough graph-based techniques have received a great\\r\\ndeal of attention for understanding stock relationships,\\r\\nwe consider that there are still two main interactive facts\\r\\nbetween stocks that may affect the capture of potential\\r\\ninterdependencies: (a) Time-varying Correlation. As shown\\r\\nin Fig. 1, we respectively visualize the trend of one-day\\r\\nreturns for three stocks in two different industries. We can\\r\\nfind that the correlation strengths between these stocks\\r\\nevolve over time. Some stocks (e.g., APPL and NICE) have\\r\\nsynchronous evolving trends while some (e.g., EXPE and\\r\\nPHIIK) maintain competitive trends. In the real world,\\r\\nthis phenomenon is prevalent in stock markets due to\\r\\nthe Momentum Spillover Effect [4]. For instance, the stock\\r\\nprice of Intel typically has an impact on the stock prices\\r\\nof other semiconductor firms. However, existing studies\\r\\nusually treat each stock’s dynamics as isolated, adhering\\r\\nto the standard (deep learning-based) time series modeling\\r\\nparadigm [17], [18], and do not consider the interactive\\r\\nsignals between stocks regarding their past movements.\\r\\nThis makes it impossible to specify how stock correlations\\r\\nhave changed over time. Therefore, We consider that in\\x02corporating such time-varying correlations behind the stock\\r\\nmovements could boost the understanding of distinct stock\\r\\nvolatility trends. (b) Domain-aware Dependency. Despite the\\r\\nfact that predetermined stock relationships can be extracted\\r\\nfrom numerous publicly available domains (e.g., industry\\r\\nand shareholder) to enhance the stock selection performance\\r\\nowing to the powerful capability of GNN-based models,\\r\\nsuch as [19], [20], we argue that existing solutions only\\r\\nexploit the simple stock relations within a specific domain\\r\\n(e.g., the stocks from the same industry) while the underly\\x02ing cross-dependencies among different domains are under\\x02explored. For instance, United Airlines, FedEx and Delta Air\\r\\nLines all operate Boeing planes, but Delta Air Lines has a\\r\\nstronger relationship with United Airlines than FedEx and\\r\\nDelta Air Lines as Delta Air Lines and United Airlines belong to\\r\\nthe same industry. Thus, we consider that employing shal\\x02low knowledge can only bring limited benefits to selection\\r\\nperformance as the complex higher-order collaborations on\\r\\nthe predefined stock relations are not fully explored.\\r\\nIn addition, stock selection, as a typical time-dependent\\r\\nlearning task, naturally includes a series of indicator obser\\x02vations that reflect stock trends. Furthermore, stock move\\x02ments may change significantly in a short period, e.g.,\\r\\nsignificant volatility in the hours after the stock market\\r\\nopens [21], [22]. Existing approaches based on deep neural\\r\\nnetworks should have the capability of modeling time\\x02series data in continuous dynamic systems to address stock\\r\\nvolatility. However, in practice, they can only take the\\r\\ndiscrete-time observations (e.g., daily level) as the input and\\r\\nuse the static topological information structure to predict\\r\\nthe most profitable stocks, which could result in the gap\\r\\nbetween deep neural networks and dynamic systems, ig\\x02nore the uncertainty behind stock observations, and even\\r\\ncause the failure of predicting continuous-time dynamics\\r\\nregarding stock movements. For example, we usually em\\x02ploy RNN-based models to handle the historical price data\\r\\nderived from daily observations, whereafter we use them to\\r\\npredict future stock trends at a fixed time interval [17], [23].\\r\\nHowever, these models could lead to investment risk and\\r\\nfinancial loss when the stock prices fluctuate rapidly within\\r\\nminutes or hours [15], [24].\\r\\nTo remedy the aforementioned drawbacks, we introduce\\r\\na novel framework named StockODE to solve the stock\\r\\nselection problem, which is motivated by recent success\\x02ful applications of Neural Ordinary Differential Equations\\r\\n(NODEs) in time-series data [21], [25]. In detail, we first\\r\\ndevelop a Movement Trend Correlation module to initiate\\r\\nthe capture of time-varying correlations between differ\\x02ent stocks’ historical movements (e.g., return ratio). Next,\\r\\nwe design an attention-inspired Neural Recursive ODE\\r\\n(NRODE) block to model the historical multivariate stock\\r\\nmovements in a continuous dynamic manner. In particular,\\r\\nStockODE performs as a latent variable model to relieve\\r\\nthe uncertainty of stock fluctuations via Gaussian assump\\x02tion. Finally, a Hierarchical Hypergraph Convolution Net\\x02work (HHCN) is presented to address the domain-aware\\r\\ndependencies among the stocks extracted from relational\\r\\nknowledge of market environmental variables. Notably, our\\r\\nhierarchical hypergraph network constructs the stock in\\x02teractions from intra-domain knowledge and cross-domain\\r\\nknowledge to learn the complexly higher-order interactions\\r\\namong the stocks. The main contributions of our study are\\r\\nsummarized as follows:\\r\\n• We propose a flexibly dynamic neural framework–\\r\\nStockODE, which provides a new perspective on stock\\r\\nmovement learning. To the best of our knowledge,\\r\\nStockODE is the first attempt to involve the neural ODE\\r\\nto capture the temporal evolution of stock volatility in\\r\\na continuous dynamic manner.\\r\\n• We design a Movement Trend Correlation module that\\r\\nis capable of integrating the co-evolution and anti\\x02evolution relationships between different stocks regard\\x02ing historical movements.\\r\\n• To capture the domain-aware dependencies behind var\\x02ious stocks, we devise a hierarchical hypergraph to\\r\\ndescribe the intra-domain and inter-domain knowledge\\r\\nfrom real-world relation sources.\\r\\n• Our experimental results conducted on two real-world\\r\\ndatasets demonstrate that the proposed StockODE sig-\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 3\\r\\nnificantly outperforms the existing solutions regarding\\r\\nquantitative stock trading.\\r\\nIn the rest of this paper, we first review the relevant\\r\\nstudies in Section 2, and then formalize the problem in\\r\\nSection 3. The details of the proposed StockODE framework\\r\\nare discussed in Section 4, and the results of the experimen\\x02tal evaluations quantifying the benefits of our approach are\\r\\npresented in Section 5. Section 6 concludes this study and\\r\\noutlines the directions of future work.\\r\\n2 RELATED WORK\\r\\nWe divide the relevant research into three key categories,\\r\\nincluding conventional stock movement learning, deep\\r\\nlearning-based approaches, and market relation learning.\\r\\n2.1 Conventional Stock Movement Learning\\r\\nThe early efforts attempt to leverage technical analysis and\\r\\nfundamental analysis for stock market modeling, where the\\r\\nformer mainly relies upon the past stock trends as future\\r\\nindicators while the latter aims to investigate the intrin\\x02sic value of stock price, i.e., fair value [26]. For technical\\r\\nanalysis, it aims at extracting the volume indicators from\\r\\nhistorical stock movement data, whereafter adopting the\\r\\nlinear models, e.g., ARIMA [5], to predict stock price trends.\\r\\nMoreover, several machine learning-based methods, e.g.,\\r\\nSVM and Random Forest, have been successfully involved\\r\\nin learning stock dynamics [7], [8]. In contrast to technical\\r\\nanalysis, the fundamental analysis presents a newer per\\x02spective for the stock movement prediction, which consid\\x02ers the impact factors from the third-party data, such as\\r\\nsocial media, earning calls, and financial news [27], [28].\\r\\nIn essence, the existing studies concentrate on the stock\\r\\ntrend prediction task and use a classification or regression\\r\\nscheme to infer the binary trend or stock future price. For\\r\\ninstance, researchers exploit the volume indicators of his\\x02torical transaction data with respect to stock movement and\\r\\nendeavor to model the stock dynamics with popular ma\\x02chine learning approaches such as Logistic Regression [29].\\r\\nIn contrast, stock selection (or ranking/recommendation)\\r\\naims at providing optimal stock choices by ranking a stock\\r\\nlist, which helps in achieving more profit expectations as\\r\\nwell as relieving the investment risks [16], [30].\\r\\n2.2 Deep Learning-based Approaches\\r\\nSince the stock market is a dynamic system affected by\\r\\nmultiple time-varying signals, the emerging deep neural\\r\\nnetworks especially for the recurrent networks, e.g., LSTM\\r\\n(long-short term memory) [31] and GRU (gated recurrent\\r\\nunits) [32], have the capability of capturing the intricate\\r\\ntemporal dependencies behind the multivariate time series\\r\\ntransactions, which spurs the researchers to apply them\\r\\nfor historical stock signal modeling [33]. For instance, [1]\\r\\nextended the LSTM model to enhance the accuracy of stock\\r\\nreturn prediction. [34] predicted future trends of stock prices\\r\\nbased on the price history and technical analysis indica\\x02tors, whereafter it concluded that the RNN-based models\\r\\nachieve significant gains compared to the earlier machine\\r\\nlearning approaches. Other popular methods, e.g., seq2seq\\x02based [35], [36], attention-based [10], [18], [37], have also\\r\\nattracted researchers’ interest in stock movement learning.\\r\\nDue to the inherent limitation of existing deep recursive\\r\\nneural networks that can only receive or predict discrete\\r\\ntime-series signals, we argue that modeling stock market\\r\\nshould have the capability of adapting to the continuous\\r\\ndynamic scenarios, enhancing the perception of market\\r\\nvolatility or uncertainty. Motivated by recent neural ordi\\x02nary differential equations (ODE), we design an attention\\x02inspired neural recursive ODE, named StockODE, which is\\r\\nmore flexible for alleviating stock selection risks as well as\\r\\npromoting investment profits.\\r\\n2.3 Market Relation Learning\\r\\nAccording to Efficient Market Hypothesis (EMH) [12], the\\r\\nstock movements are affected by several interactive facts,\\r\\nthat is, the significant movements of a specific stock could\\r\\nresult in the fluctuations of other stocks in the same in\\x02dustrial markets and the subsidiary stock price changes\\r\\ncan also cause the upstream parent company’s stock price\\r\\nmovements. To this end, recent efforts turn to use the pop\\x02ular graph neural networks to incorporate the higher-order\\r\\ninteractive correlations behind the stocks [4], [16], [19]. For\\r\\nexample, GCN [38] and GAT [39] are two common but effi\\x02cient solutions to aggregate the interaction impacts between\\r\\nthe given stock and their relative stocks. Another solution\\r\\nturns to leverage the hypergraph for stock relation modeling\\r\\nto alleviate the information loss in traditional graph learn\\x02ing [30]. Nevertheless, we argue that the domain-level inter\\x02actions behind the stocks are under-explored. That is to say,\\r\\nour StockODE not only aggregates the given relationships\\r\\namong distinct stocks but also considers the potentially\\r\\nhigher-order interactions upon pre-defined relation prior.\\r\\n3 PROBLEM STATEMENT\\r\\nWe aim at ranking a more profitable stock list for investors,\\r\\nassuming they have enough budget while lacking insights\\r\\nfor investment decisions. Let S = {s1, s2, · · · , si, · · · , sN }\\r\\nbe a set of N stocks. For each stock si on trading day\\r\\ntτ , it is associated with end-of-day data x\\r\\nτ\\r\\ni\\r\\n(∈ Xτ) includ\\x02ing open, high, low, close prices and trading volume (i.e.,\\r\\nx\\r\\nτ\\r\\ni = [x\\r\\no,τ\\r\\ni\\r\\n, x\\r\\nh,τ\\r\\ni\\r\\n, x\\r\\nl,τ\\r\\ni\\r\\n, x\\r\\nc,τ\\r\\ni\\r\\n, x\\r\\nv,τ\\r\\ni\\r\\n] ). The indicator details are\\r\\nillustrated in Table 1. As the return ratio of a stock reveals\\r\\nthe expected revenue of the stock [2], [30], we also set a\\r\\none-day return ratio r\\r\\nτ\\r\\ni =\\r\\nx\\r\\nc,τ\\r\\ni −x\\r\\nc,τ−1\\r\\ni\\r\\nx\\r\\nc,τ−1\\r\\ni\\r\\nfor each stock si on\\r\\ntrading day tτ . Basically and formally, given historical stock\\r\\ndata {Xτ }\\r\\nT\\r\\nT −w+1 with lookup window w, StockODE targets\\r\\nat predicting a ranking list of stocks RT +1 for the following\\r\\ntrading day, where we have RT +1 = {R\\r\\nT +1\\r\\n1 > RT +12\\r\\n· · · >\\r\\nR\\r\\nT +1\\r\\nN }. Specifically, for any two stocks si and sj (i 6= j), we\\r\\nlet R\\r\\nT +1\\r\\ni > RT +1j\\r\\nif r\\r\\nT +1\\r\\ni > rT +1j\\r\\n.\\r\\n4 METHODOLOGY\\r\\nIn this section, we first introduce the skeleton of our pro\\x02posed StockODE, followed by the details of each compo\\x02nent. Finally, the optimization strategy and algorithm details\\r\\nare provided.\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 4 Stocks\\r\\nDays\\r\\nDays\\r\\nStocks\\r\\nStocks\\r\\nDays\\r\\nODE Solve … ODE Solve\\r\\nReconstruction\\r\\nIndicators\\r\\nQ K V\\r\\n×\\r\\n!\\r\\nNeural Recursive ODEs\\r\\n…\\r\\n…\\r\\nzT\\r\\npT\\r\\nP\\r\\nF\\r\\nG\\r\\nG(G)\\r\\nMovement Trend Correlation\\r\\nDomain Sources\\r\\nStock Ranking\\r\\nHierarchical Hypergraph Convolution Network\\r\\n{zτ }\\r\\nT\\r\\nT −ω+1\\r\\n{Xτ}\\r\\nT\\r\\nT −ω+1\\r\\n{Xτ}\\r\\nT\\r\\nT −ω+1\\r\\nFig. 2. The framework of StockODE.\\r\\nTABLE 1\\r\\nThe details of used stock indicators.\\r\\nIndicator Description\\r\\nOpen The first price of the stock on a given trading day.\\r\\nClose The final price of the stock on a given trading day.\\r\\nHigh The highest price of the stock on a given trading day.\\r\\nLow The lowest price of the stock on a given trading day.\\r\\nVolume The total amount of shares or contracts traded\\r\\nfor a particular security.\\r\\n4.1 Architecture Overview\\r\\nAs illustrated in Fig. 2, it presents the overall framework of\\r\\nour StockODE, which mainly contains three components,\\r\\ni.e., Movement Trend Correlation, Neural Recursive ODE, and\\r\\nHierarchical Hypergraph Convolution Network. First, Move\\x02ment Trend Correlation is to capture the explicit and im\\x02plicit correlations among different stocks regarding the time\\x02evolving return ratio, which results in a generation of aggre\\x02gated results P by a standard self-attention neural network.\\r\\nNext, we devise a Neural Recursive ODE (NRODE) cell that\\r\\noperates P with the ODE solver in a recursive manner,\\r\\nwhereby we can use it to generate the latent variables\\r\\n{zτ }\\r\\nT\\r\\nT −ω+1 via Variational Bayes. Especially, NRODE cou\\x02pling with an attention mechanism models a continuous\\r\\ndynamic system based on parameterizing the derivative\\r\\nof the latent state of each stock, instead of specifying the\\r\\ndiscrete sequence of hidden states’ transformation. Sub\\x02sequently, our Hierarchical Hypergraph Convolution Net\\x02work (HHCN) turns to extract the intra- and inter-domain\\r\\nknowledge from multi-source domains, and then we fuse\\r\\nthem to a unified representation F. In the end, we use a\\r\\nsimple dense layer to produce the predicted stock ranking,\\r\\nwhere the input contains the last hidden state hT , zT and F.\\r\\nWe will elaborate on each component in the following part.\\r\\n4.2 Movement Trend Correlation\\r\\nGiven stock set S, we have their historical end-of-day data\\r\\n{Xτ }\\r\\nT\\r\\nT −w+1. We first transform {Xτ }\\r\\nT\\r\\nT −w+1 into a tensor\\r\\nform [XT −w+1, · · · , Xτ, · · · , XT], where Xτ ∈ R\\r\\nN×de pre\\x02serves N stocks’ end-of-day indicators on trading day tτ , de\\r\\nis the indicator number of each stock. For simplicity, we use\\r\\nX ∈ R\\r\\nw×N×de\\r\\nto denote such a tensor form. Since the 1-day\\r\\nreturn ratio only shows the daily return, we argue that con\\x02sidering the historical (middle) long-term earning impacts\\r\\nof each stock could help reduce the future investment risk.\\r\\nTo this end, we also use return ratios of 5-, 10-, 20-, and\\r\\n30-day moving averages to supplement stock features for\\r\\naddressing the influences of weekly and monthly trends. In\\r\\npractice, the return ratio can reveal the expected revenue\\r\\nof a specific stock. As shown in Fig. 3, we randomly select\\r\\nfive NASDAQ stocks and visualize their mutual correlations\\r\\nregarding the movement trends from Monday to Thursday\\r\\nin a given week. We can find that their mutual correlations,\\r\\nwhich vary with time, have drastically changed. For exam\\x02ple, the stock AAXN has an anti-evolved relationship with\\r\\nABMD from Monday to Wednesday, while AAXN shows a\\r\\nco-evolved relationship with ABMD on Thursday. we thus\\r\\ndesign an evolution-aware attention block containing an\\r\\nExplicit Correlation Aggregation and an Implicit Correlation\\r\\nAggregation to expose the underlying correlations among\\r\\ndifferent stocks regarding the time-evolving return ratios.\\r\\nAA\\x0c\\x05 A\\x01A\\x0c A\\x01\\x02\\x01 A\\x01\\x02\\x06 A\\x01\\x04\\x03\\r\\nAA\\x0c\\x05\\r\\nA\\x01A\\x0c\\r\\nA\\x01\\x02\\x01\\r\\nA\\x01\\x02\\x06\\r\\nA\\x01\\x04\\x03\\r\\n\\t\\x08\\x07\\n\\r\\n\\t\\n\\x07\\x0b\\r\\n\\n\\x07\\n\\r\\n\\n\\x07\\x0b\\r\\n\\x08\\x07\\n\\r\\n(a) Monday.\\r\\nAA\\x0c\\x05 A\\x01A\\x0c A\\x01\\x02\\x01 A\\x01\\x02\\x06 A\\x01\\x04\\x03\\r\\nAA\\x0c\\x05\\r\\nA\\x01A\\x0c\\r\\nA\\x01\\x02\\x01\\r\\nA\\x01\\x02\\x06\\r\\nA\\x01\\x04\\x03\\r\\n\\t\\x08\\x07\\n\\r\\n\\t\\n\\x07\\x0b\\r\\n\\n\\x07\\n\\r\\n\\n\\x07\\x0b\\r\\n\\x08\\x07\\n\\r\\n(b) Tuesday.\\r\\nAA\\x0c\\x05 A\\x01A\\x0c A\\x01\\x02\\x01 A\\x01\\x02\\x06 A\\x01\\x04\\x03\\r\\nAA\\x0c\\x05\\r\\nA\\x01A\\x0c\\r\\nA\\x01\\x02\\x01\\r\\nA\\x01\\x02\\x06\\r\\nA\\x01\\x04\\x03\\r\\n\\t\\x08\\x07\\n\\r\\n\\t\\n\\x07\\x0b\\r\\n\\n\\x07\\n\\r\\n\\n\\x07\\x0b\\r\\n\\x08\\x07\\n\\r\\n(c) Wednesday.\\r\\nAA\\x0c\\x05 A\\x01A\\x0c A\\x01\\x02\\x01 A\\x01\\x02\\x06 A\\x01\\x04\\x03\\r\\nAA\\x0c\\x05\\r\\nA\\x01A\\x0c\\r\\nA\\x01\\x02\\x01\\r\\nA\\x01\\x02\\x06\\r\\nA\\x01\\x04\\x03\\r\\n\\t\\x08\\x07\\n\\r\\n\\t\\n\\x07\\x0b\\r\\n\\n\\x07\\n\\r\\n\\n\\x07\\x0b\\r\\n\\x08\\x07\\n\\r\\n(d) Thursday.\\r\\nFig. 3. A toy example of movement trend correlation from January 7,\\r\\n2013 to January 10, 2013. Note that the yellow cell indicates the co\\x02evolved relation while the green cell reflects the anti-evolved relation.\\r\\nExplicit Correlation Aggregation. We propose a corre-\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 5\\r\\nlation tensor Υ (∈ R\\r\\nw×N×N ) regarding the return ratio\\r\\nto expose the co-evolved and anti-evolved relationships\\r\\namong different stocks. Specifically, the 1-day return ratio\\r\\nindicates the difference between the current trading day and\\r\\nthe previous trading day. For a given trading day tτ , we thus\\r\\ndefine the following formulation to obtain the correlation\\r\\ncoefficient among different stocks w.r.t the trading day tτ ’s\\r\\nreturn ratio:\\r\\nΥτ = sign\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\nr\\r\\nτ\\r\\n1\\r\\nr\\r\\nτ\\r\\n2\\r\\n· · · r\\r\\nτ\\r\\nN\\r\\nr\\r\\nτ\\r\\n1\\r\\nr\\r\\nτ\\r\\n2\\r\\n· · · r\\r\\nτ\\r\\nN\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\nr\\r\\nτ\\r\\n1\\r\\nr\\r\\nτ\\r\\n2\\r\\n· · · r\\r\\nτ\\r\\nN\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n⊗ sign\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\nr\\r\\nτ\\r\\n1\\r\\nr\\r\\nτ\\r\\n2\\r\\n· · · r\\r\\nτ\\r\\nN\\r\\nr\\r\\nτ\\r\\n1\\r\\nr\\r\\nτ\\r\\n2\\r\\n· · · r\\r\\nτ\\r\\nN\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\nr\\r\\nτ\\r\\n1\\r\\nr\\r\\nτ\\r\\n2\\r\\n· · · r\\r\\nτ\\r\\nN\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\n⊤\\r\\n,\\r\\n(1)\\r\\nwhere sign is a Sign function and ⊗ refers to element-wise\\r\\nmultiplication. Υτis able to indicate either a positive or\\r\\nnegative correlation between different stocks in terms of\\r\\ntheir return ratios. As such, we can eventually formulate\\r\\na correlation tensor Υ to describe the return ratio-aware\\r\\nrelations on each trading day, i.e., Υ = [Υ1, Υ2, · · · , Υω].\\r\\nNext, we operate a one-layer graph convolutional layer\\r\\nto aggregate explicit correlations among different stocks,\\r\\nwhich can be summarized as follows:\\r\\nH = ΥXWΥ, (2)\\r\\nH = H + XWX,\\r\\nwhere WΥ(∈ R\\r\\nde×d\\r\\n) and WX(∈ R\\r\\nde×d\\r\\n) are trainable\\r\\nparameters.\\r\\nImplicit Correlation Aggregation. Inspired by [40], we\\r\\nemploy a self-attention layer to capture the attentive stock\\r\\ncorrelations w.r.t H. Note that this self-attention aims to\\r\\nevaluate the correlation scores among the stocks instead\\r\\nof capturing the temporal dependencies of the stock itself.\\r\\nSpecifically, it first computes the query Q, key K and value\\r\\nV, and then uses dot-product attention with an activation\\r\\nfunction to generate the latent states H′(∈ R\\r\\nw×N×d\\r\\n). Thus,\\r\\nthe process can be written as:\\r\\nH′ = ReLU \\x12Norm \\x12\\r\\nΩ · HWV\\r\\n√\\r\\nd\\r\\n\\x13\\x13 ,\\r\\nΩ = softmax \\x10HWQ · (HWK)\\r\\n⊤\\r\\n\\x11\\r\\n,\\r\\n(3)\\r\\nwhere HWQ, HWK and HWV refer to Q, K and V in the\\r\\nself-attention neural network, respectively. Norm denotes\\r\\nthe Layer Normalization operation for fast and stable train\\x02ing. And the self-attention score matrix Ω is normalized by\\r\\na softmax function. Notably, WQ,WK,WV ∈ R\\r\\nd×d\\r\\nare\\r\\ntrainable matrices and d refers to the size of dimensionality.\\r\\nNow we reuse the transpose operation to shift H′to tempo\\x02ral perspective:\\r\\nP = tran(H′), (4)\\r\\nwhere P ∈ R\\r\\nN×w×d\\r\\n. We will take P as the input of the\\r\\nfollowing Neural Recursive ODEs.\\r\\n4.3 Neural Recursive ODEs\\r\\nWe now introduce our devised Neural Recursive ODE\\r\\n(NRODE) for multivariate stock movement learning. Prior\\r\\nto that, we first go over the technical specifics of current\\r\\nneural ODEs and show how our NRODE is inspired and\\r\\nwhat it contributes.\\r\\nNeural ODE. Recent neural ODE enables generating\\r\\ncontinuous observations by giving the initial condition [21],\\r\\nwhich stimulates us to develop the neural ODE-based mod\\x02ule to tackle the discrete stock indicators (observations).\\r\\nNeural ODEs (NODEs) intuitively build the infinite-steps\\r\\nhidden state update in neural networks for bridging the gap\\r\\nbetween discrete neural networks and continuous dynamic\\r\\nsystems [21], [41]. Specifically, NODEs, starting from the\\r\\ninput hidden layer h(t0), parameterize the continuous dy\\x02namics of hidden units using an ODE specified by a neural\\r\\nnetwork:\\r\\ndh(t)\\r\\ndt = f(h(t), t; ψ), where (5)\\r\\nh (t1) = h (t0) + Z t1\\r\\nt0\\r\\nf (h(t), t; ψ) dt.\\r\\nNotably, ODE function f (h(t), t; ψ) is a neural network\\r\\nparameterized by ψ. And the above process also can be\\r\\nrewritten as:\\r\\nh (t1) = ODESolve(fψ, h (t0),(t0, t1)) (6)\\r\\nAs such, the most significant benefit of ODE is that we can\\r\\nobtain the results of dynamic hidden representation at any\\r\\ntime (e.g., t1) when we have received the initial state at t0\\r\\n(t0 < t1).\\r\\nAs one of the popular neural ODEs, the ODE-GRUs [42]\\r\\ncould provide us with a new paradigm for stock movement\\r\\nlearning, which can be summarized as:\\r\\nh\\r\\n′\\r\\ni = ODESolve(fψ, hi−1,(ti−1, ti)), (7)\\r\\nhi = GRUCell(δi, h\\r\\n′\\r\\ni\\r\\n), (8)\\r\\nwhere δi denotes the input feature and hiis the hidden\\r\\nstate of current input δi. Nevertheless, the trend of stock\\r\\nindicators such as price is different from the representa\\x02tive time series modeling problems as the former usu\\x02ally does not has strong regularity which is dependent\\r\\non time while the latter usually follows obvious transi\\x02tional patterns/regularities (e.g., human mobility [43], ur\\x02ban flow [44], residential electricity consumption [45], and\\r\\netc). The reason is stock movements in the future are usually\\r\\ndisturbed by multiple external factors (e.g., public opinion,\\r\\nstimulus policy, investor psychology, and etc.). As a result,\\r\\nwe conjecture that the unobserved or unconsidered uncer\\x02tainty of the stock market could also bring the risk of stock\\r\\nselection. To this end, we devise a lightweight recursive\\r\\nmodel–Neural Recursive ODE (NRODE)–which enables co\\x02evolving the stock movements recursively with the atten\\x02tion mechanism. As shown in Fig. 4, our NRODE discards\\r\\nthe complex gate operations in vanilla RNN (e.g., LSTM\\r\\nand GRU). Instead, we propose an attention-inspired ODE\\r\\nblock to generate the candidate of continuous hidden state\\r\\nwhereafter incorporating the current discrete observation to\\r\\nformulate the current hidden state. In particular, we encode\\r\\nthese hidden states into latent codes via Variational Bayes to\\r\\naccount for the uncertainty in stock movements.\\r\\nNRODE. Given the initial hidden states of stock move\\x02ments P, we first adopt a linear transformation layer to\\r\\ntackle each trading day data pτ (∈ P) and set the obtained\\r\\nresults as the input of NRODE:\\r\\nvτ = pτWv + bv, (9)\\r\\nwhere Wv ∈ R\\r\\nd×d\\r\\nand bv ∈ R\\r\\nd\\r\\nare learnable parameters.\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 6\\r\\nFor each vτ along with time tτ , we have:\\r\\nh\\r\\n′\\r\\nτ = ODESolve(fψ,(hτ−1, a(hτ−1)),(tτ−1, tτ )), (10)\\r\\nh\\r\\n′′\\r\\nτ = [vτ , h\\r\\n′\\r\\nτ\\r\\n]Wh + bh, (11)\\r\\nhτ = (1 − I(vτ ))h\\r\\n′′\\r\\nτ + I(vτ )hτ−1 (12)\\r\\nwhere h0 is set as 0 and I(vτ ) denotes the update gate.\\r\\nhτ is the updated hidden state and fψ is the differentiable\\r\\nnetwork parameterized by ψ. Herein, we choose the Euler\\r\\nsolution as the numerical solver in our StockODE instead\\r\\nof the adjoint method due to its numerical instability of\\r\\nbackward ODE solve [21]. Inspired by previous works [25],\\r\\n[46], we also employ three-layer multilayer perceptrons\\r\\n(MLPs) as the ODE function where each layer has d units.\\r\\nAnd we notice that Eq. (10) can be theoretically described as\\r\\nfollows:\\r\\nh\\r\\n′\\r\\n(tτ ) = h(tτ−1) + Z tτ\\r\\ntτ−1\\r\\nf (h(t), a(t), t; ψ) dt, (13)\\r\\nwhere a(t) has the same dimension as h(t) and it aims\\r\\nto regularize the contribution of each value in h(t). In\\r\\nparticular, the ODE function f can be further described as:\\r\\nf (h(t), a(t), t; ψ) = f (g(h(t), a(t)), t; ψ), (14)\\r\\nwhere g(·, ·) is an attention-integrated layer, defined as:\\r\\ng(h(t), a(h(t))) = h(t) ⊗ sigmoid(a(h(t))), (15)\\r\\nwhere ⊗ is element-wise multiplication. Notably, the input\\r\\nof attention state a(t) is initialized by h(t):\\r\\na(t) = h(t)Wa + ba, (16)\\r\\nwhere Wa ∈ R\\r\\nd×d\\r\\nand ba ∈ R\\r\\nd denote the trainable\\r\\nparameters, respectively.\\r\\npτ\\r\\nODE\\r\\n×\\r\\n+\\r\\nhτ−1 hτ\\r\\nσ\\r\\n1−\\r\\n×\\r\\nσ\\r\\nFig. 4. A circuit diagram of NRODE cell. Note that σ is the ‘sigmoid’\\r\\noperation, ⊗ is the element-wise multiplication, and ⊕ refers to the sum\\r\\noperation.\\r\\nPosterior Approximation. To consider the uncer\\x02tainty/stochasticity that is hard to observe in stock move\\x02ments, we turn to operate each hidden state into a latent\\r\\nspace based on Variational Bayes (VB) [47]. Specifically, we\\r\\ncan obtain a sequence of latent variables Z = {zτ }\\r\\nT\\r\\nT −w+1,\\r\\nwhich enables tackling the uncertain factors in stock move\\x02ments. Each latent variable zτ can be formulated as follows:\\r\\nzτ = hτ + στ ǫ, (17)\\r\\n[µτ,σi] = hτWz + bz. (18)\\r\\nHerein, ǫ is sampled from Gaussian noise, i.e., ǫ ∼ N (0, 1).\\r\\nAlgorithm 1: the workflow of NRODE.\\r\\nInput: {pτ }\\r\\nT\\r\\nτ=T −ω+1\\r\\n1 let h0 = 0;\\r\\n2 for τ = T − ω + 1; τ = τ + 1; τ ≤ T do\\r\\n3 vτ = pτWv + bv;\\r\\n4 a(hτ−1) = hτ−1Wa + ba;\\r\\n5 h\\r\\n′\\r\\nτ = ODESolve(fψ,(hτ−1, a(hτ−1)),(tτ−1, tτ ));\\r\\n6 h\\r\\n′′\\r\\nτ = [vτ , h\\r\\n′\\r\\nτ\\r\\n]Wµ + bµ;\\r\\n7 hτ = I(vi)h\\r\\n′′\\r\\nτ + (1 − I(vτ ))hτ−1;\\r\\n8 zτ = hτ + στ ǫ;\\r\\n9 end\\r\\nOutput: {zτ }\\r\\nT\\r\\nτ=T −ω+1\\r\\nThe general pipeline of NRODE operated in a recursive\\r\\nmanner is summarized in Algorithm 1.\\r\\n4.4 Hierarchical Hypergraph Convolution Network\\r\\nAs the popular hypergraph learning enables modeling\\r\\ncomplex high-order relations, we construct a hierarchical\\r\\nhypergraph convolution network (HHCN) to describe the\\r\\ndomain-aware dependencies regarding the stocks from two\\r\\nviews, i.e., intra-domain and inter-domain. In short, we first\\r\\nconstruct a hypergraph G from various kinds of domain in\\x02teractions to extract the intra-domain Knowledge. Then, we\\r\\nbuild a meta-hypergraph G(G) based on the hypergraph G\\r\\nto address the domain-level interactions for the acquisition\\r\\nof inter-domain knowledge. Below are the details.\\r\\n4.4.1 Intra-domain Knowledge\\r\\nLet G = (S, E) denote a hypergraph, where S is a set\\r\\ncontaining N stocks ( i.e., si ∈ S, N = |S|) and E represents\\r\\na set of hyperedges. Each hyperedge ej ∈ E contains two\\r\\nor more stocks, reflecting an intra-domain fact, e.g., they\\r\\nare from the same industry. Note that the real-world stock\\r\\nrelations (e.g., supplier-consumer relation and ownership\\r\\nrelation) are derived from the third-party open data Dr\\r\\nsuch as Wiki [48]. Additionally, we assign a positive weight\\r\\nwjj to each hyperedge ej, and finally formulate a diagonal\\r\\nmatrix Ψ ∈ R\\r\\n|E |×|E |. Similar to previous works [30], [49], we\\r\\nset Ψ as the identity matrix, which indicates equal weights\\r\\nfor all hyperedges. Actually, the hypergraph G can be equiv\\x02alently denoted by an incidence matrix M ∈ R\\r\\nN×|E | where\\r\\nMij = 1 if the hyperedge ej ∈ E contains a stock si ∈ S,\\r\\notherwise 0. Correspondingly, we can respectively define\\r\\nDii and Ojj as the degree of a stock si and a hyperedge ej,\\r\\nwhere Dii =\\r\\nP\\r\\nej∈E ΨjjMij and Ojj =\\r\\nPN\\r\\ni=0 Mij . Finally,\\r\\nwe obtain the degree matrices D ∈ R\\r\\nN×N and O ∈ R|E |×|E |\\r\\n,\\r\\nwhere D and O are diagonal matrices. Following [49],\\r\\nwe use multi-layer hypergraph convolutions to aggregate\\r\\nthe feature information, which inherently comprises two\\x02stage knowledge passing, followed by stock-hyperedge and\\r\\nhyperedge-stock. For instance, the l-th layer can be repre\\x02sented as:\\r\\nU(l+1) = D−1MΨO−1M⊺U(l)W(l)\\r\\nU\\r\\n, (19)\\r\\nwhere W(l)\\r\\nU\\r\\nis a trainable matrix. Finally, the intra-domain\\r\\nknowledge among the stock will be successfully incorpo-\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 7\\r\\nrated in U(L) ∈ R\\r\\nN×d\\r\\n′\\r\\nafter passing through L hypergraph\\r\\nlayers.\\r\\n4.4.2 Inter-domain Knowledge\\r\\nInter-domain knowledge reflects the domain-level interac\\x02tion behind stocks. Given the hypergraph G = (S, E),\\r\\nwe present a meta-hypergraph G(G) to explore the inter\\x02domain knowledge where each meta-node ci\\r\\nin G(G) is a\\r\\nhyperedge in G. Actually, meta-hypergraph G(G) depicts\\r\\nthe connectivity of hyperedges in G. That is to say, the meta\\x02edge between meta-node ci and meta-node cj refers to their\\r\\ncorresponding hyperedges in G have at least one common\\r\\nstock. In a nutshell, G(G)(C, E) contains a meta-node set\\r\\nC = {ce : ce ∈ E} and a meta-edge set E = {(cem, cen) :\\r\\ncem , cen ∈ E, kem ∩ enk ≥ 1}. In addition, each edge\\r\\n(cem , cen) is associated with a weight Ωmn calculated by\\r\\n|em∩en|\\r\\n|em∪en|\\r\\n. As such, we can obtain a weight matrix Ω for G(G)\\r\\nand regard it as the incidence matrix. Now we can operate\\r\\nthe simple graph convolutional networks for information\\r\\ndistillation, where each layer can be defined as:\\r\\nB\\r\\n(l+1) = Dˆ −1ΩBˆ (l)W(l)\\r\\n, (20)\\r\\nwhere Dˆ is diagonal degree matrix of Ωˆ , Ωˆ = Ω + I, and\\r\\nW(l)\\r\\nB ∈ R\\r\\nd\\r\\n′×d′\\r\\nis a learnable matrix. Herein, I is an identity\\r\\nmatrix. In the end, we obtain the the final inter-domain\\r\\nknowledge B(L) ∈ R\\r\\n|E |×d\\r\\n′\\r\\nafter L convolutional operations.\\r\\n4.4.3 Knowledge Interaction\\r\\nTo incorporate both intra- and inter- domain knowledge for\\r\\nstock recommendation, we design an interactive operation\\r\\nto formulate the fused knowledge F, which is denoted as:\\r\\nF = U(L)B\\r\\n(L)\\r\\n⊺WF , (21)\\r\\nwhere WF ∈ R\\r\\n|E |×d\\r\\nis a learnable matrix. We will incorpo\\x02rate the fused knowledge for downstream stock selection.\\r\\n4.5 Optimization\\r\\nWe now introduce how to incorporate a variety of extracted\\r\\nfeatures into a unified formulation for task prediction. And\\r\\nthen we illustrate the training details including the final\\r\\noptimization object and algorithmic aspect.\\r\\nKnowledge Fusion. As we obtain the temporal correla\\x02tion information and domain knowledge as well as learned\\r\\nlatent variables zT, we concatenate them and employ a\\r\\nsimple dense neural network as a fusion layer to generate\\r\\nthe predicted return ratios rˆ\\r\\nT +1, which can be formulated\\r\\nas:\\r\\nrˆ\\r\\nT +1 = LeakyReLU([pT ; zT ; F]Wr + br), (22)\\r\\nwhere LeakyReLU is the activation function and ; refers to\\r\\nthe concatenation operation. In addition, Wr ∈ R\\r\\nd×d\\r\\nand\\r\\nbr ∈ R\\r\\nd\\r\\nare trainable parameters. Finally, we can produce\\r\\nthe ranked stock list according to the predicted results.\\r\\nELBO. StockODE using latent variables also should op\\x02timize the variational divergence by maximizing the follow\\x02ing evidence lower bound (ELBO):\\r\\nJELBO(θ, φ) = Eqφlog[pθ(X|Z)] + Eqφlog[pθ(Z)] (23)\\r\\n−Eqφ\\r\\nlog[qφ(Z|X)].\\r\\nAlgorithm 2: StockODE Training.\\r\\nInput: stock set S, historical end-of-day data\\r\\n{X}\\r\\nT\\r\\nT −ω+1, and stock relation data Dr.\\r\\n/* Preparation */\\r\\n1 initialize the parameters Θ in StockODE;\\r\\n2 Formulate the correlation tensor Υ via Eq.(1);\\r\\n3 Construct hypergraph G from Dr;\\r\\n4 Construct meta-hypergraph G(G) based on G;\\r\\n5 for epoch i=0; i=i+1; i < N do\\r\\n/* Movement Trend Correlation */\\r\\n6 Obtain explicit correlations H via Eq.(2);\\r\\n7 Generate P via Eq.(3) and Eq.(4);\\r\\n/* NRODE */\\r\\n8 Operate the Algorithm 1 to obtain zT ;\\r\\n/* HHCN */\\r\\n9 Explore the intra-domain knowledge U(L) via\\r\\nEq.(19);\\r\\n10 Explore the inter-domain knowledge B(L) via\\r\\nEq.(20);\\r\\n11 Obtain fused graph knowledge F via Eq.(21);\\r\\n/* Optimization */\\r\\n12 Generate the stock ranking Rˆ\\r\\nT +1\\r\\nvia Eq.(22);\\r\\n13 Compute the training loss according to Eq.(24);\\r\\n14 Update the learnable parameters;\\r\\n15 end\\r\\nOutput: the optimal Model Θ∗.\\r\\nWherein, the first term is the reconstruction likelihood and\\r\\nthe last two terms are the KL-divergence of prior assump\\x02tion (i.e., Gaussian distribution) and the variational poste\\x02rior distribution. We regard our NRODE as the cognition\\r\\nnetwork qφ while using another NRODE as the generative\\r\\nnetwork pθ.\\r\\nMulti-task Learning. In order to minimize the loss be\\x02tween the predicted return ratios and ground truth while\\r\\nkeeping the ranked stocks with higher returns for invest\\x02ment, StockODE which is operated in a multi-task learning\\r\\nmanner uses a point-wise regression and pairwise ranking\\x02aware loss function to optimize the trainable parameters.\\r\\nThe reason is that our goal, which is to discover more\\r\\nprofitable stocks, cannot be reflected by merely applying\\r\\nthe point-wise regression function (e.g., mean squared er\\x02ror). Thus, we also employ the pairwise ranking-aware loss\\r\\nfunction to optimize the ranking of stocks [39]. In addition,\\r\\nwe also should maximize the ELBO to explore the intricate\\r\\ndistribution behind the stock movement patterns. In the\\r\\nend, the optimization object can be summarized as:\\r\\nL(Θ) =\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nrˆ\\r\\nT +1 − r\\r\\nT +1\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n2\\r\\n− βJELBO (24)\\r\\n+\\r\\nX\\r\\n|S|\\r\\ni=0\\r\\nX\\r\\n|S|\\r\\nj=0\\r\\nmax \\x100, −\\r\\n\\x10\\r\\nrˆ\\r\\nT +1\\r\\ni − rˆ\\r\\nT +1\\r\\nj\\r\\n\\x11 \\x10r\\r\\nT +1\\r\\ni − r\\r\\nT +1\\r\\nj\\r\\n\\x11\\x11 ,\\r\\nwhere the first term is point-wise regression loss, the second\\r\\nterm is ELBO, and the last one is pairwise ranking loss\\r\\nregarding return ratio. Herein, Θ refers to the trainable\\r\\nparameters in StockODE. β is a trade-off factor. The general\\r\\ntraining pipeline of StockODE is presented in Algorithm 2.\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 8\\r\\n5 EXPERIMENTS\\r\\nWe now present the details of experimental evaluations to\\r\\nverify the effectiveness of our proposed StockODE. Specif\\x02ically, we first introduce experimental settings including\\r\\ndatasets, metrics, and baselines, followed by implementa\\x02tion details. Next, we compare the experimental results of\\r\\nStockODE with several state-of-the-art baselines. Finally, the\\r\\nablation study, continuity analysis, and sensitivity analysis\\r\\nare presented.\\r\\n5.1 Experimental Setup\\r\\n5.1.1 Datasets.\\r\\nTo verify the performance of our proposed method, we\\r\\nconduct the experiments on two real-world datasets col\\x02lected from NASDAQ and NYSE market [2]. Specifically,\\r\\nthe collected stocks from the NASDAQ and NYSE have\\r\\ntransaction records between January 2, 2013 and November\\r\\n8, 2017, where each selected stock has been traded on more\\r\\nthan 98% of trading days and has never been traded at\\r\\nless than $ 5 per share. In the end, we respectively obtain\\r\\n1,026 and 1,737 stocks for our experiments. Notably, each\\r\\ndataset contains three types of data, including historical\\r\\nindicator data, sector-industry relations, and Wiki relations\\r\\nbetween their companies such as supplier-consumer rela\\x02tions and ownership relations. Specifically, we follow pre\\x02vious work [30] to formulate three types of predefined\\r\\nhyperedges that describe the first-order and second-order\\r\\nrelations stemming from Wikidata besides sector-industry\\r\\nrelations. The details are shown in Table 2. We choose the\\r\\nfirst 756 days’ stock trades for training, the next 252 days’\\r\\nstock trades for validating, and the remaining for testing.\\r\\nTABLE 2\\r\\nStatistics of stock trading data.\\r\\nDescription NASDAQ NYSE\\r\\nPeriod 01/02/2013-12/08/2017 01/02/2013-12/08/2017\\r\\nDays(Train:Val:Test) 756:252:237 756:252:237\\r\\nStocks 1026 1737\\r\\nIndustries Types 112 130\\r\\nWiki Types 42 32\\r\\nHyperedges 1142 1979\\r\\n5.1.2 Baselines.\\r\\nWe compare our StockODE with representative conven\\x02tional and neural network-based methods. The details are:\\r\\n• ARIMA [5] first applies a differencing transformation\\r\\nto a given time series, then uses autoregressive (AR)\\r\\nand moving average (MA) for stock ranking.\\r\\n• LSTM [1] operates a simple LSTM neural network\\r\\nfor stock return prediction, where multiple indicators\\r\\nregarding the stocks are considered, including trading\\r\\nvolume, open price, close price, and etc.\\r\\n• GRU [17] is another widely used RNN model. Similar\\r\\nto LSTM [1], we use it to model the stock movements.\\r\\n• DA-RNN [23] designs a dual-stage attention-based\\r\\nRNN for historical stock movement modeling.\\r\\n• CNNpred [50] aggregates several market factors in a\\r\\nCNN-based framework for feature extraction and finan\\x02cial markets behavior modeling.\\r\\n• StockNet [36] is a deep generative model that uses the\\r\\nvariational Bayes to exploit stock price signals as well\\r\\nas other textual information.\\r\\n• LSTM+GCN [38] uses a standard LSTM model to\\r\\ntackle historical stock prices and adopts the simple\\r\\nGCN to explore the stock correlations.\\r\\n• RSR [2] is a temporal graph-based method, which\\r\\nconsiders the temporal evolution and relation network\\r\\nof stocks.\\r\\n• HATR [10] uses the multiple self-attention layers to\\r\\nlearn the historical stock movements while using the\\r\\nGCN to explore various relations among the stocks.\\r\\n• STHAN-SR [30] incorporates the Hawkes process to\\r\\nenhance the temporal evolution of stock movements,\\r\\nwhere a neural hypergraph is proposed to integrate the\\r\\nintra-domain knowledge.\\r\\nIn each baseline, we first produce the predicted results\\r\\nregarding return ratio and then yield a ranked list of stocks.\\r\\n5.1.3 Metrics.\\r\\nFollowing [2], [30], we evaluate all methods with three\\r\\ncommon metrics for stock investment decision: Sharpe Ratio\\r\\n(SR), Mean Reciprocal Rank (MRR) and NDCG@K.\\r\\nSR is to measure the return of a portfolio against its risk.\\r\\nThe ratio is the average earned return per unit of volatility\\r\\nor total risk over the risk-free rate, which can be calculated\\r\\nas follow:\\r\\nSR =\\r\\nRp − Rf\\r\\nσp\\r\\n, (25)\\r\\nwhere Rp represents return of portfolio investment, Rf rep\\x02resents risk-free rate, and σp represents standard deviation\\r\\nof the portfolio excess returns. We use this classical ratio\\r\\nto show the return level of those stocks that are ranked\\r\\nand recommended by our model after balancing the risk.\\r\\nIn our experiments, we attempt to analyze the return effect\\r\\nof selected results by showing the average return of the\\r\\nselected top five stocks’ SR.\\r\\nAs we aim to select the most profitable stocks for in\\x02vestment, several error-based metrics such as RMSE (Root\\r\\nMean Square Error) and MAE (Mean Absolute Error) are not\\r\\nsuitable for evaluating the ranking performance. We follow\\r\\nprevious studies [2], [39] and choose MRR as one of the\\r\\nsignificant metrics, which evaluates the predicted rank of\\r\\nthe top-1 return ratio stock in the ground truth. To this end,\\r\\nMRR is defined as:\\r\\nMRR = 1\\r\\nT\\r\\nX\\r\\nT\\r\\nτ=1\\r\\n1\\r\\nrankτ\\r\\n, (26)\\r\\nwhere rankτ returns the real position of the predicted\\r\\nhighest-ranked stock in the ground truth on the τ-th testing\\r\\nday and T is the number of testing days.\\r\\nAdditionally, to display the effectiveness of the portfolio,\\r\\nwe also choose NDCG@K as the last metric for model\\r\\nevaluation. NDCG@K is a widely used metric to measure\\r\\nranking quality. Following [30], we report the evaluation\\r\\nresults of NDCG@5 in this paper.\\r\\n5.1.4 Implement Details\\r\\nWe implemented our StockODE and baselines in Python\\r\\nwhere the deep learning-based methods are built upon\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 9\\r\\nTABLE 3\\r\\nPerformance comparison on NASDAQ and NYSE.\\r\\nDataset NASDAQ NYSE\\r\\nWeekly Monthly Weekly Monthly\\r\\nMethod SR MRR NDCG@5 SR MRR NDCG@5 SR MRR NDCG@5 SR MRR NDCG@5\\r\\nARIMA 0.6484 0.0026 0.8090 0.6341 0.0076 0.7418 0.1921 0.0032 0.8057 0.3313 0.0034 0.8236\\r\\nLSTM 0.3135 0.0250 0.8301 0.2127 0.0342 0.8148 1.4760 0.0385 0.7735 0.4124 0.0212 0.7275\\r\\nGRU 0.3937 0.0238 0.6919 0.2370 0.0249 0.7107 - 0.0315 0.8774 0.4974 0.0426 0.8005\\r\\nDA-RNN 1.1010 0.0212 0.6667 0.2664 0.0341 0.8380 1.4318 0.0407 0.7435 1.5184 0.0383 0.7232\\r\\nCNNpred 0.6869 0.0168 0.8997 0.7886 0.0175 0.8085 1.9209 0.0241 0.8891 0.7302 0.0329 0.6613\\r\\nStockNet 0.8283 0.0325 0.6965 1.0999 0.0128 0.7307 0.4257 0.0434 0.8653 - 0.0067 0.8739\\r\\nLSTM+GCN 0.9721 0.0259 0.8785 0.4929 0.0142 0.8832 1.0316 0.0150 0.8462 1.4876 0.0046 0.8862\\r\\nRSR 0.5637 0.0389 0.8283 0.6876 0.0132 0.8200 1.4715 0.0446 0.7735 0.0453 0.0325 0.7849\\r\\nHATR 1.4083 0.0210 0.7972 0.9403 0.0167 0.8039 0.5130 0.0424 0.8403 0.7699 0.0183 0.8549\\r\\nSTHAN-SR 1.4139 0.0240 0.8012 0.9924 0.0190 0.8299 1.1901 0.0312 0.8857 0.4046 0.0405 0.6496\\r\\nStockODE 1.6764 0.0380 0.9433 1.5840 0.0343 0.9432 1.9843 0.0447 0.8892 1.6497 0.0442 0.9420\\r\\nHerein, ‘-’ means that the indicator is negative. For each method, we report average results over five runs.\\r\\nthe PyTorch library, accelerated by the NVIDIA RTX 3090\\r\\nGPU 24G. Specifically, the dimensionality of hidden state\\r\\nin Movement Trend Correlation is 64. The dimensionality\\r\\nof hidden state in NRODE is 64. The output space in our\\r\\nhierarchical hypergraph is 32. We use the popular Adam\\r\\noptimizer for StockODE training, where the initial learning\\r\\nrate is 0.001. Our evaluation function in ODE solvers is\\r\\nthree-layer MLPs with 64 hidden units in each layer. We\\r\\nempirically set β = 0.1.\\r\\n5.2 Comparison and Analysis\\r\\n5.2.1 Overall Performance\\r\\nTable 3 presents the stock ranking performance of different\\r\\nmethods including ours. Specifically, we respectively report\\r\\nthe results conducted on weekly-level and monthly-level\\r\\nstock movements. We note that the best gain is shown in\\r\\nbold, and the second best is shown as underlined.\\r\\nBaseline Performance. Among the non-graph-based\\r\\nbaselines, we first find that ARIMA as a simple time se\\x02ries forecasting model achieves good results in terms of\\r\\nNDCG@5. Especially, ARIMA even performs better than\\r\\nRNN-based (e.g., LSTM and DA-RNN) methods, which\\r\\ndemonstrates that there exist significant temporal depen\\x02dencies behind the historical stock transactions. However,\\r\\nwe observe that the performance of ARIMA in different\\r\\ncontexts has obvious fluctuations regarding SR and MRR.\\r\\nWe also find that LSTM and GRU have similar observations\\r\\nwhen tackling the different datasets or the different levels of\\r\\nhistorical stock movements. We consider the plausible rea\\x02son is that stock movements are usually affected by multiple\\r\\nfactors besides inherent temporal dependencies, as revealed\\r\\nby the Efficient Market Hypothesis (EMH). Compared to\\r\\nGRU, DA-RNN with the attention mechanism performs bet\\x02ter regarding SR, which indicates that learning the potential\\r\\nhierarchical interactions behind the temporal dependencies\\r\\ndoes help make a promising trade-off between the risk\\r\\nand return. For StockNet, it employs the neural variational\\r\\ninference to understand the stock movements by addressing\\r\\nthe intractable posterior inference, which archives more\\r\\nstable results than other RNN-based methods. Among the\\r\\ngraph-based baselines, these methods obtain more robust\\r\\nand higher gains than solely RNN-based methods, which\\r\\ndemonstrates that incorporating the high-order dependen\\x02cies among stocks does facilitate the recommendation of\\r\\nmore profitable stocks. For LSTM+GCN and RSR, they de\\x02vise the graph convolutional neural network-based models\\r\\nto incorporate the corporation relationships, yielding more\\r\\nstable performance when tackling different temporal levels\\r\\nof historical stock transactions. For instance, RSR achieves\\r\\nthe best results in terms of MRR when the lookup window\\r\\nis at the weekly level. STHAN-SR is a recent attention-based\\r\\nmodel that uses the vanilla neural hypergraph network to\\r\\nconsider the intra-domain knowledge behind the corpora\\x02tion relationships, which achieves the best SR results against\\r\\nthe other graph-based methods. However, it does not bring\\r\\nmore effective ranking quality as it performs poorly on MRR\\r\\nand NDCG@5.\\r\\nOur solution. We can observe that our proposed Stock\\x02ODE significantly outperforms all the baselines over both\\r\\ntwo datasets except for weekly-level movements in NAS\\x02DAQ regarding MRR, which demonstrates that StockODE\\r\\nis a more effective method to tackle dependencies behind\\r\\nthe stock movements as well as uncover the complex in\\x02teractions among the stocks. Regarding the longitudinal\\r\\ncomparison, SR shows the return level of the selected stocks\\r\\nand comprehensively considers the level of risk. The higher\\r\\nSR, the better-selected stocks are obtained. Compared with\\r\\nother baselines, our model obtained is well behaved with\\r\\nSR. MRR and NDCG@5 show the difference between the\\r\\nselected stocks and the real stock ranking. They can provide\\r\\na basis for the selection of stocks to a certain extent. Overall,\\r\\nthe prediction difference between the stocks recommended\\r\\nby the model and the best ranking of real stock returns is the\\r\\nsmallest, so the recommended ratios are the highest. We will\\r\\nconduct more empirical investigations on each of the design\\r\\ncomponents of StockODE in the following to evaluate their\\r\\ndistinctive contributions.\\r\\nODE Comparison. We also select the most recent au\\x02toregressive models with ODE as the variants of our Stock\\x02ODE. Specifically, we adopt the Latent ODE [21] for stock\\r\\nmovement learning where the ODE is only used in the\\r\\nreconstruction part. And we define it as StockODEL. In\\r\\naddition, we use the ODE-RNN [42] as our encoder part,\\r\\nand we call this model as StockODEG where we select the\\r\\nGRU as the recursive cell. As shown in Fig. 5, It is clear\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 10\\r\\nthat most evaluation results obtained from our StockODE\\r\\nachieve the best performance compared to traditional ODEs,\\r\\nwhich indicates that our devised NRODE is a reasonable\\r\\nand competitive time-dependent learning module.\\r\\n\\t\\x06\\x0c\\x07\\x06\\x0b\\x0e\\x17\\x10\\x10\\x12 \\t\\x06\\x0c\\x07\\x06\\x0b\\x0e\\x13\\x15\\x14\\x16\\x11 \\t\\r\\x0c\\x08\\x0e\\x17\\x10\\x10\\x12 \\t\\r\\x0c\\x08\\x0e\\x13\\x15\\x14\\x16\\x11\\r\\n\\x02\\x01\\x02\\r\\n\\x02\\x01\\x05\\r\\n\\x03\\x01\\x02\\r\\n\\x03\\x01\\x05\\r\\n\\x04\\x01\\x02\\r\\n\\x0c\\x16\\x15\\x0f\\x12\\n\\x07\\x08 \\x0c\\x16\\x15\\x0f\\x12\\n\\x07\\x08\\x01 \\x0c\\x16\\x15\\x0f\\x12\\n\\x07\\x08\\r\\n(a) SR.\\r\\n\\n\\x07\\r\\x08\\x07\\x0c\\x0f\\x18\\x11\\x11\\x13 \\n\\x07\\r\\x08\\x07\\x0c\\x0f\\x14\\x16\\x15\\x17\\x12 \\n\\x0e\\r\\t\\x0f\\x18\\x11\\x11\\x13 \\n\\x0e\\r\\t\\x0f\\x14\\x16\\x15\\x17\\x12\\r\\n\\x02\\x01\\x02\\x02\\r\\n\\x02\\x01\\x02\\x03\\r\\n\\x02\\x01\\x02\\x04\\r\\n\\x02\\x01\\x02\\x05\\r\\n\\x02\\x01\\x02\\x06\\r\\n\\r\\x17\\x16\\x10\\x13\\x0b\\x08\\t \\r\\x17\\x16\\x10\\x13\\x0b\\x08\\t\\x01 \\r\\x17\\x16\\x10\\x13\\x0b\\x08\\t\\r\\n(b) MRR.\\r\\n\\x0b\\x08\\x0e\\t\\x08\\r\\x10\\x19\\x12\\x12\\x14 \\x0b\\x08\\x0e\\t\\x08\\r\\x10\\x15\\x17\\x16\\x18\\x13 \\x0b\\x0f\\x0e\\n\\x10\\x19\\x12\\x12\\x14 \\x0b\\x0f\\x0e\\n\\x10\\x15\\x17\\x16\\x18\\x13\\r\\n\\x02\\x01\\x02\\r\\n\\x02\\x01\\x04\\r\\n\\x02\\x01\\x05\\r\\n\\x02\\x01\\x06\\r\\n\\x02\\x01\\x07\\r\\n\\x03\\x01\\x02\\r\\n\\x0e\\x18\\x17\\x11\\x14\\x0c\\t\\n \\x0e\\x18\\x17\\x11\\x14\\x0c\\t\\n\\x01 \\x0e\\x18\\x17\\x11\\x14\\x0c\\t\\n\\r\\n(c) NDCG@5.\\r\\nFig. 5. ODE comparison.\\r\\n\\x03\\x0e\\t\\x0b\\x03\\r\\n\\n\\x0f\\x10\\x0b\\r\\n\\x07\\x0e\\x11\\r\\n\\x05\\x03\\x02\\x0e\\x0c\\x0c\\r\\n\\x04\\x0c\\x0c\\x17\\x18\\x14\\x13\\r\\n\\x0f\\x19\\x16\\x12\\x15\\x0c\\x14\\x19\\r\\n\\n\\x0f\\x10\\x0b\\x01\\x07\\x04\\x0c\\r\\n\\x0e\\x0f\\x0e\\r\\n\\x08\\x03\\x10\\x0e\\r\\n\\x0f\\x10\\x08\\x03\\x0c\\x02\\x0f\\x0e\\r\\n\\x0f\\x19\\x16\\x12\\x15\\r\\x05\\x06\\r\\n\\r\\n\\x0f\\x19\\x16\\x12\\x15\\r\\x05\\x06\\r\\n\\x01\\r\\n\\x0f\\x19\\x16\\x12\\x15\\r\\x05\\x06\\r\\n\\r\\n\\x01\\r\\n\\x02\\r\\n\\x03\\r\\n\\x04\\r\\n\\x05\\r\\n\\x06\\r\\n\\x0b\\x02\\t\\x01\\x03\\x02\\n\\x05\\x07\\x02\\x04\\x07\\x05\\x08\\x06\\r\\n\\x0f\\x19\\x16\\x12\\x15\\r\\x05\\x06\\r\\nFig. 6. Training time comparison.\\r\\nModel Efficiency. Finally, we visualize the training time\\r\\nof the baselines as well as our StockODE. As shown in Fig. 6,\\r\\nthe blue one represents the training time after reaching opti\\x02mal performance, and the orange one represents the average\\r\\ntime of 50 training epochs. We can observe that StockODE\\r\\nobtains competitive performance in terms of training time.\\r\\nTABLE 4\\r\\nExperimental results on the NASDAQ dataset at the weekly level.\\r\\nMethod SR MRR NDCG@5\\r\\nGRU 0.3937 0.0238 0.6919\\r\\nStockODE-B 1.2228 0.0346 0.8665\\r\\nStockODE-I 0.5004 0.0334 0.9299\\r\\nStockODE-H 1.4508 0.0320 0.9432\\r\\nStockODE-A 1.1182 0.0317 0.8812\\r\\nStockODE 1.6764 0.0380 0.9433\\r\\n0 \\x03 \\x04 \\x05 \\x06 \\x07 \\x08 \\t \\n \\x0b \\x030 \\x03\\x03 \\x03\\x04 \\x03\\x05 \\x03\\x06 \\x03\\x07 \\x03\\x08 \\x03\\t \\x03\\n \\x03\\x0b \\x040 \\x04\\x03 \\x04\\x04 \\x04\\x05 \\x04\\x06 \\x04\\x07 \\x04\\x08 \\x04\\t \\x04\\n \\x04\\x0b \\x050\\r\\n\\x07\\x11\\t\\n\\x0e\\x0f\\r\\x08\\n\\t\\x15\\x12\\r\\n\\x0c\\x0c\\x12\\x12\\r\\n\\x0c\\x0c\\x13\\x0f\\r\\n\\x0c\\x0c\\x13\\x10\\r\\n\\x0c\\r\\x0c\\x13\\r\\n\\x0c\\r\\x0e\\r\\r\\n\\x02\\x03\\x06\\x04\\x01\\x12\\x02\\x13\\x13\\x0b\\x0f\\x13\\x0e\\x10\\x0f\\r\\n\\x0c\\r\\x0e\\x11\\r\\n0 \\x07 \\x030 \\x03\\x07 \\x040 \\x04\\x07 \\x050\\r\\n0\\x010\\x04\\x07\\r\\n0\\x01000\\r\\n0\\x010\\x04\\x07\\r\\n\\x05\\x0b\\x13\\x14\\x11\\x0f\\x11\\t\\x13\\x0e\\x10\\x10\\x0c\\x02\\x03\\x06\\x04\\r\\n\\x18\\x1e\\x1c \\x1b\\x16\\x14\\x1f\\x1e \\x1f\\x19 \\x1d\\x1e\\x17\\x16\\x1a\\x15\\x1f\\x1a\\x1c\\x1b\\r\\nFig. 7. The visualization of implicit correlation in StockODE.\\r\\n5.2.2 Ablation Analysis\\r\\nNow we further conduct ablation analysis to investigate the\\r\\ncontribution of each significant component in our Stock\\x02ODE. The variants of StockODE are as follows:\\r\\n• StockODE-B: It uses our Movement Trend Correlation\\r\\nmodule for capturing the correlations among different\\r\\nstocks regarding the time-evolving movements, and then\\r\\nthe standard GRU is employed for the stock movement\\r\\nmodeling.\\r\\n• StockODE-I: This variant does not consider the interac\\x02tive facts in StockODE.\\r\\n• StockODE-H: It does not consider the Inter-domain\\r\\nknowledge part in hypergraph learning.\\r\\n• StockODE-A: It replaces our NRODE in StockODE with\\r\\nthe standard GRU for stock movement pattern learning.\\r\\nAs shown in Table 4, we also report experimental results\\r\\nobtained from a simple GRU [17]. Generally, we can observe\\r\\nthat the experimental results regarding each variant are\\r\\nsignificantly distinguishable, which demonstrates that each\\r\\ndevised component does help facilitate task performance.\\r\\nIn detail, StockODE-B outperforms GRU, which demon\\x02strates that correlating the movement trends among differ\\x02ent stocks is a positive signal to promote the selection of\\r\\nprofitable stocks. StockODE-I performs worse than Stock\\x02ODE especially on SR, which indicates that incorporating\\r\\nthe higher-order stock relations from multiple domains is\\r\\ncapable of providing higher returns as well as relieving\\r\\nthe investment risks. For StockODE-H, it performs worse\\r\\nthan our StockODE, which demonstrates that inter-domain\\r\\nknowledge is also a key signal for promoting stock ranking.\\r\\nThe plausible reason is such a piece of knowledge learned\\r\\nby our hypergraph network is capable of exploiting the\\r\\nimplicit stock correlations. Finally, StockODE outperforming\\r\\nStockODE-A reveals that our proposed NRODE is a highly\\r\\npredominant module that enables dynamically the temporal\\r\\ndependencies from the historical stock movements.\\r\\nAs shown in Fig 7, we take stock AAWW as an example\\r\\nto show the results of our Implicit Correlation Aggregation\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 11\\r\\nFig. 8. The comparison of the average returns of the top five stocks.\\r\\nin StockODE, as well as the prediction results compared to\\r\\nthe ground truth. As the top of Fig 7 presents, our StockODE\\r\\nis able to predict return ratios that closely match the ground\\r\\ntruth over the long term, especially for the evolutionary\\r\\ntrends. During inference, we visualize part of the attention\\r\\nscores of AAWW regarding other stocks. The darker the\\r\\ncolor, the higher the attention score. We can find that their\\r\\nscores evolve over time, suggesting that considering time\\x02varying correlations can help us reveal the strength of the\\r\\nrelationship between different stocks in the time domain.\\r\\nIn the end, we use each of the above models including\\r\\nour StockODE to predict a ranked list and then use each list\\r\\nto evaluate investment returns. Specifically, we use the real\\r\\nstock prices of the five selected stocks to calculate the daily\\r\\nreturn ratio, and then judge the profit margin brought by\\r\\nthe selection results for investors. Fig. 8 shows the average\\r\\nreturn ratios of the top five stocks over the 50 testing days.\\r\\nWe can find that the selected results from StockODE bring\\r\\nus higher profits on most of the trading days than other\\r\\nmethods.\\r\\n5.2.3 Continuity Analysis\\r\\nNow we turn to make continuity analysis to demonstrate\\r\\nthe advantage of ODE regarding the stock market fluctua\\x02tions. As one of the inherent advantages, StockODE follows\\r\\nthe paradigm of continuous dynamics systems, which can\\r\\ngenerate flexible but extensive results with varying obser\\x02vation time intervals. We visualize averaged return ratio of\\r\\nthe top-five stocks of NASDAQ selected by our StockODE.\\r\\nAs shown in Fig. 9, the red curve is the ground-truth results\\r\\nbased on selected stocks. And we use the StockODE with\\r\\n10 different time steps, ranging from (0.0,1.0), to generate\\r\\na more fine-grained curve to mimic the changes of these\\r\\nstocks, we can find the blue curve significantly approxi\\x02mates the ground truth across long-term testing days. As we\\r\\nmentioned in Sec. 1, the stock prices could fluctuate rapidly\\r\\nwithin minutes or hours, traditional RNN-based model can\\r\\nonly infer the results with a discrete fixed time interval (i.e.,\\r\\ndaily observation.). In contrast, our StockODE is capable of\\r\\ngenerating observations at any flexible time. That is to say,\\r\\nStockODE can generate more fine-grained stock movements\\r\\nand incorporate immediate volatility and uncertainty in\\r\\nmovements to help investors mitigate investment risks. As\\r\\nshown in Fig. 10, we visualize the predicted evolution of\\r\\naverage returns with a fine-grained time step, i.e., 0.1. We\\r\\nfind that the results in different time horizons have obvious\\r\\ndisturbances within each test trading day, suggesting that\\r\\nour StockODE is capable of simulating the uncertainty of\\r\\nstock movements.\\r\\nFig. 9. The evolution of long-term daily average returns.\\r\\n\\x04 \\x06 \\x07 \\t \\x0b \\x05\\x04\\r\\n\\n\\x08\\x01\\x02\\x05\\x06\\x04\\x02\\x01\\r\\t\\r\\n\\x04\\x03\\x04\\x08\\r\\n\\x04\\x03\\x05\\x04\\r\\n\\x04\\x03\\x05\\x08\\r\\n\\x04\\x03\\x06\\x04\\r\\n\\x08\\x03\\x0b\\x0c\\x08\\x06\\x08\\x01\\x0b\\x05\\x07\\r\\n\\n\\x0c\\x05\\r\\n\\n\\x04\\x02\\x05\\x01\\r\\n(a) short-term.\\r\\n\\n\\x04 \\n\\x06 \\n\\x07 \\n\\t \\n\\x0b\\r\\n\\n\\x08\\x01\\x02\\x05\\x06\\x04\\x02\\x01\\r\\t\\r\\n\\x04\\x03\\x05\\x04\\r\\n\\x04\\x03\\x05\\x08\\r\\n\\x04\\x03\\x06\\x04\\r\\n\\x04\\x03\\x06\\x08\\r\\n\\x08\\x03\\x0b\\x0c\\x08\\x06\\x08\\x01\\x0b\\x05\\x07\\r\\n\\x0c\\r\\x05\\r\\n\\x0c\\x04\\x02\\x05\\x01\\r\\n(b) middle-term.\\r\\n\\x05\\x04\\x04 \\x05\\x04\\x06 \\x05\\x04\\x08 \\x05\\x04\\n \\x05\\x04\\x0c \\x05\\x05\\x04\\r\\n\\n\\x08\\x01\\x02\\x05\\x06\\x04\\x02\\x01\\r\\t\\r\\n\\x04\\x03\\x05\\x04\\r\\n\\x04\\x03\\x05\\t\\r\\n\\x04\\x03\\x06\\x04\\r\\n\\x04\\x03\\x06\\t\\r\\n\\x04\\x03\\x07\\x04\\r\\n\\x04\\x03\\x07\\t\\r\\n\\x04\\x03\\x08\\x04\\r\\n\\x08\\x03\\x0b\\x0c\\x08\\x06\\x08\\x01\\x0b\\x05\\x07\\r\\n\\x0b\\r\\x05\\r\\n\\x0b\\x04\\x02\\x05\\x01\\r\\n(c) long-term.\\r\\nFig. 10. The evolution of average returns over different time horizons.\\r\\nFig. 11. The heat map of the ranking results of the selected 20 stocks\\r\\nover the first 20 testing days. Specifically, the left figure is the best\\x02ranking position of each stock according to each day’s stock price, the\\r\\nright figure presents the predicted ranking results.\\r\\nSince most investors may pay attention to the investment\\r\\nreturns brought by short-term fluctuations in the stock mar\\x02ket and ignore the long-term returns of stock investments,\\r\\nresulting in potential investment risks. According to the left\\r\\npart of Fig. 11, we generate the daily best rankings by calcu\\x02lating the return ratio of the stocks for each test trading day,\\r\\nwe can find the ranking score of each selected stock shifts\\r\\nfrequently due to the fluctuations of stock prices, which\\r\\ncould confuse investors when picking potentially profitable\\r\\nstocks to invest in. In contrast, the results of the right part\\r\\nof Fig. 11 derived from our StockODE demonstrate that\\r\\nthe predicted ranking score of each stock is significantly\\r\\npersistent. The reason is that StockODE coupling with ODE\\r\\ncan consider stock trend fluctuations over a long period of\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 12\\r\\ntime to relieve the risk of unreasonable decision-making and\\r\\ninvestment losses.\\r\\n\\n\\t \\r\\x0b \\x08\\t\\x0f\\r\\n\\x03\\x04\\x01\\x01\\x02\\x05\\x06\\x04\\x07\\x02\\r\\n\\x07\\x05\\x07\\x07\\r\\n\\x07\\x05\\t\\x0c\\r\\n\\x07\\x05\\x0c\\x07\\r\\n\\x07\\x05\\x0e\\x0c\\r\\n\\x08\\x05\\x07\\x07\\r\\n\\x08\\x05\\t\\x0c\\r\\n\\x08\\x05\\x0c\\x07\\r\\n\\x08\\x05\\x0e\\x0c\\r\\n\\t\\x05\\x07\\x07\\r\\n\\x06\\x02\\x01\\x03\\x0c \\x10\\x11 \\x04\\x11\\x11\\r\\n\\x07\\x05\\x07\\x08\\x07\\r\\n\\x07\\x05\\x07\\x08\\x0c\\r\\n\\x07\\x05\\x07\\t\\x07\\r\\n\\x07\\x05\\x07\\t\\x0c\\r\\n\\x07\\x05\\x07\\n\\x07\\r\\n\\x07\\x05\\x07\\n\\x0c\\r\\n\\x07\\x05\\x07\\x0b\\x07\\r\\n\\x07\\x05\\x07\\x0b\\x0c\\r\\n\\x07\\x05\\x07\\x0c\\x07\\r\\n(a) The metric results.\\r\\n5\\x02 \\x03\\x02\\x02 \\x035\\x02 \\x04\\x02\\x02\\r\\n\\x08\\x06\\x01\\x02\\x04\\x05\\x03\\x02\\x01\\t\\x07\\r\\n\\x02\\x01\\x02\\x02\\t5\\r\\n\\x02\\x01\\x02\\x025\\x02\\r\\n\\x02\\x01\\x02\\x02\\x045\\r\\n\\x02\\x01\\x02\\x02\\x02\\x02\\r\\n\\x02\\x01\\x02\\x02\\x045\\r\\n\\x02\\x01\\x02\\x025\\x02\\r\\n\\x02\\x01\\x02\\x02\\t5\\r\\n\\x02\\x01\\x02\\x03\\x02\\x02\\r\\n\\x0f\\x10\\r\\r\\x0e\\x11\\x0c\\x12\\x10\\x13\\x0e\\x0b\\x05\\x04\\r\\n\\x0f\\x10\\r\\r\\x0e\\x11\\x0c\\x12\\x10\\x13\\x0e\\x0b\\x08\\x06\\r\\n\\x0f\\x10\\r\\r\\x0e\\x11\\x0c\\x12\\x10\\x13\\x0e\\x0b\\x03\\x04\\n\\r\\n(b) The movements of returns.\\r\\n\\t\\x08\\x08\\x03 \\x11\\x08\\x03 \\r\\x08\\x03\\r\\n\\x06\\x01\\x03\\x07\\x02\\x05\\x04\\r\\n\\x08\\x06\\x08\\x08\\r\\n\\x08\\x06\\n\\r\\r\\n\\x08\\x06\\r\\x08\\r\\n\\x08\\x06\\x0f\\r\\r\\n\\t\\x06\\x08\\x08\\r\\n\\t\\x06\\n\\r\\r\\n\\t\\x06\\r\\x08\\r\\n\\t\\x06\\x0f\\r\\r\\n\\n\\x06\\x08\\x08\\r\\n\\x07\\x02\\x01\\x04\\r \\x0b\\x0c \\x05\\x0c\\x0c\\r\\n\\x08\\x06\\x08\\t\\x08\\r\\n\\x08\\x06\\x08\\t\\r\\r\\n\\x08\\x06\\x08\\n\\x08\\r\\n\\x08\\x06\\x08\\n\\r\\r\\n\\x08\\x06\\x08\\x0e\\x08\\r\\n\\x08\\x06\\x08\\x0e\\r\\r\\n\\x08\\x06\\x08\\x10\\x08\\r\\n\\x08\\x06\\x08\\x10\\r\\r\\n\\x08\\x06\\x08\\r\\x08\\r\\n(c) The metric results.\\r\\n5\\x02 \\x03\\x02\\x02 \\x035\\x02 \\x04\\x02\\x02\\r\\n\\x08\\x06\\x01\\x02\\x04\\x05\\x03\\x02\\x01\\t\\x07\\r\\n\\x05\\x02\\x01\\x02\\x02\\x075\\r\\n\\x05\\x02\\x01\\x02\\x025\\x02\\r\\n\\x05\\x02\\x01\\x02\\x02\\x045\\r\\n\\x02\\x01\\x02\\x02\\x02\\x02\\r\\n\\x02\\x01\\x02\\x02\\x045\\r\\n\\x02\\x01\\x02\\x025\\x02\\r\\n\\x02\\x01\\x02\\x02\\x075\\r\\n\\x02\\x01\\x02\\x03\\x02\\x02\\r\\n\\x03\\x02\\x02\\r\\n\\x08\\x02\\r\\n5\\x02\\r\\n(d) The movements of returns.\\r\\n\\x08 \\t \\n \\x0b \\x0c\\r\\n\\x06\\x01\\x0b\\x02\\t\\n\\x07\\x03\\x04\\t\\x01\\x08\\x05\\r\\n\\x07\\x05\\x07\\x07\\r\\n\\x07\\x05\\t\\x0c\\r\\n\\x07\\x05\\x0c\\x07\\r\\n\\x07\\x05\\x0e\\x0c\\r\\n\\x08\\x05\\x07\\x07\\r\\n\\x08\\x05\\t\\x0c\\r\\n\\x08\\x05\\x0c\\x07\\r\\n\\x08\\x05\\x0e\\x0c\\r\\n\\t\\x05\\x07\\x07\\r\\n\\x06\\x02\\x01\\x03\\x0c \\r\\x0f \\x04\\x0f\\x0f\\r\\n\\x07\\x05\\x07\\x08\\x07\\r\\n\\x07\\x05\\x07\\x08\\x0c\\r\\n\\x07\\x05\\x07\\t\\x07\\r\\n\\x07\\x05\\x07\\t\\x0c\\r\\n\\x07\\x05\\x07\\n\\x07\\r\\n\\x07\\x05\\x07\\n\\x0c\\r\\n\\x07\\x05\\x07\\x0b\\x07\\r\\n\\x07\\x05\\x07\\x0b\\x0c\\r\\n\\x07\\x05\\x07\\x0c\\x07\\r\\n(e) The metric results.\\r\\n5\\x04 \\x05\\x04\\x04 \\x055\\x04 \\x06\\x04\\x04\\r\\n\\x08\\x06\\x01\\x02\\x04\\x05\\x03\\x02\\x01\\t\\x07\\r\\n\\x07\\x04\\x03\\x04\\x04\\x0b5\\r\\n\\x07\\x04\\x03\\x04\\x045\\x04\\r\\n\\x07\\x04\\x03\\x04\\x04\\x065\\r\\n\\x04\\x03\\x04\\x04\\x04\\x04\\r\\n\\x04\\x03\\x04\\x04\\x065\\r\\n\\x04\\x03\\x04\\x045\\x04\\r\\n\\x04\\x03\\x04\\x04\\x0b5\\r\\n\\x04\\x03\\x04\\x05\\x04\\x04\\r\\n\\x02\\r\\x01\\x08\\x0e\\x05\\r\\n\\x02\\r\\x01\\x08\\x0e\\x06\\r\\n\\x02\\r\\x01\\x08\\x0e\\n\\r\\n\\x02\\r\\x01\\x08\\x0e\\x0c\\r\\n\\x02\\r\\x01\\x08\\x0e5\\r\\n(f) The movements of returns.\\r\\n\\x08 \\t \\n \\x0b \\x0c\\r\\n\\x05\\x01\\x0b\\x02\\x08\\t\\x07\\x03\\x01\\n\\n\\x02\\x06\\n\\x04\\x07\\x06\\r\\n\\x07\\x05\\x07\\x07\\r\\n\\x07\\x05\\t\\x0c\\r\\n\\x07\\x05\\x0c\\x07\\r\\n\\x07\\x05\\x0e\\x0c\\r\\n\\x08\\x05\\x07\\x07\\r\\n\\x08\\x05\\t\\x0c\\r\\n\\x08\\x05\\x0c\\x07\\r\\n\\x08\\x05\\x0e\\x0c\\r\\n\\t\\x05\\x07\\x07\\r\\n\\x06\\x02\\x01\\x03\\x0c \\r\\x0f \\x04\\x0f\\x0f\\r\\n\\x07\\x05\\x07\\x08\\x07\\r\\n\\x07\\x05\\x07\\x08\\x0c\\r\\n\\x07\\x05\\x07\\t\\x07\\r\\n\\x07\\x05\\x07\\t\\x0c\\r\\n\\x07\\x05\\x07\\n\\x07\\r\\n\\x07\\x05\\x07\\n\\x0c\\r\\n\\x07\\x05\\x07\\x0b\\x07\\r\\n\\x07\\x05\\x07\\x0b\\x0c\\r\\n\\x07\\x05\\x07\\x0c\\x07\\r\\n(g) The metric results.\\r\\n5\\x04 \\x05\\x04\\x04 \\x055\\x04 \\x06\\x04\\x04\\r\\n\\x08\\x06\\x01\\x02\\x04\\x05\\x03\\x02\\x01\\t\\x07\\r\\n\\x07\\x04\\x03\\x04\\x04\\x0b5\\r\\n\\x07\\x04\\x03\\x04\\x045\\x04\\r\\n\\x07\\x04\\x03\\x04\\x04\\x065\\r\\n\\x04\\x03\\x04\\x04\\x04\\x04\\r\\n\\x04\\x03\\x04\\x04\\x065\\r\\n\\x04\\x03\\x04\\x045\\x04\\r\\n\\x04\\x03\\x04\\x04\\x0b5\\r\\n\\x04\\x03\\x04\\x05\\x04\\x04\\r\\n\\x02\\r\\x01\\x08\\x0e\\x05\\r\\n\\x02\\r\\x01\\x08\\x0e\\x06\\r\\n\\x02\\r\\x01\\x08\\x0e\\n\\r\\n\\x02\\r\\x01\\x08\\x0e\\x0c\\r\\n\\x02\\r\\x01\\x08\\x0e5\\r\\n(h) The movements of returns.\\r\\nFig. 12. Sensitivity Analysis on NASDAQ at the weekly level.\\r\\n5.2.4 Sensitivity Analysis\\r\\nFinally, we turn to investigate the sensitivity of StockODE,\\r\\ne.g., hidden size and the edge rate in HHCN. Note that we\\r\\ndo not change other hyperparameters when varying any one\\r\\ninvestigated hyperparameters.\\r\\nHidden Size. As shown in Fig. 12(a) and Fig. 12(b), the\\r\\nleft y-axis corresponds to the collected results of SR and\\r\\nNDCF@5 while the right is related to the results of MRR.\\r\\nThey demonstrate that the hidden size significantly affects\\r\\nthe model performance, especially for MRR. Nevertheless,\\r\\nthe larger size of the hidden layer does not bring us better\\r\\nresults, and thus we set the hidden size to 64 to trade off the\\r\\nnumber of trainable parameters and model performance.\\r\\nRelationship Sensitivity. To quantify the impact of ex\\x02tracted relationships among stocks, we randomly remove\\r\\n10% and 50% relationships from sector-industry relations\\r\\nand Wiki relations, the results in Fig. 12(c) and Fig. 12(d)\\r\\ndemonstrate that domain-aware dependencies do affect the\\r\\nmodel performance. And it also indicates that incorporating\\r\\nricher prior knowledge among the stock could bring higher\\r\\ngains in terms of investment decisions.\\r\\nDepth of HHCN. We turn to study the impact of\\r\\ngraph layers in HHCN. Specifically, we increase or decrease\\r\\nthe same number of graph layers in intra-domain knowl\\x02edge and inter-domain knowledge learning, respectively.\\r\\nAs Fig. 12(e) and Fig. 12(f) show, we find that using more\\r\\ngraph layers does not bring higher achievements. The most\\r\\nplausible reason is the over-fitting issue. Thus, choosing one\\r\\nlayer is enough.\\r\\nDepth of Attention Layers. We finally investigate the\\r\\nimpact of attention layers in the Movement Trend Cor\\x02relation module. As Fig. 12(g) and Fig. 12(h) show, we\\r\\nfind a similar observation as the impact of graph layers in\\r\\nHHCN. Hence, we only use a one-layer attention network\\r\\nfor movement correlation learning.\\r\\n6 CONCLUSIONS AND FUTURE WORK\\r\\nThis paper presented a novel framework, i.e., StockODE,\\r\\nfor stock selection, a practical but intricate task in the field\\r\\nof investment decision-making. In contrast to most existing\\r\\nstock movement prediction efforts, we primarily seek to\\r\\nprovide a valuable stock ranking list for investors, aiming\\r\\nto help them alleviate the investment risks. The proposed\\r\\nStockODE inspired by the recent neural dynamic system\\r\\nprovides a new perspective on stock movement learning.\\r\\nIn particular, we devised a Movement Trend Correlation\\r\\nmodule to capture the stock relations regarding the time\\x02varying return ratio aspect. Also, we proposed a hierar\\x02chical hypergraph to consider both explicit and implicit\\r\\ndependencies among different domains, which aimed to\\r\\nstrengthen the impact of higher-order collaboration in the\\r\\nevolution of stocks. Our experimental results demonstrate\\r\\nthat StockODE enables promoting the ranking performance\\r\\ncompared to state-of-the-art baselines. In the future, we will\\r\\nconsider employing financial news and public reviews to\\r\\nprovide more evidence impacts behind the stocks.\\r\\nACKNOWLEDGMENT\\r\\nThis work was supported by the National Natural Sci\\x02ence Foundation of China under Grant 62102326, the Key\\r\\nResearch and Development Project of Sichuan Province\\r\\nunder Grant 2022YFG0314, Guanghua Talent Project, and\\r\\nthe Financial Intelligence and Financial Engineering Key\\r\\nLaboratory of Sichuan Province.\\r\\nREFERENCES\\r\\n[1] K. Chen, Y. Zhou, and F. Dai, “A lstm-based method for stock\\r\\nreturns prediction: A case study of china stock market,” in IEEE\\r\\nBig Data. IEEE, 2015, pp. 2823–2824.\\r\\n[2] F. Feng, X. He, X. Wang, C. Luo, Y. Liu, and T.-S. Chua, “Tempo\\x02ral relational ranking for stock prediction,” ACM Transactions on\\r\\nInformation Systems (TOIS), vol. 37, pp. 1 – 30, 2019.\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 13\\r\\n[3] J. R. Nofsinger, “Social mood and financial economics,” The Journal\\r\\nof Behavioral Finance, vol. 6, no. 3, pp. 144–160, 2005.\\r\\n[4] R. Cheng and Q. Li, “Modeling the momentum spillover effect for\\r\\nstock prediction via attribute-driven graph attention networks,” in\\r\\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 35,\\r\\nno. 1, 2021, pp. 55–62.\\r\\n[5] A. A. Ariyo, A. O. Adewumi, and C. K. Ayo, “Stock price predic\\x02tion using the arima model,” in 2014 UKSim-AMSS 16th Interna\\x02tional Conference on Computer Modelling and Simulation. IEEE, 2014,\\r\\npp. 106–112.\\r\\n[6] P. H. Franses and D. Van Dijk, “Forecasting stock market volatility\\r\\nusing (non-linear) garch models,” Journal of Forecasting, vol. 15,\\r\\nno. 3, pp. 229–235, 1996.\\r\\n[7] Y. Lin, H. Guo, and J. Hu, “An svm-based approach for stock\\r\\nmarket trend prediction,” in IJCNN. IEEE, 2013, pp. 1–7.\\r\\n[8] P. Sadorsky, “A random forests approach to predicting clean\\r\\nenergy stock prices,” Journal of Risk and Financial Management,\\r\\nvol. 14, no. 2, p. 48, 2021.\\r\\n[9] M. R. Hassan and B. Nath, “Stock market forecasting using hidden\\r\\nmarkov model: a new approach,” in 5th International Conference on\\r\\nIntelligent Systems Design and Applications (ISDA’05). IEEE, 2005,\\r\\npp. 192–196.\\r\\n[10] H. Wang, S. Li, T. Wang, and J. Zheng, “Hierarchical adaptive\\r\\ntemporal-relational modeling for stock trend prediction,” in IJCAI,\\r\\n2021, pp. 3691–3698.\\r\\n[11] L. Zhang, C. Aggarwal, and G.-J. Qi, “Stock price prediction via\\r\\ndiscovering multi-frequency trading patterns,” in Proceedings of the\\r\\n23rd ACM SIGKDD international conference on knowledge discovery\\r\\nand data mining, 2017, pp. 2141–2149.\\r\\n[12] E. F. Fama, “The behavior of stock-market prices,” The Journal of\\r\\nBusiness, vol. 38, no. 1, pp. 34–105, 1965.\\r\\n[13] L. Shi, Z. Teng, L. Wang, Y. Zhang, and A. Binder, “Deepclue:\\r\\nvisual interpretation of text-based deep stock prediction,” IEEE\\r\\nTransactions on Knowledge and Data Engineering, vol. 31, no. 6, pp.\\r\\n1094–1108, 2018.\\r\\n[14] Q. Li, J. Tan, J. Wang, and H. Chen, “A multimodal event\\x02driven lstm model for stock prediction using online news,” IEEE\\r\\nTransactions on Knowledge and Data Engineering, vol. 33, no. 10, pp.\\r\\n3323–3337, 2020.\\r\\n[15] R. Sawhney, A. Wadhwa, S. Agarwal, and R. Shah, “Fast: Financial\\r\\nnews and tweet based time aware network for stock trading,”\\r\\nin Proceedings of the 16th Conference of the European Chapter of the\\r\\nAssociation for Computational Linguistics: Main Volume, 2021, pp.\\r\\n2164–2175.\\r\\n[16] J. Gao, X. Ying, C. Xu, J. Wang, S. Zhang, and Z. Li, “Graph\\x02based stock recommendation by time-aware relational attention\\r\\nnetwork,” ACM Transactions on Knowledge Discovery from Data\\r\\n(TKDD), vol. 16, no. 1, pp. 1–21, 2021.\\r\\n[17] G. Shen, Q. Tan, H. Zhang, P. Zeng, and J. Xu, “Deep learning with\\r\\ngated recurrent unit networks for financial sequence predictions,”\\r\\nProcedia computer science, vol. 131, pp. 895–903, 2018.\\r\\n[18] Q. Ding, S. Wu, H. Sun, J. Guo, and J. Guo, “Hierarchical multi\\x02scale gaussian transformer for stock movement prediction.” in\\r\\nIJCAI, 2020, pp. 4640–4646.\\r\\n[19] X. Ying, C. Xu, J. Gao, J. Wang, and Z. Li, “Time-aware graph\\r\\nrelational attention network for stock recommendation,” in Pro\\x02ceedings of the 29th ACM International Conference on Information &\\r\\nKnowledge Management, 2020, pp. 2281–2284.\\r\\n[20] R. Sawhney, S. Agarwal, A. Wadhwa, and R. Shah, “Exploring\\r\\nthe scale-free nature of stock markets: Hyperbolic graph learning\\r\\nfor algorithmic trading,” in Proceedings of the Web Conference 2021,\\r\\n2021, pp. 11–22.\\r\\n[21] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, “Neural\\r\\nordinary differential equations,” in Proceedings of the 32nd Interna\\x02tional Conference on Neural Information Processing Systems, 2018, pp.\\r\\n6572–6583.\\r\\n[22] Y. Lu, A. Zhong, Q. Li, and B. Dong, “Beyond finite layer neural\\r\\nnetworks: Bridging deep architectures and numerical differential\\r\\nequations,” in ICML. PMLR, 2018, pp. 3276–3285.\\r\\n[23] Y. Qin, D. Song, H. Cheng, W. Cheng, G. Jiang, and G. W. Cottrell,\\r\\n“A dual-stage attention-based recurrent neural network for time\\r\\nseries prediction,” in IJCAI, 2017, pp. 2627–2633.\\r\\n[24] M. Scholtus, D. Van Dijk, and B. Frijns, “Speed, algorithmic trad\\x02ing, and market quality around macroeconomic news announce\\x02ments,” Journal of Banking & Finance, vol. 38, pp. 89–105, 2014.\\r\\n[25] F. Zhou and L. Li, “Forecasting reservoir inflow via recurrent\\r\\nneural odes,” in Proceedings of the AAAI Conference on Artificial\\r\\nIntelligence, vol. 35, no. 17, 2021, pp. 15 025–15 032.\\r\\n[26] W. Jiang, “Applications of deep learning in stock market predic\\x02tion: recent progress,” Expert Systems with Applications, vol. 184, p.\\r\\n115537, 2021.\\r\\n[27] T. H. Nguyen and K. Shirai, “Topic modeling based sentiment\\r\\nanalysis on social media for stock market prediction,” in ACL,\\r\\n2015, pp. 1354–1364.\\r\\n[28] H. Wang, T. Wang, and Y. Li, “Incorporating expert-based in\\x02vestment opinion signals in stock prediction: A deep learning\\r\\nframework,” in Proceedings of the AAAI Conference on Artificial\\r\\nIntelligence, vol. 34, no. 01, 2020, pp. 971–978.\\r\\n[29] A. Dutta, G. Bandopadhyay, and S. Sengupta, “Prediction of stock\\r\\nperformance in the indian stock market using logistic regression,”\\r\\nInternational Journal of Business and Information, vol. 7, no. 1, p. 105,\\r\\n2012.\\r\\n[30] R. Sawhney, S. Agarwal, A. Wadhwa, T. Derr, and R. R. Shah,\\r\\n“Stock selection via spatiotemporal hypergraph attention network:\\r\\nA learning to rank approach,” in Proceedings of the AAAI Conference\\r\\non Artificial Intelligence, vol. 35, no. 1, 2021, pp. 497–504.\\r\\n[31] S. Hochreiter and J. Schmidhuber, “Long short-term memory.”\\r\\nNeural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.\\r\\n[32] J. Chung, C. Gulcehre, K. H. Cho, and Y. Bengio, “Empirical evalu\\x02ation of gated recurrent neural networks on sequence modeling,”\\r\\narXiv preprint arXiv:1412.3555, 2014.\\r\\n[33] L.-C. Cheng, Y.-H. Huang, and M.-E. Wu, “Applied attention\\x02based lstm neural networks in stock prediction,” in 2018 IEEE\\r\\nInternational Conference on Big Data (Big Data). IEEE, 2018, pp.\\r\\n4716–4718.\\r\\n[34] D. M. Nelson, A. C. Pereira, and R. A. de Oliveira, “Stock market’s\\r\\nprice movement prediction with lstm neural networks,” in IJCNN.\\r\\nIEEE, 2017, pp. 1419–1426.\\r\\n[35] J. Wang, T. Sun, B. Liu, Y. Cao, and H. Zhu, “Clvsa: a convolutional\\r\\nlstm based variational sequence-to-sequence model with attention\\r\\nfor predicting trends of financial markets,” in Proceedings of the\\r\\n28th International Joint Conference on Artificial Intelligence, 2019, pp.\\r\\n3705–3711.\\r\\n[36] Y. Xu and S. B. Cohen, “Stock movement prediction from tweets\\r\\nand historical prices,” in Proceedings of the 56th Annual Meeting of\\r\\nthe Association for Computational Linguistics, 2018, pp. 1970–1979.\\r\\n[37] H. Wang, T. Wang, S. Li, and S. Guan, “Hatr-i: Hierarchical adap\\x02tive temporal relational interaction for stock trend prediction,”\\r\\nIEEE Transactions on Knowledge and Data Engineering, 2022.\\r\\n[38] Y. Chen, Z. Wei, and X. Huang, “Incorporating corporation rela\\x02tionship via graph convolutional neural networks for stock price\\r\\nprediction,” in CIKM, 2018, pp. 1655–1658.\\r\\n[39] Y.-L. Hsu, Y.-C. Tsai, and C.-T. Li, “Fingat: Financial graph atten\\x02tion networks for recommending top-k profitable stocks,” IEEE\\r\\nTransactions on Knowledge and Data Engineering, 2021.\\r\\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\r\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\\r\\nin NIPS, 2017, pp. 5998–6008.\\r\\n[41] S. Y. Jhin, M. Jo, T. Kong, J. Jeon, and N. Park, “Ace-node: Attentive\\r\\nco-evolving neural ordinary differential equations,” in Proceedings\\r\\nof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data\\r\\nMining, 2021, pp. 736–745.\\r\\n[42] Y. Rubanova, R. T. Chen, and D. Duvenaud, “Latent odes for\\r\\nirregularly-sampled time series,” in Advances in neural information\\r\\nprocessing systems, 2019, pp. 5320–5330.\\r\\n[43] Y. Liang, K. Ouyang, H. Yan, Y. Wang, Z. Tong, and R. Zim\\x02mermann, “Modeling trajectories with neural ordinary differential\\r\\nequations.” in IJCAI, 2021, pp. 1498–1504.\\r\\n[44] F. Zhou, L. Li, T. Zhong, G. Trajcevski, K. Zhang, and J. Wang,\\r\\n“Enhancing urban flow maps via neural odes,” in Proceed\\x02ings of the Twenty-Ninth International Joint Conference on Artificial\\r\\nIntelligence,{IJCAI} 2020, 2020.\\r\\n[45] B. Dong, Z. Li, S. M. Rahman, and R. Vega, “A hybrid model ap\\x02proach for forecasting future residential electricity consumption,”\\r\\nEnergy and Buildings, vol. 117, pp. 341–351, 2016.\\r\\n[46] Y. Liang, K. Ouyang, H. Yan, Y. Wang, Z. Tong, and R. Zim\\x02mermann, “Modeling trajectories with neural ordinary differential\\r\\nequations,” in Proceedings of the Thirtieth International Joint Con\\x02ference on Artificial Intelligence, IJCAI-21, Z.-H. Zhou, Ed. Inter\\x02national Joint Conferences on Artificial Intelligence Organization,\\r\\n2021, pp. 1498–1504.\\nIEEE TRANSACTIONS ON XXX, VOL. XX, NO. XX, NOVEMBER 2022 14\\r\\n[47] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”\\r\\nin International Conference on Learning Representations (ICLR), 2014,\\r\\npp. 1–14.\\r\\n[48] D. Vrandeˇci´c and M. Kr ¨otzsch, “Wikidata: a free collaborative\\r\\nknowledgebase,” Communications of the ACM, vol. 57, no. 10, pp.\\r\\n78–85, 2014.\\r\\n[49] X. Xia, H. Yin, J. Yu, Q. Wang, L. Cui, and X. Zhang, “Self\\x02supervised hypergraph convolutional networks for session-based\\r\\nrecommendation,” in AAAI, 2021, pp. 4503–4511.\\r\\n[50] E. Hoseinzade and S. Haratizadeh, “Cnnpred: Cnn-based stock\\r\\nmarket prediction using a diverse set of variables,” Expert Systems\\r\\nwith Applications, vol. 129, pp. 273–285, 2019.'},\n",
       " {'name': '2306.02326v1.pdf',\n",
       "  'content': 'Cross-LKTCN:\\r\\nModern Convolution Utilizing Cross-Variable\\r\\nDependency for Multivariate Time Series Forecasting\\r\\nDonghao Luo, Xue Wang\\r\\nDepartment of Precision Instrument, Tsinghua University, China\\r\\nldh21@mails.tsinghua.edu.cn\\r\\nAbstract\\r\\nThe past few years have witnessed the rapid development in multivariate time\\r\\nseries forecasting. The key to accurate forecasting results is capturing the long\\x02term dependency between each time step (cross-time dependency) and modeling\\r\\nthe complex dependency between each variable (cross-variable dependency) in\\r\\nmultivariate time series. However, recent methods mainly focus on the cross-time\\r\\ndependency but seldom consider the cross-variable dependency. To fill this gap, we\\r\\nfind that convolution, a traditional technique but recently losing steam in time series\\r\\nforecasting, meets the needs of respectively capturing the cross-time and cross\\x02variable dependency. Based on this finding, we propose a modern pure convolution\\r\\nstructure, namely Cross-LKTCN, to better utilize both cross-time and cross-variable\\r\\ndependency for time series forecasting. Specifically in each Cross-LKTCN block, a\\r\\ndepth-wise large kernel convolution with large receptive field is proposed to capture\\r\\ncross-time dependency, and then two successive point-wise group convolution feed\\r\\nforward networks are proposed to capture cross-variable dependency. Experimental\\r\\nresults on real-world benchmarks show that Cross-LKTCN achieves state-of-the\\x02art forecasting performance and improves the forecasting accuracy significantly\\r\\ncompared with existing convolutional-based models and cross-variable methods.\\r\\n1 Introduction\\r\\nMultivariate time series records M variables’ variations during multiple time steps. It mainly contains\\r\\ntwo types of dependencies, namely cross-time dependency and cross-variable dependency. The former\\r\\nindicates the long-term temporal dependency between each time step. The latter is the dependency\\r\\namong different variables. And time series forecasting means using the historical information of\\r\\nlength L to predict the future of length T, where T represents a long-term period of time. It has been\\r\\nwidely used in industrial production planning, sensor network monitoring and health management.\\r\\nThe past few years have witnessed the rapid development in time series forecasting[31, 19]. Among\\r\\nthem, the rise of Transformer-based methods [25, 33, 39, 41, 38, 18, 4, 29, 15, 22] and Linear\\r\\nmodels [36] is especially compelling. However, the cross-variable dependency has been seldom\\r\\nconsidered in these previous works. Most previous models simply mix and embed M variables\\r\\nat a specific time step into a D-dimensional vector, trying to capture the cross-variable dependency\\r\\nvia the embedding layer [33, 39, 41, 32]. However, just an embedding projection layer fails to learn\\r\\nthe complex dependency across variables and even loses their independent characteristics for not\\r\\nconsidering the different behaviors of different variables. Other models embed and analyse each\\r\\nunivariate time series individually to maintain the independence of different variables but still omit the\\r\\ndependency between them [25, 36, 4, 40]. Some cross-variable methods design explicit mechanisms\\r\\nto capture cross-variable dependency [34, 38, 17]. But they come with high computational complexity.\\r\\nMore efficient methods to capture both cross-variable and cross-time dependency are still in need.\\r\\nPreprint. Under review.\\r\\narXiv:2306.02326v1 [cs.LG] 4 Jun 2023\\nFrom the perspective of computational complexity, convolution1\\r\\nis an efficient way to capture cross\\x02variable dependency[17]. Meanwhile, the decoupling property of group convolution and depth-wise\\r\\nseparable convolution[11, 16, 37] meets the needs of respectively capturing cross-time dependency\\r\\nand cross-variable dependency. However, convolution-based models are losing steam in time series\\r\\nforecasting due to their limited receptive field and weak cross-time dependency modeling ability [1].\\r\\nAlthough some methods [30, 20] use multi-resolution down-sampling to alleviate this problem, there\\r\\nis still a performance gap between them and the state-of-the-art models with global view [36, 25]. A\\r\\nturnaround occurred recently. In computer vision, it is proven that large kernel 2D convolution-based\\r\\nmodels can obtain as large receptive field as vision transformers have[24, 5]. Some recipes for\\r\\ntraining the large kernel convolutions are provided in [5]. Therefore we can also enlarge the kernel\\r\\nsize of 1D convolution-based model in time series forecasting to get rid of the limited receptive field.\\r\\nBased on above motivations, we bring convolution-based model back to the arena of time series\\r\\nforecasting and propose Cross-LKTCN as a modern pure convolution structure2to efficiently utilize\\r\\ncross-time and cross-variable dependency for time series forecasting. Specifically, we introduce the\\r\\npatch-style embedding strategy3for time series to enhance locality and aggregate more semantic\\r\\ninformation from adjacent time points. Then we propose the Cross-LKTCN block, which contains\\r\\na depth-wise large kernel convolution to capture the cross-time dependency and two successive\\r\\npoint-wise group convolution feed forward networks (FFNs) to capture cross-variable dependency.\\r\\nAnd we remove causal convolution from our design as it was proven unnecessary in time series\\r\\nforecasting [20]. Experimentally, Cross-LKTCN achieves competitive performance on nine real\\x02world benchmarks against state-of-the-arts, indicating the great potential of convolution-based models\\r\\nand cross-variable methods in time series forecasting. Our contributions are as follows:\\r\\n• To better utilize the cross-variable dependency in multivariate time series forecasting, we\\r\\npropose successive point-wise group convolution FFNs based on the decoupling property of\\r\\ngroup convolution and depth-wise separable convolution. Our method surpasses existing\\r\\ncross-variable methods and achieves state-of-the-art performance, indicating the importance\\r\\nof cross-variable dependency in time series forecasting.\\r\\n• Inspired by the latest large kernel 2D convolution in computer vision, we introduce the\\r\\ndepth-wise large kernel convolution into time series forecasting to enlarge the receptive field\\r\\nand improve the cross-time dependency modeling ability. Experimental results show that\\r\\nour method can better unleash the potential of convolution in time series forecasting than\\r\\nother existing convolution-based models.\\r\\n• Extensive experimental results on nine real-world benchmarks show the effectiveness of our\\r\\nCross-LKTCN. Our method surpasses existing convolution-based models and cross-variable\\r\\nmethods by a large margin in multivariate time series forecasting (respectively 27.4% and\\r\\n52.3% relative improvement) and achieves state-of-the-art performance.\\r\\n2 Related Work\\r\\n2.1 Utilizing Cross-variable Dependency in Multivariate Time Series\\r\\nUnlike the variable-mixing methods [33, 39, 41, 32] and variable-independent methods [25, 36,\\r\\n4, 40] mentioned in Section 1, some cross-variable methods focus on capturing cross-variable\\r\\ndependency and design some explicit mechanisms. LSTnet[17] employs convolution to capture\\r\\ncross-variable dependency and uses the recurrent component[10, 3] to capture cross-time dependency.\\r\\nThis study proves the effectiveness of convolution in capturing cross-variable dependency. However,\\r\\nfollowing studies like MTGNN[34] and Crossformer[38] omit to further optimize the efficiency and\\r\\nperformance of convolution but turn to design dedicated graph convolution or attention mechanism to\\r\\ncapture the cross-variable dependency. These cross-variable methods can explicitly capture cross\\x02variable dependency. But they come with high computational complexity and are still not comparable\\r\\n1The convolution in this paper refers to 1D convolution by default, unless specified as 2D convolution.\\r\\n2A modern pure convolution structure means a convolution-based model incorporating some architectural\\r\\ndesigns in Transformers but free of any attention mechanism[24].\\r\\n3\\r\\nPatch-style embedding strategy means doing patching and embedding on the input series, while the\\r\\ncommonly used non-patch-style embedding strategy only embeds the input series without patching process.\\r\\n2\\nto the state-of-the-art variable-independent methods[36, 25] in long-term forecasting. More efficient\\r\\ndesigns for capturing cross-variable dependency are still in need.\\r\\n2.2 Capturing Cross-time Dependency via Convolution in Time Series Forecasting\\r\\nConvolution is widely used in time series forecasting. TCN[1] uses causal convolution to model the\\r\\ntemporal causality and stacks many layers for larger receptive field. Similar ideas are also adopted\\r\\nby [2, 26, 7]. Going beyond causal convolution, MICN[30] proposes a multi-scale branch structure\\r\\nwith down-sampled convolution and isometric convolution to combine local features and global\\r\\ncorrelations in time series. However, these pure convolution-based models haven’t fully unleashed\\r\\nconvolution’s potential in time series for not adopting modern convolution structures. Considering the\\r\\nlimited receptive field, many models only introduce convolution to enhance locality. LogTrans[18]\\r\\nintroduces convolution to Transformer to enhance locality and uses LogSparse attention for long-term\\r\\ndependency modeling. SCINet[20] removes the idea of causal convolution and introduces a recursive\\r\\ndownsample-convolve-interact architecture to model time series with complex temporal dynamics.\\r\\nBut they still have difficulty in modeling long-term dependency due to the limited receptive filed.\\r\\n2.3 Large Kernel 2D Convolution in Computer Vision\\r\\nLarge kernel 2D convolution has a long history in computer vision but was abandoned for a long time.\\r\\nIt is brought back in some modern ConvNets nowadays and proved to be an effective way to enlarge\\r\\nthe receptive field. AlexNet[16] uses a large kernel size such as 7×7 and 11×11 in 2010s. However,\\r\\nwith the introduction of VGG[27], it has become a consensus to stack small convolutional kernels\\r\\nsuch as 1×1 and 3×3 to obtain large receptive field[8, 12]. In 2020s, Vision Transformers (ViTs)[6, 23]\\r\\nare proposed and outperform previous standard ConvNets[8, 35]. Inspried by the architectural designs\\r\\nin ViTs, modern ConvNets in 2020s are introduced. ConvNeXt[24] adopts a depth-wise separable\\r\\n7×7 kernel, surpassing the performance of Swin Transformer[23]. Further more, RepLKNet[5] scales\\r\\nthe kernel size to 31×31 with the help of Structural Reparameter technique. SLaK[21] enlarges the\\r\\nkernel size to 51×51 by decomposing a large kernel into two rectangular, parallel kernels and by\\r\\nusing dynamic sparsity. Inspired by above studies, we also apply large kernel 1D convolution in time\\r\\nseries forecasting to enlarge the receptive field and improve cross-time dependency modeling abiltiy.\\r\\n3 Cross-LKTCN\\r\\nFigure 1: Cross-LKTCN. The patch-style embedding method is applied to time series to enhance\\r\\nlocality. The backbone stacked by Cross-LKTCN blocks is proposed to learn the representation by\\r\\ncapturing both cross-time and cross-variable dependency. The linear head with a flatten layer is used\\r\\nto obtain the final predictions. And RevIN [14] is to mitigate the distribution shift.\\r\\nTime series forecasting aims to predict the future values of prediction length T based on the historical\\r\\nvalues of input length L. We design Cross-LKTCN as a pure convolution structure to utilize both\\r\\ncross-time and cross-variable dependency for time series forecasting. In Section 3.1, we introduce the\\r\\noverall structure of Cross-LKTCN. In Section 3.2, we introduce the patch-style embedding strategy\\r\\nfor time series to enhance locality and aggregate more local semantic information by patching\\r\\nthe adjacent time points. In Section 3.3, we propose the Cross-LKTCN block, which can capture\\r\\nlong-term temporal dependency by depth-wise large kernel convolution and capture cross-variable\\r\\ndependency based on successive point-wise group convolution FFNs.\\r\\n3.1 Overall Structure\\r\\nForward Process. The overall structure of Cross-LKTCN is shown on Figure 1. We denote\\r\\nXin ∈ RM×L as the M variables input time series of length L. And it will be further divided into\\r\\nN patches and embedded into D-dimensional embedding vectors by using patch-style embedding\\r\\n3\\nstrategy. The embedding process is shown as follows:\\r\\nXemb = Embedding(Xin) (1)\\r\\nWhere Embedding(·) is the patch-style embedding method designed for time series. Details will\\r\\nbe introduced in Section 3.2. After the embedding process, we have the input embedding Xemb ∈\\r\\nRM×D×N .\\r\\nThen the input embedding vector Xemb is fed into the backbone to capture both the cross-time and\\r\\ncross-variable dependency and to learn the informative representation Z ∈ RM×D×N :\\r\\nZ = Backbone(Xemb) (2)\\r\\nBackbone(·) is the stacked Cross-LKTCN blocks we proposed and will be described in Section 3.3.\\r\\nFinally, the linear head with a flatten layer is used to obtain the final prediction:\\r\\nXb = Head(Flatten(Z)) (3)\\r\\nWhere Xb ∈ RM×Tis the prediction of length T with M variables. Flatten(·) denotes a flatten\\r\\nlayer that changes the final representation’s shape to Z ∈ RM×(D×N). Head(·) indicates the linear\\r\\nprojection layer that maps the final representation to the final prediction.\\r\\nRevIN. RevIN[14] is a special instance normalization for time series to mitigate the distribution\\r\\nshift between the training and testing data. In norm phase, we normalize the input time series per\\r\\nvariable with zero mean and unit standard deviation before patching and embedding. Then in de-norm\\r\\nphase, we add the mean and deviation back to the final prediction per variable after the forward\\r\\nprocess.\\r\\n3.2 Patch-style Embedding Strategy\\r\\nFigure 2: Patch-style Embedding. It employs a 1D convolution layer with kernel size P and stride S\\r\\nto divide the length-L time series into N patches and embed them into D-dimensional vectors. A 1D\\r\\nconvolution layer is shown as (kernel size, /stride, #in channels→#out channels, group number).\\r\\nBased on previous studies, it is necessary to divide the time series into patches before embedding[25,\\r\\n38]. And it is also common in 2D convolution-based models to split the original input into patches at\\r\\nthe network’s beginning for downsampling [24, 8]. Therefore, we adopt a patch-style embedding\\r\\nstrategy in our 1D convolution-based model to divide the input time series into patches and embed\\r\\nthem.\\r\\nAs shown in Figure 2, we have Xin ∈ RM×L as the M variables input time series of length L. In\\r\\nthe patching process, the time series is divided into N patches of patch size P. And the stride in\\r\\nthe patching process is S, which also serves as the length of non overlapping region between two\\r\\nconsecutive patches. Technically, after unsqueezing its shape to Xin ∈ RM×1×L, we can divide the\\r\\ntime series into N patches and then embed them to D-dimensional vectors by using a 1D convolution\\r\\nstem layer as follows:\\r\\nXemb = Conv1d(Padding(Xin))kernel size=P,stride=S,channels:1→D (4)\\r\\nWhere Xemb ∈ RM×D×N is the input embedding. Padding(·) denotes the padding operation\\r\\napplied to the original time series Xin to keep the number of patches N = L//S. Specifically, we\\r\\nrepeat Xin’s last value (P − S) times and then pad them back to the end of Xin before patching and\\r\\nembedding. For Conv1d(·) here, we set its kernel size as P and stride as S. And it maps 1 input\\r\\nchannel into D output channels. The overall embedding process in Section 3.2 can also be briefly\\r\\nsummarized by Equation 1.\\r\\n4\\nFigure 3: Structure of Cross-LKTCN block. It contains a depth-wise large kernel convolution\\r\\nto capture cross-time dependency and two successive point-wise group convolution FFNs to cap\\x02ture the cross-variable dependency. A 1D convolution layer is shown as (kernel size, /stride, #in\\r\\nchannels→#out channels, group number). DW and PW are short for depth-wise and point-wise.\\r\\n3.3 Cross-LKTCN Block\\r\\nCross-time dependency and cross-variable dependency are critical for multivariate time series fore\\x02casting. To this end, we propose Cross-LKTCN block (Figure 3 ) as a basic module to capture both\\r\\ncross-time and cross-variable dependency in time series. Therefore the backbone containing K Cross\\x02LKTCN blocks can learn informative representation from the input embedding Xemb. Cross-LKTCN\\r\\nblock is organized in a residual way [8]. The forward process in the i-th Cross-LKTCN block is:\\r\\nZi+1 = Block(Zi) + Zi (5)\\r\\nWhere Zi ∈ RM×D×N , i ∈ {1, ..., K} is the i-th block’s input,\\r\\nZi =\\r\\n\\x1a\\r\\nXemb , i = 1\\r\\nBlock(Zi−1) + Zi−1 , i > 1\\r\\n(6)\\r\\nBlock(·) denotes the Cross-LKTCN block. In details, a Cross-LKTCN block contains a depth-wise\\r\\nlarge kernel convolution to capture long-term temporal dependency and two successive point-wise\\r\\ngroup convolution FFNs to capture cross-variable dependency. Therefore, the detailed forward\\r\\nprocess in the i-th Cross-LKTCN block can be introduced in the following paragraphs.\\r\\nDepth-wise Large Kernel Convolution. Firstly, we merge the first two axes of the i-th block’s\\r\\ninput and unsqueeze its shape to Zi ∈ R\\r\\n1×(M×D)×N . Then a depth-wise large kernel convolution is\\r\\nused to capture cross-time dependency:\\r\\nZ\\r\\ntime1\\r\\ni = BN(DW1Conv1d(Zi))kernel size=large size,channels:(M×D)→(M×D),groups=(M×D)\\r\\nZ\\r\\ntime2\\r\\ni = BN(DW2Conv1d(Zi))kernel size=small size,channels:(M×D)→(M×D),groups=(M×D)\\r\\nZ\\r\\ntime\\r\\ni = Z\\r\\ntime1\\r\\ni + Z\\r\\ntime2\\r\\ni\\r\\n(7)\\r\\nZ\\r\\ntime\\r\\ni ∈ R\\r\\n1×(M×D)×N is the representation after capturing the cross-time dependency. BN(·)\\r\\nmeans 1D Batch Normalization[13]. DW1Conv1d(·) and DW2Conv1d(·) are two parallel depth\\x02wise convolution layers to map (M × D) input channels into (M × D) output channels with different\\r\\nkernel sizes. Here we set the kernel size in DW1Conv1d(·) as large size to enlarge the receptive\\r\\nfield.\\r\\nAnd we set the kernel size in DW2Conv1d(·) as small size. DW2Conv1d(·) serves as an additional\\r\\nStructural Re-parameterization branch which helps to make up the optimization issue of large\\r\\nkernel convolutions according to [5, 21]. More details about Structural Re-parameterization are in\\r\\nsupplementary materials. The large size and small size are two hyperparameters to be defined.\\r\\n5\\nSuccessive Point-wise Group Convolution FFNs. We adopt two successive point-wise group\\r\\nconvolution FFNs to capture cross-variable dependency. The process is as follows:\\r\\nZ\\r\\nvariable1\\r\\ni =ConvFFN1(Z\\r\\ntime\\r\\ni\\r\\n)groups=M\\r\\nZ\\r\\nvariable2\\r\\ni =Permute&Reshape(Z\\r\\nvariable1\\r\\ni\\r\\n)\\r\\nZ\\r\\nvariable\\r\\ni =ConvFFN2(Z\\r\\nvariable2\\r\\ni\\r\\n)groups=D\\r\\nZi+1 =Permute&Reshape(Z\\r\\nvariable\\r\\ni\\r\\n)\\r\\n(8)\\r\\nThe former Permute&Reshape(·) after ConvFFN1(·) changes the shape from Z\\r\\nvariable1\\r\\ni ∈\\r\\nR\\r\\n1×(M×D)×N to Zvariable2\\r\\ni ∈ R\\r\\n1×(D×M)×N . And Zvariable\\r\\ni ∈ R\\r\\n1×(D×M)×N is the represen\\x02tation after capturing the cross-variable dependency. After the latter Permute&Reshape(·), we\\r\\nhave Zi+1 ∈ RM×D×N as the output of the i-th Cross-LKTCN block. We set the group number of\\r\\nConvFFN1(·) as M to learn the new D features of each variable per time step and set the group\\r\\nnumber of ConvFFN2(·) as D to capture the cross-variable dependency per feature in each time\\r\\nstep. Weights in ConvFFN1(·) and ConvFFN2(·) are shared among all time steps.\\r\\nAnd the forward process in ConvFFN1(·) is as follows:\\r\\nZ\\r\\nvariable1\\r\\ni\\r\\n′\\r\\n=Drop(PW1\\r\\n1Conv1d(Z\\r\\ntime\\r\\ni\\r\\n)kernelsize=1,channels:(M×D)→(r×M×D),groups=M)\\r\\nZ\\r\\nvariable1\\r\\ni\\r\\n′\\r\\n=GELU(Z\\r\\nvariable1\\r\\ni\\r\\n′\\r\\n)\\r\\nZ\\r\\nvariable1\\r\\ni =Drop(PW12Conv1d(Z\\r\\nvariable1\\r\\ni\\r\\n′\\r\\n)kernelsize=1,channels:(r×M×D)→(M×D),groups=M)\\r\\n(9)\\r\\nSimilarly, the forward process in ConvFFN2(·) is as follows:\\r\\nZ\\r\\nvariable2\\r\\ni\\r\\n′\\r\\n=Drop(PW2\\r\\n1Conv1d(Z\\r\\nvariable2\\r\\ni\\r\\n)kernelsize=1,channels:(D×M)→(r×D×M),groups=D)\\r\\nZ\\r\\nvariable2\\r\\ni\\r\\n′\\r\\n=GELU(Z\\r\\nvariable2\\r\\ni\\r\\n′\\r\\n)\\r\\nZ\\r\\nvariable\\r\\ni =Drop(PW22Conv1d(Z\\r\\nvariable2\\r\\ni\\r\\n′\\r\\n)kernelsize=1,channels:(r×D×M)→(D×M),groups=D)\\r\\n(10)\\r\\nGELU(·) is the non-linear activation [9]. Drop(·) is dropout operation[28]. PW1\\r\\n1Conv1d(·) and\\r\\nPW1\\r\\n2Conv1d(·) are the first and the second point-wise group convolutions in ConvFFN1(·). The\\r\\nsame goes for ConvFFN2(·). And r is the FFN ratio, which means in the FFN process, the channel\\r\\nnumber is mapping from (D × M) to (r × D × M) and then mapping back to (D × M) with group\\r\\nconvolutions.\\r\\nIn conclusion, in a Cross-LKTCN block, the DW1Conv1d(·) is used to capture cross-time de\\x02pendency. The ConvFFN1(·) is responsible for applying the linear transformation and non-linear\\r\\nactivation to D previous learned features per variable to further learn the new representation of each\\r\\nvariable independently. And ConvFFN2(·) is in charge of capturing the cross-variable dependency\\r\\nper feature. Details are shown in Figure 4.\\r\\nIn the following process, Zi+1 will be sent to the next Cross-LKTCN block as its input. And\\r\\nthe output of the last Cross-LKTCN block ZK+1 = Block(ZK) + ZK will be treated as the final\\r\\nrepresentation Z generated by the backbone in Equation 2.\\r\\n4 Experiments\\r\\nDatasets. We evaluate the performance of our proposed Cross-LKTCN on 9 popular real-world\\r\\ndatasets, including Weather, Traffic, Electricity, Exchange, ILI and 4 ETT datasets (ETTh1, ETTh2,\\r\\nETTm1, ETTm2). More details about the datasets are described in supplementary materials.\\r\\nBaselines. For multi-variate time series forecasting, we choose three state-of-the-art transformer\\x02based models: PatchTST[25], FEDformer[41] and Autoformer[33], two recent cross-variable models:\\r\\nMTGNN[34] and Crossformer[38], two latest convolution-based models: MICN[30] and SCINet[20],\\r\\nand one state-of-the-art Linear model: DLinear[36] as our baselines. For some baselines that have\\r\\ndifferent variants, we perform experiments on all the variants and collect the best results.\\r\\n6\\nFigure 4: An example to describe the roles of each module in a Cross-LKTCN block. Here the input\\r\\nof Cross-LKTCN block contains three tokens with variable number as 2 and channel number as 5.\\r\\nTable 1: Multivariate long-term forecasting results. We set the prediction lengths T ∈ {24, 36, 48, 60}\\r\\nfor ILI dataset and T ∈ {96, 192, 336, 720} for other datasets. A lower MSE or MAE indicates a\\r\\nbetter performance. The best results are in bold and the second best are underlined.\\r\\nModels Cross-LKTCN PatchTST DLinear Crossformer MTGNN MICN SCINet FEDfromer Autoformer\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Electricity\\r\\n96 0.129 0.226 0.129 0.222 0.140 0.237 0.187 0.283 0.198 0.294 0.159 0.267 0.171 0.256 0.186 0.302 0.196 0.313\\r\\n192 0.143 0.239 0.147 0.240 0.153 0.249 0.258 0.330 0.266 0.339 0.168 0.279 0.177 0.265 0.197 0.311 0.211 0.324\\r\\n336 0.161 0.259 0.163 0.259 0.169 0.267 0.323 0.369 0.328 0.373 0.196 0.308 0.197 0.285 0.213 0.328 0.214 0.327\\r\\n720 0.191 0.286 0.197 0.290 0.203 0.301 0.404 0.423 0.422 0.410 0.203 0.312 0.234 0.318 0.233 0.344 0.236 0.342\\r\\nETTh2\\r\\n96 0.263 0.332 0.274 0.336 0.289 0.353 0.628 0.563 0.690 0.614 0.289 0.357 0.295 0.361 0.332 0.374 0.332 0.368\\r\\n192 0.320 0.374 0.339 0.379 0.383 0.418 0.703 0.624 0.745 0.662 0.409 0.438 0.349 0.383 0.407 0.446 0.426 0.434\\r\\n336 0.313 0.376 0.329 0.380 0.448 0.465 0.827 0.675 0.886 0.721 0.417 0.452 0.365 0.409 0.400 0.447 0.477 0.479\\r\\n720 0.392 0.433 0.379 0.422 0.605 0.551 1.181 0.840 1.299 0.936 0.426 0.473 0.475 0.488 0.412 0.469 0.453 0.490\\r\\nWeather\\r\\n96 0.149 0.204 0.149 0.198 0.176 0.237 0.153 0.217 0.161 0.223 0.161 0.226 0.178 0.233 0.238 0.314 0.249 0.329\\r\\n192 0.196 0.248 0.194 0.241 0.220 0.282 0.197 0.269 0.206 0.278 0.220 0.283 0.235 0.277 0.275 0.329 0.325 0.370\\r\\n336 0.238 0.281 0.245 0.282 0.265 0.319 0.252 0.311 0.261 0.322 0.275 0.328 0.337 0.345 0.339 0.377 0.351 0.391\\r\\n720 0.314 0.334 0.314 0.334 0.323 0.362 0.318 0.363 0.324 0.366 0.311 0.356 0.396 0.413 0.389 0.409 0.415 0.426\\r\\nILI\\r\\n24 1.347 0.717 1.319 0.754 2.215 1.081 3.040 1.186 4.268 1.385 2.684 1.112 2.150 1.005 2.624 1.095 2.906 1.182\\r\\n36 1.250 0.778 1.430 0.834 1.963 0.963 3.356 1.230 4.768 1.494 2.507 1.013 2.103 0.983 2.516 1.021 2.585 1.038\\r\\n48 1.388 0.781 1.553 0.815 2.130 1.024 3.441 1.223 5.333 1.592 2.423 1.012 2.432 1.061 2.505 1.041 3.024 1.145\\r\\n60 1.774 0.868 1.470 0.788 2.368 1.096 3.608 1.302 5.083 1.556 2.653 1.085 2.325 1.035 2.742 1.122 2.761 1.114\\r\\nTraffic\\r\\n96 0.373 0.263 0.360 0.249 0.410 0.282 0.512 0.290 0.527 0.316 0.508 0.301 0.613 0.395 0.576 0.359 0.597 0.371\\r\\n192 0.383 0.257 0.379 0.256 0.423 0.287 0.523 0.297 0.534 0.320 0.536 0.315 0.559 0.363 0.610 0.380 0.607 0.382\\r\\n336 0.391 0.263 0.392 0.264 0.436 0.296 0.530 0.300 0.540 0.335 0.525 0.310 0.555 0.358 0.608 0.375 0.623 0.387\\r\\n720 0.435 0.288 0.432 0.286 0.466 0.315 0.573 0.313 0.557 0.343 0.571 0.323 0.620 0.394 0.621 0.375 0.639 0.395\\r\\nExchange\\r\\n96 0.080 0.196 0.093 0.214 0.081 0.203 0.186 0.346 0.203 0.381 0.102 0.235 0.061 0.188 0.139 0.276 0.197 0.323\\r\\n192 0.166 0.288 0.192 0.312 0.157 0.293 0.467 0.522 0.459 0.512 0.172 0.316 0.106 0.244 0.256 0.369 0.300 0.369\\r\\n336 0.307 0.398 0.350 0.432 0.305 0.414 0.783 0.721 0.707 0.697 0.272 0.407 0.181 0.323 0.426 0.464 0.509 0.524\\r\\n720 0.656 0.582 0.911 0.716 0.643 0.601 1.367 0.943 1.323 0.912 0.714 0.658 0.525 0.571 1.090 0.800 1.447 0.941\\r\\n1 More results about other ETT benchmarks are provided in supplementary materials.\\r\\nImplementation. All models are following the same experimental setup with prediction length\\r\\nT ∈ {24, 36, 48, 60} for ILI dataset and T ∈ {96, 192, 336, 720} for other datasets as [25]. We\\r\\ncollect some baseline results from [25]. Following the protocols in [25], for other baselines, we\\r\\nfollow their official implementation but re-run them with various input length L and choose the\\r\\nbest results to avoid under-estimating the baselines. All experiments are repeated three times. We\\r\\ncalculate the MSE and MAE of multivariate time series forecasting as metrics. More details about\\r\\nthe implementation are described in supplementary materials.\\r\\n4.1 Main Results\\r\\nTable 1 shows the multivariate long-term forecasting results. Overall, our model outperforms all\\r\\nbaseline methods. We discuss the results from the following aspects. (1) Compared with convolution\\x02based models like MICN[30] and SCINet[20], Cross-LKTCN achieves an overall 27.4% reduction\\r\\non MSE and 15.3% reduction on MAE, becoming the best performing convolution-based model.\\r\\nThese experimental results indicate that our structure can better unleash the potential of convolution\\r\\nin time series forecasting. (2) Compared with cross-variable baselines like MTGNN[34] and\\r\\nCrossformer[38], Cross-LKTCN achieves an overall 52.3% reduction on MSE and 33.5% reduction\\r\\non MAE, indicating our proposed method is more efficient in capturing cross-variable dependency. It’s\\r\\nworth noting that Cross-LKTCN also outperforms the previous state-of-the-art variable-independent\\r\\nmethods in general, indicating the importance of cross-variable dependency capturing. (3) Compared\\r\\nwith state-of-the-art models, Cross-LKTCN can still achieve comparable or even better performance.\\r\\n7\\nTable 2: Ablation of successive point-wise group convolution FFNs. We compare our method with\\r\\nthree different variants on three datasets. The best results are highlighted in blod.\\r\\nDatasets ILI ETTh1 Electricity\\r\\nPrediction length 24 36 48 60 96 192 336 720 96 192 336 720\\r\\nM Groups + D Groups. MSE 1.347 1.250 1.388 1.774 0.368 0.405 0.391 0.450 0.130 0.143 0.161 0.191\\r\\nMAE 0.717 0.778 0.781 0.868 0.394 0.413 0.412 0.461 0.226 0.239 0.261 0.286\\r\\nM Groups. MSE 1.858 1.269 1.658 1.883 0.375 0.426 0.403 0.467 0.135 0.151 0.168 0.199\\r\\nMAE 0.895 0.789 0.876 0.902 0.403 0.429 0.423 0.483 0.233 0.249 0.272 0.300\\r\\nD Groups. MSE 2.525 1.318 1.662 1.991 0.377 0.412 0.409 0.467 0.136 0.153 0.170 0.197\\r\\nMAE 0.935 0.730 0.907 0.957 0.406 0.429 0.438 0.479 0.235 0.250 0.276 0.298\\r\\nno Group. MSE 2.231 1.482 1.551 1.991 0.381 0.423 0.418 0.479 0.138 0.161 0.173 0.203\\r\\nMAE 0.906 0.796 0.873 0.926 0.412 0.427 0.432 0.489 0.238 0.254 0.278 0.304\\r\\nCross-LKTCN can outperform DLinear[36] in general. Compared with PatchTST[25], Cross-LKTCN\\r\\ncan surpass it in many datasets. For example, in Exchange dataset that is of unclear periodicity, while\\r\\nPatchTST suffers from performance degradation for it cannot capture useful cross-time information,\\r\\nCross-LKTCN can utilize cross-variable dependency as additiaonal information and still achieves\\r\\ntop-2 perfomance, which is much better that PatchTST. And SCINet has advantages in Exchange\\r\\ndataset for its hierarchical structure can utilize multi-scale information of input series, which makes\\r\\nit more possible to match the actual periodicity. And in other settings, the prediction accuracy of\\r\\nCross-LKTCN and PatchTST are very close. The experimental results prove that a cross-variable\\r\\nmodel and a pure convolution-based model can also achieve state-of-the-art performance and\\r\\nhave great potential in time series forecasting. (4) See supplementary materials for showcases.\\r\\n4.2 Ablation Study\\r\\nSuccessive Point-wise Group Convolution FFNs. We propose two successive point-wise group\\r\\nconvolution FFNs to capture cross-variable dependency. To validate its effectiveness, we consider\\r\\nfollowing 4 cases: (a) D Groups + M Groups. means our two successive point-wise group convolution\\r\\nFFNs whose group numbers equal to M and D respectively. (b) M Groups. means we only keep\\r\\nthe first point-wise group convolution FFN whose group number equals to M. (c) D Groups. means\\r\\nwe only keep the second point-wise group convolution FFN whose group number equals to D.\\r\\n(d) no Group. means we replace the two successive point-wise group convolution FFNs with one\\r\\nnon-group convolution FFN. Results are shown on Table 2. Case (a) achieves the best results. As\\r\\ncomparison, case (b) can only apply the linear transformation and non-linear activation to D features\\r\\nper variable to learn each variable’s deep representation independently. But it omits to capture the\\r\\ncross-variable dependency. In contrast, case (c) can capture the cross-variable dependency per feature.\\r\\nBut to each variable, it doesn’t apply any linear interaction across features to learn the new deep\\r\\nrepresentation. And case (d) is not a decoupling method, it cannot meet the needs of capturing\\r\\ncross-variable dependency and learning the new deep representation of each variable respectively.\\r\\n4.3 Model Analysis\\r\\nImpact of Kernel Size. According to[5], a large kernel size is the key to obtain a large receptive\\r\\nfield in 2D convolution. To verify whether this finding still works on 1D convolution and to figure out\\r\\nthe impact of kernel size, we perform experiments with 3 different kernel sizes ranging from small\\r\\nto large on 3 datasets. Results on Table 3 show that increasing the kernel size leads to performance\\r\\nimprovement. The experiment results indicate that directly enlarging the kernel size in 1D convolution\\r\\nlayer and training it with Structural Re-parameterization technique[5] can effectively improve the\\r\\nreceptive field and help convolution layer to better capture cross-time dependency.\\r\\nImpact of Input Length. Since a longer input length indicates more historical information an\\r\\nalgorithm can utilize in time series forecasting, a model with strong ability to capture long-term\\r\\ntemporal dependency should perform better when input length increases[36, 30, 25]. To validate\\r\\nour model, we conduct experiments with different input lengths and the same prediction length. As\\r\\nshown in Figure 5,in general, our model gains performance improvement with increasing input length,\\r\\nindicating our model can effectively extract useful information from longer history and capture long\\x02term dependency. However, some Transformer-based models[33, 39, 29] suffer from performance\\r\\ndegradation with increasing input length owing to the repeated short-term patterns according to[39].\\r\\n8\\nTable 3: Impact of kernel size. We compare three different kernel sizes ranging from small to large.\\r\\nA lower MSE or MAE indicates a better performance. The best results are highlighted in blod.\\r\\nDatasets ILI ETTh1 Electricity\\r\\nPrediction length 24 36 48 60 96 192 336 720 96 192 336 720\\r\\nkernel size = 3 MSE 1.906 1.546 1.754 1.893 0.381 0.416 0.403 0.460 0.143 0.155 0.175 0.203\\r\\nMAE 0.862 0.841 0.891 0.900 0.405 0.423 0.419 0.470 0.237 0.249 0.270 0.299\\r\\nkernel size = 31 MSE 1.687 1.486 1.376 1.855 0.367 0.405 0.389 0.449 0.133 0.147 0.159 0.192\\r\\nMAE 0.848 0.855 0.797 0.929 0.393 0.413 0.410 0.460 0.228 0.243 0.257 0.288\\r\\nkernel size = 51 MSE 1.347 1.250 1.388 1.774 0.368 0.403 0.391 0.449 0.130 0.143 0.161 0.189\\r\\nMAE 0.717 0.778 0.781 0.868 0.393 0.412 0.411 0.460 0.226 0.240 0.260 0.286\\r\\nFigure 5: The MSE results with different input lengths and same prediction length(192 time steps).\\r\\n4.4 Efficiency Analysis\\r\\nWe list each layer’s complexity to capture cross-time and cross-variable dependency in two latest\\r\\ncross-variable/convolution-based models on Table 4 for comparison. For Crossformer[38], we only\\r\\nanalyse the complexity in the attention module in an Encoder block. And for MICN[30], since it is\\r\\na variable-mixing method, we don’t take variable number M into consideration when analysing its\\r\\ncomplexity and don’t consider its complexity to capture cross-variable dependency. Cross-LKTCN\\r\\nand Crossformer’s complexity to capture cross-variable dependency are quadratic with respect to\\r\\nvariable number M. But in terms of capturing cross-time dependency, while Crossformer suffers from\\r\\nquadratic complexity due to the attention mechanism, Cross-LKTCN can reduce the computational\\r\\ncost to linear complexity.\\r\\nCross-LKTCN and MICN’s complexity to capture cross-time dependency are linear with respect to\\r\\ninput length L since they are pure convolution structure. But with the help of patch-style embedding,\\r\\nthe coefficient 1\\r\\nS\\r\\nterm can significantly reduce Cross-LKTCN’s practical complexity. More running\\r\\ntime and memory usage analysis are in supplementary materials.\\r\\n5 Conclusion and Future Work\\r\\nThis paper proposes a pure convolution-based model, namely cross-LKTCN, as an efficient way to\\r\\ncapture cross-time and cross-variable dependency for time-series forecasting. Specifically, the patch\\x02style embedding strategy is applied to time series to enhance locality and aggregate more semantic\\r\\ninformation. Then in each Cross-LKTCN block, a depth-wise large kernel convolution is used to\\r\\ncapture cross-time dependency, following which two successive point-wise group convolution FFNs\\r\\nare used to capture cross-variable dependency. Extensive experimental results on nine real-world\\r\\ndatasets demonstrate the effectiveness of Cross-LKTCN against state-of-the-arts. Our study also\\r\\nTable 4: Complexity analysis of different forecasting models.\\r\\nModel Cross-time Cross-variable\\r\\nCross-LKTCN O( L\\r\\nS MD) O( LS MD2 +\\r\\nL\\r\\nS DM2\\r\\n)\\r\\nCrossformer O( L\\r\\n2\\r\\nS2 MD) O(M2 LS D)\\r\\nMICN O(LD2) -\\r\\n9\\ndemonstrates the importance of cross-variable dependency and the effectiveness of convolution in\\r\\ntime series forecasting. Both two aspects are worth further study in the future.\\r\\n10\\nReferences\\r\\n[1] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\\r\\nand recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\\r\\n[2] Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. Conditional time series forecast\\x02ing with convolutional neural networks. arXiv preprint arXiv:1703.04691, 2017.\\r\\n[3] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation\\r\\nof gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555,\\r\\n2014.\\r\\n[4] Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, and Shirui Pan.\\r\\nTriformer: Triangular, variable-specific attentions for long sequence multivariate time series\\r\\nforecasting–full version. arXiv preprint arXiv:2204.13767, 2022.\\r\\n[5] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to\\r\\n31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF Conference on\\r\\nComputer Vision and Pattern Recognition, pages 11963–11975, 2022.\\r\\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\r\\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\\r\\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\r\\narXiv:2010.11929, 2020.\\r\\n[7] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré.\\r\\nCombining recurrent, convolutional, and continuous-time models with linear state space layers.\\r\\nAdvances in neural information processing systems, 34:572–585, 2021.\\r\\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning. Image\\r\\nRecognition, 7, 2015.\\r\\n[9] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\\r\\narXiv:1606.08415, 2016.\\r\\n[10] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\r\\n9(8):1735–1780, 1997.\\r\\n[11] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias\\r\\nWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural\\r\\nnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\\r\\n[12] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\\r\\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern\\r\\nrecognition, pages 4700–4708, 2017.\\r\\n[13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\\r\\nby reducing internal covariate shift. In International conference on machine learning, pages\\r\\n448–456. pmlr, 2015.\\r\\n[14] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.\\r\\nReversible instance normalization for accurate time-series forecasting against distribution shift.\\r\\nIn International Conference on Learning Representations, 2021.\\r\\n[15] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\\r\\narXiv preprint arXiv:2001.04451, 2020.\\r\\n[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep\\r\\nconvolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.\\r\\n[17] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term\\r\\ntemporal patterns with deep neural networks. In The 41st international ACM SIGIR conference\\r\\non research & development in information retrieval, pages 95–104, 2018.\\r\\n11\\n[18] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng\\r\\nYan. Enhancing the locality and breaking the memory bottleneck of transformer on time series\\r\\nforecasting. Advances in neural information processing systems, 32, 2019.\\r\\n[19] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosoph\\x02ical Transactions of the Royal Society A, 379(2194):20200209, 2021.\\r\\n[20] Minhao Liu, Ailing Zeng, Z Xu, Q Lai, and Q Xu. Scinet: time series modeling and forecasting\\r\\nwith sample convolution and interaction. In 36th Conference on Neural Information Processing\\r\\nSystems (NeurIPS), 2022.\\r\\n[21] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola\\r\\nPechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up\\r\\nkernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022.\\r\\n[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.\\r\\nPyraformer: Low-complexity pyramidal attention for long-range time series modeling and\\r\\nforecasting. In International conference on learning representations, 2021.\\r\\n[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\r\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\\r\\nof the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\\r\\n[24] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\\r\\nXie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision\\r\\nand Pattern Recognition, pages 11976–11986, 2022.\\r\\n[25] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is\\r\\nworth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730,\\r\\n2022.\\r\\n[26] Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think globally, act locally: A deep neural\\r\\nnetwork approach to high-dimensional time series forecasting. Advances in neural information\\r\\nprocessing systems, 32, 2019.\\r\\n[27] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\\r\\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\\r\\n[28] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\\r\\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine\\r\\nlearning research, 15(1):1929–1958, 2014.\\r\\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\r\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\r\\nprocessing systems, 30, 2017.\\r\\n[30] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn:\\r\\nMulti-scale local and global context modeling for long-term series forecasting. In The Eleventh\\r\\nInternational Conference on Learning Representations, 2023.\\r\\n[31] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun.\\r\\nTransformers in time series: A survey. arXiv preprint arXiv:2202.07125, 2022.\\r\\n[32] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Times\\x02net: Temporal 2d-variation modeling for general time series analysis. arXiv preprint\\r\\narXiv:2210.02186, 2022.\\r\\n[33] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition trans\\x02formers with auto-correlation for long-term series forecasting. Advances in Neural Information\\r\\nProcessing Systems, 34:22419–22430, 2021.\\r\\n[34] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang.\\r\\nConnecting the dots: Multivariate time series forecasting with graph neural networks. In\\r\\nProceedings of the 26th ACM SIGKDD international conference on knowledge discovery &\\r\\ndata mining, pages 753–763, 2020.\\r\\n12\\n[35] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual\\r\\ntransformations for deep neural networks. In Proceedings of the IEEE conference on computer\\r\\nvision and pattern recognition, pages 1492–1500, 2017.\\r\\n[36] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\\r\\nforecasting? arXiv preprint arXiv:2205.13504, 2022.\\r\\n[37] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient\\r\\nconvolutional neural network for mobile devices. In Proceedings of the IEEE conference on\\r\\ncomputer vision and pattern recognition, pages 6848–6856, 2018.\\r\\n[38] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency\\r\\nfor multivariate time series forecasting. In International Conference on Learning Representa\\x02tions, 2023.\\r\\n[39] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\\r\\nZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In\\r\\nProceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106–11115,\\r\\n2021.\\r\\n[40] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Rong Jin, et al. Film: Fre\\x02quency improved legendre memory model for long-term time series forecasting. arXiv preprint\\r\\narXiv:2205.08897, 2022.\\r\\n[41] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:\\r\\nFrequency enhanced decomposed transformer for long-term series forecasting. In International\\r\\nConference on Machine Learning, pages 27268–27286. PMLR, 2022.\\r\\n13'},\n",
       " {'name': '2407.15516v1.pdf',\n",
       "  'content': 'Attention Is All You Need But You Don’t Need All Of It\\r\\nFor Inference of Large Language Models\\r\\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\\r\\nAbstract\\r\\nThe inference demand for LLMs has skyrocketed\\r\\nin recent months, and serving models with low\\r\\nlatencies remains challenging due to the quadratic\\r\\ninput length complexity of the attention layers.\\r\\nIn this work, we investigate the effect of drop\\x02ping MLP and attention layers at inference time\\r\\non the performance of Llama-v2 models. We\\r\\nfind that dropping dreeper attention layers only\\r\\nmarginally decreases performance but leads to the\\r\\nbest speedups alongside dropping entire layers.\\r\\nFor example, removing 33% of attention layers\\r\\nin a 13B Llama2 model results in a 1.8% drop in\\r\\naverage performance over the OpenLLM bench\\x02mark. We also observe that skipping layers except\\r\\nthe latter layers reduces performances for more\\r\\nlayers skipped, except for skipping the attention\\r\\nlayers.\\r\\n1. Introduction\\r\\nThe ubiquitous deployment of Large Language Models\\r\\n(LLMs) results in ever-growing amounts of compute spent\\r\\non inference (Patterson et al., 2021; Chen et al., 2023; Kad\\x02dour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur\\x02ther, serving models with low latencies remains challenging\\r\\nbecause contemporary Transformer architectures employ\\r\\nthe self-attention mechanism with quadratic input complex\\x02ity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\\r\\nIn this work, we delve deeper into the concept of layer\\r\\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\\r\\nthe computation on superfluous LLM components. Our\\r\\nfindings demonstrate that pruning deeper attention layers\\r\\ndoes not significantly affect performance. When applied\\r\\nto Llama-v2 (Touvron et al., 2023b), we maintain good\\r\\nperformance on the OpenLLM (ARC (Clark et al., 2018),\\r\\n*Equal contribution 1University College London, UK\\r\\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\\r\\n<tyukinegor@gmail.com>.\\r\\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\\r\\nPMLR 235, 2024. Copyright 2024 by the author(s).\\r\\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\\r\\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech\\x02ing et al., 2023), recording only minimal performance devi\\x02ations compared to the full model.\\r\\n2. Method\\r\\n0 5 10 15 20 25 30 35 40\\r\\nLayer\\r\\n0.65\\r\\n0.70\\r\\n0.75\\r\\n0.80\\r\\n0.85\\r\\n0.90\\r\\n0.95\\r\\n1.00\\r\\nCosine Similarity\\r\\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13b\\r\\nLLaMA-v2 7b\\r\\nLLaMA-v2 13b\\r\\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\\r\\nlayer: We observe that the deeper the layer, the more its features\\r\\nare similar to the previous layer except for the very last layer.\\r\\n2.1. Layer skipping\\r\\nConsider a Transformer model M with L layers, each\\r\\nconsisting of an attention sub-layer followed by a multi\\x02layer perceptron (MLP) sub-layer. We denote each layer as\\r\\nMi = (Attentioni, MLPi) for i ∈ {1, 2, . . . , L}.\\r\\nTo compare the performance of Transformer models when\\r\\nskipping specific sub-layers, we create two variants of the\\r\\nmodel:\\r\\n1. Skipping MLP Layers: We construct a model Mskip MLP\\r\\n1\\r\\narXiv:2407.15516v1 [cs.LG] 22 Jul 2024\\nAttention Is All You Need But You Don’t Need All Of It\\r\\nby skipping the MLP sub-layer from the last k layers. The\\r\\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i ∈\\r\\n{1, 2, . . . , L − k}} ∪ {(Attentioni, ∅) | i ∈ {L − k +\\r\\n1, . . . , L}}.\\r\\n2. Skipping Attention Layers: We construct a model\\r\\nMskip Attention by skipping the attention sub-layer from the\\r\\nlast k layers. The resulting model is Mskip Attention =\\r\\n{(Attentioni, MLPi) | i ∈ {1, 2, . . . , L − k}} ∪\\r\\n{(∅, MLPi) | i ∈ {L − k + 1, . . . , L}}.\\r\\n3. Skipping Transformer Blocks: We construct a model\\r\\nMskip Attention by skipping the entire last k layers. The re\\x02sulting model is Mskip Block = {(Attentioni\\r\\n, MLPi) | i ∈\\r\\n{1, 2, . . . , L − k}} ∪ {(∅) | i ∈ {L − k + 1, . . . , L}}.\\r\\nWe then evaluate the performance of these modified models\\r\\non the OpenLLM benchmark (Beeching et al., 2023), com\\x02paring metrics such as accuracy, computational efficiency,\\r\\nand memory usage. This comparison helps in understand\\x02ing the individual contributions of the attention and MLP\\r\\nsub-layers to the overall performance of the Transformer\\r\\nmodel.\\r\\n(a) Skip attention lay\\x02ers.(b) Skip attention lay\\x02ers, keep last full\\r\\nblock.\\r\\n(c) Skip ffwd layers.\\r\\n(d) Skip ffwd layers,\\r\\nkeep last full block.\\r\\n(e) Skip full blocks. (f) Skip full blocks,\\r\\nkeep last full block.\\r\\nFigure 2. Skip mechanisms for skipping single layers and entire\\r\\nTransformer blocks (ffwd and attention layers) during inference.\\r\\n2.2. Motivation: Are Deeper Layers More Redundant?\\r\\nIn Transformer models, the last layers have been shown to\\r\\ncontribute less information than earlier layers, making it\\r\\npossible to drop those layers at a minimal performance cost\\r\\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\\r\\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\\r\\n2023).\\r\\nTo verify this, we experiment with removing either the at\\x02tention sublayers or the MLP sublayers. Figure 1 shows the\\r\\ncosine similarities between a layer’s features and the previ\\x02ous layer showing that deeper layers have a lower impact\\r\\non the features than earlier layers. One notable exception\\r\\nto this trend is that the last layer for both Llama-v2 7B and\\r\\n13B has the lowest cosine similarity with the previous layer.\\r\\nPrevious analysis of the attention mechanism has shown\\r\\nthat they can converge to the same value due to attention\\r\\ncollapse (Zhai et al., 2023) and token features that also con\\x02verge to the same value due to over-smoothing (Wang et al.,\\r\\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\\r\\n2023), with solutions to these issues typically improving\\r\\nperformance (Ali et al., 2023; Choi et al., 2024).\\r\\n3. Results\\r\\nExperimental Setup For all experiments, we use either\\r\\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\\r\\ntwo LLMs trained on trillions of publically available tokens.\\r\\nWe experiment with keeping 66%, 75%, 90% and 100% of\\r\\nthe network and report the corresponding results in Table 1.\\r\\nWe also experiment with removing attention sublayers in\\r\\nTable 2, MLP sublayers in Table 3, and a varying number of\\r\\nlayers similar to Table 1 but keeping the last layer in Table 4.\\r\\n3.1. Chopping Layers\\r\\nTable 1. Llama-v2 skipping full layer\\r\\nModel Performances\\r\\nARC HellaSwag TruthfulQA MMLU Average\\r\\n7B-66% 35.2 46.8 46.2 40.3 42.1\\r\\n7B-75% 38.3 53.0 45.1 45.9 45.6\\r\\n7B-90% 47.7 69.3 39.6 46.4 50.8\\r\\n7B-100% 53.1 78.6 38.8 46.6 54.3\\r\\n13B-66% 37.8 46.8 45.3 51.8 45.4\\r\\n13B-75% 40.9 53.6 42.5 53.2 47.6\\r\\n13B-90% 51.3 71.3 37.1 54.8 53.6\\r\\n13B-100% 59.6 82.1 36.9 55.4 58.5\\r\\nOn all datasets except TruthfulQA, performance drops\\r\\nwhich is expected. It had already been observed that larger\\r\\nlanguage models are less truthful (Lin et al., 2022), but we\\r\\nnow also observe that reducing the size of already trained\\r\\nmodels can also make them more truthful. The observa\\x02tion still holds when the last layer is preserved. Skipping\\r\\n2\\nAttention Is All You Need But You Don’t Need All Of It\\r\\nTable 2. Llama-v2 skipping attention sublayers\\r\\nModel Performances\\r\\nARC HellaSwag TruthfulQA MMLU Average\\r\\n7B-66% 51.2 77.0 42.2 39.4 52.5\\r\\n7B-75% 52.5 78.3 42.3 41.4 53.6\\r\\n7B-90% 52.8 78.9 40.0 44.0 53.9\\r\\n7B-100% 53.1 78.6 38.8 46.6 54.3\\r\\n13B-66% 55.6 80.1 40.1 51.3 56.8\\r\\n13B-75% 55.9 79.7 39.9 52.1 56.9\\r\\n13B-90% 57.0 81.3 38.2 54.8 57.8\\r\\n13B-100% 59.6 82.1 36.9 55.4 58.5\\r\\nTable 3. Llama-v2 skipping ffwd sublayers\\r\\nModel Performances\\r\\nARC HellaSwag TruthfulQA MMLU Average\\r\\n7B-66% 35.1 52.5 42.2 43.9 43.4\\r\\n7B-75% 40.4 60.3 39.2 46.3 46.6\\r\\n7B-90% 48.5 71.4 38.0 46.1 51.0\\r\\n7B-100% 53.1 78.6 38.8 46.6 54.3\\r\\n13B-66% 41.6 56.9 40.7 53.4 48.2\\r\\n13B-75% 47.3 65.2 40.0 53.2 51.4\\r\\n13B-90% 54.2 75.8 38.3 54.7 55.8\\r\\n13B-100% 59.6 82.1 36.9 55.4 58.5\\r\\nattention layers only leads to better results with only a 1.8%\\r\\ndecrease in performance when keeping 66% of the network\\r\\ncompared to a 13.1% decrease in performance when drop\\x02ping dropping the MLP layers only. This seems to indicate\\r\\nthat MLP layers are more important than attention layers, at\\r\\nleast in deeper parts of the network.\\r\\n3.2. Last Layer Inclusion\\r\\nTable 4. Llama-v2 skip full layers with last layer\\r\\nModel Performances\\r\\nARC HellaSwag TruthfulQA MMLU Average\\r\\n7B-66% 32.0 45.8 46.9 40.7 41.3\\r\\n7B-75% 34.5 49.4 45.9 38.3 42.0\\r\\n7B-90% 46.5 73.1 41.8 41.4 50.7\\r\\n7B-100% 53.1 78.6 38.8 46.6 54.3\\r\\n13B-66% 35.1 50.0 46.9 19.1 37.8\\r\\n13B-75% 38.7 56.6 43.7 25.2 41.1\\r\\n13B-90% 51.2 78.1 38.0 27.1 47.9\\r\\n13B-100% 59.6 82.1 36.9 55.4 58.5\\r\\nSurprisingly, we notice that skipping layers except the lat\\x02ter layers reduces performances for more layers skipped,\\r\\nexcept for skipping the attention layers. This is even more\\r\\nexaggerated compared to just dropping layers, including the\\r\\nlast one. The reason for this could be attributed to the (lack\\r\\nof) robustness of feedforward sublayers, as the last layer\\r\\nnow has to process perturbed information from earlier lay\\x02ers. For future work, it would be interesting to see if these\\r\\nperformance drops can be compensated by a small amount\\r\\nTable 5. Llama-v2 skip attention sublayers with last layer\\r\\nModel Performances\\r\\nARC HellaSwag TruthfulQA MMLU Average\\r\\n7B-66% 49.3 77.1 40.5 42.5 52.4\\r\\n7B-75% 51.8 78.3 41.1 44.1 53.8\\r\\n7B-90% 51.9 78.7 39.4 45.7 53.9\\r\\n7B-100% 53.1 78.6 38.8 46.6 54.3\\r\\n13B-66% 56.8 82.1 38.0 50.3 56.8\\r\\n13B-75% 57.5 82.1 37.0 51.4 57.0\\r\\n13B-90% 58.9 82.4 36.6 54.5 58.1\\r\\n13B-100% 59.6 82.1 36.9 55.4 58.5\\r\\nTable 6. Llama-v2 skip ffwd sublayers with last layer\\r\\nModel Performances\\r\\nARC HellaSwag TruthfulQA MMLU Average\\r\\n7B-66% 32.0 45.8 46.9 39.4 41.0\\r\\n7B-75% 34.5 49.4 45.9 40.2 42.5\\r\\n7B-90% 46.5 73.1 41.8 40.2 50.4\\r\\n7B-100% 53.1 78.6 38.8 46.6 54.3\\r\\n13B-66% 35.1 50.0 46.9 20.4 38.1\\r\\n13B-75% 38.7 56.6 43.7 33.6 43.2\\r\\n13B-90% 51.2 78.1 38.0 34.4 50.4\\r\\n13B-100% 59.6 82.1 36.9 55.4 58.5\\r\\nof continued training; since model growing techniques for\\r\\ntraining seem to not suffer from instabilities (Kaddour et al.,\\r\\n2023b).\\r\\n3.3. Compute-matched Comparison\\r\\nTo measure the efficiency of the networks we conducted\\r\\na separate experiment, where we record the time it takes\\r\\nfor the model to output a sequence of length 1, averaging\\r\\nover 1000 sequences. We conducted this experiment for\\r\\nboth 50 and 100 length input sequences. We notice that full\\r\\nlayer droppings do improve time costs the best, followed by\\r\\nattention sublayers, and then feedforward sublayers which\\r\\ndo not impact the speed of processing a lot.\\r\\nWe report the time×102(for clarity) it takes to predict 1\\r\\ntoken for 1000 sequences as well as the percentage improve\\x02ment. We show the results of this experiment for Llama 2\\r\\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\\r\\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\\r\\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\\r\\nModel Full Attention ffwd\\r\\nTime(s) ×102(%) Time(s) ×102(%) Time(s) ×102(%)\\r\\n7B-66% 31.35 32.96 36.72 21.47 43.51 6.95\\r\\n7B-75% 35.48 24.12 39.46 15.61 42.88 8.30\\r\\n7B-90% 43.31 7.38 42.93 8.19 44.17 5.53\\r\\n7B-100% 46.76 0 - - - -\\r\\n3\\nAttention Is All You Need But You Don’t Need All Of It\\r\\nTable 8. Llama-v2 time results, 50 length sequence, last layer in\\x02cluded\\r\\nModel Full Attention ffwd\\r\\nTime(s) ×102(%) Time(s) ×102(%) Time(s) ×102(%)\\r\\n7B-66% 31.78 32.04 36.92 21.04 41.31 11.66\\r\\n7B-75% 34.98 25.19 40.24 13.94 42.62 8.85\\r\\n7B-90% 40.92 12.49 42.43 9.26 43.51 6.95\\r\\n7B-100% 46.76 0 - - - -\\r\\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\\r\\nModel Full Attention ffwd\\r\\nTime(s) ×102(%) Time(s) ×102(%) Time(s) ×102(%)\\r\\n7B-66% 32.36 32.58 38.97 18.18 43.08 10.25\\r\\n7B-75% 36.58 23.79 41.27 14.02 44.13 8.06\\r\\n7B-90% 43.65 9.06 44.62 7.04 46.30 3.54\\r\\n7B-100% 48.00 0 - - - -\\r\\nTable 10. Llama-v2 time results, 100 length sequence, last layer\\r\\nincluded\\r\\nModel Full Attention ffwd\\r\\nTime(s) ×102(%) Time(s) ×102(%) Time(s) ×102(%)\\r\\n7B-66% 32.05 33.23 38.52 19.75 42.66 11.13\\r\\n7B-75% 36.41 24.15 41.00 14.58 43.92 8.50\\r\\n7B-90% 43.28 9.83 44.27 7.77 45.20 5.83\\r\\n7B-100% 48.00 0 - - - -\\r\\n4. Related Work\\r\\nEarly Exit during inference Early exit methods have also\\r\\nbeen proposed in other domains (Graves, 2017; Teerapit\\x02tayanon et al., 2017) before getting adapted to autoregressive\\r\\nmodels (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\\r\\n2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\\r\\n2024). The idea works by dynamically allocating compute\\r\\nbased on the difficulty of the input sequence. Our method\\r\\nprunes the deepest layers and does not involve any level of\\r\\nadaptability. This is beneficial because it does not require\\r\\nthe entire model to be loaded in memory. Dropping layers\\r\\nduring inference has been done on BERT-like models in\\r\\n(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\\r\\nanalysis to more recent LLMs and study the impact of skip\\x02ping attention and/or MLP layers in more detail. Concurrent\\r\\nwork to ours by Gromov et al. (2024) yields similar results\\r\\nby pruning deeper layers and applying fine-tuning on the\\r\\npruned model.\\r\\nLayer dropping/growing during training There are var\\x02ious works studying the dropping/growing layers dynami\\x02cally during training (Fan et al., 2019; Gong et al., 2019;\\r\\nKaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\\r\\ncontrast, this work focuses on dropping layers of an already\\r\\npre-trained model in a way similar to Men et al. (2024).\\r\\nOther Inference Speedup Methods Other works to speed\\r\\nup inference include compressing KV caches (Nawrot et al.,\\r\\n2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\\r\\n(Chen et al., 2023), efficient memory management (Kwon\\r\\net al., 2023), or subqudratic attention architectures (Fu et al.,\\r\\n2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\\r\\nbeen provided by Kaddour et al. (2023a).\\r\\n5. Conclusion\\r\\nWe investigated the effect of dropping the last layers from\\r\\nthe 7B and 13B Llama2 models. We observe that dropping\\r\\nattention sublayers lead to much lower drops in performance\\r\\nthan dropping the MLP sublayers, whether the last layer\\r\\nis included or not, while also leading to better inference\\r\\nspeedups. For example, removing 33% of attention layers\\r\\nleads to an 18% speedup in a 13B Llama2 model at the cost\\r\\nof a 1.8% drop in average performance. This shows that\\r\\nmassive improvements can be made over dropping entire\\r\\nlayers from just dropping the attention sublayer.\\r\\nReferences\\r\\nAli, A., Galanti, T., and Wolf, L. Centered self-attention\\r\\nlayers, 2023.\\r\\nBeeching, E., Fourrier, C., Habib, N., Han, S.,\\r\\nLambert, N., Rajani, N., Sanseviero, O., Tun\\x02stall, L., and Wolf, T. Open llm leader\\x02board. https://huggingface.co/spaces/\\r\\nHuggingFaceH4/open_llm_leaderboard,\\r\\n2023.\\r\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\r\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\r\\nlatent predictions from transformers with the tuned lens.\\r\\narXiv preprint arXiv:2303.08112, 2023.\\r\\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\\r\\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\\r\\nScaling open-source language models with longtermism.\\r\\narXiv preprint arXiv:2401.02954, 2024.\\r\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\r\\nJumper, J. Accelerating large language model decoding\\r\\nwith speculative sampling. CoRR, abs/2302.01318, 2023.\\r\\ndoi: 10.48550/ARXIV.2302.01318. URL https://\\r\\ndoi.org/10.48550/arXiv.2302.01318.\\r\\nChen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\\r\\nLarge-scale training and inference of early-exit large lan\\x02guage models with 3d parallelism, 2024.\\r\\nChoi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\\r\\nPark, N. Graph convolutions enrich the self-attention in\\r\\ntransformers!, 2024.\\r\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\r\\nSchoenick, C., and Tafjord, O. Think you have solved\\r\\n4\\nAttention Is All You Need But You Don’t Need All Of It\\r\\nquestion answering? try arc, the ai2 reasoning challenge,\\r\\n2018.\\r\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\r\\nto conclusions: Short-cutting transformers with linear\\r\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\r\\nDong, Y., Cordonnier, J.-B., and Loukas, A. Attention\\r\\nis not all you need: Pure attention loses rank doubly\\r\\nexponentially with depth, 2023.\\r\\nDovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\\r\\nSetting the record straight on transformer oversmoothing,\\r\\n2024.\\r\\nElbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\\r\\ntransformer. In International Conference on Learning\\r\\nRepresentations, 2020. URL https://openreview.\\r\\nnet/forum?id=SJg7KhVKPH.\\r\\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\\r\\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\\r\\nS., Roman, A., et al. Layer skip: Enabling early exit\\r\\ninference and self-speculative decoding. arXiv preprint\\r\\narXiv:2404.16710, 2024.\\r\\nFan, A., Grave, E., and Joulin, A. Reducing transformer\\r\\ndepth on demand with structured dropout, 2019.\\r\\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\\r\\nA., Wang, Y., and Wang, Z. Not all layers of llms are\\r\\nnecessary during inference, 2024.\\r\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\\r\\nA., and Re, C. Hungry hungry hippos: Towards lan- ´\\r\\nguage modeling with state space models. arXiv preprint\\r\\narXiv:2212.14052, 2022.\\r\\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\\r\\nEfficient training of bert by progressively stacking. In\\r\\nInternational conference on machine learning, pp. 2337–\\r\\n2346. PMLR, 2019.\\r\\nGraves, A. Adaptive computation time for recurrent neural\\r\\nnetworks, 2017.\\r\\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\\r\\nRoberts, D. A. The unreasonable ineffectiveness of the\\r\\ndeeper layers, 2024.\\r\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\\r\\nwith selective state spaces, 2023.\\r\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\\r\\nSong, D., and Steinhardt, J. Measuring massive multitask\\r\\nlanguage understanding, 2021.\\r\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\r\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\r\\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\\r\\narXiv:2310.06825, 2023.\\r\\nJiang, Y.-G., Cheng, C., Lin, H., and Fu, Y. Learning\\r\\nlayer-skippable inference network. IEEE Transactions on\\r\\nImage Processing, 29:8747–8759, 2020. doi: 10.1109/\\r\\nTIP.2020.3018269.\\r\\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\\r\\nR., and McHardy, R. Challenges and applications of\\r\\nlarge language models. CoRR, abs/2307.10169, 2023a.\\r\\ndoi: 10.48550/ARXIV.2307.10169. URL https://\\r\\ndoi.org/10.48550/arXiv.2307.10169.\\r\\nKaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\\r\\nM. J. No train no gain: Revisiting efficient training\\r\\nalgorithms for transformer-based language models. In\\r\\nOh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\\r\\nM., and Levine, S. (eds.), Advances in Neural Information\\r\\nProcessing Systems 36: Annual Conference on Neural\\r\\nInformation Processing Systems 2023, NeurIPS 2023,\\r\\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023b.\\r\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\r\\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\\r\\nmemory management for large language model serving\\r\\nwith pagedattention. In Proceedings of the 29th Sym\\x02posium on Operating Systems Principles, pp. 611–626,\\r\\n2023.\\r\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\\r\\nhow models mimic human falsehoods, 2022.\\r\\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\\r\\nShrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\\r\\nB. Deja vu: Contextual sparsity for efficient LLMs at\\r\\ninference time. In Krause, A., Brunskill, E., Cho, K.,\\r\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro\\x02ceedings of the 40th International Conference on Ma\\x02chine Learning, volume 202 of Proceedings of Machine\\r\\nLearning Research, pp. 22137–22176. PMLR, 23–29 Jul\\r\\n2023. URL https://proceedings.mlr.press/\\r\\nv202/liu23am.html.\\r\\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\\r\\nX., and Chen, W. Shortgpt: Layers in large language\\r\\nmodels are more redundant than you expect, 2024. URL\\r\\nhttps://arxiv.org/abs/2403.03853.\\r\\nNawrot, P., Łancucki, A., Chochowski, M., Tarjan, D., and ´\\r\\nPonti, E. M. Dynamic memory compression: Retrofitting\\r\\nllms for accelerated inference, 2024.\\r\\nPatterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\\r\\nL., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car\\x02bon emissions and large neural network training. CoRR,\\r\\n5\\nAttention Is All You Need But You Don’t Need All Of It\\r\\nabs/2104.10350, 2021. URL https://arxiv.org/\\r\\nabs/2104.10350.\\r\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\\r\\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\\r\\net al. Rwkv: Reinventing rnns for the transformer era.\\r\\narXiv preprint arXiv:2305.13048, 2023.\\r\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli\\x02crap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\\r\\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi\\x02modal understanding across millions of tokens of context.\\r\\narXiv preprint arXiv:2403.05530, 2024.\\r\\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\\r\\neffect of dropping layers of pre-trained transformer mod\\x02els. Computer Speech & Language, 77:101429, jan\\r\\n2023. doi: 10.1016/j.csl.2022.101429. URL https:\\r\\n//doi.org/10.1016%2Fj.csl.2022.101429.\\r\\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\\r\\nTran, V., Tay, Y., and Metzler, D. Confident adaptive\\r\\nlanguage modeling. Advances in Neural Information\\r\\nProcessing Systems, 35:17456–17472, 2022.\\r\\nTeerapittayanon, S., McDanel, B., and Kung, H. T.\\r\\nBranchynet: Fast inference via early exiting from deep\\r\\nneural networks, 2017.\\r\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\r\\nM.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., `\\r\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam\\x02ple, G. Llama: Open and efficient foundation language\\r\\nmodels, 2023a.\\r\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\r\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\r\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\r\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\r\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\r\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\r\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\r\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\r\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\r\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\r\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\r\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\r\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\r\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\r\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\r\\nchat models, 2023b.\\r\\nWang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\\r\\nSkipbert: Efficient inference with shallow layer skipping.\\r\\nIn Proceedings of the 60th Annual Meeting of the Asso\\x02ciation for Computational Linguistics (Volume 1: Long\\r\\nPapers), pp. 7287–7301, 2022a.\\r\\nWang, P., Zheng, W., Chen, T., and Wang, Z. Anti\\x02oversmoothing in deep vision transformers via the fourier\\r\\ndomain analysis: From theory to practice. In In\\x02ternational Conference on Learning Representations,\\r\\n2022b. URL https://openreview.net/forum?\\r\\nid=O476oWmiNNp.\\r\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\r\\ninference of large language models, 2024.\\r\\nXia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\\r\\nLi, W., and Sui, Z. Unlocking efficiency in large language\\r\\nmodel inference: A comprehensive survey of speculative\\r\\ndecoding. arXiv preprint arXiv:2401.07851, 2024.\\r\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\r\\nHellaswag: Can a machine really finish your sentence?,\\r\\n2019.\\r\\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\\r\\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta\\x02bilizing transformer training by preventing attention en\\x02tropy collapse, 2023.\\r\\nZhang, M. and He, Y. Accelerating training of transformer\\x02based language models with progressive layer dropping.\\r\\nAdvances in neural information processing systems, 33:\\r\\n14011–14023, 2020.\\r\\n6'},\n",
       " {'name': '1408.0087v1.pdf',\n",
       "  'content': 'arXiv:1408.0087v1 [stat.AP] 1 Aug 2014\\r\\nThe Annals of Applied Statistics\\r\\n2014, Vol. 8, No. 2, 1256–1280\\r\\nDOI: 10.1214/14-AOAS739\\r\\n\\rc Institute of Mathematical Statistics, 2014\\r\\nPROBABILITY AGGREGATION IN TIME-SERIES: DYNAMIC\\r\\nHIERARCHICAL MODELING OF SPARSE EXPERT BELIEFS1\\r\\nBy Ville A. Satopa¨a, Shane T. Jensen, Barbara A. Mellers, ¨\\r\\nPhilip E. Tetlock and Lyle H. Ungar\\r\\nUniversity of Pennsylvania\\r\\nMost subjective probability aggregation procedures use a single\\r\\nprobability judgment from each expert, even though it is common for\\r\\nexperts studying real problems to update their probability estimates\\r\\nover time. This paper advances into unexplored areas of probability\\r\\naggregation by considering a dynamic context in which experts can\\r\\nupdate their beliefs at random intervals. The updates occur very in\\x02frequently, resulting in a sparse data set that cannot be modeled by\\r\\nstandard time-series procedures. In response to the lack of appropri\\x02ate methodology, this paper presents a hierarchical model that takes\\r\\ninto account the expert’s level of self-reported expertise and produces\\r\\naggregate probabilities that are sharp and well calibrated both in\\x02and out-of-sample. The model is demonstrated on a real-world data\\r\\nset that includes over 2300 experts making multiple probability fore\\x02casts over two years on different subsets of 166 international political\\r\\nevents.\\r\\n1. Introduction. Experts’ probability assessments are often evaluated on\\r\\ncalibration, which measures how closely the frequency of event occurrence\\r\\nagrees with the assigned probabilities. For instance, consider all events that\\r\\nan expert believes to occur with a 60% probability. If the expert is well\\r\\ncalibrated, 60% of these events will actually end up occurring. Even though\\r\\nseveral experiments have shown that experts are often poorly calibrated\\r\\n[see, e.g., Cooke (1991), Shlyakhter et al. (1994)], these are noteworthy ex\\x02ceptions. In particular, Wright et al. (1994) argue that higher self-reported\\r\\nexpertise can be associated with better calibration.\\r\\nReceived September 2013; revised March 2014.\\r\\n1\\r\\nSupported by a research contract to the University of Pennsylvania and the University\\r\\nof California from the Intelligence Advanced Research Projects Activity (IARPA) via the\\r\\nDepartment of Interior National Business Center contract number D11PC20061.\\r\\nKey words and phrases. Probability aggregation, dynamic linear model, hierarchical\\r\\nmodeling, expert forecast, subjective probability, bias estimation, calibration, time series.\\r\\nThis is an electronic reprint of the original article published by the\\r\\nInstitute of Mathematical Statistics in The Annals of Applied Statistics,\\r\\n2014, Vol. 8, No. 2, 1256–1280. This reprint differs from the original in pagination\\r\\nand typographic detail.\\r\\n1\\n2 V. A. SATOPA¨A ET AL. ¨\\r\\nCalibration by itself, however, is not sufficient for useful probability es\\x02timation. Consider a relatively stationary process, such as rain on different\\r\\ndays in a given geographic region, where the observed frequency of occur\\x02rence in the last 10 years is 45%. In this setting an expert could always as\\x02sign a constant probability of 0.45 and be well-calibrated. This assessment,\\r\\nhowever, can be made without any subject-matter expertise. For this rea\\x02son the long-term frequency is often considered the baseline probability—a\\r\\nnaive assessment that provides the decision-maker very little extra informa\\x02tion. Experts should make probability assessments that are as far from the\\r\\nbaseline as possible. The extent to which their probabilities differ from the\\r\\nbaseline is measured by sharpness [Gneiting et al. (2008), Winkler and Jose\\r\\n(2008)]. If the experts are both sharp and well calibrated, they can forecast\\r\\nthe behavior of the process with high certainty and accuracy. Therefore, use\\x02ful probability estimation should maximize sharpness subject to calibration\\r\\n[see, e.g., Raftery et al. (2005), Murphy and Winkler (1987)].\\r\\nThere is strong empirical evidence that bringing together the strengths\\r\\nof different experts by combining their probability forecasts into a sin\\x02gle consensus, known as the crowd belief, improves predictive performance.\\r\\nPrompted by the many applications of probability forecasts, including medi\\x02cal diagnosis [Wilson et al. (1998), Pepe (2003)], political and socio-economic\\r\\nforesight [Tetlock (2005)], and meteorology [Sanders (1963), Vislocky and\\r\\nFritsch (1995), Baars and Mass (2005)], researchers have proposed many\\r\\napproaches to combining probability forecasts [see, e.g., Ranjan and Gneit\\x02ing (2010), Satop¨a¨a et al. (2014a), Batchelder, Strashny and Romney (2010)\\r\\nfor some recent studies, and Genest and Zidek (1986), Wallsten, Budescu\\r\\nand Erev (1997), Clemen and Winkler (2007), Primo et al. (2009) for a com\\x02prehensive overview]. The general focus, however, has been on developing\\r\\none-time aggregation procedures that consult the experts’ advice only once\\r\\nbefore the event resolves.\\r\\nConsequently, many areas of probability aggregation still remain rather\\r\\nunexplored. For instance, consider investors aiming to assess whether a stock\\r\\nindex will finish trading above a threshold on a given date. To maximize their\\r\\noverall predictive accuracy, they may consult a group of experts repeatedly\\r\\nover a period of time and adjust their estimate of the aggregate probability\\r\\naccordingly. Given that the experts are allowed to update their probability\\r\\nassessments, the aggregation should be performed by taking into account\\r\\nthe temporal correlation in their advice.\\r\\nThis paper adds another layer of complexity by assuming a heterogeneous\\r\\nset of experts, most of whom only make one or two probability assessments\\r\\nover the hundred or so days before the event resolves. This means that\\r\\nthe decision-maker faces a different group of experts every day, with only a\\r\\nfew experts returning later on for a second round of advice. The problem\\r\\nat hand is therefore strikingly different from many time-series estimation\\nPROBABILITY AGGREGATION IN TIME-SERIES 3\\r\\nproblems, where one has an observation at every time point—or almost\\r\\nevery time point. As a result, standard time-series procedures like ARIMA\\r\\n[see, e.g., Mills (1991)] are not directly applicable. This paper introduces a\\r\\ntime-series model that incorporates self-reported expertise and captures a\\r\\nsharp and well-calibrated estimate of the crowd belief. The model is highly\\r\\ninterpretable and can be used for the following:\\r\\n• analyzing under and overconfidence in different groups of experts,\\r\\n• obtaining accurate probability forecasts, and\\r\\n• gaining question-specific quantities with easy interpretations, such as ex\\x02pert disagreement and problem difficulty.\\r\\nThis paper begins by describing our geopolitical database. It then in\\x02troduces a dynamic hierarchical model for capturing the crowd belief. The\\r\\nmodel is estimated in a two-step procedure: first, a sampling step produces\\r\\nconstrained parameter estimates via Gibbs sampling [see, e.g., Geman and\\r\\nGeman (1984)]; second, a calibration step transforms these estimates to\\r\\ntheir unconstrained equivalents via a one-dimensional optimization proce\\x02dure. The model introduction is followed by the first evaluation section that\\r\\nuses synthetic data to study how accurately the two-step procedure can es\\x02timate the crowd belief. The second evaluation section applies the model to\\r\\nour real-world geopolitical forecasting database. The paper concludes with\\r\\na discussion of future research directions and model limitations.\\r\\n2. Geopolitical forecasting data. Forecasters were recruited from profes\\x02sional societies, research centers, alumni associations, science bloggers and\\r\\nword of mouth (n = 2365). Requirements included at least a Bachelor’s de\\x02gree and completion of psychological and political tests that took roughly\\r\\ntwo hours. These measures assessed cognitive styles, cognitive abilities, per\\x02sonality traits, political attitudes and real-world knowledge. The experts\\r\\nwere asked to give probability forecasts (to the second decimal point) and\\r\\nto self-assess their level of expertise (on a 1-to-5 scale with 1 = Not At All\\r\\nExpert and 5 = Extremely Expert) on a number of 166 geopolitical binary\\r\\nevents taking place between September 29, 2011 and May 8, 2013. Each ques\\x02tion was active for a period during which the participating experts could up\\x02date their forecasts as frequently as they liked without penalty. The experts\\r\\nknew that their probability estimates would be assessed for accuracy using\\r\\nBrier scores.2 This incentivized them to report their true beliefs instead of\\r\\nattempting to game the system [Winkler and Murphy (1968)]. In addition\\r\\nto receiving $150 for meeting minimum participation requirements that did\\r\\n2The Brier score is the squared distance between the probability forecast and the\\r\\nevent indicator that equals 1.0 or 0.0 depending on whether the event happened or not,\\r\\nrespectively. See Brier (1950) for the original introduction.\\n4 V. A. SATOPA¨A ET AL. ¨\\r\\nTable 1\\r\\nFive-number summaries of our real-world data\\r\\nStatistic Min. Q1 Median Mean Q3 Max.\\r\\n# of days a question is active 4 35.6 72.0 106.3 145.20 418\\r\\n# of experts per question 212 543.2 693.5 783.7 983.2 1690\\r\\n# forecasts given by each expert on a question 1 1.0 1.0 1.8 2.0 131\\r\\n# questions participated by an expert 1 14.0 36.0 55.0 90.0 166\\r\\nnot depend on prediction accuracy, the experts received status rewards for\\r\\ntheir performance via leader-boards displaying Brier scores for the top 20\\r\\nexperts. Given that a typical expert participated only in a small subset of\\r\\nthe 166 questions, the experts are considered indistinguishable conditional\\r\\non the level of self-reported expertise.\\r\\nThe average number of forecasts made by a single expert in one day was\\r\\naround 0.017, and the average group-level response rate was around 13.5\\r\\nforecasts per day. Given that the group of experts is large and diverse, the\\r\\nresulting data set is very sparse. Tables 1 and 2 provide relevant summary\\r\\nstatistics on the data. Notice that the distribution of the self-reported exper\\x02tise is skewed to the right and that some questions remained active longer\\r\\nthan others. For more details on the data set and its collection see Ungar\\r\\net al. (2012).\\r\\nTo illustrate the data with some concrete examples, Figure 1(a) and 1(b)\\r\\nshow scatterplots of the probability forecasts given for (a) Will the expansion\\r\\nof the European bailout fund be ratified by all 17 Eurozone nations before 1\\r\\nNovember 2011? and (b) Will the Nikkei 225 index finish trading at or above\\r\\n9500 on 30 September 2011? The points have been shaded according to the\\r\\nlevel of self-reported expertise and jittered slightly to make overlaps visible.\\r\\nThe solid line gives the posterior mean of the calibrated crowd belief as es\\x02timated by our model. The surrounding dashed lines connect the point-wise\\r\\n95% posterior intervals. Given that the European bailout fund was ratified\\r\\nbefore November 1, 2011 and that the Nikkei 225 index finished trading at\\r\\naround 8700 on September 30, 2011, the general trend of the probability fore\\x02casts tends to converge toward the correct answers. The individual experts,\\r\\nhowever, sometimes disagree strongly, with the disagreement persisting even\\r\\nnear the closing dates of the questions.\\r\\nTable 2\\r\\nFrequencies of the self-reported expertise (1 = Not At All Expert and 5 = Extremely\\r\\nExpert) levels across all the 166 questions in our real-world data\\r\\nExpertise level 1 2 3 4 5\\r\\nFrequency (%) 25.3 30.7 33.6 8.2 2.1\\nPROBABILITY AGGREGATION IN TIME-SERIES 5\\r\\nFig. 1. Scatterplots of the probability forecasts given for two questions in our data set.\\r\\nThe solid line gives the posterior mean of the calibrated crowd belief as estimated by our\\r\\nmodel. The surrounding dashed lines connect the point-wise 95% posterior intervals.\\r\\n3. Model. Let pi,t,k ∈ (0, 1) be the probability forecast given by the ith\\r\\nexpert at time t for the kth question, where i = 1, . . . , Ik, t = 1, . . . , Tk, and\\r\\nk = 1, . . . ,K. Denote the logit probabilities with\\r\\nYi,t,k = logit(pi,t,k) = log\\x12\\r\\npi,t,k\\r\\n1 − pi,t,k \\x13\\r\\n∈ R\\r\\nand collect the logit probabilities for question k at time t into a vector\\r\\nYt,k = [Y1,t,kY2,t,k · · ·YIk,t,k]\\r\\nT\\r\\n. Partition the experts into J groups based on\\r\\nsome individual feature, such as self-reported expertise, with each group\\r\\nsharing a common multiplicative bias term bj ∈ R for j = 1, . . . , J. Collect\\r\\nthese bias terms into a bias vector b = [b1 b2 · · · bJ ]\\r\\nT\\r\\n. Let Mk be a Ik × J\\r\\nmatrix denoting the group memberships of the experts in question k; that is,\\r\\nif the ith expert participating in the kth question belongs to the jth group,\\r\\nthen the ith row of Mk is the jth standard basis vector ej . The bias vector\\r\\nb is assumed to be identical across all K questions. Under this notation, the\\r\\nmodel for the kth question can be expressed as\\r\\n(3.1) Yt,k = MkbXt,k + vt,k,\\n6 V. A. SATOPA¨A ET AL. ¨\\r\\n(3.2) Xt,k = γkXt−1,k + wt,k,\\r\\nX0,k ∼ N (µ0, σ2\\r\\n0\\r\\n),\\r\\nwhere (3.1) denotes the observed process, (3.2) shows the hidden process that\\r\\nis driven by the constant γk ∈ R, and (µ0, σ2\\r\\n0\\r\\n) ∈ (R,R\\r\\n+) are hyperparameters\\r\\nfixed a priori to 0 and 1, respectively. The error terms follow:\\r\\nvt,k|σ\\r\\n2\\r\\nk\\r\\ni.i.d. ∼ NIk\\r\\n(0, σ2\\r\\nk\\r\\nIIk),\\r\\nwt,k|τ\\r\\n2\\r\\nk\\r\\ni.i.d. ∼ N (0, τ 2\\r\\nk\\r\\n).\\r\\nTherefore, the parameters of the model are b, σ\\r\\n2\\r\\nk\\r\\n, γk and τ\\r\\n2\\r\\nk\\r\\nfor k = 1, . . . ,K.\\r\\nTheir prior distributions are chosen to be noninformative, p(b, σ2\\r\\nk\\r\\n|Xk) ∝ σ\\r\\n2\\r\\nk\\r\\nand p(γk, τ 2\\r\\nk\\r\\n|Xk) ∝ τ\\r\\n2\\r\\nk\\r\\n.\\r\\nThe hidden state Xt,k represents the aggregate logit probability for the\\r\\nkth event given all the information available up to and including time t. To\\r\\nmake this more specific, let Zk ∈ {0, 1} indicate whether the event associ\\x02ated with the kth question happened (Zk = 1) or did not happen (Zk = 0). If\\r\\n{Ft,k}\\r\\nTk\\r\\nt=1 is a filtration representing the information available up to and in\\x02cluding a given time point, then according to our model E[Zk|Ft,k] = P(Zk =\\r\\n1|Ft,k) = logit−1(Xt,k). Ideally this probability maximizes sharpness subject\\r\\nto calibration [for technical definitions of calibration and sharpness see Ran\\x02jan and Gneiting (2010), Gneiting and Ranjan (2013)]. Even though a single\\r\\nexpert is unlikely to have access to all the available information, a large and\\r\\ndiverse group of experts may share a considerable portion of the available\\r\\ninformation. The collective wisdom of the group therefore provides an at\\x02tractive proxy for Ft,k.\\r\\nGiven that the experts may believe in false information, hide their true\\r\\nbeliefs or be biased for many other reasons, their probability assessments\\r\\nshould be aggregated via a model that can detect potential bias, separate\\r\\nsignal from noise and use the collective opinion to estimate Xt,k. In our\\r\\nmodel the experts are assumed to be, on average, a multiplicative constant\\r\\nb away from Xt,k. Therefore, an individual element of b can be interpreted\\r\\nas a group-specific systematic bias that labels the group either as overcon\\x02fident [bj ∈ (1,∞)] or as underconfident [bj ∈ (0, 1)]. See Section 3 for a\\r\\nbrief discussion on different bias structures. Any other deviation from Xt,k\\r\\nis considered random noise. This noise is measured in terms of σ\\r\\n2\\r\\nk\\r\\nand can\\r\\nbe assumed to be caused by momentary over-optimism (or pessimism), false\\r\\nbeliefs or other misconceptions.\\r\\nThe random fluctuations in the hidden process are measured by τ\\r\\n2\\r\\nk\\r\\nand\\r\\nare assumed to represent changes or shocks to the underlying circumstances\\r\\nthat ultimately decide the outcome of the event. The systematic component\\r\\nγk allows the model to incorporate a constant signal stream that drifts the\\nPROBABILITY AGGREGATION IN TIME-SERIES 7\\r\\nhidden process. If the uncertainty in the question diminishes [γk ∈ (1,∞)],\\r\\nthe hidden process drifts to positive or negative infinity. Alternatively, the\\r\\nhidden process can drift to zero, in which case any available information does\\r\\nnot improve predictive accuracy [γk ∈ (0, 1)]. Given that all the questions\\r\\nin our data set were resolved within a prespecified timeframe, we expect\\r\\nγk ∈ (1,∞) for all k = 1, . . . ,K.\\r\\nAs for any future time T\\r\\n∗ ≥ t,\\r\\nXT ∗,k = γ\\r\\nT\\r\\n∗−t\\r\\nk Xt +\\r\\nT\\r\\nX∗\\r\\ni=t+1\\r\\nγ\\r\\nT\\r\\n∗−i\\r\\nk wi\\r\\n∼ N γ\\r\\nT\\r\\n∗−t\\r\\nk Xt,k, τ 2\\r\\nk\\r\\nT\\r\\nX∗\\r\\ni=t+1\\r\\nγ\\r\\nT\\r\\n∗−i\\r\\nk\\r\\n!\\r\\n,\\r\\nthe model can be used for time-forward prediction as well. The prediction\\r\\nfor the aggregate logit probability at time T\\r\\n∗\\r\\nis given by an estimate of\\r\\nγ\\r\\nT\\r\\n∗−tXt,k. Naturally the uncertainty in this prediction grows in T. To make\\r\\nsuch time-forward predictions, it is necessary to assume that the past pop\\x02ulation of experts is representative of the future population. This is a rea\\x02sonable assumption because even though the future population may consist\\r\\nof entirely different individuals, on average the population is likely to look\\r\\nvery similar to the past population. In practice, however, social scientists\\r\\nare generally more interested in an estimate of the current probability than\\r\\nthe probability under unknown conditions in the future. For this reason, our\\r\\nanalysis focuses on probability aggregation only up to the current time t.\\r\\nFor the sake of model identifiability, it is sufficient to share only one of\\r\\nthe elements of b among the K questions. In this paper, however, all the\\r\\nelements of b are assumed to be identical across the questions because some\\r\\nof the questions in our real-world data set involve very few experts with the\\r\\nhighest level of self-reported expertise. The model can be extended rather\\r\\neasily to estimate bias at a more general level. For instance, by assuming a\\r\\nhierarchical structure bik ∼ N (bj(i,k), σ2\\r\\nj(i,k)\\r\\n), where j(i, k) denotes the self\\x02reported expertise of the ith expert in question k, the bias can be estimated\\r\\nat an individual-level. These estimates can then be compared across ques\\x02tions. Individual-level analysis was not performed in our analysis for two\\r\\nreasons. First, most experts gave only a single prediction per problem, which\\r\\nmakes accurate bias estimation at the individual-level very difficult. Second,\\r\\nit is unclear how the individually estimated bias terms can be validated.\\r\\nIf the future event can take upon M > 2 possible outcomes, the hidden\\r\\nstate Xt,k is extended to a vector of size M − 1 and one of the outcomes,\\r\\nfor example, the Mth one, is chosen as the base case to ensure that the\\r\\nprobabilities will sum to one at any given time point. Each of the remaining\\n8 V. A. SATOPA¨A ET AL. ¨\\r\\nM − 1 possible outcomes is represented by an observed process similar to\\r\\n(3.1). Given that this multinomial extension is equivalent to having M − 1\\r\\nindependent binary-outcome models, the estimation and properties of the\\r\\nmodel are easily extended to the multi-outcome case. This paper focuses on\\r\\nbinary outcomes because it is the most commonly encountered setting in\\r\\npractice.\\r\\n4. Model estimation. This section introduces a two-step procedure, called\\r\\nSample-And-Calibrate (SAC), that captures a well-calibrated estimate of the\\r\\nhidden process without sacrificing the interpretability of our model.\\r\\n4.1. Sampling step. Given that (ab, Xt,k/a, a2τ\\r\\n2\\r\\nk\\r\\n) 6= (b, Xt,k, τ 2\\r\\nk\\r\\n) for any\\r\\na > 0 yield the same likelihood for Yt,k, the model as described by (3.1)\\r\\nand (3.2) is not identifiable. A well-known solution is to choose one of the\\r\\nelements of b, say, b3, as the reference point and fix b3 = 1. In Section 5 we\\r\\nprovide a guideline for choosing the reference point. Denote the constrained\\r\\nversion of the model by\\r\\nYt,k = Mkb(1)Xt,k(1) + vt,k,\\r\\nXt,k(1) = γk(1)Xt−1,k(1) + wt,k,\\r\\nvt,k|σ\\r\\n2\\r\\nk\\r\\n(1) i.i.d. ∼ NIk(0, σ2\\r\\nk\\r\\n(1)IIk),\\r\\nwt,k|τ\\r\\n2\\r\\nk\\r\\n(1) i.i.d. ∼ N (0, τ 2\\r\\nk\\r\\n(1)),\\r\\nwhere the trailing input notation, (a), signifies the value under the con\\x02straint b3 = a. Given that this version is identifiable, estimates of the model\\r\\nparameters can be obtained. Denote the estimates by placing a hat on the\\r\\nparameter symbol. For instance, bˆ(1) and Xˆ\\r\\nt,k(1) represent the estimates of\\r\\nb(1) and Xt,k(1), respectively.\\r\\nThese estimates are obtained by first computing a posterior sample via\\r\\nGibbs sampling and then taking the average of the posterior sample. The\\r\\nfirst step of our Gibbs sampler is to sample the hidden states via the Forward\\x02Filtering-Backward-Sampling (FFBS) algorithm. FFBS first predicts the\\r\\nhidden states using a Kalman filter and then performs a backward sam\\x02pling procedure that treats these predicted states as additional observations\\r\\n[see, e.g., Carter and Kohn (1994), Migon et al. (2005) for details on FFBS].\\r\\nGiven that the Kalman filter can handle varying numbers or even no fore\\x02casts at different time points, it plays a very crucial role in our probability\\r\\naggregation under sparse data.\\r\\nOur implementation of the sampling step is written in C++ and runs\\r\\nquite quickly. To obtain 1000 posterior samples for 50 questions each with\\r\\n100 time points and 50 experts takes about 215 seconds on a 1.7 GHz Intel\\nPROBABILITY AGGREGATION IN TIME-SERIES 9\\r\\nCore i5 computer. See the supplemental article for the technical details of\\r\\nthe sampling steps [Satop¨a¨a et al. (2014b)] and, for example, Gelman et al.\\r\\n(2003) for a discussion on the general principles of Gibbs sampling.\\r\\n4.2. Calibration step. Given that the model parameters can be estimated\\r\\nby fixing b3 to any constant, the next step is to search for the constant that\\r\\ngives an optimally sharp and calibrated estimate of the hidden process. This\\r\\nsection introduces an efficient procedure that finds the optimal constant\\r\\nwithout requiring any additional runs of the sampling step. First, assume\\r\\nthat parameter estimates bˆ(1) and Xˆ\\r\\nt,k(1) have already been obtained via\\r\\nthe sampling step described in Section 4.1. Given that for any β ∈ R/{0},\\r\\nYt,k = Mkb(1)Xt,k(1) + vt,k\\r\\n= Mk(b(1)β)(Xt,k(1)/β) + vt,k\\r\\n= Mkb(β)Xt,k(β) + vt,k,\\r\\nwe have that b(β) = b(1)β and Xt,k(β) = Xt,k(1)/β. Recall that the hidden\\r\\nprocess Xt,k is assumed to be sharp and well calibrated. Therefore, b3 can\\r\\nbe estimated with the value of β that simultaneously maximizes the sharp\\x02ness and calibration of Xˆ\\r\\nt,k(1)/β. A natural criterion for this maximization\\r\\nis given by the class of proper scoring rules that combine sharpness and\\r\\ncalibration [Gneiting et al. (2008), Buja, Stuetzle and Shen (2005)]. Due to\\r\\nthe possibility of complete separation in any one question [see, e.g., Gelman\\r\\net al. (2008)], the maximization must be performed over multiple questions.\\r\\nTherefore,\\r\\nβˆ = arg max\\r\\nβ∈R/{0}\\r\\nX\\r\\nK\\r\\nk=1\\r\\nX\\r\\nTk\\r\\nt=1\\r\\nS(Zk, Xˆ (4.1) k,t(1)/β),\\r\\nwhere Zk ∈ {0, 1} is the event indicator for question k. The function S is a\\r\\nstrictly proper scoring rule such as the negative Brier score [Brier (1950)]\\r\\nSBRI(Z, X) = −(Z − logit−1(X))2\\r\\nor the logarithmic score [Good (1952)]\\r\\nSLOG(Z, X) = Z log(logit−1(X)) + (1 − Z) log(1 − logit−1(X)).\\r\\nThe estimates of the unconstrained model parameters are then given by\\r\\nXˆ\\r\\nt,k = Xˆk,t(1)/β, ˆ\\r\\nbˆ = bˆ(1)β, ˆ\\r\\nτˆ\\r\\n2\\r\\nk = ˆτ\\r\\n2\\r\\nk\\r\\n(1)/βˆ2,\\r\\nσˆ\\r\\n2\\r\\nk = ˆσ\\r\\n2\\r\\nk\\r\\n(1),\\r\\nγˆk = ˆγk(1).\\r\\nNotice that estimates of σ\\r\\n2\\r\\nk\\r\\nand γk are not affected by the constraint.\\n10 V. A. SATOPA¨A ET AL. ¨\\r\\n5. Synthetic data results. This section uses synthetic data to evaluate\\r\\nhow accurately the SAC-procedure captures the hidden states and bias vec\\x02tor. The hidden process is generated from standard Brownian motion. More\\r\\nspecifically, if Zt,k denotes the value of a path at time t, then\\r\\nZk = 1(ZTk,k > 0),\\r\\nXt,k = logit\\x14Φ\\r\\n\\x12\\r\\nZt,k\\r\\n√\\r\\nTk − t\\r\\n\\x13\\x15\\r\\ngives a sequence of Tk calibrated logit probabilities for the event Zk = 1.\\r\\nA hidden process is generated for K questions with a time horizon of Tk =\\r\\n101. The questions involve 50 experts allocated evenly among five expertise\\r\\ngroups. Each expert gives one probability forecast per day with the exception\\r\\nof time t = 101 when the event resolves. The forecasts are generated by\\r\\napplying bias and noise to the hidden process as described by (3.1). Our\\r\\nsimulation study considers a three-dimensional grid of parameter values:\\r\\nσ\\r\\n2 ∈ {1/2, 1, 3/2, 2, 5/2},\\r\\nβ ∈ {1/2, 3/4, 1, 4/3, 2/1},\\r\\nK ∈ {20, 40, 60, 80, 100},\\r\\nwhere β varies the bias vector by b = [1/2, 3/4, 1, 4/3, 2/1]T β. Forty syn\\x02thetic data sets are generated for each combination of σ\\r\\n2\\r\\n, β and K values.\\r\\nThe SAC-procedure runs for 200 iterations of which the first 100 are used\\r\\nfor burn-in.\\r\\nSAC under the Brier (SACBRI) and logarithm score (SACLOG) are com\\x02pared with the Exponentially Weighted Moving Average (EWMA). EWMA,\\r\\nwhich serves as a baseline, can be understood by first denoting the (expertise\\x02weighted) average forecast at time t for the kth question with\\r\\np¯t,k =\\r\\nX\\r\\nJ\\r\\nj=1\\r\\nωj\\r\\n\\x12\\r\\n1\\r\\n|Ej |\\r\\nX\\r\\ni∈Ej\\r\\npi,t,k\\x13(5.1) ,\\r\\nwhere Ej refers to an index set of all experts in the jth expertise group and\\r\\nωj denotes the weight associated with the jth expertise group. The EWMA\\r\\nforecasts for the kth problem are then constructed recursively from\\r\\npˆt,k(α) = \\x1a\\r\\np¯1,k, for t = 1,\\r\\nαp¯t,k + (1 − α)ˆpt−1,k(α), for t > 1,\\r\\nwhere α and ω are learned from the training set by\\r\\n(ˆα,ωˆ ) = arg min\\r\\nα,ωj∈[0,1]\\r\\nX\\r\\nK\\r\\nk=1\\r\\nX\\r\\nTk\\r\\nt=1\\r\\n(Zk − pˆt,k(α,ω))2s.t. X\\r\\nJ\\r\\nj=1\\r\\nωj = 1.\\nPROBABILITY AGGREGATION IN TIME-SERIES 11\\r\\nTable 3\\r\\nSummary measures of the estimation accuracy under\\r\\nsynthetic data. As EWMA does not produce an estimate of the\\r\\nbias vector, its accuracy on the bias term cannot be reported\\r\\nModel Quadratic loss Absolute loss\\r\\nHidden process\\r\\nSACBRI 0.00226 0.0334\\r\\nSACLOG 0.00200 0.0313\\r\\nEWMA 0.00225 0.0339\\r\\nBias vector\\r\\nSACBRI 0.147 0.217\\r\\nSACLOG 0.077 0.171\\r\\nIf pt,k = logit−1(Xt,k) and ˆpt,k is the corresponding probability estimated\\r\\nby the model, the model’s accuracy to estimate the hidden process is mea\\x02sured with the quadratic loss, (pt,k −pˆt,k)\\r\\n2\\r\\n, and the absolute loss, |pt,k −pˆt,k|.\\r\\nTable 3 reports these losses averaged over all conditions, simulations and\\r\\ntime points. The three competing methods, SACBRI, SACLOG and EWMA,\\r\\nestimate the hidden process with great accuracy. Based on other perfor\\x02mance measures that are not shown for the sake of brevity, all three methods\\r\\nsuffer from an increasing level of noise in the expert logit probabilities but\\r\\ncan make efficient use of extra data.\\r\\nSome interesting differences emerge from Figure 2 which shows the marginal\\r\\neffect of β on the average quadratic loss. As can be expected, EWMA per\\x02Fig. 2. The marginal effect of β on the average quadratic loss.\\n12 V. A. SATOPA¨A ET AL. ¨\\r\\nforms well when the experts are, on average, close to unbiased. Interest\\x02ingly, SAC estimates the hidden process more accurately when the experts\\r\\nare overconfident (large β) compared to underconfident (small β). To un\\x02derstand this result, assume that the experts in the third group are highly\\r\\nunderconfident. Their logit probabilities are then expected to be closer to\\r\\nzero than the corresponding hidden states. After adding white noise to these\\r\\nexpected logit probabilities, they are likely to cross to the other side of zero.\\r\\nIf the sampling step fixes b3 = 1, as it does in our case, the third group is\\r\\ntreated as unbiased and some of the constrained estimates of the hidden\\r\\nstates are likely to be on the other side of zero as well. Unfortunately, this\\r\\ndiscrepancy cannot be corrected by the calibration step that is restricted\\r\\nto shifting the constrained estimates either closer or further away from zero\\r\\nbut not across it. To maximize the likelihood of having all the constrained\\r\\nestimates on the right side of zero and hence avoiding the discrepancy, the\\r\\nreference point in the sampling step should be chosen with care. A helpful\\r\\nguideline is to fix the element of b that is a priori believed to be the largest.\\r\\nThe accuracy of the estimated bias vector is measured with the quadratic\\r\\nloss, (bj − ˆbj )\\r\\n2\\r\\n, and the absolute loss, |bj − ˆbj |. Table 3 reports these losses\\r\\naveraged over all conditions, simulations and elements of the bias vector.\\r\\nUnfortunately, EWMA does not produce an estimate of the bias vector.\\r\\nTherefore, it cannot be used as a baseline for the estimation accuracy in\\r\\nthis case. Given that the losses for SACBRI and SACLOG are quite small,\\r\\nthey estimate the bias vector accurately.\\r\\n6. Geopolitical data results. This section presents results for the real\\x02world data described in Section 2. The goal is to provide application specific\\r\\ninsight by discussing the specific research objectives itemized in Section 1.\\r\\nFirst, however, we discuss two practical matters that must be taken into\\r\\naccount when aggregating real-world probability forecasts.\\r\\n6.1. Incoherent and imbalanced data. The first matter regards human\\r\\nexperts making probability forecasts of 0.0 or 1.0 even if they are not com\\x02pletely sure of the outcome of the event. For instance, all 166 questions in\\r\\nour data set contain both a zero and a one. Transforming such forecasts into\\r\\nthe logit space yields infinities that can cause problems in model estimation.\\r\\nTo avoid this, Ariely et al. (2000) suggest changing p = 0.00 and 1.00 to\\r\\np = 0.02 and 0.98, respectively. This is similar to winsorising that sets the\\r\\nextreme probabilities to a specified percentile of the data [see, e.g., Hastings\\r\\net al. (1947) for more details on winsorising]. Allard, Comunian and Re\\x02nard (2012), on the other hand, consider only probabilities that fall within\\r\\na constrained interval, say, [0.001, 0.999], and discard the rest. Given that\\r\\nthis implies ignoring a portion of the data, we adopt a censoring approach\\r\\nsimilar to Ariely et al. (2000) by changing p = 0.00 and 1.00 to p = 0.01\\r\\nand 0.99, respectively. Our results remain insensitive to the exact choice of\\nPROBABILITY AGGREGATION IN TIME-SERIES 13\\r\\ncensoring as long as this is done in a reasonable manner to keep the extreme\\r\\nprobabilities from becoming highly influential in the logit space.\\r\\nThe second matter is related to the distribution of the class labels in the\\r\\ndata. If the set of occurrences is much larger than the set of nonoccurrences\\r\\n(or vice versa), the data set is called imbalanced. On such data the modeling\\r\\nprocedure can end up over-focusing on the larger class and, as a result,\\r\\ngive very accurate forecast performance over the larger class at the cost\\r\\nof performing poorly over the smaller class [see, e.g., Chen (2008), Wallace\\r\\nand Dahabreh (2012)]. Fortunately, it is often possible to use a well-balanced\\r\\nversion of the data. The first step is to find a partition S0 and S1 of the\\r\\nquestion indices {1, 2, . . . ,K} such that the equality P\\r\\nk∈S0\\r\\nTk =\\r\\nP\\r\\nk∈S1\\r\\nTk\\r\\nis as closely approximated as possible. This is equivalent to an NP-hard\\r\\nproblem known in computer science as the Partition Problem: determine\\r\\nwhether a given set of positive integers can be partitioned into two sets such\\r\\nthat the sums of the two sets are equal to each other [see, e.g., Karmarkar and\\r\\nKarp (1982), Hayes (2002)]. A simple solution is to use a greedy algorithm\\r\\nthat iterates through the values of Tk in descending order, assigning each Tk\\r\\nto the subset that currently has the smaller sum [see, e.g., Kellerer, Pferschy\\r\\nand Pisinger (2004), Gent and Walsh (1996) for more details on the Partition\\r\\nProblem]. After finding a well-balanced partition, the next step is to assign\\r\\nthe class labels such that the labels for the questions in Sx are equal to x for\\r\\nx = 0 or 1. Recall from Section 4.2 that Zk represents the event indicator\\r\\nfor the kth question. To define a balanced set of indicators Z˜\\r\\nk for all k ∈ Sx,\\r\\nlet\\r\\nZ˜\\r\\nk = x,\\r\\np˜i,t,k =\\r\\n\\x1a\\r\\n1 − pi,t,k, if Zk = 1 − x,\\r\\npi,t,k, if Zk = x,\\r\\nwhere i = 1, . . . , Ik, and t = 1, . . . , Tk. The resulting set\\r\\n{(Z˜\\r\\nk, {p˜i,t,k|i = 1, . . . , Ik, t = 1, . . . , Tk})}\\r\\nK\\r\\nk=1\\r\\nis a balanced version of the data. This procedure was used to balance our\\r\\nreal-world data set both in terms of events and time points. The final output\\r\\nsplits the events exactly in half (|S0| = |S1| = 83) such that the number of\\r\\ntime points in the first and second halves are 8737 and 8738, respectively.\\r\\n6.2. Out-of-sample aggregation. The goal of this section is to evaluate\\r\\nthe accuracy of the aggregate probabilities made by SAC and several other\\r\\nprocedures. The models are allowed to utilize a training set before making\\r\\naggregations on an independent testing set. To clarify some of the upcom\\x02ing notation, let Strain and Stest be index sets that partition the data into\\r\\ntraining and testing sets of sizes |Strain| = Ntrain and |Stest| = 166 − Ntrain,\\r\\nrespectively. This means that the kth question is in the training set if and\\n14 V. A. SATOPA¨A ET AL. ¨\\r\\nonly if k ∈ Strain. Before introducing the competing models, note that all\\r\\nchoices of thinning and burn-in made in this section are conservative and\\r\\nhave been made based on pilot runs of the models. This was done to ensure\\r\\na posterior sample that has low autocorrelation and arises from a converged\\r\\nchain. The competing models are as follows:\\r\\n1. Simple Dynamic Linear Model (SDLM). This is equivalent to the dynamic\\r\\nmodel from Section 3 but with b = 1 and β = 1. Thus,\\r\\nYt,k = Xt,k + vt,k,\\r\\nXt,k = γkXt−1,k + wt,k,\\r\\nwhere Xt,k is the aggregate logit probability. Given that this model does\\r\\nnot share any parameters across questions, estimates of the hidden pro\\x02cess can be obtained directly for the questions in the testing set without\\r\\nfitting the model first on the training set. The Gibbs sampler is run for\\r\\n500 iterations of which the first 200 are used for burn-in. The remaining\\r\\n300 iterations are thinned by discarding every other observation, leaving\\r\\na final posterior sample of 150 observations. The average of this sample\\r\\ngives the final estimates.\\r\\n2. The Sample-And-Calibrate procedure both under the Brier (SACBRI) and\\r\\nthe Logarithmic score (SACLOG). The model is first fit on the training\\r\\nset by running the sampling step for 3000 iterations of which the first\\r\\n500 iterations are used for burn-in. The remaining 2500 observations are\\r\\nthinned by keeping every fifth observation. The calibration step is per\\x02formed for the final 500 observations. The out-of-sample aggregation is\\r\\ndone by running the sampling step for 500 iterations with each consec\\x02utive iteration reading in and conditioning on the next value of β and\\r\\nb found during the training period. The first 200 iterations are used for\\r\\nburn-in. The remaining 300 iterations are thinned by discarding every\\r\\nother observation, leaving a final posterior sample of 150 observations.\\r\\nThe average of this sample gives the final estimates.\\r\\n3. A fully Bayesian version of SACLOG (BSACLOG). Denote the calibrated\\r\\nlogit probabilities and event indicators across all K questions with X(1)\\r\\nand Z, respectively. The posterior distribution of β conditional on X(1)\\r\\nis given by p(β|X(1), Z) ∝ p(Z|β,X(1))p(β|X(1)). The likelihood is\\r\\np(Z|β,X(1))\\r\\n(6.1)\\r\\n∝\\r\\nY\\r\\nK\\r\\nk=1\\r\\nY\\r\\nTk\\r\\nt=1\\r\\nlogit−1(Xt,k(1)/β)\\r\\nZk\\r\\n(1 − logit−1(Xt,k(1)/β))1−Zk.\\r\\nAs in Gelman et al. (2003), the prior for β is chosen to be locally uniform,\\r\\np(1/β) ∝ 1. Given that this model estimates Xt,k(1) and β simultaneously,\\r\\nit is a little more flexible than SAC. Posterior estimates of β can be sam-\\nPROBABILITY AGGREGATION IN TIME-SERIES 15\\r\\npled from (6.1) using generic sampling algorithms such as the Metropolis\\r\\nalgorithm [Metropolis et al. (1953)] or slice sampling [Neal (2003)]. Given\\r\\nthat the sampling procedure conditions on the event indicators, the full\\r\\nconditional distribution of the hidden states is not in a standard form.\\r\\nTherefore, the Metropolis algorithm is also used for sampling the hidden\\r\\nstates. Estimation is made with the same choices of thinning and burn-in\\r\\nas described under Sample-And-Calibrate.\\r\\n4. Due to the lack of previous literature on dynamic aggregation of expert\\r\\nprobability forecasts, the main competitors are exponentially weighted\\r\\nversions of procedures that have been proposed for static probability\\r\\naggregation:\\r\\n(a) Exponentially Weighted Moving Average (EWMA) as described in\\r\\nSection 5.\\r\\n(b) Exponentially Weighted Moving Logit Aggregator (EWMLA). This\\r\\nis a moving version of the aggregator ˆpG(b) that was introduced\\r\\nin Satop¨a¨a et al. (2014a). The EWMLA aggregate probabilities are\\r\\nfound recursively from\\r\\npˆt,k(α,b) = \\x1a\\r\\nG1,k(b), for t = 1,\\r\\nαGt,k(b) + (1 − α)ˆpt−1,k(α,b), for t > 1,\\r\\nwhere the vector b ∈ R\\r\\nJ\\r\\ncollects the bias terms of the expertise\\r\\ngroups, and\\r\\nGt,k(ν) = N\\r\\nYt,k\\r\\ni=1\\r\\n\\x12\\r\\npi,t,k\\r\\n1 − pi,t,k \\x13bj(i,k)/Nt,k!\\x1e \\r\\n1+\\r\\nN\\r\\nYt,k\\r\\ni=1\\r\\n\\x12\\r\\npi,t,k\\r\\n1 − pi,t,k \\x13bj(i,k)/Nt,k!\\r\\n.\\r\\nThe parameters α and b are learned from the training set by\\r\\n(ˆα,bˆ) = arg min\\r\\nb∈R5,α∈[0,1]\\r\\nX\\r\\nk∈Strain\\r\\nX\\r\\nTk\\r\\nt=1\\r\\n(Zk − pˆt,k(α,b))2.\\r\\n(c) Exponentially Weighted Moving Beta-transformed Aggregator\\r\\n(EWMBA). The static version of the Beta-transformed aggregator\\r\\nwas introduced in Ranjan and Gneiting (2010). A dynamic version\\r\\ncan be obtained by replacing Gt,k(ν) in the EWMLA description with\\r\\nHν,τ (¯pt,k), where Hν,τ is the cumulative distribution function of the\\r\\nBeta distribution and ¯pt,k is given by (5.1). The parameters α, ν, τ\\r\\nand ω are learned from the training set by\\r\\n(ˆα, ν, ˆ τ, ˆ ωˆ ) = arg min\\r\\nν,τ>0 α,ωj∈[0,1]\\r\\nX\\r\\nk∈Strain\\r\\nX\\r\\nTk\\r\\nt=1\\r\\n(Zk − pˆt,k(α, ν, τ,ω))2\\r\\n(6.2)\\r\\ns.t. X\\r\\nJ\\r\\nj=1\\r\\nωj = 1.\\n16 V. A. SATOPA¨A ET AL. ¨\\r\\nTable 4\\r\\nBrier scores based on 10-fold cross-validation. Scores by Day weighs a question by the\\r\\nnumber of days the question remained open. Scores by Problem gives each question an\\r\\nequal weight regardless of how long the question remained open. The bolded values\\r\\nindicate the best scores in each column. The values in the parenthesis represent standard\\r\\nerrors in the scores\\r\\nModel All Short Medium Long\\r\\nScores by day\\r\\nSDLM 0.100 (0.156) 0.066 (0.116) 0.098 (0.154) 0.102 (0.157)\\r\\nBSACLOG 0.097 (0.213) 0.053 (0.147) 0.100 (0.215) 0.098 (0.215)\\r\\nSACBRI 0.096 (0.190) 0.056 (0.134) 0.097 (0.190) 0.098 (0.192)\\r\\nSACLOG 0.096 (0.191) 0.056 (0.134) 0.096 (0.189) 0.098 (0.193)\\r\\nEWMBA 0.104 (0.204) 0.057 (0.120) 0.113 (0.205) 0.105 (0.206)\\r\\nEWMLA 0.102 (0.199) 0.061 (0.130) 0.111 (0.214) 0.103 (0.200)\\r\\nEWMA 0.111 (0.146) 0.080 (0.101) 0.116 (0.152) 0.112 (0.146)\\r\\nScores by problem\\r\\nSDLM 0.089 (0.116) 0.064 (0.085) 0.106 (0.141) 0.092 (0.117)\\r\\nBSACLOG 0.083 (0.160) 0.052 (0.103) 0.110 (0.198) 0.085 (0.162)\\r\\nSACBRI 0.083 (0.142) 0.055 (0.096) 0.106 (0.174) 0.085 (0.144)\\r\\nSACLOG 0.082 (0.142) 0.055 (0.096) 0.105 (0.174) 0.085 (0.144)\\r\\nEWMBA 0.091 (0.157) 0.057 (0.095) 0.121 (0.187) 0.093 (0.164)\\r\\nEWMLA 0.090 (0.159) 0.064 (0.109) 0.120 (0.200) 0.090 (0.159)\\r\\nEWMA 0.102 (0.108) 0.080 (0.075) 0.123 (0.130) 0.103 (0.110)\\r\\nThe competing models are evaluated via a 10-fold cross-validation3that\\r\\nfirst partitions the 166 questions into 10 sets such that each set has approx\\x02imately the same number of questions (16 or 17 questions in our case) and\\r\\nthe same number of time points (between 1760 and 1764 time points in our\\r\\ncase). The evaluation then iterates 10 times, each time using one of the 10\\r\\nsets as the testing set and the remaining 9 sets as the training set. Therefore,\\r\\neach question is used nine times for training and exactly once for testing.\\r\\nThe testing proceeds sequentially one testing question at a time as follows:\\r\\nFirst, for a question with a time horizon of Tk, give an aggregate probability\\r\\nat time t = 2 based on the first two days. Compute the Brier score for this\\r\\nprobability. Next give an aggregate probability at time t = 3 based on the\\r\\nfirst three days and compute the Brier score for this probability. Repeat this\\r\\nprocess for all of the Tk −1 days. This leads to Tk −1 Brier scores per testing\\r\\nquestion and a total of 17,475 Brier scores across the entire data set.\\r\\nTable 4 summarizes these scores in different ways. The first option, de\\x02noted by Scores by Day, weighs each question by the number of days the\\r\\nquestion remained open. This is performed by computing the average of the\\r\\n3A 5-fold cross-validation was also performed. The results were, however, very similar\\r\\nto the 10-fold cross-validation and hence not presented in the paper.\\nPROBABILITY AGGREGATION IN TIME-SERIES 17\\r\\n17,475 scores. The second option, denoted by Scores by Problem, gives each\\r\\nquestion an equal weight regardless of how long the question remained open.\\r\\nThis is done by first averaging the scores within a question and then aver\\x02aging the average scores across all the questions. Both scores can be further\\r\\nbroken down into subcategories by considering the length of the questions.\\r\\nThe final three columns of Table 4 divide the questions into Short questions\\r\\n(30 days or fewer), Medium questions (between 31 and 59 days) and Long\\r\\nProblems (60 days or more). The number of questions in these subcategories\\r\\nwere 36, 32 and 98, respectively. The bolded scores indicate the best score\\r\\nin each column. The values in the parenthesis quantify the variability in the\\r\\nscores: Under Scores by Day the values give the standard errors of all the\\r\\nscores. Under Scores by Problem, on the other hand, the values represent\\r\\nthe standard errors of the average scores of the different questions.\\r\\nAs can be seen in Table 4, SACLOG achieves the lowest score across all\\r\\ncolumns except Short where it is outperformed by BSACLOG. It turns out\\r\\nthat BSACLOG is overconfident (see Section 6.3). This means that BSACLOG\\r\\nunderestimates the uncertainty in the events and outputs aggregate proba\\x02bilities that are typically too near 0.0 or 1.0. This results into highly variable\\r\\nperformance. The short questions generally involved very little uncertainty.\\r\\nOn such easy questions, overconfidence can pay off frequently enough to\\r\\ncompensate for a few large losses arising from the overconfident and drasti\\x02cally incorrect forecasts.\\r\\nSDLM, on the other hand, lacks sharpness and is highly underconfident\\r\\n(see Section 6.3). This behavior is expected, as the experts are underconfi\\x02dent at the group level (see Section 6.4) and SDLM does not use the train\\x02ing set to explicitly calibrate its aggregate probabilities. Instead, it merely\\r\\nsmooths the forecasts given by the experts. The resulting aggregate prob\\x02abilities are therefore necessarily conservative, resulting into high average\\r\\nscores with low variability.\\r\\nSimilar behavior is exhibited by EWMA that performs the worst of all\\r\\nthe competing models. The other two exponentially weighted aggregators,\\r\\nEWMLA and EWMBA, make efficient use of the training set and present\\r\\nmoderate forecasting performance in most columns of Table 4. Neither ap\\x02proach, however, appears to dominate the other. The high variability and\\r\\naverage of their performance scores indicate that their performance suffers\\r\\nfrom overconfidence.\\r\\n6.3. In- and out-of-sample sharpness and calibration. A calibration plot\\r\\nis a simple tool for visually assessing the sharpness and calibration of a\\r\\nmodel. The idea is to plot the aggregate probabilities against the observed\\r\\nempirical frequencies. Therefore, any deviation from the diagonal line sug\\x02gests poor calibration. A model is considered underconfident (or overconfi\\x02dent) if the points follow an S-shaped (or S-shaped) trend. To assess sharp\\x02ness of the model, it is common practice to place a histogram of the given\\n18 V. A. SATOPA¨A ET AL. ¨\\r\\nFig. 3. The top and bottom rows show in- and out-of-sample calibration and sharpness,\\r\\nrespectively.\\r\\nforecasts in the corner of the plot. Given that the data were balanced, any\\r\\ndeviation from the the baseline probability of 0.5 suggests improved sharp\\x02ness.\\r\\nThe top and bottom rows of Figure 3 present calibration plots for SDLM,\\r\\nSACLOG, SACBRI and BSACLOG under in- and out-of-sample probability\\r\\naggregation, respectively. Each setting is of interest in its own right: Good\\r\\nin-sample calibration is crucial for model interpretability. In particular, if the\\r\\nestimated crowd belief is well calibrated, then the elements of the bias vector\\r\\nb can be used to study the amount of under or overconfidence in the different\\r\\nexpertise groups. Good out-of-sample calibration and sharpness, on the other\\r\\nhand, are necessary properties in decision making. To guide our assessment,\\r\\nthe dashed bands around the diagonal connect the point-wise, Bonferroni\\x02corrected [Bonferroni (1936)] 95% lower and upper critical values under\\r\\nthe null hypothesis of calibration. These have been computed by running\\r\\nthe bootstrap technique described in Br¨ocker and Smith (2007) for 10,000\\r\\niterations. The in-sample predictions were obtained by running the models\\r\\nfor 10,200 iterations, leading to a final posterior sample of 1000 observations\\r\\nafter thinning and using the first 200 iterations for burn-in. The out-of\\x02sample predictions were given by the 10-fold cross-validation discussed in\\r\\nSection 6.2.\\nPROBABILITY AGGREGATION IN TIME-SERIES 19\\r\\nFig. 4. Posterior distributions of bj for j = 1, . . . , 5.\\r\\nOverall, SAC is sharp and well calibrated both in- and out-of-sample with\\r\\nonly a few points barely falling outside the point-wise critical values. Given\\r\\nthat the calibration does not change drastically from the top to the bottom\\r\\nrow, SAC can be considered robust against overfitting. This, however, is not\\r\\nthe case with BSACLOG that is well calibrated in-sample but presents over\\x02confidence out-of-sample. Figure 3(a) and (e) serve as baselines by showing\\r\\nthe calibration plots for SDLM. Given that this model does not perform any\\r\\nexplicit calibration, it is not surprising to see most points outside the critical\\r\\nvalues. The pattern in the deviations suggests strong underconfidence. Fur\\x02thermore, the inset histogram reveals drastic lack of sharpness. Therefore,\\r\\nSAC can be viewed as a well-performing compromise between SDLM and\\r\\nBSACLOG that avoids overconfidence without being too conservative.\\r\\n6.4. Group-level expertise bias. This section explores the bias among the\\r\\nfive expertise groups in our data set. Figure 4 compares the posterior dis\\x02tributions of the individual elements of b with side-by-side boxplots. Given\\r\\nthat the distributions fall completely below the no-bias reference line at 1.0,\\r\\nall the expertise groups are deemed underconfident. Even though the exact\\r\\nlevel of underconfidence is affected slightly by the extent to which the ex\\x02treme probabilities are censored (see Section 6.1), the qualitative results in\\r\\nthis section remain insensitive to different levels of censoring.\\r\\nFigure 4 shows that underconfidence decreases as expertise increases. The\\r\\nposterior probability that the most expert group is the least underconfident\\n20 V. A. SATOPA¨A ET AL. ¨\\r\\nis approximately equal to 1.0, and the posterior probability of a strictly\\r\\ndecreasing level of underconfidence is approximately 0.87. The latter prob\\x02ability is driven down by the inseparability of the two groups with the low\\x02est levels of self-reported expertise. This inseparability suggests that the\\r\\nexperts are poor at assessing how little they know about a topic that is\\r\\nstrange to them. If these groups are combined into a single group, the pos\\x02terior probability of a strictly decreasing level of underconfidence is approx\\x02imately 1.0.\\r\\nThe decreasing trend in underconfidence can be viewed as a process of\\r\\nBayesian updating. A completely ignorant expert aiming to minimize a rea\\x02sonable loss function, such as the Brier score, has no reason to give anything\\r\\nbut 0.5 as his probability forecast. However, as soon as the expert gains some\\r\\nknowledge about the event, he produces an updated forecast that is a com\\x02promise between his initial forecast and the new information acquired. The\\r\\nupdated forecast is therefore conservative and too close to 0.5 as long as the\\r\\nexpert remains only partially informed about the event. If most experts fall\\r\\nsomewhere on this spectrum between ignorance and full information, their\\r\\naverage forecast tends to fall strictly between 0.5 and the most informed\\r\\nprobability forecast [see Baron et al. (2014) for more details]. Given that\\r\\nexpertise is to a large extent determined by subject matter knowledge, the\\r\\nlevel of underconfidence can be expected to decrease as a function of the\\r\\ngroup’s level of self-reported expertise.\\r\\nFinding underconfidence in all the groups may seem like a surprising re\\x02sult given that many previous studies have shown that experts are often\\r\\noverconfident [see, e.g., Lichtenstein, Fischhoff and Phillips (1977), Morgan\\r\\n(1992), Bier (2004) for a summary of numerous calibration studies]. It is,\\r\\nhowever, worth emphasizing three points: First, our result is a statement\\r\\nabout groups of experts and hence does not invalidate the possibility of\\r\\nthe individual experts being overconfident. To make conclusions at the in\\x02dividual level based on the group level bias terms would be considered an\\r\\necological inference fallacy [see, e.g., Lubinski and Humphreys (1996)]. Sec\\x02ond, the experts involved in our data set are overall very well calibrated\\r\\n[Mellers et al. (2014)]. A group of well-calibrated experts, however, can pro\\x02duce an aggregate forecast that is underconfident. In fact, if the aggregate is\\r\\nlinear, the group is necessarily underconfident [see Theorem 1 of Ranjan and\\r\\nGneiting (2010)]. Third, according to Erev, Wallsten and Budescu (1994),\\r\\nthe level of confidence depends on the way the data were analyzed. They\\r\\nexplain that experts’ probability forecasts suggest underconfidence when the\\r\\nforecasts are averaged or presented as a function of independently defined\\r\\nobjective probabilities, that is, the probabilities given by logit−1(Xt,k) in our\\r\\ncase. This is similar to our context and opposite to many empirical studies\\r\\non confidence calibration.\\nPROBABILITY AGGREGATION IN TIME-SERIES 21\\r\\n6.5. Question difficulty and other measures. One advantage of our model\\r\\narises from its ability to produce estimates of interpretable question-specific\\r\\nparameters γk, σ\\r\\n2\\r\\nk\\r\\nand τ\\r\\n2\\r\\nk\\r\\n. These quantities can be combined in many in\\x02teresting ways to answer questions about different groups of experts or the\\r\\nquestions themselves. For instance, being able to assess the difficulty of a\\r\\nquestion could lead to more principled ways of aggregating performance\\r\\nmeasures across questions or to novel insight on the kinds of questions that\\r\\nare found difficult by experts [see, e.g., a discussion on the Hard-Easy Ef\\x02fect in Wilson (1994)]. To illustrate, recall that higher values of σ\\r\\n2\\r\\nk\\r\\nsuggest\\r\\ngreater disagreement among the participating experts. Given that experts\\r\\nare more likely to disagree over a difficult question than an easy one, it is\\r\\nreasonable to assume that σ\\r\\n2\\r\\nk\\r\\nhas a positive relationship with question dif\\x02ficulty. An alternative measure is given by τ\\r\\n2\\r\\nk\\r\\nthat quantifies the volatility\\r\\nof the underlying circumstances that ultimately decide the outcome of the\\r\\nevent. Therefore, a high value of τ\\r\\n2\\r\\nk\\r\\ncan cause the outcome of the event to\\r\\nappear unstable and difficult to predict.\\r\\nAs a final illustration of our model, we return to the two example questions\\r\\nintroduced in Figure 1. Given that ˆσ\\r\\n2\\r\\nk = 2.43 and ˆσ\\r\\n2\\r\\nk = 1.77 for the questions\\r\\ndepicted in Figure 1(a) and 1(b), respectively, the first question provokes\\r\\nmore disagreement among the experts than the second one. Intuitively this\\r\\nmakes sense because the target event in Figure 1(a) is determined by several\\r\\nconditions that may change radically from one day to the next while the\\r\\ntarget event in Figure 1(b) is determined by a relatively steady stock market\\r\\nindex. Therefore, it is not surprising to find that in Figure 1(a) ˆτ\\r\\n2\\r\\nk = 0.269,\\r\\nwhich is much higher than ˆτ\\r\\n2\\r\\nk = 0.039 in Figure 1(b). We may conclude that\\r\\nthe first question is inherently more difficult than the second one.\\r\\n7. Discussion. This paper began by introducing a rather unorthodox\\r\\nbut nonetheless realistic time-series setting where probability forecasts are\\r\\nmade very infrequently by a heterogeneous group of experts. The resulting\\r\\ndata is too sparse to be modeled well with standard time-series methods.\\r\\nIn response to this lack of appropriate modeling procedures, we propose\\r\\nan interpretable time-series model that incorporates self-reported expertise\\r\\nto capture a sharp and well-calibrated estimate of the crowd belief. This\\r\\nprocedure extends the forecasting literature into an under-explored area of\\r\\nprobability aggregation.\\r\\nOur model preserves parsimony while addressing the main challenges in\\r\\nmodeling sparse probability forecasting data. Therefore, it can be viewed\\r\\nas a basis for many future extensions. To give some ideas, recall that most\\r\\nof the model parameters were assumed constant over time. It is intuitively\\r\\nreasonable, however, that these parameters behave differently during differ\\x02ent time intervals of the question. For instance, the level of disagreement\\r\\n(represented by σ\\r\\n2\\r\\nk\\r\\nin our model) among the experts can be expected to\\n22 V. A. SATOPA¨A ET AL. ¨\\r\\ndecrease toward the final time point when the question resolves. This hy\\x02pothesis could be explored by letting σ\\r\\n2\\r\\nt,k evolve dynamically as a function\\r\\nof the previous term σ\\r\\n2\\r\\nt−1,k and random noise.\\r\\nThis paper modeled the bias separately within each expertise group. This\\r\\nis by no means restricted to the study of bias or its relation to self-reported\\r\\nexpertise. Different parameter dependencies could be constructed based on\\r\\nmany other expert characteristics, such as gender, education or specialty,\\r\\nto produce a range of novel insights on the forecasting behavior of experts.\\r\\nIt would also be useful to know how expert characteristics interact with\\r\\nquestion types, such as economic, domestic or international. The results\\r\\nwould be of interest to the decision-maker who could use the information as\\r\\na basis for hiring only a high-performing subset of the available experts.\\r\\nOther future directions could remove some of the obvious limitations of\\r\\nour model. For instance, recall that the random components are assumed\\r\\nto follow a normal distribution. This is a strong assumption that may not\\r\\nalways be justified. Logit probabilities, however, have been modeled with the\\r\\nnormal distribution before [see, e.g., Erev, Wallsten and Budescu (1994)].\\r\\nFurthermore, the normal distribution is a rather standard assumption in\\r\\npsychological models [see, e.g., signal-detection theory in Tanner, Wilson\\r\\nand Swets (1954)].\\r\\nA second limitation resides in the assumption that both the observed and\\r\\nhidden processes are expected to grow linearly. This assumption could be\\r\\nrelaxed, for instance, by adding higher order terms to the model. A more\\r\\ncomplex model, however, is likely to sacrifice interpretability. Given that our\\r\\nmodel can detect very intricate patterns in the crowd belief (see Figure 1),\\r\\ncompromising interpretability for the sake of facilitating nonlinear growth\\r\\nis hardly necessary.\\r\\nA third limitation appears in an online setting where new forecasts are\\r\\nreceived at a fast rate. Given that our model is fit in a retrospective fash\\x02ion, it is necessary to refit the model every time a new forecast becomes\\r\\navailable. Therefore, our model can be applied only to offline aggregation\\r\\nand online problems that tolerate some delay. A more scalable and efficient\\r\\nalternative would be to develop an aggregator that operates recursively on\\r\\nstreams of forecasts. Such a filtering perspective would offer an aggregator\\r\\nthat estimates the current crowd belief accurately without having to refit the\\r\\nentire model each time a new forecast arrives. Unfortunately, this typically\\r\\nimplies being less accurate in estimating the model parameters such as the\\r\\nbias term. However, as estimation of the model parameters was addressed\\r\\nin this paper, designing a filter for probability forecasts seems like the next\\r\\nnatural development in time-series probability aggregation.\\r\\nAcknowledgments. The U.S. Government is authorized to reproduce and\\r\\ndistribute reprints for Government purposes notwithstanding any copyright\\nPROBABILITY AGGREGATION IN TIME-SERIES 23\\r\\nannotation thereon. Disclaimer: The views and conclusions expressed herein\\r\\nare those of the authors and should not be interpreted as necessarily repre\\x02senting the official policies or endorsements, either expressed or implied, of\\r\\nIARPA, DoI/NBC or the U.S. Government.\\r\\nWe deeply appreciate the project management skills and work of Terry\\r\\nMurray and David Wayrynen, which went far beyond the call of duty on\\r\\nthis project.\\r\\nSUPPLEMENTARY MATERIAL\\r\\nSampling step (DOI: 10.1214/14-AOAS739SUPP; .pdf). This supplemen\\x02tary material provides a technical description of the sampling step of the\\r\\nSAC-algorithm.\\r\\nREFERENCES\\r\\nAllard, D., Comunian, A. and Renard, P. (2012). Probability aggregation methods\\r\\nin geoscience. Math. Geosci. 44 545–581. MR2947804\\r\\nAriely, D., Au, W. T., Bender, R. H., Budescu, D. V., Dietz, C. B., Gu, H.,\\r\\nWallsten, T. S. and Zauberman, G. (2000). The effects of averaging subjective\\r\\nprobability estimates between and within judges. Journal of Experimental Psychology:\\r\\nApplied 6 130–147.\\r\\nBaars, J. A. and Mass, C. F. (2005). Performance of national weather service forecasts\\r\\ncompared to operational, consensus, and weighted model output statistics. Weather\\r\\nand Forecasting 20 1034–1047.\\r\\nBaron, J., Mellers, B. A., Tetlock, P. E., Stone, E. and Ungar, L. H. (2014).\\r\\nTwo reasons to make aggregated probability forecasts more extreme. Decis. Anal. 11.\\r\\nDOI:10.1287/deca.2014.0293.\\r\\nBatchelder, W. H., Strashny, A. and Romney, A. K. (2010). Cultural consensus\\r\\ntheory: Aggregating continuous responses in a finite interval. In Advances in Social\\r\\nComputing (S.-K. Chaim, J. J. Salerno and P. L. Mabry, eds.) 98–107. Springer,\\r\\nBerlin.\\r\\nBier, V. (2004). Implications of the research on expert overconfidence and dependence.\\r\\nReliability Engineering & System Safety 85 321–329.\\r\\nBonferroni, C. E. (1936). Teoria Statistica Delle Classi e Calcolo Delle Probabilit´a.\\r\\nPubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze\\r\\n8 3–62.\\r\\nBrier, G. W. (1950). Verification of forecasts expressed in terms of probability. Monthly\\r\\nWeather Review 78 1–3.\\r\\nBrocker, J. ¨ and Smith, L. A. (2007). Increasing the reliability of reliability diagrams.\\r\\nWeather and Forecasting 22 651–661.\\r\\nBuja, A., Stuetzle, W. and Shen, Y. (2005). Loss functions for binary class proba\\x02bility estimation and classification: Structure and applications. Statistics Department,\\r\\nUniv. Pennsylvania, Philadelphia, PA. Available at http://stat.wharton.upenn.edu/\\r\\n~buja/PAPERS/paper-proper-scoring.pdf.\\r\\nCarter, C. K. and Kohn, R. (1994). On Gibbs sampling for state space models.\\r\\nBiometrika 81 541–553. MR1311096\\r\\nChen, Y. (2008). Learning classifiers from imbalanced, only positive and unlabeled data\\r\\nsets. Project Report for UC San Diego Data Mining Contest. Dept. Computer Science,\\n24 V. A. SATOPA¨A ET AL. ¨\\r\\nIowa State Univ., Ames, IA. Available at https://www.cs.iastate.edu/~yetianc/\\r\\ncs573/files/CS573_ProjectReport_YetianChen.pdf.\\r\\nClemen, R. T. and Winkler, R. L. (2007). Aggregating probability distributions.\\r\\nIn Advances in Decision Analysis: From Foundations to Applications (W. Edwards,\\r\\nR. F. Miles and D. von Winterfeldt, eds.) 154–176. Cambridge Univ. Press, Cam\\x02bridge.\\r\\nCooke, R. M. (1991). Experts in Uncertainty: Opinion and Subjective Probability in\\r\\nScience. Clarendon Press, New York. MR1136548\\r\\nErev, I., Wallsten, T. S. and Budescu, D. V. (1994). Simultaneous over- and under\\x02confidence: The role of error in judgment processes. Psychological Review 66 519–527.\\r\\nGelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (2003). Bayesian data\\r\\nanalysis. CRC press, Boca Raton.\\r\\nGelman, A., Jakulin, A., Pittau, M. G. and Su, Y.-S. (2008). A weakly informative\\r\\ndefault prior distribution for logistic and other regression models. Ann. Appl. Stat. 2\\r\\n1360–1383. MR2655663\\r\\nGeman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the\\r\\nBayesian restoration of images. Institute of Electrical and Electronics Engineer (IEEE)\\r\\nTransactions on Pattern Analysis and Machine Intelligence 6 721–741.\\r\\nGenest, C. and Zidek, J. V. (1986). Combining probability distributions: A critique and\\r\\nan annotated bibliography. Statist. Sci. 1 114–148. MR0833278\\r\\nGent, I. P. and Walsh, T. (1996). Phase transitions and annealed theories: Number\\r\\npartitioning as a case study. In Proceedings of European Conference on Artificial Intel\\x02ligence (ECAI 1996) 170–174. Wiley, New York.\\r\\nGneiting, T. and Ranjan, R. (2013). Combining predictive distributions. Electron. J.\\r\\nStat. 7 1747–1782. MR3080409\\r\\nGneiting, T., Stanberry, L. I., Grimit, E. P., Held, L. and Johnson, N. A. (2008).\\r\\nRejoinder on: Assessing probabilistic forecasts of multivariate quantities, with an ap\\x02plication to ensemble predictions of surface winds [MR2434318]. TEST 17 256–264.\\r\\nMR2434326\\r\\nGood, I. J. (1952). Rational decisions. J. R. Stat. Soc. Ser. B Stat. Methodol. 14 107–114.\\r\\nMR0077033\\r\\nHastings, C. Jr., Mosteller, F., Tukey, J. W. and Winsor, C. P. (1947). Low mo\\x02ments for small samples: A comparative study of order statistics. Ann. Math. Statistics\\r\\n18 413–426. MR0022335\\r\\nHayes, B. (2002). The easiest hard problem. American Scientist 90 113–117.\\r\\nKarmarkar, N. and Karp, R. M. (1982). The differencing method of set partition\\x02ing. Technical Report UCB/CSD 82/113, Computer Science Division, Univ. California,\\r\\nBerkeley, CA.\\r\\nKellerer, H., Pferschy, U. and Pisinger, D. (2004). Knapsack Problems. Springer,\\r\\nDordrecht. MR2161720\\r\\nLichtenstein, S., Fischhoff, B. and Phillips, L. D. (1977). Calibration of Prob\\x02abilities: The State of the Art. In Decision Making and Change in Human Affairs\\r\\n(H. Jungermann and G. De Zeeuw, eds.) 275–324. Springer, Berlin.\\r\\nLubinski, D. and Humphreys, L. G. (1996). Seeing the forest from the trees: When\\r\\npredicting the behavior or status of groups, correlate means. Psychology, Public Policy,\\r\\nand Law 2 363.\\r\\nMellers, B., Ungar, L., Baron, J., Ramos, J., Gurcay, B., Fincher, K.,\\r\\nScott, S. E., Moore, D., Atanasov, P. and Swift, S. A. et al. (2014). Psy\\x02chological strategies for winning a geopolitical forecasting tournament. Psychological\\r\\nScience 25. DOI:10.1177/0956797614524255.\\nPROBABILITY AGGREGATION IN TIME-SERIES 25\\r\\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. and\\r\\nTeller, E. (1953). Equation of state calculations by fast computing machines. The\\r\\nJournal of Chemical Physics 21 1087–1092.\\r\\nMigon, H. S., Gamerman, D., Lopes, H. F. and Ferreira, M. A. R. (2005). Dynamic\\r\\nmodels. In Bayesian Thinking: Modeling and Computation. Handbook of Statist. 25\\r\\n553–588. Elsevier/North-Holland, Amsterdam. MR2490539\\r\\nMills, T. C. (1991). Time series techniques for economists. Cambridge Univ. Press,\\r\\nCambridge.\\r\\nMorgan, M. G. (1992). Uncertainty: A Guide to Dealing with Uncertainty in Quantitative\\r\\nRisk and Policy Analysis. Cambridge Univ. Press, Cambridge.\\r\\nMurphy, A. H. and Winkler, R. L. (1987). A general framework for forecast verification.\\r\\nMonthly Weather Review 115 1330–1338.\\r\\nNeal, R. M. (2003). Slice sampling. Ann. Statist. 31 705–767. MR1994729\\r\\nPepe, M. S. (2003). The Statistical Evaluation of Medical Tests for Classification and Pre\\x02diction. Oxford Statistical Science Series 28. Oxford Univ. Press, Oxford. MR2260483\\r\\nPrimo, C., Ferro, C. A., Jolliffe, I. T. and Stephenson, D. B. (2009). Calibration\\r\\nof probabilistic forecasts of binary events. Monthly Weather Review 137 1142–1149.\\r\\nRaftery, A. E., Gneiting, T., Balabdaoui, F. and Polakowski, M. (2005). Using\\r\\nBayesian model averaging to calibrate forecast ensembles. Monthly Weather Review 133\\r\\n1155–1174.\\r\\nRanjan, R. and Gneiting, T. (2010). Combining probability forecasts. J. R. Stat. Soc.\\r\\nSer. B Stat. Methodol. 72 71–91. MR2751244\\r\\nSanders, F. (1963). On subjective probability forecasting. Journal of Applied Meteorology\\r\\n2 191–201.\\r\\nSatopa¨a, V. A. ¨ , Baron, J., Foster, D. P., Mellers, B. A., Tetlock, P. E. and\\r\\nUngar, L. H. (2014a). Combining multiple probability predictions using a simple logit\\r\\nmodel. International Journal of Forecasting 30 344–356.\\r\\nSatopa¨a, V. A. ¨ , Jensen, S. T., Mellers, B. A., Tetlock, P. E. and Ungar, L. H.\\r\\n(2014b). Supplement to “Probability aggregation in time-series: Dynamic hierarchical\\r\\nmodeling of sparse expert beliefs.” DOI:10.1214/14-AOAS739SUPP.\\r\\nShlyakhter, A. I., Kammen, D. M., Broido, C. L. and Wilson, R. (1994). Quantifying\\r\\nthe credibility of energy projections from trends in past data: The US energy sector.\\r\\nEnergy Policy 22 119–130.\\r\\nTanner, J., Wilson, P. and Swets, J. A. (1954). A decision-making theory of visual\\r\\ndetection. Psychological Review 61 401–409.\\r\\nTetlock, P. E. (2005). Expert Political Judgment: How Good Is It? How Can We Know?\\r\\nPrinceton Univ. Press, Princeton, NJ.\\r\\nUngar, L., Mellers, B., Satopa¨a, V. ¨ , Tetlock, P. and Baron, J. (2012). The good\\r\\njudgment project: A large scale test of different methods of combining expert pre\\x02dictions. In The Association for the Advancement of Artificial Intelligence 2012 Fall\\r\\nSymposium Series, Univ. Pennsylvania, Philadelphia, PA.\\r\\nVislocky, R. L. and Fritsch, J. M. (1995). Improved model output statistics forecasts\\r\\nthrough model consensus. Bulletin of the American Meteorological Society 76 1157–\\r\\n1164.\\r\\nWallace, B. C. and Dahabreh, I. J. (2012). Class probability estimates are unreliable\\r\\nfor imbalanced data (and how to fix them). In Institute of Electrical and Electronics\\r\\nEngineers (IEEE) 12th International Conference on Data Mining (International Con\\x02ference on Data Mining) 695–704. IEEE Computer Society, Washington, DC.\\r\\nWallsten, T. S., Budescu, D. V. and Erev, I. (1997). Evaluating and combining\\r\\nsubjective probability estimates. Journal of Behavioral Decision Making 10 243–268.\\n26 V. A. SATOPA¨A ET AL. ¨\\r\\nWilson, A. G. (1994). Cognitive factors affecting subjective probability assessment. Dis\\x02cussion Paper 94-02, Institute of Statistics and Decision Sciences, Duke Univ., Chapel\\r\\nHill, NC.\\r\\nWilson, P. W., D’Agostino, R. B., Levy, D., Belanger, A. M., Silbershatz, H.\\r\\nand Kannel, W. B. (1998). Prediction of coronary heart disease using risk factor\\r\\ncategories. Circulation 97 1837–1847.\\r\\nWinkler, R. L. and Jose, V. R. R. (2008). Comments on: Assessing probabilistic fore\\x02casts of multivariate quantities, with an application to ensemble predictions of surface\\r\\nwinds [MR2434318]. TEST 17 251–255. MR2434325\\r\\nWinkler, R. L. and Murphy, A. H. (1968). “Good” probability assessors. Journal of\\r\\nApplied Meteorology 7 751–758.\\r\\nWright, G., Rowe, G., Bolger, F. and Gammack, J. (1994). Coherence, calibration,\\r\\nand expertise in judgmental probability forecasting. Organizational Behavior and Hu\\x02man Decision Processes 57 1–25.\\r\\nV. A. Satopa¨a¨\\r\\nS. T. Jensen\\r\\nDepartment of Statistics\\r\\nThe Wharton School\\r\\nUniversity of Pennsylvania\\r\\nPhiladelphia, Pennsylvania 19104-6340\\r\\nUSA\\r\\nE-mail: satopaa@wharton.upenn.edu\\r\\nstjensen@wharton.upenn.edu\\r\\nB. A. Mellers\\r\\nP. E. Tetlock\\r\\nDepartment of Psychology\\r\\nUniversity of Pennsylvania\\r\\nPhiladelphia, Pennsylvania 19104-6340\\r\\nUSA\\r\\nE-mail: mellers@wharton.upenn.edu\\r\\ntetlock@wharton.upenn.edu\\r\\nL. H. Ungar\\r\\nDepartment of Computer\\r\\nand Information Science\\r\\nUniversity of Pennsylvania\\r\\nPhiladelphia, Pennsylvania 19104-6309\\r\\nUSA\\r\\nE-mail: ungar@cis.upenn.edu'},\n",
       " {'name': '2309.04478v1.pdf',\n",
       "  'content': '1\\r\\nMultimodal machine learning for materials science: composition-structure \\r\\nbimodal learning for experimentally measured properties\\r\\nSheng Gong1*, Shuo Wang1*, Taishan Zhu1, Yang Shao-Horn1,2, and Jeffrey C. Grossman1\\r\\n1Department of Materials Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA \\r\\n02139, USA\\r\\n2Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\\r\\n* These authors contribute equally\\n2\\r\\nAbstract\\r\\nThe widespread application of multimodal machine learning models like GPT-4 has revolutionized \\r\\nvarious research fields including computer vision and natural language processing. However, its \\r\\nimplementation in materials informatics remains underexplored, despite the presence of materials data \\r\\nacross diverse modalities, such as composition and structure. The effectiveness of machine learning \\r\\nmodels trained on large calculated datasets depends on the accuracy of calculations, while \\r\\nexperimental datasets often have limited data availability and incomplete information. This paper \\r\\nintroduces a novel approach to multimodal machine learning in materials science via composition\\x02structure bimodal learning. The proposed COmposition-Structure Bimodal Network (COSNet) is \\r\\ndesigned to enhance learning and predictions of experimentally measured materials properties that \\r\\nhave incomplete structure information. Bimodal learning significantly reduces prediction errors across \\r\\ndistinct materials properties including Li conductivity in solid electrolyte, band gap, refractive index, \\r\\ndielectric constant, energy, and magnetic moment, surpassing composition-only learning methods. \\r\\nFurthermore, we identified that data augmentation based on modal availability plays a pivotal role in \\r\\nthe success of bimodal learning.\\n3\\r\\nIntroduction\\r\\nRecently, multimodal machine learning models, such as GPT-4(1), have profoundly \\r\\ntransformed the influence of artificial intelligence on society(2). By definition, multimodal machine \\r\\nlearning obtains information from different modalities, such as text, image, and audio(3), and fuses \\r\\nall the information for downstream tasks. Multimodal machine learning has attracted increasing \\r\\nattention in many research communities such as computer vision and natural language processing, \\r\\nbecause in many cases multimodal machine learning outperforms the single modal learning(4). There \\r\\nalso exist many modalities in materials science, such as composition, structure, spectrum, image, and \\r\\ntext(5), which in principle can be simultaneously incorporated into multimodal machine learning \\r\\nmodels that potentially outperform machine learning models trained on single modality. Despite the \\r\\npotential, however, there is still a lack of widespread application of multimodal machine learning in \\r\\nthe field of materials science. Therefore, it is important to demonstrate the effectiveness of multimodal \\r\\nmachine learning in materials science.\\r\\nOne of the ultimate goals of materials informatics is to predict materials properties that are \\r\\nclose to experimental measurements(6). Although most of current machine learning models applied to \\r\\nmaterials are trained on theoretical datasets(7-16) due to the high availability, the inherit gap between \\r\\ntheoretical calculations and experiments indicates that the usefulness of such machine learning models \\r\\nlargely depends on the accuracy of the theoretical calculations(15, 17-19). To bypass the dependency\\r\\nof accuracy of training data, recently experimentally measured properties, the “ground truth”, have \\r\\nbeen used as the training set for machine learning models(12, 20, 21). \\r\\nHowever, learning experimental data faces many challenges, such as limited number of data \\r\\npoints, conflicted results in different literatures, and incomplete information recorded in literatures. \\r\\nWhile data cleaning has been used to assess the issue of conflicts(22), and machine learning techniques \\r\\nsuch as transfer learning(12, 21, 23, 24) and multi-fidelity learning(20) have been developed to \\n4\\r\\nalleviate the issue of limited number of data points, the challenge of incomplete information has not \\r\\nyet been addressed properly. This is because, single modal machine learning cannot encode all \\r\\ninformation. For example, as illustrated in Figure 1a, in most materials datasets the composition \\r\\ninformation of most materials is available(25, 26), but the data points with available structure \\r\\ninformation is a subset of data points with composition(22, 25). Therefore, although structure is more \\r\\ninformative than composition(27), machine learning models trained on structures have to be trained \\r\\non subsets. On the other hand, machine learning models trained on composition with more data cannot \\r\\nincorporate structure information into the model. As a result, available single modal machine learning \\r\\nmodels trained on either composition or structure are not ideal models that fully utilize all available \\r\\ninformation.\\r\\nIn this work, we propose that, multimodal machine learning, specifically composition-structure \\r\\nbimodal learning, can be used to improve the learning performance of experimentally measured \\r\\nmaterials properties. We build a new machine learning framework, COSNet, to realize a COmposition\\x02Structure bimodal Network. We show that composition-structure bimodal learning has better \\r\\npredictions for various materials properties than composition-only learning, including Li conductivity \\r\\nin solid electrolyte, band gap, refractive index, dielectric constant, energy, and magnetic moment. The \\r\\nimprovement from addition of structure information exists in both data points with and without \\r\\nstructures, showing the effect of representation alignment that the addition of the structure modality \\r\\nhas globally informed the representation of the composition modality. We also find that the modal \\r\\navailability-based data-augmentation is critical to the effect of bimodal learning, as it ensures that the \\r\\ncomposition network is effectively trained by all available data.\\r\\nThe main contributions of the work are:\\r\\n1) proposing the composition-structure bimodal learning framework to improve the learning \\r\\nperformance to datasets with incomplete structure information.\\n5\\r\\n2) demonstrating that data augmentation is critical to the improvement from bimodal learning. \\r\\nFigure 1. Overview of composition-structure bimodal learning. a Illustration of the range of \\r\\nmaterials data, composition data and structure data. Composition data is a subset of all materials data, \\r\\nand structure data is a subset of composition data. b Schematic of composition-structure bimodal \\r\\nlearning and the COSNet. \\r\\nResults\\n6\\r\\nArchitecture of COSNet. In this work, we propose COSNet (COmposition-Structure Bimodal \\r\\nNetwork) for composition-structure bimodal learning on materials dataset. As shown in Figure 1b, \\r\\nCOSNet is mainly composed of four parts: a neural network to encode composition, a neural network \\r\\nto encode structure, an attention-based summation/concatenation to combine composition \\r\\nrepresentation and structure representation, and a multi-layer perceptron (MLP) to predict property \\r\\nbased on the combined representation. \\r\\nThe architecture of COSNet is summarized below. For a materials dataset 𝐷 =\\r\\n{(𝑐\\r\\n𝑖\\r\\n, 𝑠\\r\\n𝑖\\r\\n,𝑡\\r\\n𝑖\\r\\n)𝑖=1,…,𝑁}, where 𝑐\\r\\n𝑖\\r\\n, 𝑠\\r\\n𝑖\\r\\n,𝑡\\r\\n𝑖\\r\\nrepresent composition, structure, and target property of material 𝑖, \\r\\nrespectively, composition representation 𝐶\\r\\n𝑖\\r\\nand structure representation 𝑆\\r\\n𝑖\\r\\nare first obtained by the \\r\\ncomposition network 𝑔𝐶\\r\\n𝑟\\r\\nand the structure network 𝑔𝑆\\r\\n𝑟\\r\\nas in equation (1) and (2), respectively: \\r\\n𝐶\\r\\n𝑖 = 𝑔𝐶𝑟\\r\\n(𝑐\\r\\n𝑖\\r\\n) ...... (1),\\r\\n𝑆\\r\\n𝑖 = 𝑔𝑆𝑟\\r\\n(𝑠\\r\\n𝑖\\r\\n) ...... (2),\\r\\nIn this work, both 𝑔𝐶\\r\\n𝑟\\r\\nand 𝑔𝑆\\r\\n𝑟\\r\\nare graph neural networks (ROOST(10) and de-CGCNN(28, 29), \\r\\nrespectively). Note that, if material 𝑖 does not have recorded structure, we assign a null structure to it \\r\\nas a place holder: 𝑠\\r\\n𝑖 = 𝑠𝑛𝑢𝑙𝑙\\r\\n. Then, to combine the two representations from the two modalities into \\r\\nan overall materials representation 𝑀𝑖, we design an attention-based summation/concatenation as \\r\\nbelow:\\r\\n𝑀𝑖 = {\\r\\n𝐶\\r\\n𝑖\\r\\n∗ 𝑤𝑐\\r\\n𝑖 + 𝑆𝑖\\r\\n∗ 𝑤𝑠\\r\\n𝑖\\r\\nor\\r\\n𝐶\\r\\n𝑖\\r\\n∗ 𝑤𝑐\\r\\n𝑖 ⊕ 𝑆𝑖\\r\\n∗ 𝑤𝑠\\r\\n𝑖\\r\\n...... (3),\\r\\nwhere + is element-wise summation and ⊕ is vector concatenation, and 𝑤𝑐\\r\\n𝑖\\r\\nand 𝑤𝑠\\r\\n𝑖\\r\\nare the attention \\r\\nweights for 𝐶\\r\\n𝑖\\r\\nand 𝑆\\r\\n𝑖\\r\\n, respectively. Note 𝑤𝑐\\r\\n𝑖\\r\\n(𝑤𝑠\\r\\n𝑖\\r\\n) can be either scalar or vector with the same \\r\\ndimension of 𝐶\\r\\n𝑖\\r\\n(𝑆\\r\\n𝑖\\r\\n), and this choice of dimension of attention weight and the choice between + and \\n7\\r\\n⊕ are defined as hyper-parameters in COSNet. 𝑤𝑐\\r\\n𝑖\\r\\nand 𝑤𝑠\\r\\n𝑖\\r\\nare determined as below:\\r\\n𝑤𝑐\\r\\n′𝑖 = MLP𝐶𝑤(𝐶𝑖\\r\\n) ...... (4),\\r\\n𝑤𝑆\\r\\n′𝑖 = MLP𝑆𝑤(𝑆𝑖\\r\\n) ∗ 𝑝𝑆\\r\\n𝑖\\r\\n...... (5),\\r\\n𝑤𝑐\\r\\n𝑖 =\\r\\n𝑤𝑐\\r\\n′𝑖\\r\\n𝑤𝑐\\r\\n′𝑖+𝑤𝑆′𝑖\\r\\n...... (6),\\r\\n𝑤𝑠\\r\\n𝑖 =\\r\\n𝑤𝑠\\r\\n′𝑖\\r\\n𝑤𝑐\\r\\n′𝑖+𝑤𝑆′𝑖\\r\\n...... (7),\\r\\n𝑝𝑆\\r\\n𝑖 = {\\r\\n0, if 𝑠\\r\\n𝑖 = 𝑠𝑛𝑢𝑙𝑙\\r\\n1, if 𝑠\\r\\n𝑖 ≠ 𝑠𝑛𝑢𝑙𝑙 ...... (8),\\r\\nwhere MLP𝐶\\r\\n𝑤 and MLP𝑆𝑤 denote the multi-layer perceptron (MLP) for determining composition and \\r\\nstructure weights, respectively. Note that a softplus activation function is used at the end of MLP𝐶\\r\\n𝑤 and \\r\\nMLP𝑆\\r\\n𝑤 to ensure that 𝑤𝑐′𝑖\\r\\nand 𝑤𝑆\\r\\n′𝑖\\r\\nare positive. Combining equations (3) to (8), the materials \\r\\nrepresentation 𝑀𝑖can be written as:\\r\\n𝑀𝑖 = {\\r\\n𝐶\\r\\n𝑖 or 𝐶𝑖 ⊕ 𝟎, if 𝑠𝑖 = 𝑠𝑛𝑢𝑙𝑙\\r\\n𝐶\\r\\n𝑖\\r\\n∗ 𝑤𝑐\\r\\n𝑖 + (or ⊕ ) 𝑆𝑖\\r\\n∗ 𝑤𝑠\\r\\n𝑖\\r\\n, if 𝑠\\r\\n𝑖 ≠ 𝑠𝑛𝑢𝑙𝑙 ...... (9),\\r\\nwhere 𝟎 is a zero vector with the same dimension of 𝑆\\r\\n𝑖\\r\\n. Finally, a MLP is used to predict target \\r\\nproperty from 𝑀𝑖:\\r\\n𝑡\\r\\n𝑖 = MLP𝑡\\r\\n(𝑀𝑖)...... (10).\\r\\nBesides the neural network architecture of COSNet, the other critical aspect of composition\\x02structure bimodal learning in this work is the data augmentation based on the relation between \\r\\ncomposition and structure. For a composition 𝑐, in principle there are infinite number of structures 𝑠\\r\\nthat have the same composition 𝑐. If we impose a restriction that 𝑠 should only include stable or meta\\x02stable structures at room temperature and pressure, then there might be a single or multiple \\n8\\r\\n𝑠 corresponding to a given 𝑐, depending on the nature of the composition 𝑐. In a materials dataset 𝐷, \\r\\nfor a given 𝑐, there might be no or a single or multiple 𝑠 that have composition 𝑐. If only one specific \\r\\n𝑠\\r\\n𝑖\\r\\nin the dataset has the composition 𝑐\\r\\n𝑖\\r\\n, then ideally the prediction from composition-structure bimodal \\r\\nlearning should be equal to that from composition-only learning:\\r\\n∀𝑐\\r\\n𝑖 ∈ 𝐷 s.t. ∃! 𝑠𝑖 ∈ 𝐷, 𝑓𝐶(𝑠𝑖\\r\\n) = 𝑐\\r\\n𝑖\\r\\n,\\r\\nMLP𝑡(𝐶\\r\\n𝑖 or 𝐶𝑖 ⊕ 𝟎) = MLP𝑡\\r\\n(𝐶\\r\\n𝑖\\r\\n∗ 𝑤𝑐\\r\\n𝑖 + (or ⊕ ) 𝑆𝑖\\r\\n∗ 𝑤𝑠\\r\\n𝑖\\r\\n) …… (12),\\r\\nwhere 𝑓𝐶 denotes the mapping function of a structure 𝑠 to its composition 𝑐. However, different from \\r\\nthe invariance imposed by physics such as translation and rotation invariance of structure-property \\r\\nlearning(28, 30), the equality in equation (12) is dataset-dependent and might break when the dataset \\r\\nis changed, such as when a new structure 𝑠\\r\\n𝑗\\r\\ncorresponding to an existing 𝑐\\r\\n𝑖\\r\\nis added into the dataset. \\r\\nTherefore, it is not reasonable to impose equation (12) in the neural networks of COSNet. Instead, we \\r\\nimplement a data augmentation in the data preparation stage of COSNet for equation (12):\\r\\n𝐷augmented = {(𝑐\\r\\n𝑖\\r\\n, 𝑠\\r\\n𝑖\\r\\n,𝑡\\r\\n𝑖\\r\\n)𝑖=1,…,𝑁} + {(𝑐\\r\\n𝑖\\r\\n, 𝑠\\r\\n𝑛𝑢𝑙𝑙\\r\\n,𝑡\\r\\n𝑖\\r\\n)∀(𝑐\\r\\n𝑖\\r\\n, 𝑡\\r\\n𝑖\\r\\n) ∈ 𝐵\\r\\n},\\r\\n𝐵 = {(𝑐\\r\\n𝑖\\r\\n, 𝑡\\r\\n𝑖\\r\\n),∃! 𝑠\\r\\n𝑖 ∈ 𝐷 s.t. 𝑓𝐶(𝑠𝑖\\r\\n) = 𝑐\\r\\n𝑖\\r\\n} ...... (13).\\r\\nIn other words, if a composition only corresponds to one structure in the training set, then this \\r\\ncomposition is added into the training set without structure, which is to push COSNet to have similar \\r\\npredictions based on only composition and the pair of composition-structure. Note that the data\\r\\naugmentation is only implemented on training sets, not on validation sets and test sets.\\r\\nPredictions from COSNet. In Table 1, we compare the predictions from composition-structure \\r\\nbimodal learning (COSNet) and composition-only learning (COSNet with ∀𝑖 ∈ 𝐷, 𝑝𝑆\\r\\n𝑖 = 0) for four \\r\\nexperimentally measured materials properties: Li conductivity in solid electrolyte, band gap, refractive \\n9\\r\\nindex, and dielectric constant. We can see that, for the four properties, bimodal learning has better \\r\\npredictions for all the complete datasets than composition-only learning with around 7% to 10% lower \\r\\nmean absolute errors (MAEs), and the improvements are statistically significant as the difference of \\r\\nerrors are larger than the sum of the standard deviations of the two MAEs. We can also see that, the \\r\\nlower errors of bimodal learning are observed for both data points with structures and without \\r\\nstructures, which shows that structure information of a portion of data points can improve prediction \\r\\nof other data points without structure. \\r\\nTable 1. Comparison of test set mean absolute errors (MAEs) of composition-only learning, bimodal \\r\\nlearning with data augmentation and bimodal learning without data augmentation, respectively. For \\r\\neach property, the top, middle, and bottom row represent MAE of all data in the test set, data with only\\r\\ncomposition, and data with both composition and structure, respectively. The unit of each property is \\r\\nshown under the name of the property. \\r\\nHere we propose that the improvement of data points without structure is from representation \\r\\nalignment(3), which is a phenomenon that representations from different modalities (𝐶\\r\\n𝑖\\r\\nand 𝑆\\r\\n𝑖\\r\\nin this \\r\\nwork) align with each other during multi-modal training. In other words, 𝐶\\r\\n𝑖\\r\\nand 𝑆\\r\\n𝑖\\r\\nare aligned during \\r\\nthe training, and since 𝑆\\r\\n𝑖\\r\\ncontains more information than 𝐶\\r\\n𝑖\\r\\n, 𝐶\\r\\n𝑖 becomes more informative in bimodal \\r\\nlearning than composition-only learning, which leads to better predictions of data points without \\r\\nModel dataset type number of \\r\\ndata points\\r\\ncomposition \\r\\nonly\\r\\nbimodal with \\r\\naugmentation\\r\\nbimodal without \\r\\naugmentation\\r\\nLi Conductivity\\r\\n(log10 (mS/cm))\\r\\ncomplete 1499 1.022 ± 0.047 0.924 ± 0.012 1.245 ± 0.078\\r\\nw/o structure 1077 0.906 ± 0.048 0.832 ± 0.033 1.122 ± 0.099\\r\\nw/ structure 422 1.425 ± 0.091 1.243 ± 0.116 1.674 ± 0.145\\r\\nBand Gap\\r\\n(eV)\\r\\ncomplete 5901 0.502 ± 0.016 0.460 ± 0.004 0.499 ± 0.009\\r\\nw/o structure 2741 0.435 ± 0.031 0.415 ± 0.009 0.404 ± 0.008\\r\\nw/ structure 3160 0.560 ± 0.009 0.495 ± 0.010 0.577 ± 0.009\\r\\nRefractive Index\\r\\n(1)\\r\\ncomplete 1736 0.473 ± 0.008 0.430 ± 0.010 0.482 ± 0.006\\r\\nw/o structure 995 0.492 ± 0.007 0.421 ± 0.023 0.441 ± 0.009\\r\\nw/ structure 741 0.436 ± 0.015 0.439 ± 0.014 0.514 ± 0.009\\r\\nDielectric \\r\\nConstant\\r\\n(1)\\r\\ncomplete 1768 235 ± 6 219 ± 7 225 ± 4\\r\\nw/o structure 1140 295 ± 12 291 ± 10 300 ± 6\\r\\nw/ structure 628 118 ± 16 72 ± 0 71 ± 1\\n10\\r\\nstructure. To demonstrate the hypothesis, here we use a structural descriptor, volume per atom (𝑉𝑎, a \\r\\ndescriptor strongly correlated with ion conductivity(31)), to represent structure information, and use \\r\\n𝐶\\r\\n𝑖\\r\\nto predict 𝑉𝑎 by linear regression and take the R2scores of the linear regressions as a metric to \\r\\nevaluate correlation between 𝐶\\r\\n𝑖\\r\\nand structure. In Table 2, we show that 𝐶\\r\\n𝑖\\r\\nfrom bimodal learning has \\r\\nstronger correlation with 𝑉𝑎 than that from composition-only learning, which verifies the effect of \\r\\nrepresentation alignment between 𝐶\\r\\n𝑖\\r\\nand 𝑆\\r\\n𝑖\\r\\nand explains why addition of structure information helps \\r\\nlearning of data points without structure.\\r\\nTable 2. R2scores of the linear regressions between 𝐶\\r\\n𝑖\\r\\nand 𝑉𝑎 of composition-only learning, bimodal \\r\\nlearning with data augmentation and bimodal learning without data augmentation, respectively. The \\r\\ndataset for the linear regressions is composed of data points in the test set of Li conductivity that have \\r\\nstructure. \\r\\nIn addition to the four experimental datasets in Table 1, in Figure 2 we plot the MAE versus \\r\\npercentage of data points with structures for magnetic moment per formula and energy per atom. \\r\\nAlthough these two datasets are purely theoretical from the Materials Project database(32), we include \\r\\nthem here to show the impact of percentage of data points with structures as well as how bimodal \\r\\nlearning performs on larger datasets than typical experimental datasets as in Table 1. We can see that, \\r\\nfor the larger theoretical datasets, bimodal learning also has better predictions than composition-only \\r\\nlearning (percentage = 0), and the more structures, the lower errors, which demonstrates the positive \\r\\nModel R2scores\\r\\ncomposition-only 0.373 ± 0.034\\r\\nbimodal with data augmentation 0.435 ± 0.048\\r\\nbimodal without data augmentation 0.371 ± 0.019\\n11\\r\\neffect of structure on predicting materials properties. The trend of lower errors with respect to more \\r\\nstructures is inspiring, as it indicates that, in the future with more structures measured in the \\r\\nexperimental datasets, more significant improvement can be achieved by bimodal learning compared \\r\\nwith that in this work.\\r\\nFigure 2. Test set MAE of COSNet versus percentage of data points with structures for total magnetic \\r\\nmoment per formula and energy per atom. The two datasets are composed of 25,000 materials \\r\\nrandomly sampled from the Materials Project database(32).\\r\\nEffect of data augmentation. In addition to the comparison between bimodal learning and \\r\\ncomposition-only learning, we also compare bimodal learning with data augmentation in equation (13) \\r\\nand bimodal learning without data augmentation in Table 1 and Figure 2. We can see that, for all the \\r\\ncomplete datasets in Table 1, bimodal learning without data augmentation has larger errors than that \\r\\nwith data augmentation, and for Li conductivity and refractive index, bimodal learning without data \\r\\naugmentation even has larger errors than composition-only learning. As in Figure 2, for moderate \\r\\npercentages, bimodal learning without data augmentation has larger errors than that with data \\n12\\r\\naugmentation. From Table 2, we can see that the effect of representation alignment is more significant \\r\\non bimodal learning with data augmentation than that without augmentation, which is straightforward \\r\\nas data augmentation in equation (13) directly pushes 𝐶\\r\\n𝑖\\r\\nand 𝐶\\r\\n𝑖\\r\\n∗ 𝑤𝑐\\r\\n𝑖 + (or ⊕ ) 𝑆𝑖\\r\\n∗ 𝑤𝑠\\r\\n𝑖\\r\\nto align with \\r\\neach other and result in similar predictions. The stronger representation alignment might explain why \\r\\ndata augmentation leads to better predictions.\\r\\nA possible reason for the poorer predictions and weaker representation alignment from \\r\\nbimodal learning without data augmentation is that, without data augmentation the structure network\\r\\nand the composition network might be trained separately. This is because, structure is generally more \\r\\ninformative than composition for predicting properties(27). Therefore, for data points with structures,\\r\\nbimodal learning might underplay the role of the composition network. To support the argument, we \\r\\nplot the norm of 𝑤𝑐\\r\\n𝑖\\r\\nfor predictions of test set of Li conductivity. For data points without structure, \\r\\nboth bimodal learning with and without augmentation solely use the composition network to predict \\r\\nthe property with 𝑤𝑐\\r\\n𝑖 = 1. However, for data points with structures, we can see that bimodal learning \\r\\nwithout augmentation has much smaller 𝑤𝑐\\r\\n𝑖\\r\\nthan bimodal learning with augmentation, and for more \\r\\nthan a half of data points 𝑤𝑐\\r\\n𝑖 < 0.5. Although structure has more information than composition, the \\r\\nnumber of data points with structures is usually very limited as in Table 1. Consequently, bimodal \\r\\nlearning without structure somewhat splits the learning task into two subtasks, one for training the \\r\\nstructure network and the other training the composition network, leading to the result that the \\r\\ncomposition network is not adequately trained. On the contrary, with data augmentation, the \\r\\ncomposition network is trained on the complete composition dataset, and the structure channel is also \\r\\ntrained on all available structure data, which leads to better training and prediction. \\r\\nThe phenomenon in Figure 2 that, the differences between MAEs of bimodal learning with and \\r\\nwithout data augmentation are most significant at 40% and 60%, also supports the analysis above. At \\r\\nthe moderate percentages, for bimodal learning without data augmentation, the composition network\\n13\\r\\nis not well trained, which leads to the largest difference compared with that with data augmentation \\r\\nwhere both networks are trained on all available data. However, at smaller percentages the composition \\r\\nnetwork dominates and get trained by most data points without structure, and at higher percentages the \\r\\nstructure network dominates and get trained by most data points with structures. Therefore, the \\r\\ndifference between bimodal learning with and without data augmentation becomes less significant at \\r\\nsmall and high percentages of data points with structures.\\r\\nFigure 3. Visualization of composition weights for predicting Li conductivity. Note that the length of \\r\\nthe two regions in the figure is not strictly scaled to the percentage of number of data points with and \\r\\nwithout structure.\\r\\nDiscussions\\r\\nIn this work, we confront the challenge that, many materials datasets, especially experimental \\r\\ndatasets, usually have complete composition information but incomplete structure information, which \\r\\nlimits the learning performance of single modal machine learning models as a model trained on either\\r\\ncomposition or structure cannot incorporate all available information. We propose that both modalities \\n14\\r\\nof composition and structure can be used simultaneously to predict materials properties, and we design \\r\\na machine learning framework, COSNet, to realize the composition-structure bimodal learning. \\r\\nCompared with composition-only learning, we find that bimodal learning can achieve lower errors on \\r\\nfour experimental datasets (Li conductivity, band gap, refractive index, and dielectric constant) and \\r\\ntwo theoretical datasets (energy and magnetization). Moreover, these improvements exist in both data \\r\\npoints with and without structures, which shows the effect of representation alignment between \\r\\ncomposition and structure. We also find that, data augmentation is critical to the improvement from \\r\\nbimodal learning, as data augmentation strengthens representation alignment and ensures that the \\r\\ncomposition network can be adequately trained by all available composition data. \\r\\nNote that a large portion of structures used in this work are from the Materials Project \\r\\ndatabase(32), which might not correspond to the true structures measured in the experiments. \\r\\nTherefore, with more reliable experimentally measured structures in the future, we expect that \\r\\nimprovement from the bimodal learning for experimental properties can be more significant than this \\r\\nwork. As above, this work establishes a new avenue for learning and prediction of experimentally \\r\\nmeasured materials properties, a key task in the field of materials informatics.\\r\\nAs a proof of concept of multimodal machine learning for materials science, we hope this study \\r\\ncan inspire future studies further improving the machine learning architectures for multimodal learning, \\r\\nincorporating more modalities, and applying multimodal learning on more scenarios in materials \\r\\nscience. With new machine learning models being developed for each single modality, such as new \\r\\ncomposition networks and new structure networks, we hope that the improvement from composition\\x02structure bimodal learning can be more significant than this work. Beyond composition and structure, \\r\\nthere are many other modalities in materials science, such as spectrums, images, and texts. We hope \\r\\nthat future studies can simultaneously incorporate more modalities into machine learning models. With \\r\\nfuture development of multimodal machine learning for materials science, we hope that one day people \\n15\\r\\ncan build up “materials general intelligence” that obtains information from all modalities of materials \\r\\nand solves tasks for different purposes of materials development, like what GPT-4 is doing today(1). \\r\\nMethods\\r\\nDatasets. In this work, we use four experimental datasets and two theoretical datasets to test the idea \\r\\nof bimodal learning and COSNet. The dataset of Li conductivity in solid electrolyte is from Ref.(33)\\r\\nand Ref.(34), and a data cleaning is conducted that the value from the most recent report is used for \\r\\nconflicted entries. Structures in the dataset of Li conductivity are from the Materials Project \\r\\ndatabase(32) with a manual match. The dataset of band gap is from Ref.(35) and the Springer Materials \\r\\ndatabase(36). In the band gap dataset, the structures are selected from the Materials Project database(32)\\r\\nwith available space group information, and for conflicted entries of band gap we use the mean value \\r\\nas the value in our dataset. The datasets of refractive index and dielectric constant are from Ref.(26), \\r\\nand a data cleaning is conducted that removes compositions of organic materials and replaces the \\r\\nconflicted values by the mean value. Structures in the two databases are manually selected from the \\r\\nMaterials Project database(32) and the ICSD database(37) according to the available reference\\r\\ninformation in Ref.(26). The datasets of total magnetization per formula and energy per atom are\\r\\nrandomly sampled from the Materials Project database(32). All the datasets are randomly split \\r\\n(60%:20%:20%) into training, validation and test sets for training and evaluation of the machine \\r\\nlearning models.\\r\\nModels. In this work, we choose ROOST(10) and de-CGCNN(28, 29) as the composition and structure \\r\\nnetwork in COSNet, respectively. This choice is based on the fact that, as in Table 1, experimental \\r\\ndatasets typically have limited number of data points, and both ROOST(10) and de-CGCNN(29) have \\r\\nbeen shown to have strong ability for small datasets. All the models are trained by 200 epochs, and the \\n16\\r\\nepoch that has the lowest validation MAE is selected as the final checkpoint for each training. Hyper\\x02parameters search is conducted according to Table 3 to find the hyper-parameters with the lowest \\r\\nvalidation error, and the selected model is used to predict the test set as the final evaluation. The \\r\\ntraining-validation-test procedure is repeated five times with different random seeds to report the error \\r\\nbars in Table 1, Table 2 and Figure 2.\\r\\nTable 3. Hyper-parameters search space for COSNet in this work. Parameters not mentioned here are \\r\\nset to the default value as in the open source codes. \\r\\nFunding: This work was supported by Toyota Research Institute.\\r\\nAuthor Contributions:\\r\\nConceptualization: SG, SW, YSH, JCG\\r\\nMethodology: SG, SW\\r\\nInvestigation: SG, SW\\r\\nSupervision: YSH, JCG\\r\\nWriting—original draft: SG, SW, YSH, JCG\\r\\nWriting—review & editing: SG, SW, YSH, JCG\\r\\nCompeting Interests: The authors declare no competing interest.\\r\\nAcknowledgement: We thank Dr. Kiarash Gordiz and Daniele Vivona for helping collect the Li \\r\\nconductivity dataset.\\r\\nName Space\\r\\nweight decay 1e-1, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4\\r\\nlearning rate 1e-2, 5e-3, 1e-3, 5e-4, 1e-4\\r\\nlearning rate decay 0.99, 0.98, 0.97, 0.96, 0.95, 0.94\\r\\nconcatenation false, true\\r\\nscalar weight false, true\\n17\\r\\nData and Materials Availability:\\r\\nAll datasets and codes in this work are provided at: \\r\\nhttps://github.com/shenggong1996/COSNet/tree/master\\r\\nCrystal structures and materials properties from the Materials Project database (V2021.03.22) are \\r\\ndownloaded at https://materialsproject.org/.\\r\\nReferences\\r\\n1. OpenAI, GPT-4 Technical Report. arXiv:2303.08774v3, (2023).\\r\\n2. S. Noy, W. Zhang, Experimental evidence on the productivity effects of generative artificial \\r\\nintelligence. Science 318, 187-192 (2023).\\r\\n3. C. Zhang, Z. Yang, X. He, L. Deng, Multimodal Intelligence: Representation Learning, Information \\r\\nFusion, and Applications. arXiv:1911.03977v3, (2020).\\r\\n4. T. Baltrusaitis, C. Ahuja, L.-P. Morency, Multimodal Machine Learning: A Survey and Taxonomy. \\r\\narXiv:1705.09406v2, (2017).\\r\\n5. K. Choudhary, B. DeCost, C. Chen, A. Jain, F. Tavazza, R. Cohn, C. W. Park, A. Choudhary, A. \\r\\nAgrawal, S. J. L. Billinge, E. Holm, S. P. Ong, C. Wolverton, Recent advances and applications of \\r\\ndeep learning methods in materials science. npj Comput. Mater. 8, (2022).\\r\\n6. J. Schmidt, M. R. G. Marques, S. Botti, M. A. L. Marques, Recent advances and applications of \\r\\nmachine learning in solid-state materials science. npj Comput. Mater. 5, 1 (2019).\\r\\n7. A. D. Sendek, E. D. Cubuk, E. R. Antoniuk, G. Cheon, Y. Cui, E. J. Reed, Machine Learning-Assisted \\r\\nDiscovery of Solid Li-Ion Conducting Materials. Chem. Mater. 31, 342-352 (2018).\\r\\n8. E. D. Cubuk, A. D. Sendek, E. J. Reed, Screening billions of candidates for solid lithium-ion \\r\\nconductors: A transfer learning approach for small data. J Chem Phys 150, 214701 (2019).\\r\\n9. K. T. Schutt, H. E. Sauceda, P. J. Kindermans, A. Tkatchenko, K. R. Muller, SchNet - A deep learning \\r\\narchitecture for molecules and materials. J Chem Phys 148, 241722 (2018).\\r\\n10. R. E. A. Goodall, A. A. Lee, Predicting materials properties without crystal structure: deep \\r\\nrepresentation learning from stoichiometry. Nat Commun 11, 6280 (2020).\\r\\n11. O. Isayev, C. Oses, C. Toher, E. Gossett, S. Curtarolo, A. Tropsha, Universal fragment descriptors \\r\\nfor predicting properties of inorganic crystals. Nat Commun 8, 15679 (2017).\\r\\n12. D. Jha, K. Choudhary, F. Tavazza, W. K. Liao, A. Choudhary, C. Campbell, A. Agrawal, Enhancing \\r\\nmaterials property prediction by leveraging computational and experimental data using deep \\r\\ntransfer learning. Nat Commun 10, 5316 (2019).\\r\\n13. W. Ye, C. Chen, Z. Wang, I. H. Chu, S. P. Ong, Deep neural networks for accurate predictions of \\r\\ncrystal stability. Nat Commun 9, 3800 (2018).\\r\\n14. A. Chandrasekaran, D. Kamal, R. Batra, C. Kim, L. Chen, R. Ramprasad, Solving the electronic \\r\\nstructure problem with machine learning. npj Comput. Mater. 5, 1 (2019).\\r\\n15. L. Ward, A. Agrawal, A. Choudhary, C. Wolverton, A general-purpose machine learning \\r\\nframework for predicting properties of inorganic materials. npj Comput. Mater. 2, (2016).\\r\\n16. Z. Shi, E. Tsymbalov, M. Dao, S. Suresh, A. Shapeev, J. Li, Deep elastic strain engineering of \\r\\nbandgap through machine learning. Proc Natl Acad Sci U S A 116, 4117-4122 (2019).\\r\\n17. B. Meredig, A. Agrawal, S. Kirklin, J. E. Saal, J. W. Doak, A. Thompson, K. Zhang, A. Choudhary, C. \\n18\\r\\nWolverton, Combinatorial screening for new materials in unconstrained composition space with \\r\\nmachine learning. Phys. Rev. B 89, (2014).\\r\\n18. D. Jha, L. Ward, A. Paul, W. K. Liao, A. Choudhary, C. Wolverton, A. Agrawal, ElemNet: Deep \\r\\nLearning the Chemistry of Materials From Only Elemental Composition. Sci Rep 8, 17593 (2018).\\r\\n19. M. K. Horton, S. Dwaraknath, K. A. Persson, Promises and perils of computational materials \\r\\ndatabases. Nat. Comput. Sci. 1, 3-5 (2021).\\r\\n20. S. Gong, S. Wang, T. Xie, W. H. Chae, R. Liu, Y. Shao-Horn, J. C. Grossman, Calibrating DFT \\r\\nFormation Enthalpy Calculations by Multifidelity Machine Learning. JACS Au 2, 1964-1977 (2022).\\r\\n21. T. Zhu, R. He, S. Gong, T. Xie, P. Gorai, K. Nielsch, J. C. Grossman, Charting Lattice Thermal \\r\\nConductivity of Inorganic Crystals. Energy Environ. Sci., 3559 (2021).\\r\\n22. K. Persson, S. Dwaraknath, S. P. Ong, A. Jain, M. Horton, M. McDermott, R. Kingsbury, A. Wang, \\r\\nA Framework for Quantifying Uncertainty in DFT Energy Corrections. Sci. Rep. 11, 15496 (2021).\\r\\n23. H. Yamada, C. Liu, S. Wu, Y. Koyama, S. Ju, J. Shiomi, J. Morikawa, R. Yoshida, Predicting Materials \\r\\nProperties with Little Data Using Shotgun Transfer Learning. ACS Cent Sci 5, 1717-1730 (2019).\\r\\n24. V. Gupta, K. Choudhary, F. Tavazza, C. Campbell, W. K. Liao, A. Choudhary, A. Agrawal, Cross\\x02property deep transfer learning framework for enhanced predictive analytics on small materials \\r\\ndata. Nat Commun 12, 6595 (2021).\\r\\n25. G. Kim, S. V. Meschel, P. Nash, W. Chen, Experimental formation enthalpies for intermetallic \\r\\nphases and other inorganic compounds. Sci Data 4, 170162 (2017).\\r\\n26. J. Zhao, J. M. Cole, A database of refractive indices and dielectric constants auto-generated using \\r\\nChemDataExtractor. Sci Data 9, 192 (2022).\\r\\n27. C. J. Bartel, A. Trewartha, Q. Wang, A. Dunn, A. Jain, G. Ceder, A critical examination of compound \\r\\nstability predictions from machine-learned formation energies. npj Comput. Mater. 6, (2020).\\r\\n28. T. Xie, J. C. Grossman, Crystal Graph Convolutional Neural Networks for an Accurate and \\r\\nInterpretable Prediction of Material Properties. Phys Rev Lett 120, 145301 (2018).\\r\\n29. S. Gong, T. Xie, Y. Shao-Horn, R. Gomez-Bombarelli, J. C. Grossman, Examining graph neural \\r\\nnetworks for crystal structures: limitations and opportunities for capturing periodicity. arXiv \\r\\npreprint arXiv:2208.05039, (2022).\\r\\n30. L. Zhang, J. Han, H. Wang, R. Car, W. E, Deep Potential Molecular Dynamics: A Scalable Model \\r\\nwith the Accuracy of Quantum Mechanics. Phys Rev Lett 120, 143001 (2018).\\r\\n31. J. Lee, N. Ohba, R. Asahi, Design Rules for High Oxygen-Ion Conductivity in Garnet-Type Oxides. \\r\\nChem. Mater. 32, 1358-1370 (2020).\\r\\n32. S. P. O. A. Jain*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. \\r\\nCeder, K.A. Persson, The Materials Project: A materials genome approach to accelerating \\r\\nmaterials innovation. APL Mater. 1, 011002 (2013).\\r\\n33. C. J. Hargreaves, M. W. Gaultois, L. M. Daniels, E. J. Watts, V. A. Kurlin, M. Moran, Y. Dang, R. \\r\\nMorris, A. Morscher, K. Thompson, M. A. Wright, B.-E. Prasad, F. Blanc, C. M. Collins, C. A. \\r\\nCrawford, B. B. Duff, J. Evans, J. Gamon, G. Han, B. T. Leube, H. Niu, A. J. Perez, A. Robinson, O. \\r\\nRogan, P. M. Sharp, E. Shoko, M. Sonni, W. J. Thomas, A. Vasylenko, L. Wang, M. J. Rosseinsky, M. \\r\\nS. Dyer, A database of experimentally measured lithium solid electrolyte conductivities evaluated \\r\\nwith machine learning. npj Comput. Mater. 9, (2023).\\r\\n34. F. A. L. Laskowski, D. B. McHaffie, K. A. See, Identification of potential solid-state Li-ion conductors \\r\\nwith semi-supervised learning. Energy Environ. Sci. 16, 1264-1276 (2023).\\r\\n35. J. Liang, X. Zhu, Phillips-Inspired Machine Learning for Band Gap and Exciton Binding Energy \\r\\nPrediction. J Phys Chem Lett 10, 5640-5646 (2019).\\r\\n36. https://materials.springer.com/.\\n19\\r\\n37. D. Zagorac, H. Muller, S. Ruehl, J. Zagorac, S. Rehme, Recent developments in the Inorganic \\r\\nCrystal Structure Database: theoretical crystal structure data and related features. J. Appl. \\r\\nCrystallogr 52, 918-925 (2019).'},\n",
       " {'name': '2311.06190v1.pdf',\n",
       "  'content': 'FourierGNN: Rethinking Multivariate Time Series\\r\\nForecasting from a Pure Graph Perspective\\r\\nKun Yi1, Qi Zhang2, Wei Fan3, Hui He1, Liang Hu2, Pengyang Wang4\\r\\nNing An5, Longbing Cao6, Zhendong Niu1∗\\r\\n1Beijing Institute of Technology, 2Tongji University, 3University of Oxford\\r\\n4University of Macau, 5HeFei University of Technology, 6Macquarie University\\r\\n{yikun, hehui617, zniu}@bit.edu.cn, zhangqi_cs@tongji.edu.cn, weifan.oxford@gmail.com\\r\\nrailmilk@gmail.com, pywang@um.edu.mo, ning.g.an@acm.org, longbing.cao@mq.edu.au\\r\\nAbstract\\r\\nMultivariate time series (MTS) forecasting has shown great importance in nu\\x02merous industries. Current state-of-the-art graph neural network (GNN)-based\\r\\nforecasting methods usually require both graph networks (e.g., GCN) and temporal\\r\\nnetworks (e.g., LSTM) to capture inter-series (spatial) dynamics and intra-series\\r\\n(temporal) dependencies, respectively. However, the uncertain compatibility of the\\r\\ntwo networks puts an extra burden on handcrafted model designs. Moreover, the\\r\\nseparate spatial and temporal modeling naturally violates the unified spatiotemporal\\r\\ninter-dependencies in real world, which largely hinders the forecasting performance.\\r\\nTo overcome these problems, we explore an interesting direction of directly apply\\x02ing graph networks and rethink MTS forecasting from a pure graph perspective.\\r\\nWe first define a novel data structure, hypervariate graph, which regards each series\\r\\nvalue (regardless of variates or timestamps) as a graph node, and represents sliding\\r\\nwindows as space-time fully-connected graphs. This perspective considers spa\\x02tiotemporal dynamics unitedly and reformulates classic MTS forecasting into the\\r\\npredictions on hypervariate graphs. Then, we propose a novel architecture Fourier\\r\\nGraph Neural Network (FourierGNN) by stacking our proposed Fourier Graph\\r\\nOperator (FGO) to perform matrix multiplications in Fourier space. FourierGNN\\r\\naccommodates adequate expressiveness and achieves much lower complexity,\\r\\nwhich can effectively and efficiently accomplish the forecasting. Besides, our\\r\\ntheoretical analysis reveals FGO’s equivalence to graph convolutions in the time do\\x02main, which further verifies the validity of FourierGNN. Extensive experiments on\\r\\nseven datasets have demonstrated our superior performance with higher efficiency\\r\\nand fewer parameters compared with state-of-the-art methods. Code is available at\\r\\nthis repository: https://github.com/aikunyi/FourierGNN.\\r\\n1 Introduction\\r\\nMultivariate time series (MTS) forecasting plays an important role in numerous real-world sce\\x02narios, such as traffic flow prediction in transportation systems [1, 2, 3], temperature estimation\\r\\nin weather forecasting [4, 5, 6], and electricity consumption planning in the energy market [7, 8],\\r\\netc. In MTS forecasting, the core challenge is to model intra-series (temporal) dependencies and\\r\\nsimultaneously capture inter-series (spatial) correlations. Existing literature has primarily focused on\\r\\nthe temporal modeling and proposed several forecasting architectures, including Recurrent Neural\\r\\nNetwork (RNN)-based methods (e.g., DeepAR [9]), Convolution Neural Network (CNN)-based\\r\\nmethods (e.g., Temporal Convolution Network [10]) and more recent Transformer-based methods\\r\\n∗Corresponding author\\r\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\r\\narXiv:2311.06190v1 [cs.LG] 10 Nov 2023\\n(e.g., Informer [11] and Autoformer [5]). In addition, another branch of MTS forecasting methods\\r\\nhas been developed to not only model temporal dependencies but also places emphasis on spatial\\r\\ncorrelations. The most representative methods are the emerging Graph Neural Network (GNN)-based\\r\\napproaches [12, 2, 13] that have achieved state-of-the-art performance in the MTS forecasting task.\\r\\nPrevious GNN-based forecasting methods (e.g., STGCN [14] and TAMP-S2GCNets [13]) heavily rely\\r\\non a pre-defined graph structure to specify the spatial correlations, which as a matter of fact cannot\\r\\ncapture the spatial dynamics, i.e., the time-evolving spatial correlation patterns. Later advanced\\r\\napproaches (e.g., StemGNN [12], MTGNN [15], AGCRN [2]) can automatically learn inter-series\\r\\ncorrelations and accordingly model spatial dynamics without pre-defined priors, but almost all of\\r\\nthem are designed by stacking graph networks (e.g., GCN and GAT) to capture spatial dynamics and\\r\\ntemporal networks (e.g., LSTM and GRU) to capture temporal dependencies. However, the uncertain\\r\\ncompatibility of the graph networks and the temporal networks puts extra burden on handcrafted\\r\\nmodel designs, which hinders the forecasting performance. Moreover, the respective modeling for the\\r\\ntwo networks separately learn spatial/temporal correlations, which naturally violate the real-world\\r\\nunified spatiotemporal inter-dependencies. In this paper, we explore an opposite direction of directly\\r\\napplying graph networks for forecasting and investigate an interesting question: can pure graph\\r\\nnetworks capture spatial dynamics and temporal dependencies even without temporal networks?\\r\\nTo answer this question, we rethink the MTS forecasting task from a pure graph perspective. We start\\r\\nwith building a new data structure, hypervariate graph, to represent time series with a united view\\r\\nof spatial/temporal dynamics. The core idea of the hypervariate graph is to construct a space-time\\r\\nfully-connected structure. Specifically, given a multivariate time series window (say input window)\\r\\nXt ∈ R\\r\\nN×T\\r\\nat timestamp t, where N is the number of series (variates) and T is the length of input\\r\\nwindow, we construct a corresponding hypervariate graph structure represented as G\\r\\nT\\r\\nt = (XTt\\r\\n, AT\\r\\nt\\r\\n),\\r\\nwhich is initialized as a fully-connected graph of NT nodes with adjacency matrix AT\\r\\nt ∈ R\\r\\nNT ×NT\\r\\nand node features XT\\r\\nt ∈ R\\r\\nNT ×1 by regarding each value x\\r\\n(n)\\r\\nt ∈ R\\r\\n1\\r\\n(variate n at step t) of input\\r\\nwindow as a distinct node of a hypervariate graph. Such a special structure design formulates both\\r\\nintra- and inter-series correlations of multivariate series as pure node-node dependencies in the\\r\\nhypervariate graph. Different from classic formulations that make spatial-correlated graphs and learn\\r\\ndynamics in a two-stage (spatial and temporal) process [15], our perspective views spatiotemporal\\r\\ncorrelations as a whole. It abandons the uncertain compatibility of spatial/temporal modeling,\\r\\nconstructs adaptive space-time inter-dependencies, and brings up higher-resolution fusion across\\r\\nmultiple variates and timestamps in MTS forecasting.\\r\\nThen, with such a graph structure, the multivariate forecasting can be originally formulated into the\\r\\npredictions on the hypervariate graph. However, the node number of the hypervariate graph increase\\r\\nwith the number of series (N) and the window length (T), leading to a graph of large order and\\r\\nsize. This could make classic graph networks (e.g., GCN [16], GAT [17]) computationally expensive\\r\\n(usually with quadratic complexity) and suffer from optimization difficulty in obtaining accurate node\\r\\nrepresentations [18]. To this end, we propose a novel architecture, Fourier Graph Neural Network\\r\\n(FourierGNN), for MTS forecasting from a pure graph perspective. Specifically, FourierGNN is built\\r\\nupon our proposed Fourier Graph Operator (FGO), which as a replacement of classic graph operation\\r\\nunits (e.g., convolutions), performs matrix multiplications in Fourier space of graphs. By stacking\\r\\nFGO layers in Fourier space, FourierGNN can accommodate adequate learning expressiveness and in\\r\\nthe mean time achieve much lower complexity (Log-linear complexity), which thus can effectively\\r\\nand efficiently accomplish MTS forecasting. Besides, we present theoretical analysis to demonstrate\\r\\nthat the FGO is equivalent to graph convolutions in the time domain, which further explains the\\r\\nvalidity of FourierGNN.\\r\\nFinally, we perform extensive experiments on seven real-world benchmarks. Experimental results\\r\\ndemonstrate that FourierGNN achieves an average of more than 10% improvement in accuracy\\r\\ncompared with state-of-the-art methods. In addition, FourierGNN achieves higher forecasting\\r\\nefficiency, which has about 14.6% less costs in training time and 20% less parameter volumes,\\r\\ncompared with most lightweight GNN-based forecasting methods.\\r\\n2 Related Work\\r\\nGraph Neural Networks for Multivariate Time Series Forecasting Multivariate time series\\r\\n(MTS) have embraced GNN due to their best capability of modeling structural dependencies between\\r\\nvariates [19, 2, 15, 14, 13, 20, 21]. Most of these models, such as STGCN [14], DCRNN [20],\\r\\n2\\nand TAMP-S2GCNets [13], require a pre-defined graph structure which is usually unknown in\\r\\nmost cases. For this limitation, some studies enable to automatically learn the graphs by the inter\\x02series correlations, e.g., by node similarity [22, 2, 19] or self-attention mechanism [12]. However,\\r\\nthese methods always adopt a graph network for spatial correlations and a temporal network for\\r\\ntemporal dependencies separately [22, 19, 12, 2]. For example, AGCRN [2] use a GCN [16] and a\\r\\nGRU [23], GraphWaveNet [19] use a GCN and a TCN [10], etc. In this paper, we propose an unified\\r\\nspatiotemporal formulation with pure graph networks for MTS forecasting.\\r\\nMultivariate Time Series Forecasting with Fourier Transform Recently, many MTS forecasting\\r\\nmodels have integrated the Fourier theory into deep neural networks [24, 25]. For instance, SFM [26]\\r\\ndecomposes the hidden state of LSTM into multiple frequencies by Discrete Fourier Transform (DFT).\\r\\nmWDN [27] decomposes the time series into multilevel sub-series by discrete wavelet decomposition\\r\\n(DWT) and feeds them to LSTM network. ATFN [28] proposes a Discrete Fourier Transform\\x02based block to capture dynamic and complicated periodic patterns of time series data. FEDformer\\r\\n[29] proposes Discrete Fourier Transform-based attention mechanism with low-rank approximation\\r\\nin frequency. While these models only capture temporal dependencies with Fourier Transform,\\r\\nStemGNN [12] takes the advantages of both spatial correlations and temporal dependencies in the\\r\\nspectral domain by utilizing Graph Fourier Transform (GFT) to perform graph convolutions and\\r\\nDiscrete Fourier Transform (DFT) to calculate the series relationships.\\r\\n3 Problem Definition\\r\\nGiven the multivariate time series input, i.e., the lookback window Xt = [xt−T +1, ..., xt] ∈ R\\r\\nN×T\\r\\nat timestamps t with the number of series (variates) N and the lookback window size T, where\\r\\nxt ∈ R\\r\\nN denotes the multivariate values of N series at timestamp t. Then, the multivariate time\\r\\nseries forecasting task is to predict the next τ timestamps Yt = [xt+1, ..., xt+τ ] ∈ R\\r\\nN×τ based on\\r\\nthe historical T observations Xt = [xt−T +1, ..., xt]. The forecasting process can be given by:\\r\\nYˆ\\r\\nt := Fθ(Xt) = Fθ([xt−T +1, ..., xt]) (1)\\r\\nwhere Yˆ\\r\\nt are the predictions corresponding to the ground truth Yt. The forecasting function is denoted\\r\\nas Fθ parameterized by θ. In practice, many MTS forecasting models usually leverage a graph\\r\\nnetwork (assume parameterized by θg) to learn the spatial dynamics and a temporal network (assume\\r\\nparameterized by θt) to learn the temporal dependencies, respectively [19, 12, 2, 15, 13]. Thus, the\\r\\noriginal definition of Equation (1) can be rewritten to:\\r\\nYˆ\\r\\nt := Fθg,θt\\r\\n(Xt) = Fθg,θt([xt−T +1, ..., xt]) (2)\\r\\nwhere original parameters θ are exposed to the parameters of the graph network θg and the temporal\\r\\nnetwork θt to make prediction based on the learned spatial-temporal dependencies.\\r\\n4 Methodology\\r\\nIn this section, we elaborate on our proposed framework: First, we start with our pure graph\\r\\nformulation with a novel hypervariate graph structure for MTS forecasting in Section 4.1. Then,\\r\\nwe illustrate the proposed neural architecture, Fourier Graph Neural Network (FourierGNN), for\\r\\nthis formulation in Section 4.2. Besides, we theoretically analyze FourierGNN to demonstrate its\\r\\narchitecture validity, and also conduct complexity analysis to show its efficiency. Finally, we introduce\\r\\ncertain inductive bias to instantiate FourierGNN for MTS forecasting in Section 4.3.\\r\\n4.1 The Pure Graph Formulation\\r\\nTo overcome the uncertain compatibility of the graph network and the temporal network as afore\\x02mentioned in Section 1, and learn the united spatiotemporal dynamics, we propose a pure graph\\r\\nformulation that refines Equation (2) by a novel data structure, hypervariate graph, for time series.\\r\\nDefinition 1 (Hypervariate Graph). Given a multivariate time series window as input Xt ∈ R\\r\\nN×T\\r\\nof N variates at timestamp t, we construct a hypervariate graph of NT nodes, Gt = (XG\\r\\nt\\r\\n, AG\\r\\nt\\r\\n), by\\r\\nregarding each element of Xt as one node of Gt such that XG\\r\\nt ∈ R\\r\\nNT ×1\\r\\nstands for the node feature\\r\\nand A\\r\\nG\\r\\nt ∈ R\\r\\nNT ×NT is the adjacency matrix initialized to make Gt as a fully-connected graph.\\r\\n3\\nInput Window Hypervariate Graph\\r\\n...\\r\\n...\\r\\n...\\r\\nTime\\r\\nTime\\r\\nTime\\r\\n...\\r\\n...\\r\\n...\\r\\n...\\r\\nFigure 1: Illustration of a hypervariate\\r\\ngraph with three time series. Each value\\r\\nin the input window is considered as a\\r\\nnode of the graph.\\r\\nSince the prior graph structure is usually unknown in most\\r\\nmultivariate time series scenarios [12, 2, 15], and the el\\x02ements of Xt are spatially or temporally correlated with\\r\\neach other because of time lag effect [30], we assume all\\r\\nnodes in the hypervariate graph Gt are fully-connected. The\\r\\nhypervariate graph Gt contains NT nodes representing the\\r\\nvalues of each variate at each timestamp in Xt, which can\\r\\nlearn a high-resolution representation across timestamps\\r\\nand variates (more explanations of the hypervariate graph\\r\\ncan be seen in Appendix C.1). We present an example hy\\x02pervariate graph of three time series in Figure 1. Thus, with\\r\\nsuch a data structure, we can reformulate the multivariate\\r\\ntime series forecasting task into the predictions on the hypervariate graphs, and accordingly rewrite\\r\\nEquation (2) into:\\r\\nYˆ\\r\\nt := FθG\\r\\n(XG\\r\\nt\\r\\n, AG\\r\\nt\\r\\n) (3)\\r\\nwhere θG stands for the network parameters for hypervariate graphs. With such a formulation, we can\\r\\nview the spatial dynamics and the temporal dependencies from an united perspective, which benefits\\r\\nmodeling the real-world spatiotemporal inter-dependencies.\\r\\n4.2 FourierGNN\\r\\nThough the pure graph formulation can enhance spatiotemporal modeling, the order and size of\\r\\nhypervariate graphs increase with the number of variates N and the size of window T, which\\r\\nmakes classic graph networks (e.g., GCN [16] and GAT [17]) computationally expensive (usually\\r\\nquadratic complexity) and suffer from optimization difficulty in obtaining accurate hidden node\\r\\nrepresentations [18]. In this regard, we propose an efficient and effective method, FourierGNN, for\\r\\nthe pure graph formulation. The main architecture of FourierGNN is built upon our proposed Fourier\\r\\nGraph Operator (FGO), a learnable network layer in Fourier space, which is detailed as follows.\\r\\nDefinition 2 (Fourier Graph Operator). Given a graph G = (X, A) with node features X ∈ R\\r\\nn×d\\r\\nand the adjacency matrix A ∈ R\\r\\nn×n, where n is the number of nodes and d is the number of features,\\r\\nwe introduce a weight matrix W ∈ R\\r\\nd×d\\r\\nto acquire a tailored Green’s kernel κ : [n] × [n] −→ R\\r\\nd×d\\r\\nwith κ[i, j] := Aij ◦ W and κ[i, j] = κ[i − j]. We define SA,W := F(κ) ∈ C\\r\\nn×d×d as a Fourier\\r\\nGraph Operator (FGO), where F denotes Discrete Fourier Transform (DFT).\\r\\nAccording to the convolution theorem [31] (see Appendix B), we can write the multiplication between\\r\\nF(X) and FGO SA,W in Fourier space as:\\r\\nF(X)F(κ) = F((X ∗ κ)[i]) = F(\\r\\nXn\\r\\nj=1\\r\\nX[j]κ[i − j]) = F(\\r\\nXn\\r\\nj=1\\r\\nX[j]κ[i, j]), ∀i ∈ [n] (4)\\r\\nwhere\\r\\nP\\r\\n(X ∗ κ)[i] denotes the convolution of X and κ. As defined κ[i, j] = Aij ◦ W, it yields\\r\\nn\\r\\nj=1 X[j]κ[i, j] = Pnj=1 AijX[j]W = AXW. Accordingly, we can get the convolution equation:\\r\\nF(X)SA,W = F(AXW). (5)\\r\\nIn particular, turning to our case of the fully-connected hypervariate graph, we can adopt a n-invariant\\r\\nFGO S ∈ C\\r\\nd×d\\r\\nthat has a computationally low cost compared to previous C\\r\\nn×d×d\\r\\n. We provide\\r\\nmore details and explanations in Appendix C.2.\\r\\nFrom Equation (5), we can observe that performing the multiplication between F(X) and FGO\\r\\nS in Fourier space corresponds to a graph shift operation (i.e., a graph convolution) in the time\\r\\ndomain [22]. Since the multiplications in Fourier space (O(n)) have much lower complexity than the\\r\\nabove shift operations (O(n\\r\\n2\\r\\n)) in the time domain (See Complexity Analysis below), it motivates us\\r\\nto develop a highly efficient graph neural network in Fourier space.\\r\\nTo this end, we propose the Fourier Graph Neural Networks (FourierGNN) based on FGO. Specifi\\x02cally, by stacking multiple layers of FGOs, we can define the K-layer Fourier graph neural networks\\r\\ngiven a graph G = (X, A) with node features X ∈ R\\r\\nn×d\\r\\nand the adjacency matrix A ∈ R\\r\\nn×n as:\\r\\nFourierGNN(X, A) := X\\r\\nK\\r\\nk=0\\r\\nσ(F(X)S0:k + bk), S0:k =\\r\\nY\\r\\nk\\r\\ni=0\\r\\nSi. (6)\\r\\n4\\nHerein, Sk is the FGO in the k-th layer, satisfying F(X)Sk = F(AkXWk) with Wk ∈ R\\r\\nd×d being\\r\\nthe weights and Ak ∈ R\\r\\nn×n corresponding to the k-th adjacency matrix sharing the same sparsity\\r\\npattern of A, and bk ∈ C\\r\\nd\\r\\nare the complex-valued biases parameters; F stands for Discrete Fourier\\r\\nTransform; σ is the activation function. In particular, S0, W0, A0 are the identity matrix, and we\\r\\nadopt identical activation at k = 0 to obtain residual F(X). All operations in FourierGNN are\\r\\nperformed in Fourier space. Thus, all parameters, i.e., {Sk, bk}\\r\\nK\\r\\nk=1, are complex numbers.\\r\\nThe core operation of FourierGNN is the summation of recursive multiplications with nonlinear\\r\\nactivation functions. Specifically, the recursive multiplications between F(X) and S, i.e., F(X)S0:k,\\r\\nare equivalent to the multi-order convolutions on the graph structure (see Theoretical Analysis below).\\r\\nNonlinear activation functions σ are introduced to address the capability limitations of modeling\\r\\nnonlinear information diffusion on graphs in the summation.\\r\\nTheoretical Analysis We theoretically analyze the effectiveness and interpretability of FourierGNN\\r\\nand verify the validity of its architecture. For convenience, we exclude the non-linear activation\\r\\nfunction σ and learnable bias parameters b from Equation (6), and focus on F(X)S0:k.\\r\\nProposition 1. Given a graph G = (X, A) with node features X ∈ R\\r\\nn×d and adjacency matrix\\r\\nA ∈ R\\r\\nn×n, the recursive multiplication of FGOs in Fourier space is equivalent to multi-order\\r\\nconvolutions in the time domain:\\r\\nF\\r\\n−1\\r\\n(F(X)S0:k) = Ak:0XW0:k, S0:k =\\r\\nY\\r\\nk\\r\\ni=0\\r\\nSi, Ak:0 =\\r\\nY\\r\\n0\\r\\ni=k\\r\\nAi, W0:k =\\r\\nY\\r\\nk\\r\\ni=0\\r\\nWi (7)\\r\\nwhere A0, S0, W0 are the identity matrix, Ak ∈ R\\r\\nn×n corresponds to the k-th diffusion step sharing\\r\\nthe same sparsity pattern of A, Wk ∈ R\\r\\nd×d\\r\\nis the k-th weight matrix, Sk ∈ C\\r\\nd×d\\r\\nis the k-th FGO\\r\\nsatisfying F(AkXWk) = F(X)Sk, and F and F\\r\\n−1 denote DFT and its inverse, respectively.\\r\\nIn the time domain, operation Ak:0XW0:k adopts different weights Wk ∈ R\\r\\nd×d\\r\\nto weigh the\\r\\ninformation of different neighbors in different diffusion orders, beneficial to capture the extensive\\r\\ndependencies on graphs [22, 32, 33]. This indicates FourierGNN is expressive in modeling the\\r\\ncomplex correlations among graph nodes, i.e., spatiotemporal dependencies in the hypervariate graph.\\r\\nThe proof of Proposition 1 and more explanations of FourierGNN are provided in Appendix C.3.\\r\\nComplexity Analysis The time complexity of F(X)S is O(nd log n + nd2), which includes\\r\\nthe Discrete Fourier Transform (DFT), the Inverse Discrete Fourier Transform (IDFT), and the\\r\\nmatrix multiplication in the Fourier space. Comparatively, the time complexity of the equivalent\\r\\noperations of F(X)S in the time domain, i.e., AXW, is O(n\\r\\n2d + nd2\\r\\n). Then, as a K-order\\r\\nsummation of a recursive multiplication of F(X)S, FourierGNN, achieves the time complexity of\\r\\nO(nd log n + Knd2), including DFT and IDFT, and the recursive multiplication of FGOs. Overall,\\r\\nthe Log-linear O(n log n) complexity makes FourierGNN much more efficient.\\r\\nFourierGNN vs Other Graph Networks We analyze the connection and difference between our\\r\\nFourierGNN with GCN [16] and GAT [17]. From the complexity perspective, FourierGNN with\\r\\nlog-linear complexity shows much higher efficiency than GCN and GAT. Regarding the network\\r\\narchitecture, we analyze them from two main perspectives: (1) Domain. GAT implements operations\\r\\nin the time domain, while GCN and FourierGNN are in Fourier space. However, GCN achieves\\r\\nthe transformation through the Graph Fourier Transform (GFT), whereas FourierGNN utilizes the\\r\\nDiscrete Fourier Transform (DFT). (2) Information diffusion: GAT aggregates neighbor nodes with\\r\\nvarying weights to via attention mechanisms. FourierGNN and GCN update node information\\r\\nvia convoluting neighbor nodes. Different from GCN, FourierGNN assigns varying importance to\\r\\nneighbor nodes in different diffusion steps. We provide a detailed comparison in Appendix D.\\r\\n4.3 Multivariate Time Series Forecasting with FourierGNN\\r\\nIn this section, we instantiate FourierGNN for MTS forecasting. The overall architecture of our model\\r\\nis illustrated in Figure 2. Given the MTS input data Xt ∈ R\\r\\nN×T\\r\\n, first we construct a fully-connected\\r\\nhypervariate graph Gt = (XG\\r\\nt\\r\\n, AG\\r\\nt\\r\\n) with XG\\r\\nt ∈ R\\r\\nNT ×1\\r\\nand A\\r\\nG\\r\\nt ∈ {1}\\r\\nn×n. Then, we project XG\\r\\nt\\r\\ninto node embeddings XG\\r\\nt ∈ R\\r\\nNT ×d by assigning a d-dimension vector for each node using an\\r\\nembedding matrix Eϕ ∈ R\\r\\n1×d\\r\\n, i.e., XG\\r\\nt = XGt × Eϕ.\\r\\n5\\nNode\\r\\nEmbedding\\r\\nDFT\\r\\nFourier Space\\r\\nIDFT\\r\\nFFN\\r\\nFGO … FGO +\\r\\nOutput FGO\\r\\nTime Domain Time Domain\\r\\nFourier Graph Neural Network\\r\\nHypervariate Graph\\r\\nY\\u0de1t\\r\\n𝜎 𝜎 𝜎 𝜎\\r\\n...\\r\\n...\\r\\n...\\r\\n𝒮1 ෑ\\r\\n𝑖=1\\r\\n𝐾\\r\\n𝒮𝑖𝒳\\r\\nAt\\r\\n𝓖 𝘹t\\r\\n𝓖 𝓖 = ( , ) \\r\\nt\\r\\n𝓖 𝒳\\r\\nt\\r\\n𝓖 𝗬\\r\\nt\\r\\n𝒮2 𝒮𝐾\\r\\nt\\r\\n𝓖 𝒴\\r\\nAt\\r\\n𝓖\\r\\nt\\r\\n𝓖 𝐗\\r\\nFigure 2: The network architecture of MTS forecasting with FourierGNN (blue characters denote\\r\\ncomplex values, such as X\\r\\nG\\r\\nt Si). Given the hypervariate graph G = (XGt\\r\\n, AG\\r\\nt\\r\\n), we 1) embed nodes\\r\\nof XG\\r\\nt ∈ R\\r\\nNT ×1\\r\\nto obtain node embeddings XG\\r\\nt ∈ R\\r\\nNT ×d\\r\\n; 2) feed embedded hypervariate graphs\\r\\nto FourierGNN: (i) transform XG\\r\\nt with DFT to X\\r\\nG\\r\\nt ∈ C\\r\\nNT ×d\\r\\n; (ii) conduct recursive multiplications\\r\\nand make summation to output Y\\r\\nG\\r\\nt\\r\\n; (iii) transform Y\\r\\nG\\r\\nt back to time domain by IDFT, resulting in\\r\\nYG\\r\\nt ∈ R\\r\\nNT ×d\\r\\n; 3) generate τ -step predictions Yˆ\\r\\nt ∈ R\\r\\nN×τ via feeding YG\\r\\nt\\r\\nto fully-connected layers.\\r\\nSubsequently, to capture the spatiotemporal dependencies simultaneously, we aim to feed multiple\\r\\nembeded hypervariate graphs with XG\\r\\nt\\r\\nto FourierGNN. First, we perform Discrete Fourier Transform\\r\\n(DFT) F on each discrete spatio-temporal dimension of the embeddings XG\\r\\nt\\r\\nand obtain the frequency\\r\\noutput X\\r\\nG\\r\\nt\\r\\n:= F(XG\\r\\nt\\r\\n) ∈ C\\r\\nNT ×d\\r\\n. Then, we perform a recursive multiplication between X\\r\\nG\\r\\nt\\r\\nand\\r\\nFGOs S0:k in Fourier space and output the resulting representations Y\\r\\nG\\r\\nt\\r\\nas:\\r\\nY\\r\\nG\\r\\nt = FourierGNN(X\\r\\nG\\r\\nt\\r\\n, AG\\r\\nt\\r\\n) = X\\r\\nK\\r\\nk=0\\r\\nσ(F(XG\\r\\nt\\r\\n)S0:k + bk), S0:k =\\r\\nY\\r\\nk\\r\\ni=0\\r\\nSi. (8)\\r\\nThen Y\\r\\nG\\r\\nt\\r\\nare transformed back to the time domain using Inverse Discrete Fourier Transform (IDFT)\\r\\nF\\r\\n−1\\r\\n, which yields YG\\r\\nt\\r\\n:= F\\r\\n−1\\r\\n(Y\\r\\nG\\r\\nt\\r\\n) ∈ R\\r\\nNT ×d\\r\\n.\\r\\nFinally, according to the FourierGNN output YG\\r\\nt which encodes spatiotemporal inter-dependencies,\\r\\nwe use two layer feed-forward networks (FFN) (see Appendix E.4 for more details) to project it onto\\r\\nτ future steps, resulting in Yˆ\\r\\nt = FFN(YG\\r\\nt\\r\\n) ∈ R\\r\\nN×τ\\r\\n.\\r\\n5 Experiments\\r\\nTo evaluate the performance of FourierGNN, we conduct extensive experiments on seven real-world\\r\\ntime series benchmarks to compare with state-of-the-art graph neural network-based methods.\\r\\n5.1 Experimental Setup\\r\\nDatasets. We evaluate our proposed method on seven representative datasets from various application\\r\\nscenarios, including traffic, energy, web traffic, electrocardiogram, and COVID-19. All datasets\\r\\nare normalized using the min-max normalization. Except the COVID-19 dataset, we split the other\\r\\ndatasets into training, validation, and test sets with the ratio of 7:2:1 in a chronological order. For the\\r\\nCOVID-19 dataset, the ratio is 6:2:2. More detailed information about datasets are in Appendix E.1.\\r\\nBaselines. We conduct a comprehensive comparison of the forecasting performance between our\\r\\nFourierGNN and several representative and state-of-the-art (SOTA) models on the seven datasets,\\r\\nincluding classic method VAR [34], deep learning-based models such as SFM [26], LSTNet [35],\\r\\nTCN [10], DeepGLO [36], and CoST [38]. We also compare FourierGNN against GNN-based models\\r\\nlike GraphWaveNet [19], StemGNN [12], MTGNN [15], and AGCRN [2], and two representative\\r\\nTransformer-based models like Reformer [37] and Informer [11], as well as two frequency enhanced\\r\\nTransformer-based models including Autoformer [5] and FEDformer [29]. In addition, we compare\\r\\nFourierGNN with SOTA models such as TAMP-S2GCNets [13], DCRNN [20], and STGCN [1],\\r\\nwhich require pre-defined graph structures. Please refer to Appendix E.2 for more implementation\\r\\ndetails of the adopted baselines.\\r\\nExperimental Settings. All experiments are conducted in Python using Pytorch 1.8 [39] (except for\\r\\nSFM [26] which uses Keras) and performed on single NVIDIA RTX 3080 10G GPU. Our model\\r\\nis trained using RMSProp with a learning rate of 10−5and MSE (Mean Squared Error) as the loss\\r\\nfunction. The best parameters for all comparative models are chosen through careful parameter tuning\\r\\non the validation set. We use Mean Absolute Errors (MAE), Root Mean Squared Errors (RMSE), and\\r\\n6\\nTable 1: Overall performance of forecasting models on the six datasets.\\r\\nModels\\r\\nDatasets Solar Wiki Traffic\\r\\nMAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%)\\r\\nVAR [34] 0.184 0.234 577.10 0.057 0.094 96.58 0.535 1.133 550.12\\r\\nSFM [26] 0.161 0.283 362.89 0.081 0.156 104.47 0.029 0.044 59.33\\r\\nLSTNet [35] 0.148 0.200 132.95 0.054 0.090 118.24 0.026 0.057 25.77\\r\\nTCN [10] 0.176 0.222 142.23 0.094 0.142 99.66 0.052 0.067 -\\r\\nDeepGLO [36] 0.178 0.400 346.78 0.110 0.113 119.60 0.025 0.037 33.32\\r\\nReformer [37] 0.234 0.292 128.58 0.048 0.085 73.61 0.029 0.042 112.58\\r\\nInformer [11] 0.151 0.199 128.45 0.051 0.086 80.50 0.020 0.033 59.34\\r\\nAutoformer [5] 0.150 0.193 103.79 0.069 0.103 121.90 0.029 0.043 100.02\\r\\nFEDformer [29] 0.139 0.182 100.92 0.068 0.098 123.10 0.025 0.038 85.12\\r\\nGraphWaveNet [19] 0.183 0.238 603 0.061 0.105 136.12 0.013 0.034 33.78\\r\\nStemGNN [12] 0.176 0.222 128.39 0.190 0.255 117.92 0.080 0.135 64.51\\r\\nMTGNN [15] 0.151 0.207 507.91 0.101 0.140 122.96 0.013 0.030 29.53\\r\\nAGCRN [2] 0.123 0.214 353.03 0.044 0.079 78.52 0.084 0.166 31.73\\r\\nFourierGNN 0.120 0.162 116.48 0.041 0.076 64.50 0.011 0.023 28.71\\r\\nModels\\r\\nDatasets ECG Electricity COVID-19\\r\\nMAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%)\\r\\nVAR [34] 0.120 0.170 22.56 0.101 0.163 43.11 0.226 0.326 191.95\\r\\nSFM [26] 0.095 0.135 24.20 0.086 0.129 33.71 0.205 0.308 76.08\\r\\nLSTNet [35] 0.079 0.115 18.68 0.075 0.138 29.95 0.248 0.305 89.04\\r\\nTCN [10] 0.078 0.107 17.59 0.057 0.083 26.64 0.317 0.354 151.78\\r\\nDeepGLO [36] 0.110 0.163 43.90 0.090 0.131 29.40 0.169 0.253 75.19\\r\\nReformer [37] 0.062 0.090 13.58 0.078 0.129 33.37 0.152 0.209 132.78\\r\\nInformer [11] 0.056 0.085 11.99 0.070 0.119 32.66 0.200 0.259 155.55\\r\\nAutoformer [5] 0.055 0.081 11.37 0.056 0.083 25.94 0.159 0.211 136.24\\r\\nFEDformer [29] 0.055 0.080 11.16 0.055 0.081 25.84 0.160 0.219 134.45\\r\\nGraphWaveNet [19] 0.093 0.142 40.19 0.094 0.140 37.01 0.201 0.255 100.83\\r\\nStemGNN [12] 0.100 0.130 29.62 0.070 0.101 - 0.421 0.508 141.01\\r\\nMTGNN [15] 0.090 0.139 35.04 0.077 0.113 29.77 0.394 0.488 88.13\\r\\nAGCRN [2] 0.055 0.080 11.75 0.074 0.116 26.08 0.254 0.309 83.37\\r\\nFourierGNN 0.052 0.078 10.97 0.051 0.077 24.28 0.123 0.168 71.52\\r\\nTable 2: Performance comparison under different prediction lengths on the COVID-19 dataset.\\r\\nLength 3 6 9 12\\r\\nMetrics MAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%)\\r\\nGraphWaveNet [19] 0.092 0.129 53.00 0.133 0.179 65.11 0.171 0.225 80.91 0.201 0.255 100.83\\r\\nStemGNN [12] 0.247 0.318 99.98 0.344 0.429 125.81 0.359 0.442 131.14 0.421 0.508 141.01\\r\\nAGCRN [2] 0.130 0.172 76.73 0.171 0.218 79.07 0.224 0.277 82.90 0.254 0.309 83.37\\r\\nMTGNN [15] 0.276 0.379 91.42 0.446 0.513 133.49 0.484 0.548 139.52 0.394 0.488 88.13\\r\\nTAMP-S2GCNets [13] 0.140 0.190 50.01 0.150 0.200 55.72 0.170 0.230 71.78 0.180 0.230 65.76\\r\\nCoST [38] 0.122 0.246 68.74 0.157 0.318 72.84 0.183 0.364 77.04 0.202 0.377 80.81\\r\\nFourierGNN(ours) 0.071 0.103 61.02 0.093 0.131 65.72 0.109 0.148 69.59 0.123 0.168 71.52\\r\\nMean Absolute Percentage Error (MAPE) to measure the performance. The evaluation details are in\\r\\nAppendix E.3 and more experimental settings are in Appendix E.4.\\r\\n5.2 Main Results\\r\\nWe present the evaluation results with an input length of 12 and a prediction length of 12 in Table\\r\\n1. Overall, FourierGNN achieves a new state-of-the-art on all datasets. On average, FourierGNN\\r\\nmakes an improvement of 9.4% in MAE and 10.9% in RMSE compared to the best-performing\\r\\nacross all datasets. Among these baselines, Reformer, Informer, Autoformer, and FEDformer are\\r\\nTransformer-based models that demonstrate competitive performance on Electricity and COVID\\x0219 datasets, as they excel at capturing temporal dependencies. However, they have limitations in\\r\\ncapturing the spatial dependencies explicitly. GraphWaveNet, MTGNN, StemGNN, and AGCRN are\\r\\nGNN-based models that show promising results on Wiki, Traffic, Solar, and ECG datasets, primarily\\r\\ndue to their capability to handle spatial dependencies among variates. However, they are limited in\\r\\ntheir capacity to simultaneously capture spatiotemporal dependencies. FourierGNN outperforms the\\r\\nbaseline models since it can learn comprehensive spatiotemporal dependencies simultaneously and\\r\\nattends to time-varying dependencies among variates.\\r\\nMulti-Step Forecasting To further evaluate the performance in multi-step forecasting, we com\\x02pare FourierGNN with other GNN-based MTS models (including StemGNN [12], AGCRN [2],\\r\\n7\\nGraphWaveNet [19], MTGNN [15], and TAMP-S2GCNets [13]) and a representation learning model\\r\\n(CoST [38]) on COVID-19 dataset under different prediction lengths, and the results are shown in\\r\\nTable 2. It shows that FourierGNN achieves an average 30.1% and 30.2% improvement on MAE\\r\\nand RMSE respectively over the best baseline. In Appendix F, we include more experiments and\\r\\nanalysis under different prediction lengths, and further compare FourierGNN with models that require\\r\\npre-defined graph structures.\\r\\n5.3 Model Analysis\\r\\nEfficiency Analysis We investigate the parameter volumes and training time costs of FourierGNN,\\r\\nStemGNN [12], AGCRN [2], GraphWaveNet [19], and MTGNN [15] on two representative datasets,\\r\\nincluding the Wiki dataset and the Traffic dataset. The results are reported in Table 3, showing the\\r\\ncomparison of parameter volumes and average time costs over five rounds of experiments. In terms\\r\\nof parameters, FourierGNN exhibits the lowest volume of parameters among the comparative models.\\r\\nSpecifically, it achieves a reduction of 32.2% and 9.5% in parameters compared to GraphWaveNet\\r\\non Traffic and Wiki datasets, respectively. This reduction is mainly attributed that FourierGNN\\r\\nhas shared scale-free parameters for each node. Regarding training time, FourierGNN runs much\\r\\nfaster than all baseline models, and it demonstrates efficiency improvements of 5.8% and 23.3%\\r\\nover the fast baseline GraphWaveNet on Traffic and Wiki datasets, respectively. Considering variate\\r\\nnumber of Wiki dataset is about twice larger than that of Traffic dataset, FourierGNN exhibits larger\\r\\nefficiency superiority with the baselines. These findings highlight the high efficiency of FourierGNN\\r\\nin computing graph operations and its scalability to large datasets with extensive graphs, which is\\r\\nimportant for the pure graph formulation due to the larger size of hypervariate graphs with NT nodes.\\r\\nTable 3: Comparisons of parameter volumes and training time costs on datasets Traffic and Wiki.\\r\\nTraffic Wiki\\r\\nModels Parameters Training (s/epoch) Parameters Training (s/epoch)\\r\\nStemGNN 1, 606, 140 185.86±2.22 4, 102, 406 92.95±1.39\\r\\nMTGNN 707, 516 169.34±1.56 1, 533, 436 28.69±0.83\\r\\nAGCRN 749, 940 113.46±1.91 755, 740 22.48±1.01\\r\\nGraphWaveNet 280, 860 105.38±1.24 292, 460 21.23±0.76\\r\\nFourierGNN 190, 564 99.25±1.07 264, 804 16.28±0.48\\r\\nAblation Study We perform an ablation study on the METR-LA dataset to assess the individual\\r\\ncontributions of different components in FourierGNN. The results, presented in Table 4, validate\\r\\nthe effectiveness of each component. Specifically, w/o Embedding emphasizes the significance of\\r\\nperforming node embedding to improve model generalization. w/o Dynamic FGO using the same\\r\\nFGO verifies the effectiveness of applying different FGOs in capturing time-varying dependencies.\\r\\nIn addition, w/o Residual represents FourierGNN without the K = 0 layer, while w/o Summation\\r\\nadopts the last order (layer) output, i.e., X S0:K, as the output of FourierGNN. These results demon\\x02strate the importance of high-order diffusion and the contribution of multi-order diffusion. More\\r\\nresults and analysis of the ablation study are provided in Appendix G.3.\\r\\nTable 4: Ablation study on METR-LA dataset.\\r\\nmetrics w/o Embedding w/o Dynamic FGO w/o Residual w/o Summation FourierGNN\\r\\nMAE 0.053 0.055 0.054 0.054 0.050\\r\\nRMSE 0.116 0.114 0.115 0.114 0.113\\r\\nMAPE(%) 86.73 86.69 86.75 86.62 86.30\\r\\n5.4 Visualization\\r\\nTo gain a better understanding of the hypervariate graph and FourierGNN in spatiotemporal modeling\\r\\nfor MTS forecasting, We conduct visualization experiments on the METR-LA and COVID-19\\r\\ndatasets. Please refer to Appendix E.5 for more information on the visualization techniques used.\\r\\nVisualization of temporal representations learned by FourierGNN In order to showcase the\\r\\ntemporal dependencies learning capability of FourierGNN, we visualize the temporal adjacency\\r\\nmatrix of different variates. Specifically, we randomly select 8 counties from the COVID-19 dataset\\r\\nand calculate the relations of 12 consecutive time steps for each county. Then, we visualize the\\r\\n8\\n(a) N=7 (b) N=18 (c) N=24 (d) N=29\\r\\n(e) N=38 (f) N=45 (g) N=46 (h) N=55\\r\\nFigure 3: The temporal adjacency matrix of eight variates on COVID-19 dataset.\\r\\nadjacency matrix by a heatmap, and the results are illustrated in Figure 3, where N denotes the index\\r\\nof the country (variate). It shows that FourierGNN learns distinct temporal patterns for each county,\\r\\nindicating that the hypervariate graph can encode rich and discriminative temporal dependencies.\\r\\nVisualization of spatial representations learned by FourierGNN To investigate the spatial corre\\x02lations learning capability of FourierGNN, we visualize the generated adjacency matrix based on the\\r\\nrepresentations learned by FourierGNN on the METR-LA dataset. Specifically, we randomly select\\r\\n20 detectors and visualize their corresponding adjacency matrix via a heatmap, as depicted in Figure 4.\\r\\nFigure 4: The adjacency matrix (right) learned by Fouri\\x02erGNN and the corresponding road map (left).\\r\\nBy examining the adjacency matrix in con\\x02junction with the actual road map, we ob\\x02serve: 1) the detectors (7, 8, 9, 11, 13,\\r\\n18) are very close w.r.t. the physical dis\\x02tance, corresponding to the high values of\\r\\ntheir correlations with each other in the\\r\\nheatmap; 2) the detectors 4, 14 and 16\\r\\nhave small overall correlation values since\\r\\nthey are far from other detectors; 3) how\\x02ever, compared with detectors 14 and 16,\\r\\nthe detector 4 has slightly higher correla\\x02tion values to other detectors, e.g., 7, 8,\\r\\n9, which is because although they are far\\r\\napart, the detectors 4, 7, 8, 9 are on the same road. The results verify that the hypervariate graph\\r\\nstructure can represent highly interpretative correlations.\\r\\nMoreover, to gain a understanding of how FGO works, we visualize the output of each layer of\\r\\nFourierGNN, and the visualization results demonstrate that FGO can adaptively and effectively\\r\\ncapture important patterns while removing noises to a learn discriminative model. Further details\\r\\ncan be found in Appendix H.1. Additionally, to investigate the ability of FourierGNN to capture\\r\\ntime-varying dependencies among variates, we further visualize the spatial correlations at different\\r\\ntimestamps. The results illustrate that FourierGNN can effectively attend to the temporal variability\\r\\nin the data. For more information, please refer to Appendix H.2.\\r\\n6 Conclusion Remarks\\r\\nIn this paper, we explore an interesting direction of directly applying graph networks for MTS\\r\\nforecasting from a pure graph perspective. To overcome the previous separate spatial and temporal\\r\\nmodeling problem, we build a hypervariate graph, regarding each series value as a graph node, which\\r\\nconsiders spatiotemporal dynamics unitedly. Then, we formulate time series forecasting on the\\r\\nhypervariate graph and propose FourierGNN by stacking Fourier Graph Operator (FGO) to perform\\r\\nmatrix multiplications in Fourier space, which can accommodate adequate learning expressiveness\\r\\nwith much lower complexity. Extensive experiments demonstrate that FourierGNN achieves state\\x02of-the-art performances with higher efficiency and fewer parameters, and the hypervariate graph\\r\\nstructure exhibits strong capabilities to encode spatiotemporal inter-dependencies.\\r\\n9\\nAcknowledgments and Disclosure of Funding\\r\\nThe work was supported in part by the National Key Research and Development Program of China\\r\\nunder Grant 2020AAA0104903 and 2019YFB1406300, and National Natural Science Foundation of\\r\\nChina under Grant 62072039 and 62272048.\\r\\nReferences\\r\\n[1] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A\\r\\ndeep learning framework for traffic forecasting. In IJCAI, pages 3634–3640, 2018.\\r\\n[2] Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional\\r\\nrecurrent network for traffic forecasting. In NeurIPS, 2020.\\r\\n[3] Hui He, Qi Zhang, Shoujin Wang, Kun Yi, Zhendong Niu, and Longbing Cao. Learning\\r\\ninformative representation for fairness-aware multivariate time-series forecasting: A group\\x02based perspective. IEEE Transactions on Knowledge and Data Engineering, pages 1–13,\\r\\n2023.\\r\\n[4] Yu Zheng, Xiuwen Yi, Ming Li, Ruiyuan Li, Zhangqing Shan, Eric Chang, and Tianrui Li.\\r\\nForecasting fine-grained air quality based on big data. In KDD, pages 2267–2276, 2015.\\r\\n[5] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition\\r\\ntransformers with auto-correlation for long-term series forecasting. In NeurIPS, pages 22419–\\r\\n22430, 2021.\\r\\n[6] Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou, and Yanjie Fu.\\r\\nDish-ts: A general paradigm for alleviating distribution shift in time series forecasting. In AAAI,\\r\\npages 7522–7529. AAAI Press, 2023.\\r\\n[7] Hui He, Qi Zhang, Simeng Bai, Kun Yi, and Zhendong Niu. CATN: cross attentive tree-aware\\r\\nnetwork for multivariate time series forecasting. In AAAI, pages 4030–4038. AAAI Press, 2022.\\r\\n[8] Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan Liu. DEPTS:\\r\\ndeep expansion learning for periodic time series forecasting. In ICLR. OpenReview.net, 2022.\\r\\n[9] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic\\r\\nforecasting with autoregressive recurrent networks. International Journal of Forecasting,\\r\\n36(3):1181–1191, 2020.\\r\\n[10] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolu\\x02tional and recurrent networks for sequence modeling. CoRR, abs/1803.01271, 2018.\\r\\n[11] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\\r\\nZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In\\r\\nAAAI, pages 11106–11115, 2021.\\r\\n[12] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong,\\r\\nBixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. Spectral temporal graph neural network for\\r\\nmultivariate time-series forecasting. In NeurIPS, 2020.\\r\\n[13] Yuzhou Chen, Ignacio Segovia-Dominguez, Baris Coskunuzer, and Yulia Gel. TAMP-s2GCNets:\\r\\nCoupling time-aware multipersistence knowledge representation with spatio-supra graph con\\x02volutional networks for time-series forecasting. In International Conference on Learning\\r\\nRepresentations, 2022.\\r\\n[14] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A\\r\\ndeep learning framework for traffic forecasting. In IJCAI, pages 3634–3640, 2018.\\r\\n[15] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang.\\r\\nConnecting the dots: Multivariate time series forecasting with graph neural networks. In KDD,\\r\\npages 753–763, 2020.\\r\\n[16] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional\\r\\nnetworks. In ICLR (Poster). OpenReview.net, 2017.\\r\\n[17] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua\\r\\nBengio. Graph attention networks. CoRR, abs/1710.10903, 2017.\\r\\n10\\n[18] Kate Smith-Miles and Leo Lopes. Measuring instance difficulty for combinatorial optimization\\r\\nproblems. Comput. Oper. Res., 39(5):875–889, 2012.\\r\\n[19] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. Graph wavenet for\\r\\ndeep spatial-temporal graph modeling. In IJCAI, pages 1907–1913, 2019.\\r\\n[20] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural\\r\\nnetwork: Data-driven traffic forecasting. In ICLR (Poster), 2018.\\r\\n[21] Leyan Deng, Defu Lian, Zhenya Huang, and Enhong Chen. Graph convolutional adversarial\\r\\nnetworks for spatiotemporal anomaly detection. IEEE Transactions on Neural Networks and\\r\\nLearning Systems, 33(6):2416–2428, 2022.\\r\\n[22] Gonzalo Mateos, Santiago Segarra, Antonio G. Marques, and Alejandro Ribeiro. Connecting\\r\\nthe dots: Identifying network structure via graph signal processing. IEEE Signal Process. Mag.,\\r\\n36(3):16–43, 2019.\\r\\n[23] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares,\\r\\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder\\x02decoder for statistical machine translation. In EMNLP, pages 1724–1734. ACL, 2014.\\r\\n[24] Kun Yi, Qi Zhang, Longbing Cao, Shoujin Wang, Guodong Long, Liang Hu, Hui He, Zhendong\\r\\nNiu, Wei Fan, and Hui Xiong. A survey on deep learning based time series analysis with\\r\\nfrequency transformation. CoRR, abs/2302.02173, 2023.\\r\\n[25] Tian Zhou, Ziqing Ma, Xue Wang, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, and Rong\\r\\nJin. Film: Frequency improved legendre memory model for long-term time series forecasting.\\r\\n2022.\\r\\n[26] Liheng Zhang, Charu C. Aggarwal, and Guo-Jun Qi. Stock price prediction via discovering\\r\\nmulti-frequency trading patterns. In KDD, pages 2141–2149, 2017.\\r\\n[27] Jingyuan Wang, Ze Wang, Jianfeng Li, and Junjie Wu. Multilevel wavelet decomposition\\r\\nnetwork for interpretable time series analysis. In KDD, pages 2437–2446, 2018.\\r\\n[28] Zhangjing Yang, Weiwu Yan, Xiaolin Huang, and Lin Mei. Adaptive temporal-frequency\\r\\nnetwork for time-series forecasting. IEEE Trans. Knowl. Data Eng., 34(4):1576–1587, 2022.\\r\\n[29] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer:\\r\\nFrequency enhanced decomposed transformer for long-term series forecasting. In ICML, 2022.\\r\\n[30] William W. S. Wei. Time series analysis - univariate and multivariate methods. Addison-Wesley,\\r\\n1989.\\r\\n[31] Y. Katznelson. An introduction to harmonic analysis. Cambridge University Press,, 1970.\\r\\n[32] Santiago Segarra, Gonzalo Mateos, Antonio G. Marques, and Alejandro Ribeiro. Blind identifi\\x02cation of graph filters. IEEE Trans. Signal Process., 65(5):1146–1159, 2017.\\r\\n[33] Elvin Isufi, Fernando Gama, and Alejandro Ribeiro. Edgenets: Edge varying graph neural\\r\\nnetworks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\r\\n[34] Mark W. Watson. Vector autoregressions and cointegration. Working Paper Series, Macroeco\\x02nomic Issues, 4, 1993.\\r\\n[35] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and short-term\\r\\ntemporal patterns with deep neural networks. In SIGIR, pages 95–104, 2018.\\r\\n[36] Rajat Sen, Hsiang-Fu Yu, and Inderjit S. Dhillon. Think globally, act locally: A deep neural\\r\\nnetwork approach to high-dimensional time series forecasting. In NeurIPS, pages 4838–4847,\\r\\n2019.\\r\\n[37] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In\\r\\nICLR, 2020.\\r\\n[38] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Cost:\\r\\nContrastive learning of disentangled seasonal-trend representations for time series forecasting.\\r\\nIn ICLR. OpenReview.net, 2022.\\r\\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\r\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\\r\\nKöpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\\r\\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\\r\\nhigh-performance deep learning library. In NeurIPS, pages 8024–8035, 2019.\\r\\n11\\n[40] Hoang Anh Dau, Anthony J. Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu,\\r\\nShaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn J. Keogh. The UCR time\\r\\nseries archive. IEEE CAA J. Autom. Sinica, 6(6):1293–1305, 2019.\\r\\n12\\nA Notation\\r\\nTable 5: Notations\\r\\nXt multivariate time series input at timestamps t, X ∈ R\\r\\nN×T\\r\\nxt the multivariate values of N series at timestamp t, xt ∈ R\\r\\nN\\r\\nYt the next τ timestamps of multivariate time series, Yt ∈ R\\r\\nN×τ\\r\\nYˆ\\r\\nt the prediction values of multivariate time series for next τ timestamps, Yˆt ∈ R\\r\\nN×τ\\r\\nN the number of series\\r\\nT the lookback window size\\r\\nτ the prediction length of multivariate time series forecasting\\r\\nGt the hypervariate graph, Gt = {XG\\r\\nt\\r\\n, AG\\r\\nt } attributed to XG\\r\\nt\\r\\nXG\\r\\nt\\r\\nthe nodes of the hypervariate graph, XG\\r\\nt ∈ R\\r\\nNT ×1\\r\\nA\\r\\nG\\r\\nt\\r\\nthe adjacency matrix of Gt, A\\r\\nG\\r\\nt ∈ R\\r\\nNT ×NT\\r\\nS the Fourier Graph Operator\\r\\nd the embedding dimension\\r\\nXG\\r\\nt\\r\\nthe embedding of XG\\r\\nt\\r\\n, XG\\r\\nt ∈ R\\r\\nNT ×d\\r\\nX\\r\\nG\\r\\nt\\r\\nthe spectrum of XG\\r\\nt\\r\\n, X\\r\\nG\\r\\nt ∈ C\\r\\nNT ×d\\r\\nY\\r\\nG\\r\\nt\\r\\nthe output of FourierGNN, Y\\r\\nG\\r\\nt ∈ C\\r\\nNT ×d\\r\\nθg the parameters of the graph network\\r\\nθt the parameters of the temporal network\\r\\nθG the network parameters for hypervariate graphs\\r\\nEϕ the embedding matrix, Eϕ ∈ R\\r\\n1×d\\r\\nκ the kernel function\\r\\nW the weight matrix\\r\\nb the complex bias weights\\r\\nF Discrete Fourier Transform\\r\\nF\\r\\n−1\\r\\nInverse Discrete Fourier Transform\\r\\nF the forecasting model\\r\\nB Convolution Theorem\\r\\nThe convolution theorem [31] is one of the most important properties of the Fourier transform. It\\r\\nstates the Fourier transform of a convolution of two signals equals the pointwise product of their\\r\\nFourier transforms in the frequency domain. Given a signal x[n] and a filter h[n], the convolution\\r\\ntheorem can be defined as follows:\\r\\nF((x ∗ h)[n]) = F(x)F(h) (9)\\r\\nwhere (x ∗ h)[n] = PN−1\\r\\nm=0 h[m]x[(n − m)N ] denotes the convolution of x and h, (n − m)N\\r\\ndenotes (n − m) modulo N, and F(x) and F(h) denote discrete Fourier transform of x[n] and h[n],\\r\\nrespectively.\\r\\n13\\nC Explanations and Proofs\\r\\nC.1 The Explanations of the Hypervariate Graph Structure\\r\\nNote that the time lag effect between time-series variables is a common phenomenon in real-world\\r\\nmultivariate time series scenarios, for example, the time lag influence between two financial assets\\r\\n(e.g. dollar and gold) of a portfolio. It is beneficial but challenging to consider dependencies between\\r\\ndifferent variables under different timestamps.\\r\\nThe hypervariate graph connecting any two variables at any two timestamps aims to encode high\\x02resolution spatiotemporal dependencies. It embodies not only the intra-series temporal dependencies\\r\\n(node connections of each individual variable), inter-series spatial dependencies (node connections\\r\\nunder each single time step), and also the time-varying spatiotemporal dependencies (node con\\x02nections between different variables at different time steps). By leveraging the hypervariate graph\\r\\nstructure, we can effectively learn the spatial and temporal dependencies. This approach is distinct\\r\\nfrom previous methods that represent the spatial and temporal dependencies separately using two\\r\\nnetwork structures.\\r\\nC.2 The Interpretation of n-invariant FGO\\r\\nWhy F(κ) ∈ C\\r\\nn×d×d? From Definition 2, we know that the kernel κ is defined as a matrix\\x02valued projection, i.e., κ : [n] × [n] −→ R\\r\\nd×d\\r\\n. Note that we assume κ is in the special case of the\\r\\nGreen’s kernel, i.e., a translation-invariant kernel κ[i, j] = κ[i − j]. Accordingly, κ can be reduced:\\r\\nκ : [n] −→ R\\r\\nd×d where we can parameterize F(κ) with a complex-valued matrix Cn×d×d\\r\\n.\\r\\nWhat is n-invariant FGO? Turning to our case of the fully-connected hypervariate graph, we can\\r\\nconsider a special case of κ, i.e., a space-invariant kernel κ[i, j] = κ[ϱ] with ϱ being a constant scalar.\\r\\nAccordingly, we can parameterize FGO S with a n-invariant complex-valued matrix C\\r\\nd×d\\r\\n.\\r\\nThe interpretation of n-invariant FGO. An n-invariant FGO is similar to a shared-weight convo\\x02lution kernel or filter of CNNs that slide along ([n] × [n]) input features, which effectively reduces\\r\\nparameter volumes and saves computation costs. Note that although we adopt the same transforma\\x02tion (i.e., the n-invariant FGO) over NT frequency points, we embed the raw MTS inputs in the\\r\\nd-dimension distributive space beforehand and then perform FourierGNN over MTS embeddings,\\r\\nwhich can be analogized as d convolution kernels/filters in each convolutional layer in CNNs. This\\r\\ncan ensure FourierGNN is able to learn informative features/patterns to improve its model capacity\\r\\n(See the following analysis of the effectiveness of n-invariant FGO).\\r\\nThe effectiveness of n-invariant FGO. In addition, the n-invariant parameterized FGO is empirically\\r\\nproven effective to improve model generalization and achieve superior forecasting performance (See\\r\\nthe ablation study in Section 5.3 for more details). Although parameterizing F(κ) ∈ C\\r\\nn×d×d\\r\\n(i.e., an n-variant FGO) may be more powerful and flexible than the n-invariant FGO in terms of\\r\\nforecasting performance, it introduces much more parameters and training time costs, especially in\\r\\ncase of multi-layer FourierGNN, and may obtain inferior performance due to inadequate training\\r\\nor overfitting. As indicated in Table 6, the FourierGNN with the n-invariant FGO achieves slightly\\r\\nbetter performance than that with the n-variant FGO on ECG and COVID-19, respectively. Notably,\\r\\nthe FourierGNN with the n-variant FGO introduces a much larger parameter volume proportional\\r\\nto n and requires significantly more training time. In contrast, n-invariant FGO is n-agnostic and\\r\\nlightweight, which is a more wise and efficient alternative. These results confirm our design and\\r\\nverify the effectiveness and applicability of n-invariant FGO.\\r\\nTable 6: Comparison between FourierGNN models with n-invariant FGO and n-variant FGO on the\\r\\nECG and COVID-19 datasets.\\r\\nDatasets Models Parameters (M) Training (s/epoch) MAE RMSE MAPE (%)\\r\\nECG n-invariant 0.18 12.45 0.052 0.078 10.97\\r\\nn-variant 82.96 104.06 0.053 0.078 11.05\\r\\nCOVID-19 n-invariant 1.06 0.62 0.123 0.168 71.52\\r\\nn-variant 130.99 7.46 0.129 0.174 72.12\\r\\n14\\nC.3 Proof of Proposition 1 and Interpretation of FourierGNN\\r\\nProposition 1. Given a graph G = (X, A) with node features X ∈ R\\r\\nn×d and adjacency matrix\\r\\nA ∈ R\\r\\nn×n, the recursive multiplication of FGOs in Fourier space is equivalent to multi-order\\r\\nconvolutions in the time domain:\\r\\nF\\r\\n−1\\r\\n(F(X)S0:k) = Ak:0XW0:k, S0:k =\\r\\nY\\r\\nk\\r\\ni=0\\r\\nSi, Ak:0 =\\r\\nY\\r\\n0\\r\\ni=k\\r\\nAi, W0:k =\\r\\nY\\r\\nk\\r\\ni=0\\r\\nWi\\r\\nwhere A0, S0, W0 are the identity matrix, Ak ∈ R\\r\\nn×n corresponds to the k-th diffusion step sharing\\r\\nthe same sparsity pattern of A, Wk ∈ R\\r\\nd×d\\r\\nis the k-th weight matrix, Sk ∈ C\\r\\nd×d\\r\\nis the k-th FGO\\r\\nsatisfying F(AkXWk) = F(X)Sk, and F and F\\r\\n−1 denote DFT and its inverse, respectively.\\r\\nProof. The proof aims to demonstrate the equivalence between the recursive multiplication of FGOs\\r\\nin Fourier space and multi-order convolutions in the time domain. According to F(AkXWk) =\\r\\nF(X)Sk, we expand the multi-order convolutions A0:KXW0:K in the time domain using a set of\\r\\nFGOs in Fourier space:\\r\\nF(AKAK−1 · · · A0XW0 · · · WK−1WK) = F(AK(AK−1...A0XW0 · · · WK−1)WK)\\r\\n= F(AK−1...A0XW0 · · · WK−1)SK\\r\\n= F(AK−1(AK−2...A0XW0 · · · WK−2)WK−1)SK\\r\\n= F(AK−2...A0XW0 · · · WK−2)SK−1SK\\r\\n= · · ·\\r\\n= F(X)S0 · · · SK−1SK\\r\\n= F(X)S0:K\\r\\n(10)\\r\\nwhere it yields F\\r\\n−1\\r\\n(F(X)S0:K) = AK:0XW0:K with S0:K =\\r\\nQK\\r\\ni=0 Si\\r\\n,AK:0 =\\r\\nQ0\\r\\ni=K Ai and\\r\\nW0:K =\\r\\nQK\\r\\ni=0 Wi\\r\\n. Proved.\\r\\nThus, the FourierGNN can be rewritten as (for convenience, we exclude the non-linear activation\\r\\nfunction σ and learnable bias parameters b):\\r\\nF\\r\\n−1\\r\\n(\\r\\nX\\r\\nK\\r\\nk=0\\r\\nF(X)S0:K) = A0XW0 + A1(A0XW0)W1 + ... + AK:0XW0:K (11)\\r\\nFrom the right part of the above equation, we can observe that it assigns different weights to weigh\\r\\nthe information of different neighbors in each diffusion order. This property enable FourierGNN to\\r\\ncapture the complex correlations (i.e., spatiotemporal dependencies) in the hypervariate graph, which\\r\\nis empirically verified in our visualization experiments.\\r\\nD Compared with Other Graph Neural Networks\\r\\nGraph Convolutional Networks. Graph convolutional networks (GCNs) depend on the Laplacian\\r\\neigenbasis to perform the multi-order graph convolutions over a given graph structure. Compared\\r\\nwith GCNs, FourierGNN as an efficient alternative to multi-order graph convolutions has three main\\r\\ndifferences: 1) No eigendecompositions or similar costly matrix operations are required. FourierGNN\\r\\ntransforms the input into Fourier domain by discrete Fourier transform (DFT) instead of graph Fourier\\r\\ntransform (GFT); 2) Explicitly assigning various importance to nodes of the same neighborhood\\r\\nwith different diffusion steps. FourierGNN adopts different Fourier Graph Operators S in different\\r\\ndiffusion steps corresponding to different dependencies among nodes; 3) FourierGNN is invariant to\\r\\nthe discretization N, T. It parameterizes the graph convolution via Fourier Graph Operators which\\r\\nare invariant to the graph structure and graph scale.\\r\\nGraph Attention Networks. Graph attention networks (GATs) are non-spectral attention-based\\r\\ngraph neural networks. GATs use node representations to calculate the attention weights (i.e., edge\\r\\nweights) varying with different graph attention layers. Accordingly, both GATs and FourierGNN do\\r\\n15\\nnot depend on eigendecompositions and adopt varying edge weights with different diffusion steps\\r\\n(layers). However, FourierGNN can efficiently perform graph convolutions in Fourier space. For\\r\\na complete graph, the time complexity of the attention calculation of K layers is proportional to\\r\\nKn2 where n is the number of nodes, while a K-layer FourierGNN infers the graph structure in\\r\\nFourier space with the time complexity proportional to n log n. In addition, compared with GATs that\\r\\nimplicitly achieve edge-varying weights with different layers, FourierGNN adopts different FGOs in\\r\\ndifferent diffusion steps explicitly.\\r\\nE Experiment Details\\r\\nE.1 Datasets\\r\\nWe use seven public multivariate benchmarks for multivariate time series forecasting and these\\r\\nbenchmark datasets are summarized in Table 7.\\r\\nTable 7: Summary of datasets.\\r\\nDatasets Solar Wiki Traffic ECG Electricity COVID-19 METR-LA\\r\\nSamples 3650 803 10560 5000 140211 335 34272\\r\\nVariables 592 2000 963 140 370 55 207\\r\\nGranularity 1hour 1day 1hour - 15min 1day 5min\\r\\nStart time 01/01/2006 01/07/2015 01/01/2015 - 01/01/2011 01/02/2020 01/03/2012\\r\\nSolar2: This dataset is about solar power collected by National Renewable Energy Laboratory. We\\r\\nchoose the power plant data points in Florida as the data set which contains 593 points. The data is\\r\\ncollected from 2006/01/01 to 2016/12/31 with the sampling interval of every 1 hour.\\r\\nWiki [36]: This dataset contains a number of daily views of different Wikipedia articles and is\\r\\ncollected from 2015/7/1 to 2016/12/31. It consists of approximately 145k time series and we\\r\\nrandomly choose 2k from them as our experimental data set.\\r\\nTraffic [36]: This dataset contains hourly traffic data from 963 San Francisco freeway car lanes. The\\r\\ntraffic data are collected since 2015/01/01 with the sampling interval of every 1 hour.\\r\\nECG3: This dataset is about Electrocardiogram(ECG) from the UCR time-series classification archive\\r\\n[40]. It contains 140 nodes and each node has a length of 5000.\\r\\nElectricity4: This dataset contains the electricity consumption of 370 clients and is collected since\\r\\n2011/01/01. The data sampling interval is every 15 minutes.\\r\\nCOVID-195: This dataset is about COVID-19 hospitalization in the U.S. states of California (CA)\\r\\nfrom 01/02/2020 to 31/12/2020 provided by the Johns Hopkins University with the sampling interval\\r\\nof every one day.\\r\\nMETR-LA6: This dataset contains traffic information collected from loop detectors in the highway\\r\\nof Los Angeles County from 01/03/2012 to 30/06/2012. It contains 207 sensors and the data sampling\\r\\ninterval is every 5 minutes.\\r\\nE.2 Baselines\\r\\nIn experiments, we conduct a comprehensive comparison of the forecasting performance between our\\r\\nFourierGNN and representative and state-of-the-art (SOTA) models as follows.\\r\\nVAR [34]: VAR is a classic linear autoregressive model. We use the Statsmodels library (https:\\r\\n//www.statsmodels.org) which is a Python package that provides statistical computations to\\r\\nrealize the VAR.\\r\\n2\\r\\nhttps://www.nrel.gov/grid/solar-power-data.html\\r\\n3\\r\\nhttp://www.timeseriesclassification.com/description.php?Dataset=ECG5000\\r\\n4\\r\\nhttps://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\r\\n5\\r\\nhttps://github.com/CSSEGISandData/COVID-19\\r\\n6\\r\\nhttps://github.com/liyaguang/DCRNN\\r\\n16\\nDeepGLO [36]: DeepGLO models the relationships among variables by matrix factorization and\\r\\nemploys a temporal convolution neural network to introduce non-linear relationships. We download\\r\\nthe source code from: https://github.com/rajatsen91/deepglo. We follow the recommended\\r\\nconfiguration as our experimental settings for wiki, electricity, and traffic datasets. For covid datasets,\\r\\nthe vertical and horizontal batch size is set to 64, the rank of the global model is set to 64, the number\\r\\nof channels is set to [32, 32, 32, 1], and the period is set to 7.\\r\\nLSTNet [35]: LSTNet uses a CNN to capture inter-variable relationships and an RNN to discover\\r\\nlong-term patterns. We download the source code from: https://github.com/laiguokun/\\r\\nLSTNet. In our experiment, we use the recommended configuration where the number of CNN\\r\\nhidden units is 100, the kernel size of the CNN layers is 4, the dropout is 0.2, the RNN hidden units\\r\\nis 100, the number of RNN hidden layers is 1, the learning rate is 0.001 and the optimizer is Adam.\\r\\nTCN [10]: TCN is a causal convolution model for regression prediction. We download the source code\\r\\nfrom: https://github.com/locuslab/TCN. We utilize the same configuration as the polyphonic\\r\\nmusic task exampled in the open source code where the dropout is 0.25, the kernel size is 5, the\\r\\nnumber of hidden units is 150, the number of levels is 4 and the optimizer is Adam.\\r\\nReformer [37]: Reformer combines the modeling capacity of a Transformer with an architecture\\r\\nthat can be executed efficiently on long sequences and with small memory use. We download\\r\\nthe source code from: https://github.com/thuml/Autoformer. We follow the recommended\\r\\nconfiguration as the experimental settings.\\r\\nInformer [11]: Informer leverages an efficient self-attention mechanism to encode the dependen\\x02cies among variables. We download the source code from: https://github.com/zhouhaoyi/\\r\\nInformer2020. We follow the recommended configuration as our experimental settings where the\\r\\ndropout is 0.05, the number of encoder layers is 2, the number of decoder layers is 1, the learning\\r\\nrate is 0.0001, and the optimizer is Adam.\\r\\nAutoformer [5]: Autoformer proposes a decomposition architecture by embedding the series de\\x02composition block as an inner operator, which can progressively aggregate the long-term trend part\\r\\nfrom intermediate prediction. We download the source code from: https://github.com/thuml/\\r\\nAutoformer. We follow the recommended configuration as our experimental settings with 2 encoder\\r\\nlayers and 1 decoder layer.\\r\\nFEDformer [29]: FEDformer proposes an attention mechanism with low-rank approximation in\\r\\nfrequency and a mixture of expert decomposition to control the distribution shifting. We download the\\r\\nsource code from: https://github.com/MAZiqing/FEDformer. We use FEB-f as the Frequency\\r\\nEnhanced Block and select the random mode with 64 as the experimental mode.\\r\\nSFM [26]: On the basis of the LSTM model, SFM introduces a series of different frequency compo\\x02nents in the cell states. We download the source code from: https://github.com/z331565360/\\r\\nState-Frequency-Memory-stock-prediction. We follow the recommended settings where the\\r\\nlearning rate is 0.01, the frequency dimension is 10, the hidden dimension is 10 and the optimizer is\\r\\nRMSProp.\\r\\nStemGNN [12]: StemGNN leverages GFT and DFT to capture dependencies among variables in\\r\\nthe frequency domain. We download the source code from: https://github.com/microsoft/\\r\\nStemGNN. We use the recommended configuration of stemGNN as our experiment setting where the\\r\\noptimizer is RMSProp, the learning rate is 0.0001, the number of stacked layers is 5, and the dropout\\r\\nrate is 0.5.\\r\\nMTGNN [15]: MTGNN proposes an effective method to exploit the inherent dependency relation\\x02ships among multiple time series. We download the source code from: https://github.com/\\r\\nnnzhan/MTGNN. Because the experimental datasets have no static features, we set the parameter\\r\\nload_static_feature to false. We construct the graph by the adaptive adjacency matrix and add the\\r\\ngraph convolution layer. Regarding other parameters, we adopt the recommended settings.\\r\\nGraphWaveNet [19]: GraphWaveNet introduces an adaptive dependency matrix learning to cap\\x02ture the hidden spatial dependency. We download the source code from: https://github.com/\\r\\nnnzhan/Graph-WaveNet. Since our datasets have no prior defined graph structures, we use only\\r\\nadaptive adjacent matrix. We add a graph convolution layer and randomly initialize the adjacent\\r\\nmatrix. We adopt the recommended configuration as our experimental settings where the learning\\r\\nrate is 0.001, the dropout is 0.3, the number of epochs is 50, and the optimizer is Adam.\\r\\n17\\nAGCRN [2]: AGCRN proposes a data-adaptive graph generation module for discovering spatial\\r\\ncorrelations from data. We download the source code from: https://github.com/LeiBAI/\\r\\nAGCRN. We follow the recommended configuration as our experimental settings where the embedding\\r\\ndimension is 10, the learning rate is 0.003, and the optimizer is Adam.\\r\\nTAMP-S2GCNets [13]: TAMP-S2GCNets explores the utility of MP to enhance knowledge represen\\x02tation mechanisms within the time-aware DL paradigm. We download the source code from: https:\\r\\n//www.dropbox.com/sh/n0ajd5l0tdeyb80/AABGn-ejfV1YtRwjf_L0AOsNa?dl=0. TAMP\\x02S2GCNets requires predefined graph topology and we use the California State topology provided by\\r\\nthe source code as input. We adopt the recommended configuration as our experimental settings on\\r\\nCOVID-19.\\r\\nDCRNN [20]: DCRNN uses bidirectional graph random walk to model spatial dependency and\\r\\nrecurrent neural network to capture the temporal dynamics. We download the source code from:\\r\\nhttps://github.com/liyaguang/DCRNN. We follow the recommended configuration as our\\r\\nexperimental settings with the batch size is 64, the learning rate is 0.01, the input dimension is 2 and\\r\\nthe optimizer is Adam. DCRNN requires a pre-defined graph structure and we use the adjacency\\r\\nmatrix as the pre-defined structure provided by the METR-LA dataset.\\r\\nSTGCN [1]: STGCN integrates graph convolution and gated temporal convolution through\\r\\nspatial-temporal convolutional blocks. We download the source code from:https://github.com/\\r\\nVeritasYin/STGCN_IJCAI-18. We use the recommended configuration as our experimental set\\x02tings where the batch size is 50, the learning rate is 0.001 and the optimizer is Adam. STGCN requires\\r\\na pre-defined graph structure and we leverage the adjacency matrix as the pre-defined structures\\r\\nprovided by the METR-LA dataset.\\r\\nCoST [38]: CoST separates the representation learning and downstream forecasting task and proposes\\r\\na contrastive learning framework that learns disentangled season-trend representations for time series\\r\\nforecasting tasks. We download the source code from: https://github.com/salesforce/CoST.\\r\\nWe set the representation dimension to 320, the learning rate to 0.001, and the batch size to 32.\\r\\nInputs are min-max normalization, we perform a 70/20/10 train/validation/test split for the METR-LA\\r\\ndataset and 60/20/20 for the COVID-19 dataset.\\r\\nE.3 Evaluation Metrics\\r\\nWe use MAE (Mean Absolute Error), RMSE (Root Mean Square Error), and MAPE (Mean Absolute\\r\\nPercentage Error) as the evaluation metrics in the experiments.\\r\\nSpecifically, given the groudtruth at timestamps t, Yt = [xt+1, ..., xt+τ ] ∈ R\\r\\nN×τ\\r\\n, and the predictions\\r\\nYˆ\\r\\nt = [xˆt+1, ..., xˆt+τ ] ∈ R\\r\\nN×τ\\r\\nfor future τ steps at timestamp t, the metrics are defined as follows:\\r\\nMAE =\\r\\n1\\r\\nτN\\r\\nX\\r\\nN\\r\\ni=1\\r\\nXτ\\r\\nj=1\\r\\n|xij − xˆij | (12)\\r\\nRMSE =\\r\\nvuut\\r\\n1\\r\\nτN\\r\\nX\\r\\nN\\r\\ni=1\\r\\nXτ\\r\\nj=1\\r\\n(xij − xˆij )\\r\\n2\\r\\n(13)\\r\\nMAP E =\\r\\n1\\r\\nτN\\r\\nX\\r\\nN\\r\\ni=1\\r\\nXτ\\r\\nj=1\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\nxij − xˆij\\r\\nxij\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n× 100% (14)\\r\\nwith xij ∈ Yt and xˆij ∈ Yˆ\\r\\nt.\\r\\nE.4 Experimental Settings\\r\\nWe summarize the implementation details of the proposed FourierGNN as follows. Note that the\\r\\ndetails of the baselines are introduced in their corresponding descriptions (see Section E.2).\\r\\n18\\nNetwork details. The fully connected feed-forward network (FFN) consists of three linear transfor\\x02mations with LeakyReLU activations in between. The FFN is formulated as follows:\\r\\nX1 = LeakyReLU(YG\\r\\nt W1 + b1)\\r\\nX2 = LeakyReLU(X1W2 + b2)\\r\\nYˆ = X2W3 + b3\\r\\n(15)\\r\\nwhere W1 ∈ R\\r\\n(T d)×d\\r\\nffn\\r\\n1 , W2 ∈ R\\r\\nd\\r\\nffn\\r\\n1 ×d\\r\\nffn\\r\\n2 and W3 ∈ R\\r\\nd\\r\\nffn\\r\\n2 ×τ\\r\\nare the weights of the three\\r\\nlayers respectively, and b1 ∈ R\\r\\nd\\r\\nffn\\r\\n1 , b2 ∈ R\\r\\nd\\r\\nffn\\r\\n2 and b3 ∈ R\\r\\nτ\\r\\nare the biases of the three layers\\r\\nrespectively. Here, d\\r\\nf fn\\r\\n1\\r\\nand d\\r\\nf fn\\r\\n2\\r\\nare the dimensions of the three layers. In addition, we adopt a\\r\\nReLU activation function in Equation 6.\\r\\nTraining details. We carefully tune the hyperparameters, including the embedding size, batch\\r\\nsize, d\\r\\nf fn\\r\\n1\\r\\nand d\\r\\nf fn\\r\\n2\\r\\n, on the validation set and choose the settings with the best performance for\\r\\nFourierGNN on different datasets. Specifically, the embedding size and batch size are tuned over\\r\\n{32, 64, 128, 256, 512} and {2, 4, 8, 16, 32, 64, 128} respectively. For the COVID-19 dataset, the\\r\\nembedding size is 256, and the batch size is set to 4. For the Traffic, Solar, and Wiki datasets, the\\r\\nembedding size is 128, and the batch size is set to 2. For the METR-LA, ECG, and Electricity\\r\\ndatasets, the embedding size is 128, and the batch size is set to 32.\\r\\nTo reduce the number of parameters, we adopt a linear transform to reshape the original time\\r\\ndomain representation YG\\r\\nt ∈ R\\r\\nNT ×d\\r\\nto Yt ∈ R\\r\\nN×T ×d\\r\\n, and map Yt to a low-dimensional tensor\\r\\nYt ∈ R\\r\\nN×l×d with l < T. We then reshape Yt ∈ RN×(ld)\\r\\nand feed it to FFN. We perform a grid\\r\\nsearch on the dimensions of FFN, i.e., d\\r\\nf fn\\r\\n1\\r\\nand d\\r\\nf fn\\r\\n2\\r\\n, over {32, 64, 128, 256, 512} and tune the\\r\\nintermediate dimension l over {2, 4, 6, 8, 12}. The settings of the three hyperparameters over all\\r\\ndatasets are shown in Table 8. By default, we set the diffusion step (layers) K = 3 for all datasets.\\r\\nTable 8: Dimension settings of FFN on different datasets. ∗ denotes that we feed the original time\\r\\ndomain representation to FFN without the dimension reduction.\\r\\nDatasets Solar Wiki Traffic ECG Electricity COVID-19 META-LR\\r\\nl 6 2 2 ∗ 4 8 4\\r\\nd\\r\\nffn\\r\\n1\\r\\n64 64 64 64 64 256 64\\r\\nd\\r\\nffn\\r\\n2\\r\\n256 256 256 256 256 512 256\\r\\nE.5 Details for Visualization Experiments\\r\\nTo verify the effectiveness of FourierGNN in learning the spatiotemporal dependencies on the\\r\\nhypervariate graph, we obtain the output of FourierGNN as the node representation, denoted as YG\\r\\nt =\\r\\nIDFT(FourierGNN(XG\\r\\nt\\r\\n)) ∈ R\\r\\nNT ×d with Inverse Discrete Fourier Transform (IDFT). Then, we\\r\\nvisualize the adjacency matrix A calculated based the flatten node representation YG\\r\\nt ∈ R\\r\\nNT ×d\\r\\n,\\r\\nformulated as A = YG\\r\\nt\\r\\n(YG\\r\\nt\\r\\n)\\r\\nT ∈ RNT ×NT , to show the variable correlations. Note that A is\\r\\nnormalized via A/ max(A). Since it is not feasible to directly visualize the huge adjacency matrix\\r\\nA of the hypervariate graph, we visualize its different subgraphs in Figures 3, 4, 9, and 10 to better\\r\\nverify the learned spatiotemporal information on the hypervariate graph from different perspectives.\\r\\nFigure 3. We select 8 counties and visualize the correlations between 12 consecutive time steps for\\r\\neach selected county respectively. Figure 3 reflects the temporal correlations within each variable.\\r\\nFigure 4: On the METR-LA dataset, we average its adjacency matrix A over the temporal dimension\\r\\n(i.e., marginalizing T) to A′ ∈ R\\r\\nN×N . Then, we randomly select 20 detectors out of all N = 207\\r\\ndetectors and obtain their corresponding sub adjacency matrix (R\\r\\n20×20) from A′\\r\\nfor visualization.\\r\\nWe further compare the sub-adjacency with the real road map (generated by the Google map tool) to\\r\\nverify the learned dependencies between different detectors.\\r\\nFigure 9. Since we adopt a 3-layer FourierGNN, we can calculate four adjacency matrices based on\\r\\nthe spectrum input X\\r\\nG\\r\\nt of FourierGNN and the outputs of each layer in FourierGNN. Following the\\r\\n19\\nway of visualization in Figure 4, we select 10 counties and two timestamps on the four adjacency\\r\\nmatrices for visualization. Figure 9 shows the effects of each layer of FourierGNN in filtering or\\r\\nenhancing variable correlations.\\r\\nFigure 10. On the COVID-19 dataset, we randomly select 10 counties out of N = 55 counties and\\r\\nobtain their four sub-adjacency matrices of four consecutive days for visualization. Each of the four\\r\\nsub adjacency matrices R\\r\\n10×10 embodies the dependencies between counties in one day. Figure 10\\r\\nreflects the time-varying dependencies between counties (i.e., variables).\\r\\nF Additional Results\\r\\nTo further evaluate the performance of our model FourierGNN in multi-step forecasting, we conduct\\r\\nmore experiments on the Wiki, METR-LA, and ECG datasets, respectively. We compare our\\r\\nmodel FourierGNN with five models (including StemGNN [12], AGCRN [2], GraphWaveNet [19],\\r\\nMTGNN [15], and Informer [11]) on the Wiki dataset under different prediction lengths, and the\\r\\nresults are shown in Table 9. From the table, we observe that FourierGNN outperforms other\\r\\nmodels on MAE, RMSE, and MAPE metrics for all the prediction lengths. On average, FourierGNN\\r\\nimproves MAE, RMSE, and MAPE by 7.4%, 3.5%, and 22.3%, respectively. Among these models,\\r\\nAGCRN shows promising performances since it captures the spatial and temporal correlations\\r\\nadaptively. However, it fails to simultaneously capture spatiotemporal dependencies, limiting its\\r\\nforecasting performance. In contrast, our model captures comprehensive spatiotemporal dependencies\\r\\nsimultaneously on a hypervariate graph for multivariate time series forecasting.\\r\\nTable 9: Accuracy comparison under different prediction lengths on the Wiki dataset.\\r\\nLength 3 6 9 12\\r\\nMetrics MAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%)\\r\\nGraphWaveNet [19] 0.061 0.105 138.60 0.061 0.105 135.32 0.061 0.105 132.52 0.061 0.104 136.12\\r\\nStemGNN [12] 0.157 0.236 89.00 0.159 0.233 98.01 0.232 0.311 142.14 0.220 0.306 125.40\\r\\nAGCRN [2] 0.043 0.077 73.49 0.044 0.078 80.44 0.045 0.079 81.89 0.044 0.079 78.52\\r\\nMTGNN [15] 0.102 0.141 123.15 0.091 0.133 91.75 0.074 0.120 85.44 0.101 0.140 122.96\\r\\nInformer [11] 0.053 0.089 85.31 0.054 0.090 84.46 0.059 0.095 93.80 0.059 0.095 95.09\\r\\nFourierGNN 0.040 0.075 58.18 0.041 0.075 60.43 0.041 0.076 60.95 0.041 0.076 64.50\\r\\nTable 10: Accuracy comparison under different prediction lengths on the METR-LA dataset.\\r\\nHorizon 3 6 9 12\\r\\nMetrics MAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%) MAE RMSE MAPE(%)\\r\\nDCRNN [20] 0.160 0.204 80.00 0.191 0.243 83.15 0.216 0.269 85.72 0.241 0.291 88.25\\r\\nSTGCN [1] 0.058 0.133 59.02 0.080 0.177 60.67 0.102 0.209 62.08 0.128 0.238 63.81\\r\\nGraphWaveNet [19] 0.180 0.366 21.90 0.184 0.375 22.95 0.196 0.382 23.61 0.202 0.386 24.14\\r\\nMTGNN [15] 0.135 0.294 17.99 0.144 0.307 18.82 0.149 0.328 19.38 0.153 0.316 19.92\\r\\nStemGNN [12] 0.052 0.115 86.39 0.069 0.141 87.71 0.080 0.162 89.00 0.093 0.175 90.25\\r\\nAGCRN [2] 0.062 0.131 24.96 0.086 0.165 27.62 0.099 0.188 29.72 0.109 0.204 31.73\\r\\nInformer [11] 0.076 0.141 69.96 0.088 0.163 70.94 0.096 0.178 72.26 0.100 0.190 72.54\\r\\nCoST [38] 0.064 0.118 88.44 0.077 0.141 89.63 0.088 0.159 90.56 0.097 0.171 91.42\\r\\nFourierGNN 0.050 0.113 86.30 0.066 0.140 87.97 0.076 0.159 88.99 0.084 0.165 89.69\\r\\nFurthermore, we compare our model FourierGNN with seven MTS models (including STGCN [1],\\r\\nDCRNN [20], StemGNN [12], AGCRN [2], GraphWaveNet [19], MTGNN [15], Informer [11], and\\r\\nCoST [38]) on the METR-LA dataset which has a predefined graph topology in the data, and the\\r\\nresults are shown in Table 10. On average, we improve 5.7% on MAE and 1.5% on RMSE. Among\\r\\nthese models, StemGNN achieves competitive performance because it combines GFT to capture the\\r\\nspatial dependencies and DFT to capture the temporal dependencies. However, it is also limited to\\r\\nsimultaneously capturing spatiotemporal dependencies. CoST learns disentangled seasonal-trend\\r\\nrepresentations for time series forecasting via contrastive learning and obtains competitive results.\\r\\nBut, our model still outperforms CoST. Because, compared with CoST, our model not only can learn\\r\\nthe dynamic temporal representations, but also capture the discriminative spatial representations.\\r\\nBesides, STGCN and DCRNN require pre-defined graph structures. But StemGNN and our model\\r\\noutperform them for all steps, and AGCRN outperforms them when the prediction lengths are 9 and\\r\\n12. This also shows that a novel adaptive graph learning can precisely capture the hidden spatial\\r\\ndependency. In addition, we compare FourierGNN with the baseline models under the different\\r\\nprediction lengths on the ECG dataset, as shown in Figure 5. It reports that FourierGNN achieves the\\r\\nbest performances (MAE, RMSE, and MAPE) for all prediction lengths.\\r\\n20\\n(a) MAE (b) RMSE (c) MAPE\\r\\nFigure 5: Performance comparison in multi-step prediction on the ECG dataset.\\r\\nG Further Analyses\\r\\nG.1 Scalability Analysis\\r\\nWe further conduct experiments on the Wiki dataset to investigate the performance of FourierGNN\\r\\nunder different graph sizes (N ×T). The results are shown in Figure 6, where Figure 6(a), Figure 6(b)\\r\\nand Figure 6(c) show MAE, RMSE, and MAPE at the different number of nodes, respectively. From\\r\\nthese figures, we observe that FourierGNN keeps a leading edge over the other state-of-the-art MTS\\r\\nmodels as the number of nodes increases. The results demonstrate the superiority and scalability of\\r\\nFourierGNN on large-scale datasets.\\r\\n(a) MAE (b) RMSE (c) MAPE\\r\\nFigure 6: Scalability analyses in terms of MAE, RMSE, and MAPE under different number of nodes\\r\\non the Wiki dataset.\\r\\nG.2 Parameter Analysis\\r\\nTable 11: Performance at different diffusion steps (lay\\x02ers) on the COVID-19 dataset.\\r\\nK=1 K=2 K=3 K=4\\r\\nMAE 0.136 0.133 0.123 0.132\\r\\nRMSE 0.181 0.177 0.168 0.176\\r\\nMAPE(%) 72.30 71.80 71.52 72.59\\r\\nWe evaluate the forecasting performance of\\r\\nour model FourierGNN under different diffu\\x02sion steps (layers) on the COVID-19 dataset,\\r\\nas illustrated in Table 11. The table shows\\r\\nthat FourierGNN achieves increasingly bet\\x02ter performance from K = 1 to K = 4\\r\\nand achieves the best results when K = 3.\\r\\nWith the further increase of K, FourierGNN\\r\\nobtains inferior performance. The results in\\x02dicate that high-order diffusion information\\r\\nis beneficial for improving forecasting accuracy, but the diffusion information may gradually weaken\\r\\nthe effect or even bring noises to forecasting with the increase of the order.\\r\\nIn addition, we conduct additional experiments on the ECG dataset to analyze the effect of the input\\r\\nlookback window length T and the embedding dimension d, as shown in Figure 7 and Figure 8,\\r\\nrespectively. Figure 7 shows that the performance (including RMSE and MAPE) of FourierGNN\\r\\ngets better as the input lookback window length increases, indicating that FourierGNN can learn\\r\\na comprehensive hypervariate graph from long MTS inputs to capture the spatial and temporal\\r\\ndependencies. Moreover, Figure 8 shows that the performance (RMSE and MAPE) first increases and\\r\\nthen decreases with the increase of the embedding size, which is attributed that a large embedding\\r\\n21\\nFigure 7: Influence of input window. Figure 8: Influence of embedding size.\\r\\nsize improves the fitting ability of FourierGNN but it may easily lead to the overfitting issue especially\\r\\nwhen the embedding size is too large.\\r\\nG.3 Ablation Study\\r\\nWe provide more details about each variant used in this section and Section 5.3.\\r\\n• w/o Embedding. A variant of FourierGNN feeds the raw MTS input instead of its embed\\x02dings into the graph convolution in Fourier space.\\r\\n• w/o Dynamic FGO. A variant of FourierGNN uses the same FGO for all diffusion steps\\r\\ninstead of applying different FGOs in different diffusion steps. It corresponds to a vanilla\\r\\ngraph filter.\\r\\n• w/o Residual. A variant of FourierGNN does not have the K = 0 layer output, i.e., X\\r\\nG\\r\\nt\\r\\n, in\\r\\nthe summation.\\r\\n• w/o Summation. A variant of FourierGNN adopts the last order (layer) output as the final\\r\\nfrequency output of the FourierGNN.\\r\\nWe conduct another ablation study on the COVID-19 dataset to further investigate the effects of the\\r\\ndifferent components of our FourierGNN. The results are shown in Table 12, which confirms the\\r\\nresults in Table 4 and further verifies the effectiveness of each component in FourierGNN. Both Table\\r\\n12 and Table 4 report that the embedding and dynamic FGOs in FourierGNN contribute more than\\r\\nthe design of residual and summation to the state-of-the-art performance of FourierGNN.\\r\\nTable 12: Ablation studies on the COVID-19 dataset.\\r\\nMetric w/o Embedding w/o Dynamic FGO w/o Residual w/o Summation FourierGNN\\r\\nMAE 0.157 0.138 0.131 0.134 0.123\\r\\nRMSE 0.203 0.180 0.174 0.177 0.168\\r\\nMAPE(%) 76.91 74.01 72.25 72.57 71.52\\r\\nH Visualizations\\r\\nH.1 Visualization of the Diffusion Process in FourierGNN\\r\\nTo gain insight into the operation of the FGO, we visualize the frequency output of each layer in\\r\\nour FourierGNN. We select 10 counties from the COVID-19 dataset and visualize their adjacency\\r\\nmatrices at two different timestamps, as shown in Figure 9. From left to right, the results correspond\\r\\nto the original spectrum of the input, as well as the outputs of the first, second, and third layers of\\r\\nthe FourierGNN. From the top, we can find that as the number of layers increases, some correlation\\r\\nvalues are reduced, indicating that some correlations are filtered out. In contrast, the bottom case\\r\\nillustrates some correlations are enhanced as the number of layers increases. These results show that\\r\\nFGO can adaptively and effectively capture important patterns while removing noises, enabling the\\r\\nlearning of a discriminative model.\\r\\n22\\nFigure 9: The diffusion process of FourierGNN at two timestamps (top and bottom) on COVID-19.\\r\\nH.2 Visualization of Time-Varying Dependencies Learned by FourierGNN\\r\\nFurthermore, we explore the capability of FourierGNN in capturing time-varying dependencies\\r\\namong variables. To investigate this, we perform additional experiments to visualize the adjacency\\r\\nmatrix of 10 randomly-selected counties over four consecutive days on the COVID-19 dataset. The\\r\\nvisualization results, displayed as a heatmap in Figure 10, reveal clear spatial patterns that exhibit\\r\\ncontinuous evolution in the temporal dimension. This is because FourierGNN can attend to the\\r\\ntime-varying variability of the spatiotemporal dependencies. These results verify that our model\\r\\nenjoys the feasibility of exploiting the time-varying dependencies among variables.\\r\\nBased on the insights gained from these visualization results, we can conclude that the hypervariate\\r\\ngraph structure exhibits strong capabilities to encode spatiotemporal dependencies. By incorporating\\r\\nFGOs, FourierGNN can effectively attend to and exploit the time-varying dependencies among\\r\\nvariates. The synergy between the hypervariate graph structure and FGOs empowers FourierGNN to\\r\\ncapture and model intricate spatiotemporal relationships with remarkable effectiveness.\\r\\nFigure 10: The adjacency matrix for four consecutive days on the COVID-19 dataset.\\r\\n23'},\n",
       " {'name': '2412.12227v1.pdf',\n",
       "  'content': '1\\r\\nEDformer: Embedded Decomposition Transformer\\r\\nfor Interpretable Multivariate Time Series\\r\\nPredictions\\r\\nSanjay Chakraborty, Ibrahim Delibasoglu, Fredrik Heintz\\r\\nDepartment of Computer and Information Science (IDA), REAL, AIICS, Linkoping University, Link ¨ oping, ¨\\r\\nSweden, Email: sanjay.chakraborty@liu.se*\\r\\nAbstract—Time series forecasting is a crucial challenge with\\r\\nsignificant applications in areas such as weather prediction,\\r\\nstock market analysis, and scientific simulations. This paper\\r\\nintroduces an embedded decomposed transformer, ’EDformer’,\\r\\nfor multivariate time series forecasting tasks. Without altering\\r\\nthe fundamental elements, we reuse the Transformer architecture\\r\\nand consider the capable functions of its constituent parts in this\\r\\nwork. Edformer first decomposes the input multivariate signal\\r\\ninto seasonal and trend components. Next, the prominent mul\\x02tivariate seasonal component is reconstructed across the reverse\\r\\ndimensions, followed by applying the attention mechanism and\\r\\nfeed-forward network in the encoder stage. In particular, the\\r\\nfeed-forward network is used for each variable frame to learn\\r\\nnonlinear representations, while the attention mechanism uses\\r\\nthe time points of individual seasonal series embedded within\\r\\nvariate frames to capture multivariate correlations. Therefore,\\r\\nthe trend signal is added with projection and performs the final\\r\\nforecasting. The EDformer model obtains state-of-the-art pre\\x02dicting results in terms of accuracy and efficiency on complex\\r\\nreal-world time series datasets. This paper also addresses model\\r\\nexplainability techniques to provide insights into how the model\\r\\nmakes its predictions and why specific features or time steps are\\r\\nimportant, enhancing the interpretability and trustworthiness of\\r\\nthe forecasting results.\\r\\nKeywords - Time-series; Forecasting; Transformer; Multi\\x02variate; Explainable AI.\\r\\nI. INTRODUCTION\\r\\nTime-series forecasting plays an important role in a variety\\r\\nof industries, such as sensor network monitoring [1], smart\\r\\ngrid management [2], economics and finance [3], and disease\\r\\npropagation analysis [4]. By replacing the Long Short-Term\\r\\nMemory (LSTM) and Recurrent Neural Network (RNN) based\\r\\nstructure, which has been the main approach for processing\\r\\ntime-series data, with several self-attention modules, Trans\\x02former shows improved performance, especially in the time\\r\\nseries analysis field [5]. Transformer is emerging in time\\r\\nseries forecasting, driven by the tremendous success in the\\r\\nnatural language processing field. Transformer has the ability\\r\\nto extract multi-level representations from sequences and il\\x02lustrate pairwise connections [3]. In time series data analysis,\\r\\nTransformers that combine frequency-domain filtering with\\r\\ntemporal and channel-wise transformations offer a powerful\\r\\napproach to capturing complex dependencies and patterns.\\r\\nBy leveraging frequency-domain filtering, these models can\\r\\nefficiently separate and analyze the signal’s various frequency\\r\\ncomponents, isolating key seasonal or trend-related elements\\r\\n[5]. Additionally, temporal and channel-wise transformations\\r\\nallow the model to process both the sequential nature of time\\r\\nseries data and the relationships across multiple channels, mak\\x02ing it particularly adept at handling multivariate time series\\r\\nand discovering intricate dependencies between variables [6].\\r\\nThis dual capability enables the model to address both local\\r\\nand global patterns, improving performance in tasks such as\\r\\nforecasting, anomaly detection, and classification. In addition,\\r\\nthe transformer-based model suffers from slow training and\\r\\ninference speed due to the bottleneck incurred by a deep\\r\\nencoder and step-by-step decoder inference.\\r\\nIn this paper, Our main contributions are as follows:\\r\\n• We analyse the Transformer architecture and discover\\r\\nthat its native components’ efficacy for multivariate time\\r\\nseries has not received enough attention.\\r\\n• The EDformer model decomposes the input multivariate\\r\\ntime series into separate seasonal and trend components.\\r\\nEach component is then independently embedded into\\r\\nframes using reverse operation, allowing the self-attention\\r\\nmodule to capture multivariate correlations while the\\r\\nfeed-forward network encodes the representations of each\\r\\nseries.\\r\\n• EDformer is a lightweight and computationally efficient\\r\\narchitecture, that significantly reduces processing time\\r\\ncompared to existing state-of-the-art methods while main\\x02taining or even improving forecasting accuracy.\\r\\n• EDformer experimentally reaches cutting-edge perfor\\x02mance on real-world benchmarks. A promising direction\\r\\nfor future developments in Transformer-based forecasting\\r\\nmodels is shown by our thorough examination of its\\r\\nembedded modules and architectural decisions.\\r\\nWe do comprehensive tests on different time-series forecasting\\r\\nbenchmarks to support our motivation and hypothesis. When\\r\\ncompared to various forecasting techniques, the performance\\r\\nof our proposed model is at the cutting edge. Extensive\\r\\nablation investigations and analysis trials support the viability\\r\\nof suggested designs and their consequent benefit over earlier\\r\\nmethods.\\r\\nThis paper is organized as follows. Section II explains a set of\\r\\ntypical times series forecasting works from the literature which\\r\\nsets the notion and motivation of this paper. Section III briefly\\r\\narXiv:2412.12227v1 [cs.LG] 16 Dec 2024\\n2\\r\\nexplains the problem statement of this paper. In Section IV,\\r\\nwe provide a detailed discussion of our proposed architecture\\r\\nsuitable for multivariate time series forecasting. In section V,\\r\\nwe discuss the result analysis of the proposed methodology\\r\\nand describe a detailed comparison with the state-of-the-art\\r\\ntime series forecasting models. A deeper explainability of the\\r\\nproposed EDformer model is described in the section VII. A\\r\\nconclusion of the work’s findings is given in section ??.\\r\\nII. LITERATURE REVIEW\\r\\nIn long-term and short-term time series forecasting, trans\\x02formers have gained considerable attention due to their ability\\r\\nto model complex temporal dependencies [7, 8, 9, 10]. In\\x02former [11], one of the first widely recognized transformers\\r\\nfor time-series forecasting, employs a generative-style decoder\\r\\nand ProbSparse self-attention to address the challenge of\\r\\nquadratic time complexity. Subsequently, various transformer\\x02based models have been proposed, including Autoformer [12],\\r\\nPyraformer [13], iTransformer [14], Reformer [15], and FED\\x02Former [16]. Pyraformer emphasizes multiresolution attention\\r\\nfor efficient signal processing. Autoformer [12] leverages auto\\x02correlation and decomposition techniques to improve fore\\x02casting, while FEDFormer [16] integrates frequency-domain\\r\\nanalysis to enhance the representation of time series. PatchTST\\r\\n[9] focuses on the use of patches to improve the model’s\\r\\ncapacity to capture both local and global dependencies in\\r\\nthe data. Crossformer [17] introduces the Dimension-Segment\\x02Wise (DSW) embedding method, which encodes input time\\x02series data into a 2D vector array while preserving temporal\\r\\nand dimensional structure. It also employs a Two-Stage Atten\\x02tion (TSA) layer to capture cross-time and cross-dimension\\r\\ndependencies. By combining DSW embedding and TSA,\\r\\nCrossformer builds a Hierarchical Encoder-Decoder (HED)\\r\\nframework to operate across multiple scales for more effective\\r\\npredictions. The Multi-resolution Time Series Transformer\\r\\n(MTST) [18] adopts a multi-branch architecture to model\\r\\ntemporal patterns across different resolutions [19]. MTST\\r\\ndistinguishes itself by employing relative positional encoding,\\r\\nwhich is better suited for capturing periodic components at\\r\\nvarious scales, compared to the fixed or absolute positional\\r\\nencodings used in many other transformer models. Time series\\r\\ndecomposition, a standard method in time series analysis\\r\\n[20], breaks down a time series into distinct components,\\r\\neach reflecting a specific category of patterns that exhibit\\r\\ngreater predictability. This technique is particularly effective\\r\\nfor examining historical changes over time. In forecasting\\r\\ntasks, decomposition is commonly applied as a pre-processing\\r\\nstep for historical data before predicting future trends [21].\\r\\nNotable examples include Prophet [22], which uses trend\\x02seasonality decomposition, N-BEATS [23], which employs\\r\\nbasis expansion, and DeepGLO [24], which utilizes matrix\\r\\ndecomposition. However, such pre-processing approaches are\\r\\noften constrained by the basic decomposition effects of histori\\x02cal series, failing to capture the hierarchical interactions among\\r\\nunderlying patterns in long-term forecasts. In this paper, a\\r\\nbasic AvgPooling(.) decomposition technique has been used\\r\\nto separate the seasonal and trend patterns from the original\\r\\ninput signal.\\r\\nIII. PROBLEM STATEMENT\\r\\nThis paper addresses the challenge of long-term fore\\x02casting for multivariate time series, using historical data.\\r\\nWe define a multivariate time series at time t as xt =\\r\\n[xt,1, xt,2, . . . , xt,N ]\\r\\n⊤ ∈ RT ×N , where xt,n represents the\\r\\nvalue of the n-th variable at time t, for n = 1, 2, . . . , N. The\\r\\naim is to develop a model for forecasting the future values\\r\\nof the series over the next T time steps, based on the most\\r\\nrecent L time steps. The parameters L and H are referred to as\\r\\nthe look-back window and the prediction horizon, respectively.\\r\\nSpecifically, for a given initial time t0, the model takes as\\r\\ninput the sequence xt0−l, corresponding to the past L time\\r\\nsteps, and outputs the predicted sequence xˆt0+h, representing\\r\\nthe forecasted values for the next H time steps. The predicted\\r\\nvalue of xt at time t is denoted by xˆt. In brief, the goal of\\r\\nmultivariate time series forecasting is to predict future values\\r\\nXt+h ∈ R\\r\\nT ×N given past observations Xt−l\\r\\n:\\r\\nXˆ\\r\\nt+h = f(Xt−l) (1)\\r\\nThe forecasting performance of the model is assessed by\\r\\ncomputing the mean squared error (MSE) and the mean\\r\\nabsolute error (MAE) between the prediction and the ground\\r\\ntruth on the test set:\\r\\nMSE =\\r\\n1\\r\\nN\\r\\nXn\\r\\ni=1\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nX\\r\\n(i)\\r\\nt+h − Xˆ\\r\\n(i)\\r\\nt+h\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n2\\r\\n(2)\\r\\nMAE =\\r\\n1\\r\\nN\\r\\nXn\\r\\ni=1\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nX\\r\\n(i)\\r\\nt+h − Xˆ\\r\\n(i)\\r\\nt+h\\r\\n\\r\\r\\n\\r\\r\\n\\r (3)\\r\\nIV. METHODOLOGY\\r\\nIn this work, we have discussed the proposed methodology\\r\\nwith a deep decomposition architecture, which includes a\\r\\nseries decomposition block, reverse operation, embedding op\\x02eration, and corresponding encoder. Our proposed EDformer\\r\\narchitecture uses transformer’s encoder-only architecture. The\\r\\ngeneral overview of the proposed EDformer architecture is\\r\\nrepresented in Figure 1. In time-token-based transformers,\\r\\nthe embedding of points from the same time step, which\\r\\nfundamentally represent distinct physical meanings captured\\r\\nby inconsistent measurements, results in the loss of multivari\\x02ate correlations within a single token. Such tokens, formed\\r\\nfrom isolated time steps, face challenges in extracting useful\\r\\ninformation due to their overly local receptive fields and\\r\\nthe misalignment of events across simultaneous time points.\\r\\nFurthermore, while the order of sequences plays a critical role\\r\\nin influencing time series variations, the use of permutation\\x02invariant attention mechanisms on the temporal dimension is\\r\\nunsuitable, as it disregards the sequential nature of the data\\r\\n[10].\\r\\nA. Decomposition Block\\r\\nDecomposition has been a widely used method in time\\r\\nseries analysis for many years [6]. The decomposition block\\r\\nseparates a time series into its seasonality and trend-cycle com\\x02ponents, enhancing the model’s ability to accurately capture\\r\\nthese aspects. In time series analysis, decomposition involves\\n3\\r\\nFig. 1: Overall architecture of EDformer\\r\\nbreaking down a time series into three systematic components:\\r\\ntrend-cycle, seasonal variation, and random fluctuations. The\\r\\ntrend component reflects the long-term direction of the series,\\r\\nwhich may be increasing, decreasing, or stable over time.\\r\\nThe seasonal component captures recurring patterns within the\\r\\nseries, while the random (or ”irregular”) component accounts\\r\\nfor noise that cannot be explained by the trend or seasonal\\r\\nfactors. Decomposition can be performed in two main ways:\\r\\nadditive and multiplicative. By breaking down a time series\\r\\ninto these components, we gain a deeper understanding of the\\r\\nunderlying patterns, allowing for more effective modelling. As\\r\\ncan be seen, the encoder and decoder use a decomposition\\r\\nblock to aggregate the trend-cyclical part and extract the\\r\\nseasonal part from the series progressively. Example, let us\\r\\ntake an input series X ∈ RL×D with length L, and the\\r\\ndecomposition layer returns XT and XS defined as,\\r\\nXT = AvgP ooling(P adding(X))\\r\\nXS = X − XT\\r\\n(4)\\r\\nXS and XT denote the seasonal and the extracted trend\\x02cyclical parts, respectively. We use the AvgPool(.) for the mov\\x02ing average with the padding procedure to maintain the series\\r\\nlength. Applying a reverse time series transformer (EDformer)\\r\\non decomposed signals, specifically on the seasonal and trend\\r\\ncomponents separately, enhances model interpretability and\\r\\nprecision. By isolating these components, the model can better\\r\\ncapture distinct temporal patterns and dependencies within\\r\\neach, leading to improved accuracy in detecting seasonality\\r\\nand trends. This approach also reduces noise interference,\\r\\nallowing the transformer to focus on key features in each\\r\\ndecomposed signal. Consequently, the model achieves a more\\r\\nnuanced representation of the underlying data dynamics than\\r\\nwhen applied to the whole signal at once.\\r\\nB. Reversible Model Inputs and Embedding\\r\\nInstead of taking multiple temporal tokens, our approach\\r\\ntakes one whole signal (Seasonal and Trends components) as\\r\\na single frame. The concept of processing the entire signal\\r\\nwithin a single frame is inspired by the iTransformer model\\r\\n[14]. Across multivariate series, our proposed encoder-only\\r\\nEDformer architecture encourages adaptive correlation and\\r\\nrepresentation learning. Every time series is first tokenized\\r\\ninto a frame capture the distinct characteristics of every\\r\\nvariable from both the components (seasonal+trend). After\\r\\nmodeling mutual interactions with self-attention, feed-forward\\r\\nnetworks process each series (seasonal+trends) separately to\\r\\nprovide its representation. This framework is optimized for\\r\\ncapturing intricate temporal dependencies in time series. The\\r\\nprocedure of forecasting future series of each distinct frame\\r\\nY\\r\\nt\\r\\n:, n based on the lookback series XS :, n and XT:, n in\\r\\nEDformer is straightforwardly expressed as follows in light of\\r\\nthe aforementioned considerations,\\r\\nh\\r\\n0\\r\\nn = Embedding(Reverse(XS\\r\\n:, n))\\r\\n+Embedding(Reverse(XT:, n)),\\r\\nH(l+1) = IntBlock(Hl), l = 0, ....., L − 1,\\r\\nY\\r\\nt\\r\\n:, n = P rojection(h\\r\\nL\\r\\nn\\r\\n),\\r\\n(5)\\r\\nwhere the superscript indicates the layer index and H =\\r\\nh1, ..., hN ∈ RN×D contains N embedded tokens of size D.\\r\\nMulti-layer perceptrons (MLPs) are used to implement both\\r\\nembedding and projection. The shared feed-forward network\\r\\nin each IntBlock() processes the acquired frames individually\\n4\\r\\nwhile interacting with one another through self-attention. In\\r\\nparticular, the position embedding in the vanilla Transformer is\\r\\nno longer required in this case because the neuron permutation\\r\\nof the feed-forward network automatically stores the order of\\r\\nsequence. The internal architecture of EDformer is represented\\r\\nin Figure 2.\\r\\nFig. 2: Internal architecture of EDformer blocks\\r\\nC. Model Encoding\\r\\nIn the encoder block, we arrange a stack of L blocks\\r\\nmade up of the feed-forward network, self-attention, and layer\\r\\nnormalization modules.\\r\\n1) Multivariate Self-attention: The proposed model views\\r\\nthe entire series of a single variable as an independent process,\\r\\nwhereas the attention mechanism is typically used to facilitate\\r\\nthe modelling of temporal connections in forecasting. The self\\x02attention mechanism allows the model to weigh the importance\\r\\nof different tokens in a sequence relative to each other, facil\\x02itating the capture of long-range dependencies and contextual\\r\\nrelationships. It adopts linear projections to obtain queries(Q),\\r\\nKeys(K) and values(V),\\r\\nQ = SWQ, K = SWk, V = SWv (6)\\r\\nWhere, WQ, Wk, Wv are learned weight matrices. Several\\r\\nkeys, queries, and values are fed into multi-scaled dot-product\\r\\nattention blocks by multi-head attention, which then concate\\x02nates the attention to produce the desired result. The attention\\r\\nscores can be computed as,\\r\\nscores =\\r\\nQKT\\r\\n√\\r\\ndk\\r\\n(7)\\r\\nwhere n represents an input sequence of length and dk repre\\x02sents the projected dimension. The computational complexity\\r\\nof computing the attention score is O(n\\r\\n2d). Therefore, we\\r\\nexecute Softmax normalization (O(n\\r\\n2d)) and weighted sum\\r\\napproaches.\\r\\nAttention(Q, K, V ) = Sof tmax(scores) (8)\\r\\nOutput = Attention(Q, K, V ) ∗ V (9)\\r\\nTransformer uses multivariate self-attention (MVA) with mul\\x02tiple sets of learned projections (H different time series)\\r\\ninstead of a single attention function.\\r\\nMV A(Q, K, V ) = Concat(h1, ...., hH)WO (10)\\r\\nWhere, hi = Attention(QWQ\\r\\ni\\r\\n, KW K\\r\\ni\\r\\n, V WV\\r\\ni\\r\\n).\\r\\n2) Layer Normalization: The ”Layer Normalization (Lay\\x02erNorm)” block comes next [25]. Initially, layer normalisation\\r\\nwas suggested as a way to improve deep networks’ training\\r\\nstability and convergence. The module in most Transformer\\x02based forecasters gradually fuses the variables with one an\\x02other by normalising the multivariate representation of the\\r\\nsame timestamp. Our inverted version applies normalisation to\\r\\nthe individual variate’s series representation as Equation 11,\\r\\nwhich has been researched and shown to be useful in solving\\r\\nnon-stationary situations. On the other hand, an oversmooth\\r\\ntime series will result from the normalisation of various tokens\\r\\nof time steps in the prior design.\\r\\nLayerNorm(H) = \"\\r\\nhn − Mean(hn)\\r\\np\\r\\nV ar(hn)\\r\\n|n = 1, ....., N#\\r\\n(11)\\r\\n3) Feed-forward network: The feed-forward network (FFN)\\r\\nis used by the transformer as the fundamental building block\\r\\nfor token representation encoding, and it is applied consistently\\r\\nto every token. As previously indicated, several variations of\\r\\nthe same timestamp that make up the token in the vanilla\\r\\nTransformer may be malpositioned and too localised to pro\\x02vide sufficient information for predictions. FFN is used on\\r\\neach variate frame’s series representation in the reversed form.\\r\\nThey are focused on storing the observed decomposed time\\r\\nseries and decoding the representations for subsequent series\\r\\nutilising dense non-linear connections by stacking inverted\\r\\nblocks [26]. As a more effective predictive representation\\r\\nlearner than self-attention applied on time points, a logical\\r\\nexplanation has been provided in which the neurones of MLP\\r\\nare trained to depict the intrinsic properties of any time\\r\\nseries, such as the amplitude, periodicity, and even frequency\\r\\nspectrums. This is sent to a ”feed-forward (FFN)” block, the\\r\\noutput of which has a ”LayerNorm” block. In the encoder\\r\\nblock, the entire multi-head attention and feed-forward blocks\\r\\nare repeated n number of times.\\r\\nF F N(H′) = ReLU(H′W1 + b\\r\\n1\\r\\n)W2 + b\\r\\n2\\r\\n(12)\\r\\nHere, H′is an output of the previous layer, W1, W2, b2are\\r\\ntrainable parameters. In a deeper module, a residual connection\\r\\nmodule followed by a layer normalization module is inserted\\r\\naround each module.\\r\\nH′ = LayerNorm(MV SelfAtten(X) + X) (13)\\r\\nH = LayerNorm(F F N(H′) + H′) (14)\\r\\nWhere MVSelfAtten(.) defines the multivariate self-attention\\r\\nmodule and LayerNorm(.) denotes the layer normalization\\r\\ntask.\\n5\\r\\nD. EDformer Components\\r\\nThe entire architecture of the proposed EDformer model is\\r\\ndiscussed below,\\r\\n1. Decomposition:\\r\\nThe decomposition layer decomposes the input time series\\r\\ndata into trend and seasonality components. The movingavg()\\r\\nlayer is used to capture the trend, and the residual (difference\\r\\nbetween the input and trend) represents the seasonality.\\r\\n2. Model:\\r\\nThe model sets up the structure, including:\\r\\n2.1 Embedding Layer: Converts the time series data into a\\r\\nform suitable for the transformer encoder.\\r\\n2.2 Transformer Encoder: Includes multiple layers of self\\x02attention, allowing the model to capture dependencies within\\r\\nthe time series.\\r\\n3. Forecasting Process:\\r\\nThis method performs the forecasting process by:\\r\\n- Normalizing seasonality to improve stability.\\r\\n- Encoding the normalized seasonality with embeddings and\\r\\ntransformer layers.\\r\\n- Projecting and de-normalizing the output to make it\\r\\ncompatible with the prediction.\\r\\n- Adding the trend component back for the final forecast.\\r\\n4. Forward Process: Handles forward propagation, using the\\r\\nforecast method for forecasting tasks.\\r\\nV. RESULT ANALYSIS\\r\\nA. Datasets\\r\\nThe datasets used in this study, detailed in Table I, encom\\x02pass both long-term and short-term time series forecasting\\r\\nscenarios. The ’Electricity Transformer Temperature (ETT)’\\r\\ndataset contains 7 factors of electricity transformer measure\\x02ments spanning with four subsets: ETTh1 and ETTh2 recorded\\r\\nhourly, and ETTm1 and ETTm2 recorded every 15 minutes.\\r\\nThe Exchange dataset comprises daily exchange rates from\\r\\n8 countries between 1990 and 2016. Additional datasets in\\x02clude ’Weather’ (21 meteorological factors collected every 10\\r\\nminutes), ’Electricity Consumption Load (ECL)’ with hourly\\r\\ndata from 321 clients, and Traffic data consisting of hourly\\r\\nroad occupancy rates from 862 sensors in the San Francisco\\r\\nBay area. The Solar-Energy dataset records 10-minute samples\\r\\nof solar power production from 137 PV plants. PEMS traffic\\r\\ndata is widely associated with the California ’Performance\\r\\nMeasurement System (PeMS)’, a comprehensive database used\\r\\nfor freeway performance monitoring in California. For short\\x02term forecasting, we utilize four subsets of the PEMS traffic\\r\\nnetwork data (PEMS03, PEMS04, PEMS07, and PEMS08),\\r\\nwhich contain traffic flow information recorded at 5-minute\\r\\nintervals [27]. These diverse datasets, with varying tempo\\x02ral granularities and dimensionalities, enable comprehensive\\r\\nevaluation of forecasting models across different domains and\\r\\ntime scales. The M4 experimental dataset consists of 100,000\\r\\ntime series covering diverse domains, as detailed in Table II.\\r\\nThis M4 dataset allows us to evaluate the proposed EDformer\\r\\nmodel’s ability to handle short-term prediction tasks with high\\r\\nvariability. All datasets are publicly available and divided into\\r\\ntraining, validation, and test sets in the benchmark Time-Series\\r\\nLibrary (TSLib).\\r\\nB. Experimental Analysis\\r\\nIn this section, we perform in-depth experiments to assess\\r\\nhow well our proposed EDformer model forecasts in conjunc\\x02tion with state-of-the-art time-series forecasting architectures.\\r\\nAn ablation work is also applied to measure the effect of\\r\\nthe proposed modules. All the experiments are implemented\\r\\nin PyTorch, CUDA version 12.2 and conducted on a single\\r\\nNVIDIA-GeForce RTX 3090 with 24GB GPU. We have repli\\x02cated all of the compared baseline models and implemented\\r\\nthem using the benchmark Time-Series Library (TSLib) [28]\\r\\nrepository, which is based on the configurations provided by\\r\\nthe official code or actual article for each model. We train\\r\\nthe proposed model on all datasets using a batch size of\\r\\n32, a learning rate of 0.0001 except PEMS, and the ADAM\\r\\noptimizer with L2 loss. The model operates with K = 4 scales.\\r\\nThe comprehensive forecasting results are listed in Tables\\r\\nIII, IV, and V. The inclusion of an embedded reverse de\\x02composition process further optimizes the model’s perfor\\x02mance. A lower MSE/MAE reflects more accurate predictions,\\r\\nand our proposed lightweight EDformer consistently achieves\\r\\nthe best performance across most datasets for multivariate\\r\\nlong-term forecasting analysis. It outperforms state-of-the-art\\r\\nmodels such as Autoformer [12], Informer [11], Reformer\\r\\n[15], Pyraformer, Nonstationary-transformer (NS-Trans [29]),\\r\\nATFNet [30], and MICN [31], particularly excelling in high\\x02dimensional time series forecasting. Figure 4 and 5 show\\r\\nsample long-term predictions for some popular architectures:\\r\\nEDformer, Autoformer, Informer, MICN, Nonstationary trans\\x02former and Pyraformer on ETTm1 and traffic datasets. In\\r\\naddition to the results presented in Table V, further comparison\\r\\nof forecasting methods, including models listed in the table,\\r\\nis provided in Figure 3. Additionally, a comparison of these\\r\\nwidely used models based on computational cost, speed, and\\r\\naverage execution time is provided in Tables IX and X. For\\r\\nshort-term forecasting performance, the proposed model has\\r\\ncompetitive performance with Autoformer, Informer, MICN,\\r\\nNon-stationary transformer (NS-Trans) and Pyraformer as\\r\\nshown in Table VII. The additional experiment using the M4\\r\\ndataset is presented in Table VIII. Table VIII shows that\\r\\nour proposed EDformer model performs well and provides\\r\\nstate-of-the-art accuracy compared to other models. Table IX\\r\\npresents a comparison of multivariate short-term forecasting\\r\\nmodels in terms of execution time (in seconds) across four\\r\\nPEMS datasets. Figure 6 shows sample short-term predic\\x02tions for some popular architectures: EDformer, Autoformer,\\r\\nInformer, and Pyraformer on the PEMS03 dataset. From\\r\\nthe perspective of the PEMS dataset, our EDformer model\\r\\ncompetes strongly with state-of-the-art models in terms of\\r\\naccuracy. Additionally, EDformer stands out for its lightweight\\r\\ndesign, achieving comparable results in significantly less time\\r\\ncompared to other approaches. However, an extensive analysis\\n6\\r\\nTABLE I: Detailed dataset descriptions for long-term and short-term forecasting datasets\\r\\nForecasting Dataset Dim Size Frequency Information\\r\\nLong-term ETTh1, ETTh2 7 (8545,2881,2881) Hourly Electricity\\r\\nETTm1, ETTm2 7 (34465,11521,11521) 15 min Electricity\\r\\nWeather 21 (36792,5271,10540) 10 min Weather\\r\\nElectricity 321 (18317,2633,5261) Hourly Electricity\\r\\nTraffic 862 (12185,1757,3509) Hourly Transportation\\r\\nExchange 8 (5120,665,1422) Daily Economy\\r\\nShort-term PEMS03 358 (15617,5135,5135) 5 min Transportation\\r\\nPEMS04 307 (10172,3375,3375) 5 min Transportation\\r\\nPEMS07 883 (16911,5622,5622) 5 min Transportation\\r\\nPEMS08 170 (10690,3548,3548) 5 min Transportation\\r\\nTABLE II: Number of M4 series per data frequency and domain\\r\\nTime interval between successive observations Micro Industry Macro Finance Demographic Other Total\\r\\nYearly 6,538 3,716 3,903 6,519 1,088 1,236 23,000\\r\\nQuarterly 6,020 4,637 5,315 5,305 1,858 865 24,000\\r\\nMonthly 10,975 10,017 10,016 10,987 5,728 277 48,000\\r\\nWeekly 112 6 41 164 24 12 359\\r\\nDaily 1,476 422 127 1,559 10 633 4,227\\r\\nHourly 0 0 0 0 0 414 414\\r\\nTotal 25,121 18,798 19,402 24,534 8,708 3,437 100,000\\r\\nof standard deviations on the long-term forecasting values\\r\\n(MSE, MAE) of each dataset and horizon is described in\\r\\nTable VI. A moderate-to-low standard deviation is desirable.\\r\\nIt indicates that the time series has predictable patterns, and\\r\\nthe forecasting model can achieve low MSE and MAE.\\r\\nFig. 3: Comparison of models efficiency with datasets vs. avg.\\r\\nMSE vs. avg. MAE\\r\\nVI. ABLATION STUDIES AND PERFORMANCE\\r\\nCOMPARISONS\\r\\nThe ablation study presented in Table XI shows the average\\r\\nperformance of different configurations on seven datasets:\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.9\\r\\n1.8\\r\\n1.7\\r\\n1.6\\r\\n1.5\\r\\n1.4\\r\\n1.3\\r\\n1.2\\r\\nGroundTruth\\r\\nPrediction\\r\\n(a) Autoformer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\nGroundTruth\\r\\nPrediction\\r\\n(b) Informer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.5\\r\\n1.4\\r\\n1.3\\r\\n1.2\\r\\n1.1\\r\\n1.0\\r\\n0.9\\r\\n0.8\\r\\nGroundTruth\\r\\nPrediction\\r\\n(c) Pyraformer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.7\\r\\n1.6\\r\\n1.5\\r\\n1.4\\r\\n1.3\\r\\n1.2 GroundTruth\\r\\nPrediction\\r\\n(d) MICN\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\nGroundTruth\\r\\nPrediction\\r\\n(e) Reformer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.5\\r\\n1.4\\r\\n1.3\\r\\n1.2 GroundTruth\\r\\nPrediction\\r\\n(f) EDformer\\r\\nFig. 4: Visualization of predictions (length:96) on ETTm1\\r\\ndataset\\r\\nEtth1, Ettm1, Weather, Electricity, and Traffic, over four\\r\\nprediction lengths (96, 192, 336, 720). We aim to show\\r\\nthe effects of incorporating reverse-embedding operations and\\r\\ndecomposition mechanisms into the forecasting model. The\\r\\ntable compares the mean squared error (MSE) and mean abso\\x02lute error (MAE) for three key configurations: using without\\r\\ndecomposition, with decomposition, and reverse embedding\\r\\nin combination with decomposition. The results demonstrate\\n7\\r\\nTABLE III: Measure of error coefficients on multivariate long-term forecasting results with different prediction lengths (96,\\r\\n192, 336, 720)\\r\\nModels Autoformer Informer NS-Trans Reformer\\r\\nDatabase Metric MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nETTh1 96 0.505 0.482 0.649 0.554 0.574 0.513 0.857 0.689\\r\\n192 0.477 0.471 1.012 0.786 0.941 0.728 0.843 0.714\\r\\n336 0.525 0.507 1.029 0.782 0.633 0.558 1.026 0.772\\r\\n720 0.512 0.509 1.240 0.892 0.676 0.584 1.255 0.866\\r\\nETTh2 96 0.375 0.409 2.835 1.329 0.484 0.461 2.202 1.143\\r\\n192 0.463 0.461 6.161 2.079 0.549 0.500 2.699 1.283\\r\\n336 0.472 0.494 5.372 1.951 0.593 0.522 2.767 1.266\\r\\n720 0.481 0.489 4.292 1.728 0.644 0.556 2.749 1.338\\r\\nETTm1 96 0.539 0.494 0.627 0.560 0.423 0.416 0.881 0.663\\r\\n192 0.566 0.511 0.727 0.618 0.476 0.450 0.876 0.682\\r\\n336 0.621 0.534 1.248 0.891 0.579 0.491 1.063 0.750\\r\\n720 0.561 0.515 0.959 0.737 0.609 0.533 1.264 0.830\\r\\nETTm2 96 0.295 0.337 0.372 0.459 0.256 0.318 0.718 0.630\\r\\n192 0.292 0.349 0.819 0.711 0.570 0.463 1.808 1.003\\r\\n336 0.329 0.365 1.382 0.902 0.614 0.503 2.417 1.170\\r\\n720 0.436 0.424 4.292 1.541 1.128 0.718 3.097 1.334\\r\\nWeather 96 0.319 0.369 0.351 0.410 0.191 0.236 0.367 0.407\\r\\n192 0.295 0.353 0.712 0.603 0.284 0.316 0.459 0.478\\r\\n336 0.346 0.385 0.422 0.441 0.296 0.320 0.671 0.597\\r\\n720 0.559 0.524 1.026 0.737 0.351 0.386 0.645 0.604\\r\\nElectricity 96 0.216 0.335 0.336 0.419 0.170 0.271 0.311 0.399\\r\\n192 0.274 0.363 0.353 0.436 0.182 0.284 0.334 0.413\\r\\n336 0.243 0.353 0.359 0.442 0.199 0.299 0.355 0.427\\r\\n720 0.287 0.372 0.401 0.461 0.226 0.324 0.326 0.403\\r\\nTraffic 96 0.668 0.400 0.741 0.423 0.622 0.346 0.723 0.405\\r\\n192 0.658 0.417 0.766 0.434 0.645 0.355 0.710 0.390\\r\\n336 0.667 0.414 0.893 0.506 0.651 0.354 0.697 0.384\\r\\n720 0.650 0.403 1.049 0.587 0.676 0.369 0.707 0.387\\r\\nExchange 96 0.149 0.279 0.935 0.778 0.141 0.262 0.994 0.799\\r\\n192 0.285 0.389 1.116 0.843 0.216 0.335 1.456 0.978\\r\\n336 0.969 0.728 1.504 0.985 0.493 0.510 1.861 1.127\\r\\n720 1.112 0.820 2.932 1.414 1.170 0.800 1.835 1.149\\r\\nTABLE IV: Measure of error coefficients on multivariate long-term forecasting results with different prediction lengths\\r\\n(96,192,336,720) (continue)\\r\\nModels ATFNet MICN Pyraformer EDformer\\r\\nDatabase Metric MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nETTh1 96 0.418 0.442 0.413 0.442 0.644 0.597 0.404 0.415\\r\\n192 0.487 0.493 0.451 0.462 0.843 0.714 0.492 0.481\\r\\n336 0.516 0.522 0.556 0.528 1.091 0.841 0.544 0.519\\r\\n720 0.640 0.597 0.658 0.607 1.005 0.802 0.628 0.559\\r\\nETTh2 96 0.183 0.288 0.303 0.364 1.380 0.917 0.386 0.417\\r\\n192 0.226 0.342 0.403 0.446 5.927 1.917 0.521 0.490\\r\\n336 0.261 0.363 0.603 0.550 4.515 1.816 0.621 0.562\\r\\n720 0.339 0.426 1.106 0.852 4.367 1.798 0.625 0.554\\r\\nETTm1 96 0.376 0.402 0.308 0.360 0.593 0.520 0.338 0.370\\r\\n192 0.427 0.432 0.343 0.384 0.626 0.547 0.367 0.392\\r\\n336 0.480 0.464 0.395 0.411 0.719 0.616 0.426 0.446\\r\\n720 0.551 0.513 0.427 0.434 0.958 0.732 0.560 0.518\\r\\nETTm2 96 0.118 0.231 0.169 0.268 0.435 0.507 0.208 0.294\\r\\n192 0.147 0.258 0.247 0.333 0.730 0.673 0.529 0.482\\r\\n336 0.182 0.288 0.290 0.351 1.201 0.845 0.712 0.588\\r\\n720 0.231 0.327 0.417 0.434 3.625 1.415 0.726 0.597\\r\\nWeather 96 0.158 0.208 0.178 0.249 0.203 0.285 0.219 0.261\\r\\n192 0.199 0.248 0.243 0.269 0.219 0.304 0.252 0.294\\r\\n336 0.248 0.287 0.278 0.338 0.294 0.368 0.325 0.336\\r\\n720 0.312 0.336 0.320 0.360 0.388 0.407 0.423 0.402\\r\\nElectricity 96 0.136 0.230 0.157 0.266 0.279 0.374 0.169 0.268\\r\\n192 0.151 0.248 0.175 0.287 0.294 0.390 0.176 0.271\\r\\n336 0.169 0.265 0.200 0.308 0.299 0.395 0.189 0.286\\r\\n720 0.207 0.299 0.228 0.338 0.298 0.386 0.254 0.341\\r\\nTraffic 96 0.380 0.264 0.473 0.293 0.702 0.404 0.465 0.325\\r\\n192 0.399 0.275 0.483 0.298 0.678 0.387 0.446 0.314\\r\\n336 0.419 0.280 0.491 0.303 0.689 0.390 0.456 0.326\\r\\n720 0.441 0.299 0.559 0.327 0.705 0.410 0.504 0.361\\r\\nExchange 96 0.114 0.251 0.278 0.315 0.533 0.590 0.112 0.247\\r\\n192 0.465 0.511 0.392 0.441 0.960 0.774 0.470 0.527\\r\\n336 0.559 0.603 0.551 0.592 1.191 0.861 0.691 0.651\\r\\n720 0.609 0.711 0.612 0.683 1.670 1.040 0.875 0.912\\r\\nthat when average decomposition is employed (second and\\r\\nthird rows), the model consistently outperforms the model\\r\\nwithout decomposition (first row) across all datasets. This\\r\\nhighlights the significance of allowing the model to learn\\r\\nhow to decompose the input time series rather than using\\r\\na predefined approach. Moreover, the addition of reverse\\r\\nembedding further improves the results (third row), suggesting\\r\\nthat reverse embedding, which reverses the whole seasonal\\n8\\r\\nTABLE V: Comparison of average error coefficients on multivariate long-term forecasting result\\r\\nModels Autoformer Informer NS-Trans Reformer ATFNet MICN PatchTST EDformer\\r\\nDatabase MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nETTh1 0.504 0.492 1.058 0.808 0.609 0.541 1.019 0.763 0.515 0.513 0.519 0.509 0.895 0.738 0.594 0.537\\r\\nETTh2 0.447 0.463 4.665 1.771 0.567 0.509 2.604 1.257 0.252 0.354 0.603 0.553 4.047 1.625 0.538 0.505\\r\\nETTm1 0.571 0.513 0.890 0.701 0.521 0.472 1.021 0.731 0.458 0.452 0.368 0.397 0.724 0.603 0.422 0.431\\r\\nETTm2 0.338 0.368 1.716 0.903 0.642 0.500 2.010 1.034 0.169 0.276 0.281 0.347 1.497 0.869 0.543 0.479\\r\\nWeather 0.379 0.407 0.627 0.547 0.280 0.314 0.535 0.521 0.229 0.270 0.254 0.304 0.276 0.341 0.304 0.323\\r\\nElectricity 0.255 0.355 0.362 0.439 0.199 0.294 0.331 0.410 0.201 0.261 0.198 0.299 0.292 0.386 0.195 0.292\\r\\nTraffic 0.661 0.408 0.862 0.487 0.648 0.356 0.709 0.391 0.479 0.339 0.502 0.345 0.693 0.397 0.467 0.332\\r\\nExchange 0.628 0.554 1.621 1.005 0.505 0.476 1.536 1.013 0.436 0.519 0.458 0.507 1.088 0.816 0.537 0.584\\r\\n# of Total\\r\\nWins\\r\\n2 2 0 0 0 1 0 0 3 2 0 0 0 0 3 3\\r\\nTABLE VI: Comparison of Standard deviations on multivariate long-term forecasting result with prediction horizons\\r\\n(96,192,336,720)\\r\\nModels Autoformer Informer NS-Trans Reformer ATFNet MICN Pyraformer EDformer\\r\\nDatabase MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nETTh1 0.017 0.016 0.212 0.123 0.140 0.081 0.166 0.068 0.080 0.056 0.095 0.064 0.170 0.093 0.081 0.053\\r\\nETTh2 0.042 0.033 1.247 0.284 0.058 0.034 0.233 0.071 0.057 0.049 0.064 0.184 1.655 0.403 0.097 0.058\\r\\nETTm1 0.030 0.014 0.239 0.126 0.075 0.043 0.159 0.065 0.065 0.041 0.045 0.027 0.142 0.081 0.085 0.057\\r\\nETTm2 0.058 0.033 1.529 0.400 0.312 0.143 0.874 0.261 0.042 0.035 0.089 0.059 1.257 0.342 0.208 0.125\\r\\nWeather 0.125 0.068 0.266 0.131 0.057 0.053 0.127 0.082 0.057 0.047 0.052 0.046 0.073 0.048 0.078 0.052\\r\\nElectricity 0.027 0.013 0.023 0.015 0.021 0.019 0.015 0.011 0.026 0.025 0.026 0.026 0.008 0.007 0.033 0.029\\r\\nTraffic 0.007 0.007 0.121 0.065 0.019 0.008 0.009 0.008 0.022 0.012 0.033 0.013 0.011 0.009 0.021 0.017\\r\\nExchange 0.417 0.225 0.783 0.247 0.405 0.207 0.352 0140 0.193 0.170 0.131 0.141 0.410 0.162 0.284 0.239\\r\\nTABLE VII: Comparison of average error coefficients on multivariate short-term forecasting result with prediction horizon 24\\r\\nand fixed look-back 96\\r\\nModels Autoformer Informer NS-Trans Reformer ATFNet MICN PatchTST EDformer\\r\\nDatabase MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nPEMS03 0.469 0.508 0.111 0.226 0.106 0.216 0.115 0.231 0.146 0.262 0.141 0.275 0.179 0.279 0.187 0.307\\r\\nPEMS04 0.346 0.437 0.092 0.198 0.103 0.215 0.100 0.210 0.158 0.278 0.195 0.327 0.221 0.316 0.191 0.311\\r\\nPEMS07 0.223 0.351 0.108 0.216 0.100 0.211 0.108 0.219 0.144 0.264 0.123 0.253 0.189 0.294 0.165 0.291\\r\\nPEMS08 0.485 0.537 0.128 0.249 0.116 0.229 0.137 0.259 0.141 0.261 0.177 0.312 0.218 0.295 0.214 0.336\\r\\nTABLE VIII: Short-term forecasting results in the M4 dataset with a single variate. All prediction lengths are in [6, 48]. The\\r\\nvalues highlighted in red colour indicate the best results for each row.\\r\\nMetric Category EDformer iTransformer Reformer NS-Trans Informer Autoformer\\r\\nsMAPE\\r\\nYearly 14.259 14.409 14.548 15.833 15.215 16.909\\r\\nQuarterly 11.407 10.777 11.922 12.366 12.696 14.445\\r\\nMonthly 15.558 16.650 14.649 14.607 15.210 18.280\\r\\nOthers 5.222 5.543 6.694 7.005 7.183 6.676\\r\\nAverage 13.796 14.170 14.192 14.201 14.206 16.464\\r\\nMAPE\\r\\nYearly 17.558 19.191 17.789 20.485 19.837 23.266\\r\\nQuarterly 13.006 12.871 12.737 14.490 14.969 16.882\\r\\nMonthly 18.318 20.144 15.830 16.988 17.972 22.442\\r\\nOthers 7.142 7.750 10.456 10.459 10.469 11.146\\r\\nAverage 16.409 17.560 14.971 16.689 17.305 20.732\\r\\nMASE\\r\\nYearly 3.158 3.218 3.232 3.532 3.398 3.761\\r\\nQuarterly 1.426 1.284 1.313 1.519 1.561 1.854\\r\\nMonthly 1.189 1.392 1.262 1.177 1.217 1.572\\r\\nOthers 4.568 3.998 4.424 4.691 4.937 4.833\\r\\nAverage 1.868 1.916 1.894 1.910 1.987 2.306\\r\\nOWA\\r\\nYearly 0.834 0.846 0.796 0.929 0.893 0.991\\r\\nQuarterly 1.038 0.957 0.975 1.115 1.145 1.332\\r\\nMonthly 1.098 1.232 0.972 1.060 1.099 1.373\\r\\nOthers 1.375 1.214 1.402 1.502 1.534 1.465\\r\\nAverage 0.997 1.023 0.921 1.039 1.043 1.210\\r\\nTABLE IX: The cost time (sec) and speed (running time, s/iter) of some benchmark models on ETTh1 dataset\\r\\nSeries Length 96 192 336 720\\r\\nModels Speed Cost time Speed Cost time Speed Cost time Speed Cost time\\r\\nEDformer 0.0050 1.5344 0.0053 1.5714 0.0055 1.5811 0.0058 1.5299\\r\\nPyraformer 0.0194 5.3420 0.0199 5.3802 0.0199 5.2862 0.0202 5.1609\\r\\nAutoformer 0.0402 10.881 0.0525 13.952 0.0719 18.689 0.1278 28.331\\r\\nReformer 0.0361 9.7632 0.0531 14.082 0.0764 19.823 0.1451 35.682\\r\\nInformer 0.0222 6.0839 0.0269 7.2604 0.0349 9.1842 0.0542 13.477\\r\\nFEDformer 0.1428 38.693 0.1524 40.891 0.1614 42.312 0.2048 51.016\\r\\nsignal embedding, provides crucial temporal information that\\r\\nenhances forecasting accuracy. Overall, the combination of\\r\\naverage decomposition and reverse embedding leads to the\\r\\nmost significant performance gains, making these features\\n9\\r\\nTABLE X: Comparison of multivariate long-term and short-term forecasting results in terms of average execution time (sec)\\r\\nDatasets EDformer Autoformer Informer Reformer Pyraformer NS-Trans\\r\\nExecution Time (Sec)\\r\\n(Total)\\r\\nETTh1 0.831 2.715 1.591 1.928 1.541 1.220\\r\\nETTh2 0.746 4.242 1.551 1.936 1.544 1.315\\r\\nWeather 3.168 12.462 5.423 6.711 5.245 5.831\\r\\nExchange 0.514 1.826 0.928 1.114 0.884 0.991\\r\\nPEMS03 0.878 1.162 1.150 1.190 1.177 1.171\\r\\nPEMS04 0.848 1.116 1.253 1.210 1.213 1.163\\r\\nPEMS07 1.180 1.302 1.287 1.390 1.316 1.378\\r\\nPEMS08 0.703 1.059 1.099 1.126 1.066 1.078\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nGroundTruth\\r\\nPrediction\\r\\n(a) Autoformer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nGroundTruth\\r\\nPrediction\\r\\n(b) Informer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nGroundTruth\\r\\nPrediction\\r\\n(c) NSTransformer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nGroundTruth\\r\\nPrediction\\r\\n(d) MICN\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nGroundTruth\\r\\nPrediction\\r\\n(e) Reformer\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nGroundTruth\\r\\nPrediction\\r\\n(f) EDformer\\r\\nFig. 5: Visualization of predictions (length:96) on Traffic\\r\\ndataset\\r\\nessential components in improving the model’s forecasting\\r\\nability.\\r\\nVII. EXPLAINABILITY ANALYSIS\\r\\nIn multivariate time series forecasting, model explainability\\r\\nis essential because it enables stakeholders to comprehend,\\r\\nbelieve, and analyse forecasts produced by intricate models.\\r\\nUnderstanding how each feature affects the prediction outcome\\r\\nis essential for diagnosing model behaviour, enhancing model\\r\\nrobustness, and learning about variable dependencies because\\r\\nmultivariate time series data is high-dimensional and involves\\r\\nmultiple interdependent variables that affect the forecast [32].\\r\\nEach prediction can be attributed to a different characteris\\x02tic using some well-known feature explainability techniques\\r\\nlike feature ablation, feature occlusion, integrated gradients,\\r\\ngradient-SHAP, and windowed feature importance in time.\\r\\nThese techniques are briefly described below.\\r\\n0 20 40 60 80 100 120\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\nGroundTruth\\r\\nPrediction\\r\\n(a) EDformer\\r\\n0 20 40 60 80 100 120\\r\\n1.50\\r\\n1.25\\r\\n1.00\\r\\n0.75\\r\\n0.50\\r\\n0.25\\r\\n0.00\\r\\nGroundTruth\\r\\nPrediction\\r\\n(b) Autoformer\\r\\n0 20 40 60 80 100 120\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\nGroundTruth\\r\\nPrediction\\r\\n(c) Informer\\r\\n0 20 40 60 80 100 120\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\nGroundTruth\\r\\nPrediction\\r\\n(d) Pyraformer\\r\\nFig. 6: Visualization of prediction results on PEMS03 dataset\\r\\nfor prediction length 24\\r\\nTABLE XI: Ablation Study: Comparison of Multivariate\\r\\nLong-Term Forecasting for the Average Results Across Pre\\x02diction Horizons 96, 192, 336, 720\\r\\nWithout\\r\\ndecompose\\r\\nWith\\r\\ndecompose\\r\\nReverse\\r\\nEmbedding MSE MAE\\r\\n✓ − − 1.010 0.810\\r\\nEtth1 − ✓ − 0.789 0.697\\r\\n− ✓ ✓ 0.594 0.537\\r\\n✓ − − 0.905 0.799\\r\\nEttm1 − ✓ − 0.769 0.638\\r\\n− ✓ ✓ 0.422 0.431\\r\\n✓ − − 0.691 0.613\\r\\nWeather − ✓ − 0.601 0.552\\r\\n− ✓ ✓ 0.304 0.323\\r\\n✓ − − 0.441 0.502\\r\\nElectricity − ✓ − 0.361 0.399\\r\\n− ✓ ✓ 0.195 0.292\\r\\n✓ − − 0.697 0.381\\r\\nTraffic − ✓ − 0.601 0.349\\r\\n− ✓ ✓ 0.467 0.332\\r\\n• Feature Ablation (FA) is an explainability technique that\\r\\nsystematically removes one feature at a time from the in\\x02put data to assess its impact on the model’s performance.\\r\\nBy measuring the change in prediction accuracy or out\\x02put, this method identifies the importance of individual\\r\\nfeatures. It helps highlight which features contribute most\\r\\nsignificantly to the model’s decisions, making it useful for\\r\\nunderstanding the behavior of complex models [33].\\r\\n• Feature Occlusion (FO) involves masking or replacing\\r\\nparts of the input data (e.g., setting a feature to zero) to\\n10\\r\\nobserve the change in the model’s output. By selectively\\r\\noccluding features or segments of the input, this method\\r\\ncan identify which components are most influential in\\r\\nthe prediction process. It is commonly used in time series\\r\\nand image data analysis to understand spatial or temporal\\r\\nfeature importance [33].\\r\\n• Integrated Gradients (IG) is an attribution method that\\r\\nexplains model predictions by accumulating gradients\\r\\nalong a path from a baseline input (e.g., all zeros) to the\\r\\nactual input. It calculates the contribution of each feature\\r\\nby integrating the gradients of the model’s output with\\r\\nrespect to the input features. This technique ensures that\\r\\nthe attributions are consistent and satisfy properties like\\r\\ncompleteness, making it a reliable approach for model\\r\\ninterpretability [34].\\r\\n• Shapley Additive explanations (SHAP) uses coopera\\x02tive game theory to allocate feature priority, quantifying\\r\\neach variable’s contribution across all conceivable feature\\r\\nsubsets [35]. In the context of a Transformer model for\\r\\nmultivariate time series forecasting, SHAP can provide\\r\\ninsights into [36]:\\r\\n– Which time steps (lags) are most influential?\\r\\n– Which variables (features) contribute most to the\\r\\nprediction?\\r\\n– How do different values impact the forecast?\\r\\nGradient SHAP (GS) is a hybrid explainability tech\\x02nique that combines SHAP (SHapley Additive exPla\\x02nations) values with gradient-based methods. It uses\\r\\nstochastic sampling and integrates gradients over a range\\r\\nof input samples to approximate the Shapley values\\r\\nfor each feature. This method provides robust feature\\r\\nattributions and is particularly effective for complex, non\\x02linear models, offering insights into how each feature\\r\\ninfluences the predictions [34].\\r\\n• Windowed Feature Importance in Time (WinIT) is\\r\\na feature removal based explainability approach. WinIT\\r\\nexplicitly incorporates temporal dependencies by consid\\x02ering the relationship between consecutive observations\\r\\nof the same feature when calculating its importance score.\\r\\nAdditionally, it accounts for the dynamic nature of feature\\r\\nimportance over time by summarizing its significance\\r\\nacross a window of previous time steps, thereby capturing\\r\\ntemporal variations in the feature’s influence [28, 37].\\r\\nTwo metrics [28] have been used here to measure the impor\\x02tance of these above explainability methods.\\r\\nA. Comprehensiveness: It evaluates how important a subset of\\r\\nfeatures is for the model’s prediction. It measures the impact of\\r\\nremoving these key features on the model’s output. The idea is\\r\\nthat if the identified important features are truly contributing to\\r\\nthe prediction, then omitting them should lead to a significant\\r\\ndrop in the model’s confidence or prediction score. A higher\\r\\ncomprehensiveness score indicates better feature attribution,\\r\\nas it confirms that the model relies on these features to\\r\\nmake its decisions. It helps in validating the quality of the\\r\\nexplainability technique by checking if the most influential\\r\\nfeatures indeed hold crucial information for the model. Table\\r\\nXII presents a comparison of ’Comprehensiveness’ scores (av\\x02erage MAE) for some state-of-the-art explainability methods\\r\\non the Electricity dataset. Table XIII presents a comparison of\\r\\n’Comprehensiveness’ scores (average MSE) for some state\\x02of-the-art explainability methods on the Electricity dataset.\\r\\nIt is evident that our proposed EDformer model registers\\r\\nthe highest number of wins in terms of ’Comprehensiveness’\\r\\ncompared to other methods.\\r\\nB. Sufficiency: It assesses whether the identified features\\r\\nalone are sufficient to maintain the prediction outcome. It\\r\\ndoes this by measuring the performance of the model when\\r\\nonly the key attributed features are retained, while the rest\\r\\nare removed or masked. If the model’s predictions remain\\r\\nconsistent, it indicates that these features are adequate for\\r\\nmaking accurate predictions. For the sufficiency metric, a\\r\\nhigher value is considered better. The sufficiency score helps\\r\\ndetermine if the feature attribution is complete, ensuring that\\r\\nthe identified features not only contribute significantly but also\\r\\ncapture enough information to make reliable predictions. Table\\r\\nXIV presents a comparison of ’Sufficieny’ scores (average\\r\\nMAE) for some state-of-the-art explainability methods on\\r\\nthe Electricity dataset. Table XV presents a comparison of\\r\\n’Sufficieny’ scores (average MSE) for some state-of-the-art ex\\x02plainability methods on the Electricity dataset. It is evident that\\r\\nour proposed EDformer model registers the highest number of\\r\\nwins in terms of ’Sufficiency’ compared to other methods.\\r\\nFig. 7: Comparison on execution time (minutes) for model\\r\\ninterpretability on Electricity dataset\\r\\nFigure 7 shows the total execution time (minutes) taken for\\r\\ninterpretability assessment of each model on the electricity\\r\\ndataset.\\r\\nA. Discussion on Explainability and Feature Importance\\r\\nTable XII to Table XIV evaluate the comprehensiveness\\r\\nand sufficiency of explainability methods across six fore\\x02casting models (Autoformer, Informer, NS-Trans, Reformer,\\r\\nMICN, and EDformer) for the Electricity (ECL) dataset with\\r\\na prediction length of 24. Comprehensiveness measures the\\r\\nreduction in predictive accuracy when the most important\\r\\nfeatures identified by an explainability method are removed,\\r\\nwhile sufficiency assesses the model’s performance when only\\r\\nthese key features are retained. The highest scores (marked in\\r\\nred) indicate the explainability method’s ability to capture the\\r\\nmost impactful features effectively. The analysis reveals that\\n11\\r\\nTABLE XII: Comparison of Comprehensiveness (avg. of MAE) among explainability methods on Electricity dataset for\\r\\nprediction length 24. The highest value is marked by red colour.\\r\\nMethod Autoformer Informer NS-Trans Reformer MICN EDformer\\r\\nFA 12.1 4.1 9.8 3.4 12.0 11.6\\r\\nFO 14.1 6.1 12.6 4.3 11.0 14.7\\r\\nIG 13.0 12.1 14.9 8.5 13.6 14.4\\r\\nGS 11.0 5.3 7.4 5.8 10.4 8.2\\r\\nWinIT 14.1 6.5 12.4 4.8 12.6 14.3\\r\\n# of Total Wins 2 0 1 0 0 2\\r\\nTABLE XIII: Comparison of Comprehensiveness (avg. of MSE) among explainability methods on Electricity dataset for\\r\\nprediction length 24. The highest value is marked by red colour.\\r\\nMethod Autoformer Informer NS-Trans Reformer MICN EDformer\\r\\nFA 9.68 1.47 8.4 1.0 10.5 10.7\\r\\nFO 12.8 3.5 12.2 1.9 11.2 15.8\\r\\nIG 11.1 12.0 16.9 6.3 12.9 15.2\\r\\nGS 8.2 2.7 5.1 3.3 8.5 5.9\\r\\nWinIT 12.7 4.0 11.7 2.2 11.3 14.8\\r\\n# of Total Wins 0 0 1 0 1 3\\r\\nTABLE XIV: Comparison of Sufficiency (avg. of MAE) among explainability methods on Electricity dataset for prediction\\r\\nlength 24. The highest value is marked by red colour.\\r\\nMethod Autoformer Informer NS-Trans Reformer MICN EDformer\\r\\nFA 14.5 15.3 15.5 12.2 11.1 17.2\\r\\nFO 14.1 14.9 13.5 11.7 9.8 15.0\\r\\nIG 16.3 18.9 21.2 15.5 16.3 28.6\\r\\nGS 15.3 14.8 17.7 12.1 14.6 21.9\\r\\nWinIT 14.1 14.6 13.6 11.5 9.8 15.8\\r\\n# of Total Wins 0 0 0 0 0 5\\r\\nTABLE XV: Comparison of Sufficiency (avg. of MSE) among explainability methods on Electricity dataset for prediction\\r\\nlength 24. The highest value is marked by red colour.\\r\\nMethod Autoformer Informer NS-Trans Reformer MICN EDformer\\r\\nFA 13.3 16.2 16.2 10.5 8.7 19.8\\r\\nFO 12.6 15.4 12.8 9.7 7.0 15.5\\r\\nIG 16.4 22.8 27.3 16.3 17.1 51.7\\r\\nGS 14.6 14.7 20.1 10.2 14.0 31.2\\r\\nWinIT 12.6 15.0 13.1 9.4 7.1 16.9\\r\\n# of Total Wins 0 0 0 0 0 5\\r\\nEDformer consistently achieves the highest scores in both met\\x02rics, particularly excelling in Sufficiency. This suggests that\\r\\nEDformer benefits the most from the features identified by the\\r\\nexplainability methods, reinforcing its robustness in utilizing\\r\\nrelevant information. Conversely, models like Informer and\\r\\nReformer have fewer ”wins,” indicating potential room for\\r\\nimprovement in aligning their performance with explainability\\r\\ninsights. The use of this explainability analysis ensures that\\r\\nmodels are not treated as ”black boxes.” It helps identify\\r\\nthe significance of input features, improves transparency, and\\r\\nfosters trust in AI predictions. This is particularly crucial\\r\\nfor datasets like ’Electricity’, where decisions can impact\\r\\ncritical energy management systems. By understanding feature\\r\\nimportance, stakeholders can optimize model inputs, enhance\\r\\ninterpretability, and ensure alignment with real-world domain\\r\\nknowledge.\\r\\nThe feature importance analysis in multivariate time se\\x02ries forecasting models provides valuable insights into the\\r\\ncontribution of different input variables toward predicting\\r\\nthe target outcomes. Using methods such as Gradient SHAP\\r\\n(GS), and feature ablation (FA), the significance of electricity\\r\\n(ECL) dataset features is evaluated for the EDformer model.\\r\\nThe visual representations of Fig. 8 highlight variations in\\r\\nfeature importance across methods and the EDformer model.\\r\\nFigure 8 highlights that the electricity consumption/load for\\r\\nfuture time steps (OT) is the most influential feature in\\r\\nthe overall prediction of the EDformer model, as it directly\\r\\nrepresents the target class. Additionally, other features such\\r\\nas seasonality, temperature, time of day, and humidity also\\r\\nexhibit notable impacts on the prediction results. The saliency\\r\\nanalysis depicted in Fig.9, illustrates the predictive relevance\\r\\nof individual features in the multivariate time-series models\\r\\napplied to the electricity (ECL) dataset. The saliency maps\\r\\nshow variation in feature importance across models, indi\\x02cating diverse sensitivity patterns. This analysis underscores\\r\\nthe potential of interpretability methods in guiding model\\r\\nselection and enhancing understanding of process dynamics\\r\\nin multivariate time-series forecasting tasks.\\r\\nVIII. CONCLUSIONS\\r\\nIn this paper, EDformer, an architecture for multivariate\\r\\ntime series forecasting that integrates decomposition into\\r\\nseasonal and trend components with a reverse embedding\\r\\nprocess, followed by an encoding-based forecaster, has been\\r\\nintroduced. EDformer consistently achieves state-of-the-art\\r\\nperformance across a wide range of benchmarks, showcasing\\r\\nits generality and robustness for long-term and short-term\\n12\\r\\n(a) EDformer (GS)\\r\\n(b) EDformer (FA)\\r\\nFig. 8: Feature importance of EDformer model on the electric\\x02ity (ECL) dataset (prediction length: 24) using Gradient Shap\\r\\n(GS), and Feature ablation (FA) methods.\\r\\nforecasting tasks. Moreover, EDformer has demonstrated state\\x02of-the-art runtime efficiency, reduced cost time, and significant\\r\\nspeed-up compared to other models. This paper delves into\\r\\nadvanced model explainability techniques—such as feature\\r\\nablation, feature occlusion, integrated gradients, gradient\\x02SHAP, and windowed feature importance over time—within\\r\\nthe realm of time series forecasting. It aims to identify which\\r\\nblack-box algorithms derive the greatest benefits from these\\r\\nexplainability methods, thereby highlighting their robustness\\r\\nin effectively utilizing critical features and improving model\\r\\ninterpretability. Detailed visualizations and ablations are in\\x02cluded to provide insights into our architecture. In the future,\\r\\nthis model will be tested for some real-life time series datasets.\\r\\nACKNOWLEDGEMENTS\\r\\nThis work was partially supported by the ’Resurssmarta\\r\\nProcessor (RSP)’, the Wallenberg AI, Autonomous Systems\\r\\nand Software Program (WASP), and the Wallenberg Initiative\\r\\nMaterials Science for Sustainability (WISE), all funded by the\\r\\nKnut and Alice Wallenberg Foundation.\\r\\n(a) EDformer (GS)\\r\\n(b) EDformer (FA)\\r\\nFig. 9: Feature saliency of EDformer model on the electricity\\r\\n(ECL) dataset (prediction length: 24) using Gradient Shap\\r\\n(GS), and Feature ablation (FA) methods.\\r\\nREFERENCES\\r\\n[1] Wenxiang Li and KL Eddie Law. Deep learning models\\r\\nfor time series forecasting: a review. IEEE Access, 2024.\\r\\n[2] Akanksha Maurya, Alper Sinan Akyurek, Baris Aksanli,\\r\\nand Tajana Simunic Rosing. Time-series clustering for\\r\\ndata analysis in smart grid. In 2016 IEEE International\\r\\nConference on Smart Grid Communications (SmartGrid\\x02Comm), pages 606–611. IEEE, 2016.\\r\\n[3] Junting Zhang, Haifei Liu, Wei Bai, and Xiaojing Li. A\\r\\nhybrid approach of wavelet transform, arima and lstm\\r\\nmodel for the share price index futures forecasting. The\\r\\nNorth American Journal of Economics and Finance,\\r\\n69:102022, 2024.\\r\\n[4] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen,\\r\\nZiqing Ma, Junchi Yan, and Liang Sun. Transformers in\\r\\ntime series: A survey. arXiv preprint arXiv:2202.07125,\\r\\n2022.\\r\\n[5] Fj Vincent Atabay, Ryu Mendoza Pagkalinawan,\\r\\nSteven Dale Pajarillo, Alonica R Villanueva, and\\r\\nJonathan V Taylar. Multivariate time series forecast\\x02ing using arimax, sarimax, and rnn-based deep learning\\r\\nmodels on electricity consumption. In 2022 3rd Interna-\\n13\\r\\ntional Informatics and Software Engineering Conference\\r\\n(IISEC), pages 1–6. IEEE, 2022.\\r\\n[6] Sabeen Ahmed, Ian E Nielsen, Aakash Tripathi,\\r\\nShamoon Siddiqui, Ravi P Ramachandran, and Ghulam\\r\\nRasool. Transformers in time-series analysis: A tutorial.\\r\\nCircuits, Systems, and Signal Processing, 42(12):7433–\\r\\n7466, 2023.\\r\\n[7] George Zerveas, Srideepika Jayaraman, Dhaval Patel,\\r\\nAnuradha Bhamidipaty, and Carsten Eickhoff. A\\r\\ntransformer-based framework for multivariate time series\\r\\nrepresentation learning. In Proceedings of the 27th ACM\\r\\nSIGKDD conference on knowledge discovery & data\\r\\nmining, pages 2114–2124, 2021.\\r\\n[8] Zhi Zhang, Weijian Li, and Han Liu. Multivariate\\r\\ntime series forecasting by graph attention networks with\\r\\ntheoretical guarantees. In International Conference on\\r\\nArtificial Intelligence and Statistics, pages 2845–2853.\\r\\nPMLR, 2024.\\r\\n[9] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and\\r\\nJayant Kalagnanam. A time series is worth 64 words:\\r\\nLong-term forecasting with transformers. arXiv preprint\\r\\narXiv:2211.14730, 2022.\\r\\n[10] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.\\r\\nAre transformers effective for time series forecasting?\\r\\nIn Proceedings of the AAAI conference on artificial\\r\\nintelligence, volume 37, pages 11121–11128, 2023.\\r\\n[11] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang,\\r\\nJianxin Li, Hui Xiong, and Wancai Zhang. Informer:\\r\\nBeyond efficient transformer for long sequence time\\x02series forecasting. In Proceedings of the AAAI conference\\r\\non artificial intelligence, volume 35, pages 11106–11115,\\r\\n2021.\\r\\n[12] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin\\r\\nLing. Autoformer: Searching transformers for visual\\r\\nrecognition. In Proceedings of the IEEE/CVF inter\\x02national conference on computer vision, pages 12270–\\r\\n12280, 2021.\\r\\n[13] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao\\r\\nLin, Alex X Liu, and Schahram Dustdar. Pyraformer:\\r\\nLow-complexity pyramidal attention for long-range time\\r\\nseries modeling and forecasting. In # PLACE\\x02HOLDER PARENT METADATA VALUE#, 2022.\\r\\n[14] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu\\r\\nWang, Lintao Ma, and Mingsheng Long. itransformer:\\r\\nInverted transformers are effective for time series fore\\x02casting. arXiv preprint arXiv:2310.06625, 2023.\\r\\n[15] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\\r\\nReformer: The efficient transformer. arXiv preprint\\r\\narXiv:2001.04451, 2020.\\r\\n[16] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang\\r\\nSun, and Rong Jin. Fedformer: Frequency enhanced\\r\\ndecomposed transformer for long-term series forecasting.\\r\\nIn International conference on machine learning, pages\\r\\n27268–27286. PMLR, 2022.\\r\\n[17] Yunhao Zhang and Junchi Yan. Crossformer: Trans\\x02former utilizing cross-dimension dependency for multi\\x02variate time series forecasting. In The eleventh interna\\x02tional conference on learning representations, 2023.\\r\\n[18] Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue\\r\\nZhang, and Mark Coates. Multi-resolution time-series\\r\\ntransformer for long-term forecasting. In International\\r\\nConference on Artificial Intelligence and Statistics, pages\\r\\n4222–4230. PMLR, 2024.\\r\\n[19] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Ku\\x02mar, and Steven Hoi. Etsformer: Exponential smoothing\\r\\ntransformers for time-series forecasting. arXiv preprint\\r\\narXiv:2202.01381, 2022.\\r\\n[20] Robert B Cleveland, William S Cleveland, Jean E\\r\\nMcRae, Irma Terpenning, et al. Stl: A seasonal-trend\\r\\ndecomposition. J. off. Stat, 6(1):3–73, 1990.\\r\\n[21] Reza Asadi and Amelia C Regan. A spatio-temporal\\r\\ndecomposition based deep neural network for time series\\r\\nforecasting. Applied Soft Computing, 87:105963, 2020.\\r\\n[22] JT Sean and J Taylor. Forecasting at scale. Am. Stat,\\r\\n72(1):37–45, 2018.\\r\\n[23] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and\\r\\nYoshua Bengio. N-beats: Neural basis expansion analysis\\r\\nfor interpretable time series forecasting. arXiv preprint\\r\\narXiv:1905.10437, 2019.\\r\\n[24] Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think\\r\\nglobally, act locally: A deep neural network approach to\\r\\nhigh-dimensional time series forecasting. Advances in\\r\\nneural information processing systems, 32, 2019.\\r\\n[25] Jimmy Lei Ba. Layer normalization. arXiv preprint\\r\\narXiv:1607.06450, 2016.\\r\\n[26] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan\\r\\nMathur, Rajat Sen, and Rose Yu. Long-term forecasting\\r\\nwith tide: Time-series dense encoder. arXiv preprint\\r\\narXiv:2304.08424, 2023.\\r\\n[27] El Mahy. Pems dataset, 2024. Last accessed: 15 Nov\\r\\n2024.\\r\\n[28] UVA-MLSys. Sa-timeseries: Self-attention time series\\r\\nmodels, 2024. Last accessed: 15 Nov 2024.\\r\\n[29] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng\\r\\nLong. Non-stationary transformers: Exploring the sta\\x02tionarity in time series forecasting. Advances in Neural\\r\\nInformation Processing Systems, 35:9881–9893, 2022.\\r\\n[30] Hengyu Ye, Jiadong Chen, Shijin Gong, Fuxin Jiang,\\r\\nTieying Zhang, Jianjun Chen, and Xiaofeng Gao.\\r\\nAtfnet: Adaptive time-frequency ensembled network\\r\\nfor long-term time series forecasting. arXiv preprint\\r\\narXiv:2404.05192, 2024.\\r\\n[31] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang,\\r\\nJunhui Chen, and Yifei Xiao. Micn: Multi-scale local and\\r\\nglobal context modeling for long-term series forecasting.\\r\\nIn The eleventh international conference on learning\\r\\nrepresentations, 2023.\\r\\n[32] Fatma Yaprakdal and Merve Varol Arısoy. A multivariate\\r\\ntime series analysis of electrical load forecasting based\\r\\non a hybrid feature selection approach and explainable\\r\\ndeep learning. Applied Sciences, 13(23):12946, 2023.\\r\\n[33] Aya Abdelsalam Ismail, Mohamed Gunady, Hector Cor\\x02rada Bravo, and Soheil Feizi. Benchmarking deep learn\\x02ing interpretability in time series predictions. Advances\\r\\nin neural information processing systems, 33:6441–6452,\\r\\n2020.\\n14\\r\\n[34] Yongjie Wang, Tong Zhang, Xu Guo, and Zhiqi Shen.\\r\\nGradient based feature attribution in explainable ai: A\\r\\ntechnical review. arXiv preprint arXiv:2403.10415, 2024.\\r\\n[35] Sarthak Manas Tripathy, Ashish Chouhan, Marcel Dix,\\r\\nArzam Kotriwala, Benjamin Klopper, and Ajinkya Prab- ¨\\r\\nhune. Explaining anomalies in industrial multivariate\\r\\ntime-series data with the help of explainable ai. In 2022\\r\\nIEEE International Conference on Big Data and Smart\\r\\nComputing (BigComp), pages 226–233. IEEE, 2022.\\r\\n[36] Rohit Saluja, Avleen Malhi, Samanta Knapic, Kary ˇ\\r\\nFramling, and Cicek Cavdar. Towards a rigorous evalu- ¨\\r\\nation of explainability for multivariate time series. arXiv\\r\\npreprint arXiv:2104.04075, 2021.\\r\\n[37] Kin Kwan Leung, Clayton Rooke, Jonathan Smith, Saba\\r\\nZuberi, and Maksims Volkovs. Temporal dependencies\\r\\nin feature importance for time series predictions. arXiv\\r\\npreprint arXiv:2107.14317, 2021.'},\n",
       " {'name': '2103.15232v1.pdf',\n",
       "  'content': 'March 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nPortfolio Optimization with Sparse Multivariate\\r\\nModelling\\r\\nPIER FRANCESCO PROCACCI 1 and TOMASO ASTE 1,2\\r\\n1Department of Computer Science, UCL, Gower Street, London, WC1E6BT, UK\\r\\n2Systemic Risk Centre, London School of Economics and Political Sciences, London, WC2A\\r\\n2AE, UK\\r\\n(v1 released 28 Mar 2021)\\r\\nPortfolio optimization approaches inevitably rely on multivariate modeling of markets and the economy.\\r\\nIn this paper, we address three sources of error related to the modeling of these complex systems: 1.\\r\\noversimplifying hypothesis; 2. uncertainties resulting from parameters’ sampling error; 3. intrinsic non\\x02stationarity of these systems. For what concerns point 1. we propose a L0-norm sparse elliptical modeling\\r\\nand show that sparsification is effective. The effects of points 2. and 3. are quantified by studying the\\r\\nmodels’ likelihood in- and out-of-sample for parameters estimated over train sets of different lengths.\\r\\nWe show that models with larger off-sample likelihoods lead to better performing portfolios up to when\\r\\ntwo to three years of daily observations are included in the train set. For larger train sets, we found\\r\\nthat portfolio performances deteriorate and detaches from the models’ likelihood, highlighting the role\\r\\nof non-stationarity. We further investigate this phenomenon by studying the out-of-sample likelihood of\\r\\nindividual observations showing that the system changes significantly through time. Larger estimation\\r\\nwindows lead to stable likelihood in the long run, but at the cost of lower likelihood in the short-term:\\r\\nthe ‘optimal’ fit in finance needs to be defined in terms of the holding period. Lastly, we show that sparse\\r\\nmodels outperform full-models in that they deliver higher out of sample likelihood, lower realized portfolio\\r\\nvolatility and improved portfolios’ stability, avoiding typical pitfalls of the Mean-Variance optimization.\\r\\nKeywords: Portfolio Construction; Market States; Markowitz; Mean-Variance; Information Filtering\\r\\nNetworks; TMFG; Sparse inverse covariance; Correlation Structure.\\r\\nJEL Classification: C38, G61, G15, G17\\r\\n1. Introduction\\r\\nQuantitative approaches to asset management have accumulated unprecedented popularity over\\r\\nthe last few decades. Of all the algorithms and strategies developed, portfolio selection models are\\r\\namong those that have received wider attention. The essence of portfolio investing is to find the\\r\\nbest way of assigning weights to a given portfolio of assets to maximize future portfolio returns\\r\\nwhile minimizing the investment risk. The exploration of this filed starts with Markowitz’s Mean\\x02Variance optimization process (Markowitz 1952).\\r\\nThe theory of mean–variance-based portfolio selection is still today a cornerstone of modern asset\\r\\nmanagement. It rests on the presumption that rational investors choose among risky assets purely\\r\\non the basis of expected return and risk, with risk measured as portfolio variance. The theoretical\\r\\nfoundation of this framework is sound if either: investors exhibit quadratic utility, in which case\\r\\nthey ignore non-normality in the data (Gollier 2001), or all the higher moments of the portfolio\\r\\ndistribution can be expressed as a function of mean and variance and hence all optimal solutions\\r\\nsatisfy the mean-variance criterion. Also, the“optimality” of the mean-variance portfolios is based\\r\\narXiv:2103.15232v1 [q-fin.ST] 28 Mar 2021\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\non the assumption that investors live in a one-period world, while in reality they have an invest\\x02ment horizon that lasts longer than one period. Markets indeed constantly change over time and\\r\\ninvestors are subject to inflows/outflows forcing them to adjust their allocation and take corrective\\r\\nactions.\\r\\nForm a general, high level, perspective all portfolio optimization approaches are based on a mul\\x02tivariate model of the variables in the market and the economy. The optimization strategies are\\r\\ndevised to maximize profits and minimize risks based on such models. In modeling these com\\x02plex systems there are, however, several sources of inaccuracies and errors with the three main\\r\\nones being: 1. oversimplifying hypothesis (such as the use of normal distributions); 2. uncertain\\x02ties resulting from the estimation of the parameters from datasets of limited sizes; 3. intrinsic\\r\\nnon-stationarity of these systems, which makes in-sample estimations, based on past observations,\\r\\ninadequate for the estimation of off-sample, future properties. Most likely all three of these factors\\r\\n– and others – contribute to undermining the predictive power of any attempt of modeling markets.\\r\\nWhile a large deal of literature has been devoted to relaxing some of the most unrealistic model\\r\\nassumptions (point 1.) the current main pitfall of portfolio optimization is attributed to error\\r\\nmaximization (point 2.). This effect has long been established in literature (Michaud 1989, Nawrocki\\r\\n1996). Essentially, inputs into the mean-variance optimization are measured with uncertainty, and\\r\\nthe optimization procedure tends to pick those assets which appear to have the most attractive\\r\\nfeatures – but these are outlying cases where estimation error is likely to be the highest, hence\\r\\nmaximizing the impact of estimation error on portfolios’ weights. The estimation error is also\\r\\namplified by market evolution which makes the training on the past not fully representative of\\r\\nfuture market behavior (point 3.).\\r\\nIn this paper, we address all three sources of inaccuracies. For what concerns point 1. we propose\\r\\na L0-norm topologically regularized sparse elliptical modeling (Aste 2020) and show that sparsi\\x02fication is effective. We quantify the effects of estimation error and non-stationarity on portfolio\\r\\nperformances (point 2. and 3.) by assessing the goodness of models’ statistical likelihood for esti\\x02mates over train sets of different lengths. Specifically, we study how the realized portfolio variance\\r\\nreacts to different out-of-sample likelihoods of the input parameters and, particularly, to sparse\\r\\nmodels. Further, we analyze how sparse precision matrices impact the magnitude and the stability\\r\\nof portfolio weights.\\r\\nThe remainder of this paper is organized as follows: in Section 2 we briefly review the theory\\r\\naround portfolio construction, highlighting the pitfalls on assumptions and estimation error and\\r\\nthe main solutions proposed in literature; in Section 3 we outline our methodology and experiments\\r\\ndesign and in Section 4 we present the results. Appendices A and B are devoted to recalling some\\r\\nuseful aspects of Elliptical distributions.\\r\\n2. Literature Review\\r\\n2.1. Modern Portfolio Theory\\r\\nConsidering a portfolio of n assets with weights W = (w1, ..., wn), returns R = (r1, ..., rn) and\\r\\nportfolio returns\\r\\nrp = W RT, (1)\\r\\nthe standard Mean-Variance optimization problem consists in minimizing the portfolios’ variance\\r\\nσp for fixed levels of expected returns E[rp] = ¯rp:\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nmin\\r\\nW\\r\\nσ\\r\\n2\\r\\np = WΣWT\\r\\ns.t. E[rp] = ¯rp,\\r\\nand W1 = 1,\\r\\n(2)\\r\\nwhere Σ ∈ R\\r\\nn×n\\r\\nis the assets’ covariance matrix and 1 ∈ R\\r\\nn×1\\r\\nis a basis column vector with\\r\\nall elements equal 1. Solving for W for different values of ¯rp, one can obtain the optimal weights\\r\\n(i.e. the weights that minimize the portfolio variance) corresponding to different portfolio expected\\r\\nreturns ¯rp yielding the so-called efficient frontier – i.e. the set of optimal weights which provide the\\r\\nlowest variance for each level of expected return.\\r\\nAs discussed in the introduction, this optimization is only concerned with the first two moments\\r\\nof the distribution of portfolios’ returns and it does not deal with multiperiod investment decisions.\\r\\nThese pitfalls have largely been discussed in literature. Kritzman (2000) provides a clear review of\\r\\nwhat are the assumptions under which repeatedly investing in one-period-efficient portfolios will\\r\\nalso result in multiperiod efficiency. Also, many models have been proposed to deal explicitly with\\r\\nmultiperiod optimality (see, for example, Li and Ng (2000) or Arditti and Levy (1976)). With re\\x02spect to non-normality of returns, a rich literature is available on both alternative parametrization\\r\\nof data (Bamberg and Dorfleitner 2001) and optimisation frameworks that consider other distri\\x02bution moments (Hogan and Warren 1974, Bawa 1978) or other measures of risk/return (Harlow\\r\\n1991, Lwin et al. 2017).\\r\\nFrom the optimization problem in Eq.2 it is also clear that the optimization does not treat the\\r\\nerror and uncertainty around the parameters Σ and µ. The difference between the estimated and\\r\\ntrue distribution parameters is called estimation error. It arises from both the sampling procedure\\r\\nor availability of data and non-stationarity. The error coming from sampling, also referred to as\\r\\nsampling error, is due to parameters used in the portfolio optimization process being typically point\\r\\nestimates – we can only expect these estimates to equal the true distribution parameters if our\\r\\nsample is infinitely large. Assuming stationary data, sampling error could be fixed by increasing\\r\\nthe number of observations in the estimation sample. Indeed, the convergence rate is in the inverse\\r\\nof the square-root of the sample size, as dictated by the law of large numbers. This would come\\r\\nhandy in our times of increasing data availability. However, a second source of estimation error\\r\\ncomes from non-stationarity. A time series is said to be non-stationary if its distribution parameters\\r\\n(or the distribution itself) changes over time – in this case, extending the length of observations\\r\\nmight reduce the contribution of sampling error to estimation error, but at the same time, it could\\r\\nincrease that of non-stationarity (Broadie 1993).\\r\\nMany techniques have been proposed in literature to deal with this phenomenon, both relying on\\r\\nheuristic methods and decision-theoretic foundations (Scherer 2007). Heuristic approaches mainly\\r\\npropose to constrain the optimization problem in order to impose feasible optimal weights.\\r\\nMichaud and Michaud (1998) address explicitly the sampling error proposing a Monte Carlo\\r\\nbased procedure called resampling. In order to model the randomness of the input mean vector and\\r\\ncovariance matrix, portfolio resampling consists in repeatedly drawing from the return distribution\\r\\ngiven by the point estimates and creating n artificial new samples. For each sample, an efficient\\r\\nfrontier is estimated and the final, resampled-efficient frontier is given by the average weight across\\r\\nall of the resampled portfolios.\\r\\nFrom a decision-theoretic perspective, Bayesian techniques have recently played a primary role\\r\\nin literature. The rationale behind Bayesian statistics for portfolio construction is to include non\\x02sample information to tackle the effect of parameter uncertainty on optimal portfolio choice. Instead\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nof a point estimate, Bayesian approaches produce a density function for the parameters involved, by\\r\\ncombining sample information (likelihood) with prior belief, potentially coming from non-sample\\r\\ninformation. A special case of this general approach is the seminal work of Black and Litterman\\r\\n(1992). In their pioneering work, the authors assume assets’ returns to be normally distributed with\\r\\nmean equal to the “equilibrium returns” (that is, the mean returns that would return the market\\r\\nportfolio if used in a mean-variance optimization) and combine this “sample” information with\\r\\ninvestors’ views on the assets. In this way, in absence of an informative prior from investors, the\\r\\nmodel would return the market or “equilibrium” portfolio. In presence of investors’ priors, instead,\\r\\nthe allocation would diverge from the equilibrium portfolio accounting for investors’ views and\\r\\nproportionally to their confidence level. Other than being highly appealing from a practitioner’s\\r\\nperspective, the model proposed in Black and Litterman (1992) highlights the flexibility of the\\r\\nBayesian framework, with many sources of information that could potentially be used in combi\\x02nation or to update the in-sample information. This is a very active area of research with recant\\r\\nnotable examples including (Scherer et al. 2012) and (De Franco et al. 2019).\\r\\n2.2. Precision matrix estimation with topological regularization\\r\\nThe literature concerning parameter estimation has greatly developed in recent years. While the\\r\\nincreasing availability of data has represented the fuel for data-greedy machine learning models,\\r\\nit has also exacerbated problems related to the curse of dimensionality and overfitting. In essence,\\r\\nthe goal is to extract the largest amount of information from data with a model that avoids\\r\\noverfitting, generalizing well with new data. Furthermore, model interpretability (or explainability)\\r\\nhas become increasingly important and for this purpose, simpler and sparser models with a smaller\\r\\nnumber of parameters are preferred. To this extent, understanding the dependency structure among\\r\\nvariables has proven to be essential to capture the collective behaviour of the systems and many\\r\\ntechniques have been introduced in literature. Examples are dimensionality reduction methods\\r\\n(Van Der Maaten et al. 2009) or shrinkage techniques (Ledoit and Wolf 2020, Friedman et al.\\r\\n2008).\\r\\nOne possible approach to represent the set interactions in a complex system is to model them\\r\\nas a network structure where the vertices are the system’s elements and edges between vertices\\r\\nindicate the interactions between the corresponding elements (Lauritzen 1996). Information filter\\x02ing networks (Tumminello et al. 2005) aim at retrieving the relevant sub network of interactions\\r\\namong the elements of the system. In the pioneering work of Mantegna (1999), it was proposed to\\r\\ninvestigate financial systems by the extraction of a minimal set of relevant interactions associated\\r\\nwith the strongest correlations belonging to the Minimum Spanning Tree (MST). The MST struc\\x02ture is, however, a drastic filtering tool and is likely to discard valuable information. Tumminello\\r\\net al. (2005) and Aste and Di Matteo (2006) expanded the concept by introducing the general idea\\r\\nof information filtering networks. In particular, they show that graphs of different complexities can\\r\\nbe constructed by iteratively linking the most strongly connected nodes under the constraint of\\r\\ngenerating planar or hyperbolic graphs. There is now a large body of literature proving network\\r\\nfiltering to be a powerful tool to associate a sparse network to a high-dimensional dependency mea\\x02sure with applications ranging from financial markets (Barfuss et al. 2016) to biological systems\\r\\n(Song et al. 2012) and econophysics (Mantegna and Stanley 2000). Recent developments extend\\r\\nthe information filtering network methodology to chordal graphs (Barfuss et al. 2016, Massara\\r\\net al. 2015, Massara and Aste 2019). This allows to directly associate the information filtering\\r\\nnetwork with a positive definite sparse inverse covariance matrix that is a L0-norm topological\\r\\nregularization of the full covariance estimate (Aste 2020).\\r\\nIn our analysis, we use the TMFG-LoGo network filtering approach (Massara et al. 2015, Barfuss\\r\\net al. 2016). TMFG-LoGo approach has proven to be more efficient and better performing, partic\\x02ularly when few data are available (Barfuss et al. 2016, Aste and Di Matteo 2017), with respect to\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nL1-norm sparsification techniques such as GLASSO (Friedman et al. 2008). Moreover, the TMFG\\r\\nprocedure is computationally highly efficient and this is well suited in our resampling experiment.\\r\\nDespite the rich literature on portfolio construction techniques, to the best of our knowledge, the\\r\\nlink between a measure of estimation goodness and portfolio performances is still not specifically\\r\\ntreated in the literature.\\r\\n3. Methodology\\r\\nThe estimation error is quantified by measuring how well the functional form of the multivariate\\r\\nprobability distribution, fθ(X) defined via the parameters θ estimated in-sample, describes the\\r\\nactual data out-of-sample. The statistical measure that describes how likely are observations to be\\x02long to the estimated probability function is the likelihood. The likelihood principle is a cornerstone\\r\\nof statistical analysis in that maximum likelihood estimators are guaranteed to be asymptotically\\r\\nefficient under mild conditions (Wald (1949), Cram´er (1946), Daniels (1965)). In this section, we\\r\\nintroduce our methodology and discuss our results for the multivariate normal case. In Appendix\\r\\nA we discuss further the generality of this approach and show that it extends to other distributions\\r\\nof the elliptical family including, in particular, the multivariate Student-t.\\r\\nThe logarithm of the likelihood for the normal case is proportional to:\\r\\nlnL(θ; Xt) = ln |J| − (Xt − µ)J(Xt − µ)\\r\\nT + k , (3)\\r\\nwhere Xt = (xt,1, xt,2, ..., xt,n) is the n-dimensional multivariate returns observation vector at time\\r\\nt; θ is the model parameter set, which includes µ the vector of means and J the generalized\\r\\nprecision matrix and; k is a constant which is independent from µ, J or Xt (see Appendix A).\\r\\nIn the multivariate normal case J = Σ−1is the inverse of the covariance. Our results generalize\\r\\nto other elliptical distributions with defined covariance where J is proportional to the inverse\\r\\ncovariance, which we assume is defined and invertible. Results for the Student-t are explicitly\\r\\nreported in appendix A.\\r\\nOur goal is to study the log-likelihood in Eq. 3 using different estimation windows and comparing\\r\\nhow the precision matrices, estimated through maximum-likelihood and TMFG-LoGo, perform. We\\r\\nconsidered a dataset of daily closing prices of US stocks entering among the constituents of the\\r\\nS&P 500 index between 02/01/1997 and 31/12/2015. After screening for those continuously traded\\r\\nand those not displaying abnormal returns, we reached a final dataset of 342 stocks. For each asset\\r\\ni = 1, ..., n, we calculated the corresponding daily returns xt,i = Pt,i/P(t−1),i − 1, where Pt,i is the\\r\\nclosing price of stock i at time t, for a total of 4026 daily multivariate observations.\\r\\nWe designed a resampling experiment in which we select 100 stocks at random among the 342 and\\r\\na random trading day spanned in our dataset. Starting from the randomly selected trading day and\\r\\ngoing back in time, we define five train sets of different sizes by including an increasing number of\\r\\nobservations. We start at 101 observations, then 150, 250, 500, 1000 and finally 1500 observations.\\r\\nWe then use a fixed-length test set of 500 observations following the randomly selected trading\\r\\nday. We keep the test set length fixed to avoid biases and selected 500 observations so that, for all\\r\\nestimation windows, the main crisis event (i.e. Global Financial Crisis in 2008) can be randomly\\r\\nincluded in or out of sample. Figure 1 shows a sketched example of our train/test split with different\\r\\nestimation windows. We use the train set to estimate the mean vector µ, the maximum-likelihood\\r\\ncovariance matrix Σ and the sparse TMFG LoGo covariance matrix ΣTMF G. These parameters are\\r\\nthen used to compute the log-likelihood in eq. 3 for both in-sample and out-of-sample observations.\\r\\nWe then investigate how the different estimates used in a portfolio optimization procedure affect\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nFigure 1.: Training and Testing scheme. We randomly sample the ending date of\\r\\nthe training period, the ‘trading day’. We then estimate the model parameters con\\x02sidering different training windows using observations up to the randomly selected\\r\\ntrading day. The following 500 observations are used for testing.\\r\\nthe optimal weights and portfolio characteristics. To this extent, we considered the standard,\\r\\nunconstrained Markowitz optimization problem described in Section 2.1. This is done to avoid any\\r\\nbias coming from constraints in our analysis and to keep the framework as plain as possible. We\\r\\nfocus therefore our analysis on the minimum variance portfolio, that is the efficient portfolio that\\r\\nminimizes the expected variance. To obtain the solution for the minimum variance portfolio, the\\r\\nportfolio optimization problem in eq. 2 rewrites:\\r\\nmin\\r\\nW\\r\\nσ\\r\\n2\\r\\np = WΣWT\\r\\ns.t. W1 = 1,\\r\\n(4)\\r\\nwhich gives the optimal, minimum variance weights:\\r\\nW∗\\r\\nmin = c 1Σ−1\\r\\n, (5)\\r\\nwhere c =\\r\\n1\\r\\n1T Σ−11\\r\\nis a normalization constant. Considering the estimation scheme described\\r\\nabove and outlined in fig. 1, the estimated covariance matrices are used as inputs in the minimum\\r\\nvariance optimal portfolio, eq. 5, to compare the different log-likelihood levels obtained out-of\\x02sample and the corresponding effects on portfolio performances.\\r\\n4. Results\\r\\n4.1. Likelihood Comparison\\r\\nFigure 2 reports the average log-likelihood for the train data (fig. 2(a)) and for the test data\\r\\n(fig. 2(b)) computed across 100 resamplings. The larger the log-likelihood is, the better the pa\\x02rameters θ are at describing the data, for the assumed model. Fig. 2(a) shows that, as expected\\r\\nand by definition, the maximum-likelihood estimate of the covariance matrix provides a higher\\r\\nin-sample likelihood as compared to the TMFG covariance, although the latter tracks quite closely\\r\\nthe maximum-likelihood. Also, one might observe that the likelihood is strictly decreasing with the\\r\\nnumber of observations included in the estimation window. Indeed, as the number of observations\\r\\ndecreases relative to the parameters, the model overfits the sample yielding larger in-sample like\\x02lihoods. Filtering the covariance matrix and reducing the number of parameters clearly limits the\\r\\noverfitting potential of the model as shown by the lower levels of likelihood attained by TMFG\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nwhen fewer observations are used which, therefore, results in a larger gap in likelihood relative to\\r\\nthe maximum-likelihood covariance.\\r\\nPerhaps more interestingly, fig. 2(b) reports the likelihoods obtained out-of-sample using the two\\r\\ndifferent in-sample estimates of the covariance matrix. The first observation is that TMFG-LoGo\\r\\nprovides a substantially larger log-likelihood, especially for short estimation windows. This result\\r\\nis exacerbated by the fact that when 101 observations are considered, the number of stocks is very\\r\\nclose to the number of observations in our samples. While the resulting covariance is still full-rank\\r\\n(number of observations > number of variables), it leads to unstable estimation in the maximum\\x02likelihood covariance (i.e. the so-called “the curse of dimensionality”) whereas TMFG-LoGo is still\\r\\nwell defined. Note that there is a break y axis of the figure to allow a better inspection of the results.\\r\\nThe figure shows that for longer estimation windows, the out-of-sample log-likelihood computed\\r\\nwith the maximum-likelihood covariance tends to converge to the TMFG likelihood which, however,\\r\\na) always provides the best out-of-sample likelihood in our experiment and b) provides quite stable\\r\\nlikelihood values also for shorter estimation windows. We conclude that the TMFG-LoGo algorithm\\r\\ndoes a good job at filtering the correlation structure providing higher out-of-sample likelihood and\\r\\nstable results with shorter estimation windows, confirming the results with stationary time series\\r\\npreviously reported in (Barfuss et al. 2016)\\r\\n(a) Likelihood comparison in-sample (b) Likelihood comparison out-of-sample. Note\\r\\nthe y axis break to fit the scale for 101 days\\r\\nestimation window.\\r\\nFigure 2.: Log Likelihood computed in sample and out of sample. Both the likelihood\\r\\ncomputed using the maximum likelihood and the TMFG covariances decrease in\\r\\nsample as the sample size increases. The maximum likelihood covariance delivers\\r\\nby construction the highest likelihood, but the TMFG likelihood tracks it closely.\\r\\nIn test, instead, the TMFG covariance always attain the highest likelihood and\\r\\ndelivered good results also when the number of observations becomes close to the\\r\\nnumber of variables.\\r\\n4.2. Impact of precision matrix estimate on optimal portfolios\\r\\nWe now address empirically the question of what is the impact of different parameter estimates\\r\\non portfolios weights and performances, when these parameters are used as inputs in the portfolio\\r\\noptimization problem in eq. 2. Having focused our attention on the minimum variance portfolio on\\r\\nthe efficient frontier, we report in Figure 3 the realized standard deviation of portfolios obtained\\r\\nusing the same parameters which provided the log-likelihood displayed in Figure 2. The chart\\r\\nshows that, overall, the out-of-sample portfolio variance decreases as the likelihood increases up\\r\\nuntil when 750 observations are used. This is coherent with respect to the likelihood results that\\r\\nreported, indeed, increasing likelihoods for the same estimation windows. In particular, for shorter\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nestimation windows, the TMFG-LoGo covariance matrix provides portfolios with significantly lower\\r\\nrealized variance. Also, little changes are observed in the realized variance when observations from\\r\\n101 to 750 are included, signalling that the TMFG-LoGo extract the relevant dependency links\\r\\nalso when few observations are available. The gap in performance tends to reduce as the number of\\r\\nobservations in the estimation window increases, with the TMFG-LoGo portfolios always displaying\\r\\nlower volatility. However, when more than 750 daily observations are included, while the out of\\r\\nsample likelihood remains flat or slightly increases, the portfolios’ variance tends to increase.\\r\\nFigure 3.: Realized Standard Deviation. Increasing the estimation window and for\\r\\nhigher values of Likelihood (figure 2), the realized standard deviation of portfolios\\r\\ndecreases. Y axis break to fit the scale for 101 days estimation window..\\r\\nTo further investigate this pattern, we report in Figure 4 the volatilities for all 100 resamplings\\r\\nand considering steps of 25 observations in the estimation windows. The figure confirms that\\r\\nthe TMFG-LoGo covariances delivered overall less volatile portfolios across resamplings and\\r\\nestimation windows. Secondly, the figure shows that the portfolios obtain the lowest out-of-sample\\r\\nvariance when approximately 2 to 3 years of daily observations (450 to 700 observations) are\\r\\nincluded in the train set. This pattern is clear for the Maximum likelihood portfolios, with\\r\\nmeans, quintiles and outliers drifting upwards when more than 750 observations are included. The\\r\\nTMFG filtered covariance regularizes and smooths this effect as well, but still when more than\\r\\n750 observations are included, the resulting portfolios exhibit a slightly higher variance. This is\\r\\nconsistent with the literature showing that longer estimation windows provide worse forecasts\\r\\nin financial time series due to the regime-changing nature of financial markets (Procacci and\\r\\nAste 2019). It is also worth emphasizing that the same mean vectors are used as inputs in the\\r\\nMarkowitz optimization for both the TMFG-LoGo and the Maximum-Likelihood portfolios, hence\\r\\nthe differences in performances are due solely to the different estimates of the covariance matrix.\\r\\nFinally, we address the impact of sparsity on portfolio construction by analyzing what is the\\r\\neffect of a sparse precision matrix on the active bets obtained from an optimization procedure.\\r\\nFigure 5 reports the number of Long (fig. 6(a)) and Short (fig. 6(b)) positions (i.e. positive and\\r\\nnegative weights assigned to the stocks in portfolio) on average across the 100 resamplings. The\\r\\nfirst observation is that the number of long positions tends to increase as the estimation window\\r\\nincreases and coherently the short positions diminish accordingly. Using TMFG-LoGo precision\\r\\nmatrices anticipates this behavior, in that these portfolios always display a greater number of\\r\\nlong positions also for short estimation windows. The intuition behind this phenomenon is that\\r\\nover the long term, markets tend to post positive returns and possible outliers in assets’ means and\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\n(a) Realized volatilities obtained with Maxi\\x02mum Likelihood covariances(b) Realized volatilities obtained with TMFG\\r\\nfiltered covariances\\r\\nFigure 4.: Portfolio realized volatility across resamplings for different estimation windows.\\r\\nThe box-plot shows the distribution of the variances obtained for 100 resampled portfolios\\r\\nand the the blue line overlaid shows the average variance (i.e. the mean of the variances’\\r\\ndistribution).\\r\\ncorrelations are polished. This intuition is confirmed by looking at the distribution of weights across\\r\\nresamplings in Figure 6. This chart (note the different scales) shows that using the TMFG-LoGo\\r\\ncovariance matrix significantly improves the stability of the optimal solutions, reducing outliers and\\r\\navoiding “corner”, i.e. extreme solutions which are a typical pitfall of the unconstrained Markowitz\\r\\noptimization. This results shows that the correlation coefficient among assets plays an important\\r\\nrole in that for high correlation levels, the optimization procedure would prefer one stock in place\\r\\nof another for slightly more appealing mean or variance features. Having filtered the correlation\\r\\nstructure in the TMFG-LoGo procedure, we obtained a portfolio that is much more general (hence\\r\\nthe anticipated larger number of Long positions) and less sensitive to single assets features given\\r\\nthe filtered correlation among stocks.\\r\\n(a) Number of “Buy” positions. (b) Number of “Sell” positions.\\r\\nFigure 5.: Comparison of Buy/Sell Active Positions. As the number of training obser\\x02vations increases, the optimizations delivers an increasing number “Long” positions.\\r\\nThis tendency is anticipated when using TMFG fiiltered covarinace which always\\r\\ndelivers an higher number of Long positions.\\r\\nLastly, considering the standard unconstrained optimization problem in eq. 2, both the maximum\\x02likelihood and the TMFG matrices produce portfolios that are in the vast majority of cases investing\\r\\nin all assets. In other words, even considering a sparse precision matrix like in the TMFG-LoGo\\r\\ncase, we very rarely found weights equal to zero assigned to some assets.\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\n(a) Distribution of optimal weights using\\r\\nMaximum-Likelihood covariance\\r\\n(b) Distribution of optimal weights using\\r\\nTMFG covariance\\r\\nFigure 6.: Optimal Weights Distribution. Using the TMFG filtered covariance in\\r\\nthe optimization provides stable weights as compared to the maximum-likelihood\\r\\ncovariance, avoiding “corner solutions” and enhancing diversification.\\r\\n4.3. Non Stationarity\\r\\nFrom the results discussed in previous section and shown in Figure 3 and Figure 2, we found\\r\\nthat the portfolio performances improve coherently with the likelihood up until approximately 3\\r\\nyears of observations are used in the train set to estimate the models parameters. However, when\\r\\nmore onservations are included in the train set, the likelihood of the parameters detaches from the\\r\\nportfolio performances and we speculate that this is due to the role of nonstationarity. To further\\r\\ninvestigate this phenomenon, Figure 7 reports the likelihood corresponding to each out-of-sample\\r\\nobservation in our experiment.\\r\\n(a) Mean Likelihood for each observation across\\r\\nresamplings. Comparison of likelihoods obtained\\r\\nwhen 125, 750 and 1500 days are used in the train\\r\\nset.\\r\\n(b) Boxplot of likelihoods representing the quartiles\\r\\nand min-max levels for each observation across re\\x02samplings, having removed outliers. The plot is for\\r\\n750 days train set (blue plot on the left).\\r\\nFigure 7.: Out-of-sample likelihood measured observation-by-observation.\\r\\nFigure 7(a) shows the average likelihood across 100 resamplings for each out-of-sample observa\\x02tion. We note that when shorter estimation windows are used to estimate the models’ parameters\\r\\n(i.e. 125 days) the likelihood is higher in the days immediately following the estimation window, but\\r\\ntends to rapidly decrease as the observations depart from the training window. Larger estimation\\r\\nwindows (i.e. 750 or 1,500 days) instead, lead to a more stable likelihood in the long run, but at\\r\\nthe cost of a lower likelihood for the obervations closer to the estimation set. Figure 7(b) shows the\\r\\nobservation-wise box-plot of the likelihood computed across the resamplings when 750 observation\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nare used in the train set. The box plot reports the 25%-75% quantile interval (dark blue) and the\\r\\nmax-min interval (‘whiskers’ light blue) having excluded the ‘outliers’ that are below the whiskers’\\r\\n(Langford 2006). Other than decreasing means, the figure shows that as the observations depart\\r\\nfrom the train set, the amount of observations posting a significantly lower likelihood increases,\\r\\ntogether with the downside volatility. In other words, it is more likely to have observations that\\r\\nare far from the model estimated in sample, supporting the conclusions drawn from Figure 7(a).\\r\\nAs we discussed in (Procacci and Aste 2019), market states tend to be persistent in daily obser\\x02vations. Shorter estimation windows, therefore, tend to better describe the system belonging to the\\r\\nsame ‘state’ which is likely to be persistent for adjacent observations. Notice that by considering\\r\\nthe aggregate behaviors across 100 resamplings, we want to avoid specific market conditions and\\r\\nstate shifts, but rather focus on the general behaviour. The evolution of the financial system is\\r\\nobviously very dynamic and the goodness of parameters is certainly dependent on both systemic\\r\\nand idiosyncratic events.\\r\\nThese results also provide further insights on the findings discussed in Figure 2 and Figure 3 in\\r\\nthat our conclusions are dependent on the number of out-of-sample observations that in our case\\r\\ncoincides with the portfolio holding period - i.e. 500 days in our experiments. Shorter estimation\\r\\nwindows provide better fit in the short term, while larger estimation windows provide robustness in\\r\\nthe long run. The optimal balance between these two effects depends on the holding period and in\\r\\nour experiments it is achieved with approximately 3 years observations in the estimation window.\\r\\nShort holding periods do not require robustness in the long run (i.e. shorter estimation windows\\r\\nwould deliver better results). As the holding period increases, the long term robustness becomes\\r\\nmore relevant than the short term fit and larger estimations windows have to be preferred.\\r\\n5. Conclusions\\r\\nPortfolio construction is a cornerstone of financial theory and practice. However, it is still today a\\r\\ncontroversial topic for both academics and practitioners. Any portfolio optimization strategy relies\\r\\non assumptions and modeling of the future market structure. However, inferring such structure\\r\\nfrom past observations is a very challenging task, plagued by uncertainty around parameters\\r\\nestimation and relying on some non fully satisfied assumptions.\\r\\nWe identify three main sources of inaccuracies and errors: 1. model oversimplification; 2. lim\\x02ited size of the estimation set; 3. non-stationarity. We address oversimplification by introducing a\\r\\nmodeling that uses a L0-norm regularized elliptical multivariate distribution, demonstrating that\\r\\nit over-performs traditional models both in likelihood and in portfolio variance performances. We\\r\\ntest the effect of sample size by training the models on windows of different sizes and find that\\r\\nperformances initially increase with sample size but then eventually decrease for windows above\\r\\n750 days. We attribute the initial improvement in performance to sampling error, which is reduced\\r\\nwhen more observations are included, and we interpret the decay in performance when more the\\r\\n750 observations are included as an instance of non-stationarity. We further investigate this phe\\x02nomenon by studying the likelihood corresponding to individual observations out-of-sample and\\r\\nshow that shorter estimation windows deliver higher out-of-sample likelihood in the days immedi\\x02ately following the train window, but it tends to rapidly decrease afterwards. As more observations\\r\\nare included in the training set, the out-of-sample likelihood gains stability, with larger values in\\r\\nthe long term, but at the cost of lower likelihood in the short term. We conclude that the financial\\r\\nsystem changes significantly through time and the ‘optimal’ fit in finance needs to be defined in\\r\\nterms of the holding period.\\r\\nOur main contribution to the literature on portfolio construction is the demonstration of the rela\\x02tionship between the goodness of the model, measured as out-of-sample likelihood, and the realized\\r\\nportfolio volatility. We show that higher likelihood obtained with filtered TMFG-LoGo precision\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nmatrices correspond to lower portfolio volatility out-of-sample. The relationship between larger\\r\\nlikelihood and lower realized volatility is also verified in the maximum-likelihood estimate of the\\r\\ncovariance matrix when computed over train sets of different lengths. Further, we show that sparse,\\r\\nfiltered covariance matrices can significantly reduce estimation errors coming from both sampling\\r\\nerror and non-stationarity. It also reduces many of the instability problems related to mean-variance\\r\\noptimal weights.\\r\\nFinally, all of the analysis and conclusions drawn in this paper are based on different estimates of\\r\\nthe covariance matrix. While forecasting future returns remains of primary importance in trading\\r\\nand wealth management, we showed that the correlation structure, sometimes overlooked in the\\r\\nasset allocation literature, plays a key role in portfolio construction and a good deal of performances\\r\\ndepend upon it.\\r\\nAcknowledgments\\r\\nTA acknowledges partial support from ESRC (ES/K002309/1), EPSRC (EP/P031730/1) and EC\\r\\n(H2020-ICT-2018-2 825215).\\r\\nReferences\\r\\nArditti, F. and Levy, H., Portfolio Efficiency Analysis in Three Moments: The Multiperiod Case. Journal of\\r\\nFinance, 1976, pp. 797–809.\\r\\nAste, T. and Di Matteo, T., Dynamical networks from correlations. , 2006, 370, 156–161.\\r\\nAste, T., Topological regularization with information filtering networks. arXiv preprint arXiv:2005.04692,\\r\\n2020.\\r\\nAste, T. and Di Matteo, T., Causality network retrieval from short time series. arXiv preprint\\r\\narXiv:1706.01954, 2017.\\r\\nBamberg, G. and Dorfleitner, G., Fat Tails and Traditional Capital Market Theory. Working Pape, University\\r\\nof Augsburg, 2001.\\r\\nBarfuss, W., Massara, G.P., di Matteo, T. and Aste, T., Parsimonious modeling with Information Filtering\\r\\nNetworks. Phys. Rev. E, 2016, 94, 062306.\\r\\nBawa, V.S., Safety-First, Stochastic Dominance, and Optimal Portfolio Choice. Journal of Financial and\\r\\nQuantitative Analysis, 1978, 13, 255–271.\\r\\nBerkane, M. and Bentler, P., Moments of elliptically distributed random variates. Statistics & Probability\\r\\nLetters, 1986, 4, 333 – 335.\\r\\nBlack, F. and Litterman, R., Global Portfolio Optimization. Financial Analysts Journal, 1992, 48, 28–43.\\r\\nBroadie, M., Computing efficient frontiers using estimated parameters. Annals of Operations Research, 1993,\\r\\n45, 215–229.\\r\\nChamberlain, G., A characterization of the distributions that imply mean–Variance utility functions. Journal\\r\\nof Economic Theory, 1983, 29, 185–201.\\r\\nCram´er, H., Mathematical Methods of Statistics, Princeton Landmarks in Mathematics and Physics, 1946,\\r\\nPrinceton University Press.\\r\\nDaniels, H.E., The asymptotic efficiency of a maximum likelihood estimator. Matematika, 1965, 9, 149–161.\\r\\nDe Franco, C., Nicolle, J. and Pham, H., Bayesian Learning For The Markowitz Portfolio Selection Problem.\\r\\nInternational Journal of Theoretical and Applied Finance (IJTAF), 2019, 22, 1–40.\\r\\nFang, K., Kotz, S. and Ng, K., Symmetric multivariate and related distributions, no. 36 In Monographs on\\r\\nstatistics and applied probability, 1990 (Chapman & Hall: London [u.a.]).\\r\\nFriedman, J., Hastie, T. and Tibshirani, R., Sparse inverse covariance estimation with the graphical lasso.\\r\\nBiostatistics, 2008, 9, 432–441.\\r\\nGollier, C., Wealth Inequality and Asset Pricing. The Review of Economic Studies, 2001, 68, 181–203.\\r\\nHarlow, W.V., Asset Allocation in a Downside-Risk Framework. Financial Analysts Journal, 1991, 47, 28–40.\\r\\nHogan, W.W. and Warren, J.M., Toward the Development of an Equilibrium Capital-Market Model Based\\r\\non Semivariance. Journal of Financial and Quantitative Analysis, 1974, 9, 1–11.\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nKritzman, M.P., Puzzles of finance: six practical problems and their remarkable solutions, Wiley investment\\r\\nseries, 2000 (John Wiley & Sons, Inc.: New York).\\r\\nLangford, E., Quartiles in Elementary Statistics. Journal of Statistics Education, 2006, 14.\\r\\nLauritzen, S.L., Graphical Models, 1996 (Oxford University Press: Oxford).\\r\\nLedoit, O. and Wolf, M., The Power of (Non-)Linear Shrinking: A Review and Guide to Covariance Matrix\\r\\nEstimation. Journal of Financial Econometrics, 2020.\\r\\nLi, D. and Ng, W.L., Optimal Dynamic Portfolio Selection : Multi-period Mean-Variance Formulation.\\r\\nMathematical Finance, 2000, 10, 387–406.\\r\\nLwin, K.T., Qu, R. and MacCarthy, B.L., Mean-VaR portfolio optimization: A nonparametric approach.\\r\\nEuropean Journal of Operational Research, 2017, 260, 751 – 766.\\r\\nMantegna, R.N., Hierarchical structure in financial markets. The European Physical Journal B - Condensed\\r\\nMatter and Complex Systems, 1999, 11, 193–197.\\r\\nMantegna, R. and Stanley, H., An Introduction to Econophysics: Correlations and Complexity in Finance,\\r\\n2000, Cambridge University Press.\\r\\nMarkowitz, H., Portfolio Selection. The Journal of Finance, 1952, 7, 77–91.\\r\\nMaruyama, Y. and Seo, T., Estimation of Moment Parameter in Elliptical Distributions. Journal of the\\r\\nJapan Statistical Society. Japanese issue, 2003, 33, 215–229.\\r\\nMassara, G.P. and Aste, T., Learning clique forests. arXiv preprint arXiv:1905.02266, 2019.\\r\\nMassara, G.P., di Matteo, T. and Aste, T., Network Filtering for Big Data: Triangulated Maximally Filtered\\r\\nGraph. CoRR, 2015, abs/1505.02445.\\r\\nMichaud, R., The Markowitz Optimization Enigma: Is Optimized Optimal?. Working Paper, University of\\r\\nAugsburg, 1989, pp. 31–42.\\r\\nMichaud, R. and Michaud, R., Efficient Asset Management: A practical Guide to Stock Portfolio Optimiza\\x02tion and Asset Allocation, 1998 (Harvard Business School Press: Boston).\\r\\nNawrocki, D., Portfolio Analysis with a Large Universe of Assets. Applied Economics, 1996, pp. 1191–1198.\\r\\nOwen, J. and Rabinovitch, R., On the Class of Elliptical Distributions and their Applications to the Theory\\r\\nof Portfolio Choice. The Journal of Finance, 1983, 38, 745–752.\\r\\nProcacci, P.F. and Aste, T., Forecasting Market States. Quantitative Finance, 2019, 19, 1491–1498.\\r\\nScherer, B., Portfolio Construction and Risk Budgeting, 2007.\\r\\nScherer, B., Winston, K. and O’Cinneide, C., Bayesian Methods In Investing, 2012, Oxford University Press.\\r\\nSong, W.M., Di Matteo, T. and Aste, T., Building complex networks with Platonic solids. Phys. Rev. E,\\r\\n2012, 85, 046115.\\r\\nTumminello, M., Aste, T., Di Matteo, T. and Mantegna, R.N., A tool for filtering information in complex\\r\\nsystems. Proceedings of the National Academy of Science, 2005, 102, 10421–10426.\\r\\nVan Der Maaten, L., Postma, E. and Van den Herik, J., Dimensionality reduction: a comparative review.\\r\\nJournal of Machine Learning Research, 2009, 10, 66–71.\\r\\nWald, A., Note on the Consistency of the Maximum Likelihood Estimate. Annals of Mathematical Statistics,\\r\\n1949, 20, 595–601.\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nAppendix A: Elliptical Distributions\\r\\nConsider an n-dimensional vector of multivariate returns X = (x1, x2, ..., xn). If X is elliptical\\r\\ndistributed, then its probability density function is defined as:\\r\\nfX(X) = cn|J|\\r\\n1/2\\r\\ngn\\r\\n\\x02\\r\\n(X − µ)J(X − µ)\\r\\nT\\r\\n\\x03\\r\\n, (A1)\\r\\nwhere µ ∈ R\\r\\n1×n\\r\\nis the vector of location (mean) parameters and cn is a normalization constant.\\r\\nThe matrix, J = Ω−1 ∈ R\\r\\nn×n\\r\\nis the generalized precision matrix, a positively defined matrix\\r\\nwhich is the inverse of the dispersion matrix Ω. When the covariance is defined (as we assume in\\r\\nthis paper) then Ω = (−ψ\\r\\n0\\r\\n(0))−1Σ, that is, Ω is proportional to the covariance matrix and the\\r\\nproportionality factor is the inverse of the first derivative of the characteristic generator evaluated\\r\\nat 0. The function, gn(·) is called density generator.\\r\\nAlso, let us stress that (X −µ)J(X −µ)\\r\\nT\\r\\n- i.e. the generalized, square Mahalanobis distance - is\\r\\na quadratic term and hence a non-negative quantity provided that the matrix Ω is positive definite.\\r\\nTo ease the notation, for the remaining of the paper we shall refer to the generalized Mahalanobis\\r\\ndistance as d\\r\\n2\\r\\n:\\r\\nd\\r\\n2 = (X − µ)J(X − µ)T\\r\\n. (A2)\\r\\nFor dfferent density generators gn(·) we obtain different distributions of the elliptical family. It\\r\\nis easy to see, for example, that the normal distribution is obtained by using:\\r\\ng(u) = e\\r\\n−u/2\\r\\n, (A3)\\r\\nand Ω = Σ.\\r\\nSimilarly the Student-t distribution is obtained by using:\\r\\ngn(u) = \\x101 +\\r\\nu\\r\\nv\\r\\n\\x11−\\r\\nn+v\\r\\n2\\r\\n, (A4)\\r\\nwhere v is the degrees of freedom, and Ω =\\r\\nν\\r\\nν−2Σ.\\r\\nThe validity of the mean-variance framework for elliptical distributions has long been estab\\x02lished in literature (Owen and Rabinovitch 1983). This proposition is derived easily from two\\r\\nproperties of the elliptical distributions. First, for every elliptical distribution with defined mean\\r\\nand variance, the distribution is completely specified by them (Owen and Rabinovitch (1983) or\\r\\nChamberlain (1983)), with all the higher moments being either zero or proportional to the first or\\r\\nsecond moment. Second, any linear combination of multivariate elliptically distributed variables is\\r\\nalso an elliptically distributed variable. In the case of normal distribution and Stiudent-t distribu\\x02tion they also have the same density generator function. Further details on these properties are\\r\\nprovided in Appendix B.\\r\\nIt follows that, if asset returns have a multivariate elliptical distribution X ∼ En(µ, Ω, gn),\\r\\nthen the portfolio expected return and dispersion are given by, respectively, E[rp] = W µT and\\r\\nσp = WΩWT , matching the optimization framework outlined in Section 2.1.\\r\\nWith respect to our likelihood analysis, considering distributions with probability density\\r\\nfunction of the form specified in Eq. A1, the corresponding likelihood function is of the form:\\r\\nLED(θ; X) = |J|\\r\\n1/2\\r\\ngn\\r\\n\\r\\nd\\r\\n2\\r\\n\\x01\\r\\n. (A5)\\r\\nWhere ED denotes the general Elliptical Distributions and we omitted the constant of integration.\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nTo stress the general validity of our analysis for other elliptical distributions, we repeated the\\r\\nexperiments discussed in Section 3 considering the t-student generator.\\r\\nAssuming a Student - t distribution of the log returns, the log likelihood (Eq. A5) is:\\r\\nlnLStudent =\\r\\nln |J|\\r\\n2\\r\\n−\\r\\nn + ν\\r\\n2\\r\\nln \\x121 +\\r\\nd\\r\\n2\\r\\nν − 2\\r\\n\\x13\\r\\n(A6)\\r\\nwhere n is the sample size and ν is the degree of freedom. Figure 1(a) reports the likelihood\\r\\ncomparison for the same resamplings as in Figure 2 but using a student-t log likelihood as in\\r\\nEq. A6. Here we used n = 500 observations (i.e. the out-of-sample size) and ν = 3. We verified\\r\\nthat this findings are robust across different degrees of freedom in the range ν = [2.1, 4].\\r\\n(a) Log likelihood assuming a t-student distribution of\\r\\nlog returns.\\r\\nFigure A1.: Out-of-sample log likelihood likelihood computed using the maximum likeli\\x02hood and the TMFG covariances. These results are coherent with the findings related to\\r\\nthe Normal distribution presented in Section 4.1\\r\\nAppendix B: Properties of Elliptical Distributions\\r\\nIn this section we recall some useful properties of Elliptical Distribution which we referred to in\\r\\nour discussion and particularly in Section A.\\r\\nProperty 1 (Distribution Definition) Consider an n-dimensional random vector X =\\r\\n(X1, ..., Xn). X has a multivariate elliptical distribution with location parameter µ and dispersion\\r\\nparameter Ω,written as X ∼ E(µ, Ω) if its characteristic function φ can be expressed as:\\r\\nφX(w) = E(e\\r\\niwX) = eiwµψ\\r\\n\\x12\\r\\n1\\r\\n2\\r\\nwΩwT\\r\\n\\x13\\r\\n, (B1)\\r\\nfor some location parameter µ ∈ R\\r\\n1×n\\r\\n, positive-definite dispersion matrix Ω ∈ R\\r\\nn×n and for some\\r\\nfunction ψ(·) : [0, ∞) → R such that ψ\\r\\nPn\\r\\ni=1 w\\r\\n2\\r\\ni\\r\\n\\x01\\r\\nis a characteristic function, which is called\\r\\ncharacteristic generator. If X ∼ E(µ, Ω) and if its density fX(X) exists, it is of the form defined\\r\\nin Eq. A1.\\nMarch 30, 2021 PortfolioConstruction˙SparseMultivariateModelling\\r\\nProperty 2 (Density Generator) The function g(·) defined in Section A is guaranteed to be\\r\\ndensity generator if the following condition holds:\\r\\nZ ∞\\r\\n0\\r\\nx\\r\\nn/2−1\\r\\ngn(x)dx < ∞. (B2)\\r\\nProperty 3 (Affine Equivariance) If X = (X1, ..., Xn) is an n-dimensional elliptical random\\r\\nvariable with location parameter µ and dispersion parameter Ω so that X ∼ EX(µ, Ω), then for\\r\\nany vector a ∈ R\\r\\n1×m and any matrix B ∈ Rm×n\\r\\nthe following affine equivariace holds:\\r\\nY = a + BX ∼ EY (a + Bµ, BΩB). (B3)\\r\\nIn other words, any linear combination of multivariate elliptical distributions is another elliptical\\r\\ndistribution. In the special cases of normal, Student-t and Cauchy distributions, the induced density\\r\\ngenerators are m-dimensional version of the original generator of X.\\r\\nFor the proof of Properties 1,2 and 3, we refer to Fang et al. (1990).\\r\\nThis implies that any portfolio Y = β1X1 + ... + βnXn of elliptically distributed variables is\\r\\ndistributed accordingly with a (univariate) elliptical distribution, which is a location-scale distri\\x02bution. Furthermore, for any univariate elliptical distribution all moments can be obtained from\\r\\nthe first and second moments (if defined). In particular, for centered variables with zero mean\\r\\n(µY = 0), the resulting distribution of Y is symmetrical around zero and it has all odd moments\\r\\nequal to zero and all even moments given by:\\r\\nµ2m = cmµ\\r\\nm\\r\\n2\\r\\n,\\r\\nwith\\r\\ncm =\\r\\n(2m)!\\r\\n(2mm!)\\r\\nψ\\r\\n(m)\\r\\n(0)\\r\\n(ψ(1)(0))m\\r\\n.\\r\\nWhere ψ\\r\\n(m)\\r\\n(0) indicated the mth derivative of ψ(ω) computed at ω = 0.\\r\\nAs an example, in the normal (0,1) case, µ2 = 1, cm = 0 for all m = 1, 2, ..., the kurtosis is\\r\\nµ(4) =\\r\\n4!\\r\\n2\\r\\n44! = 3, and µ(2m) =\\r\\n(2m)!\\r\\n(2mm!) . For the proof we refer to Berkane and Bentler (1986), which\\r\\nderived this property by succesive differentiations of φ(·), and to Maruyama and Seo (2003), which\\r\\nattained the same result by expressing the elliptical distribution in terms of a random vector with\\r\\nuniform distribution on the unit sphere.\\r\\nTherefore the mean-variance optimization is of general applicability and relevance for any port\\x02folio generated from multivariate elliptically distributed variables.'},\n",
       " {'name': '2203.11091v2.pdf',\n",
       "  'content': 'GCNET: A novel graph-based approach for prediction of stock price movement\\r\\nAlireza Jafaria, Saman Haratizadeha,∗\\r\\naFaculty of New Sciences and Technologies, University of Tehran, North Kargar Street, 1439957131 Tehran, Iran\\r\\nAbstract\\r\\nThe importance of considering related stocks data for the prediction of stock price movement has been shown in\\r\\nmany studies, however, advanced graphical techniques for modeling, embedding and analyzing the behavior of inter\\x02related stocks have not been widely exploited for the prediction of stocks price movements yet. The main challenges in\\r\\nthis domain are to find a way for modeling the existing relations among an arbitrary set of stocks and to exploit such a\\r\\nmodel for improving the prediction performance for those stocks. The most of existing methods in this domain rely on\\r\\nbasic graph-analysis techniques, with limited prediction power, and suffer from a lack of generality and flexibility. In\\r\\nthis paper, we introduce a novel framework, called GCNET that models the relations among an arbitrary set of stocks\\r\\nas a graph structure called influence network and uses a set of history-based prediction models to infer plausible initial\\r\\nlabels for a subset of the stock nodes in the graph. Finally, GCNET uses the Graph Convolutional Network algorithm\\r\\nto analyze this partially labeled graph and predicts the next price direction of movement for each stock in the graph.\\r\\nGCNET is a general prediction framework that can be applied for the prediction of the price fluctuations of\\r\\ninteracting stocks based on their historical data. Our experiments and evaluations on a set of stocks from the NASDAQ\\r\\nindex demonstrate that GCNET significantly improves the performance of SOTA in terms of accuracy and MCC\\r\\nmeasures.\\r\\nKeywords: Stock price prediction, Deep learning, Graph convolutional network, Semi-supervised learning, GCN,\\r\\nGraph-based stock forecasting\\r\\n1. Introduction\\r\\nPredicting the stock prices and fluctuations of stock prices has been of great interest for decades since any reliable\\r\\ninformation about the possible next price or direction of price movement can be of great value for investors who need\\r\\nto decide how to invest in the market (Rather et al., 2017; Soni, 2011). Traditional stock prediction approaches are\\r\\ncategorized into technical analysis and fundamental analysis. While fundamental analysis focuses on the intrinsic\\r\\nvalue of stocks and is mainly used by human experts, in technical analysis, the history of stock prices and price\\r\\nindicators are analyzed to extract useful patterns for the prediction of the future behavior of the stocks prices.\\r\\nIn past decades, technical analysis has been widely used to build automatic stock market prediction systems\\r\\n(Bustos & Pomares-Quimbaya, 2020). Researchers have applied machine learning techniques to develop a wide\\r\\nvariety of prediction models, including regression and classification systems, for forecasting the future prices or price\\r\\nfluctuations of stocks. Especially during the past few years, along with the advancements in the deep learning domain,\\r\\nsophisticated deep models have been developed for market forecasting, outperforming their traditional predecessors\\r\\n(Hoseinzade & Haratizadeh, 2019; Li & Liao, 2017; Patel et al., 2015; Gunduz et al., 2017). These experiments show\\r\\nthat deep methods can model the complex behavior of markets more accurately. However, in most of these studies,\\r\\nthe prediction models use the historical data from the past prices of a single stock to predict the future of that stock’s\\r\\nprice and ignore the relationships between stocks as a source of information.\\r\\nOne emerging idea in this domain is that financial entities like markets or stocks are naturally connected entities\\r\\nhaving interactions with each other, and so, it is reasonable to believe that the behavior of one entity can affect the\\r\\n∗Corresponding author\\r\\nEmail addresses: alireza.jafari7@ut.ac.ir (Alireza Jafari), haratizadeh@ut.ac.ir (Saman Haratizadeh)\\r\\nPreprint submitted to Elsevier September 1, 2022\\r\\narXiv:2203.11091v2 [q-fin.TR] 31 Aug 2022\\nothers. This means that intelligent analysis and consideration of related entities can help us to predict the future\\r\\nbehavior of others.\\r\\nFollowing this idea, some studies have used historical data from other possibly related stocks/markets as well to\\r\\ntrain prediction models for a target stock/market (Hoseinzade & Haratizadeh, 2019; Zhong & Enke, 2017; Gunduz\\r\\net al., 2017). Although sophisticated deep forecasting models have been used in this class of studies, they still have\\r\\nsome limitations: They usually rely on an expert’s advice to select a preferably small set of related stocks/markets to\\r\\nthe target entity. Also, most of the existing models in this category are not flexible enough to explicitly model and\\r\\nuse the direct and indirect relationships among entities or to reflect the already known degrees of relevance between\\r\\nthem. Another drawback of this approach is that the architectures of these models are usually task-dependent and are\\r\\ndesigned for prediction over a fixed set of related entities and they may not be easily applicable for other, possibly\\r\\nlarger, sets of entities.\\r\\nA natural approach to model the inter-relations among a set of entities is to use graphical analysis that can dis\\x02cover the hidden relationship between stocks from the graph structure, unlike previous works, which use the manual\\r\\nselection of related stocks. Graph-based modeling and analysis of data is a standard technique in many domains such\\r\\nas social network analysis or recommendation systems, however, it has not been widely used for modeling relations\\r\\namong financial entities and its application in this domain has been limited to a few studies (Yin et al., 2022; Kia et al.,\\r\\n2020; Long et al., 2020; Kia et al., 2018; Kim & Sayama, 2017).\\r\\nThe common approach in these studies is to construct a (usually correlation-based) graph structure reflecting some\\r\\nkind of relation among entities and then propagate a set of already known labels to other nodes through the paths that\\r\\nexist among nodes in the graph. Although these algorithms have achieved some improvement in performance over\\r\\nhistory-based prediction techniques, they still suffer from some important flaws. Firstly, they rely on a set of known\\r\\nlabels for some entities while that may not be available in all settings. Secondly, their prediction mechanism is solely\\r\\nbased on the structure of the graph and usually ignores any local information about each entity’s current state for\\r\\nprediction of its. Finally, these algorithms in their prediction step usually use neighbor-based approaches that cannot\\r\\nbe trained or optimized based on the current state of a node and its neighbors or the structure of the current underlying\\r\\ngraph. This lack of adaptability makes the performance of the prediction step extremely dependent on the initial\\r\\nstructure of the graph, which limits the prediction performance especially when there are hidden interactions among\\r\\nentities that are not reflected in the graphical model.\\r\\nFortunately, in recent years there have been significant developments in the field of graph analysis using Graph\\r\\nNeural Networks. These techniques are able to analyze graph-structured data in order to generate new representations\\r\\nof each node while considering node features. In this paper, we introduce GCNET, a general framework for the\\r\\nprediction of stock price fluctuations that formulates this classic problem as a label prediction task in a graph. It\\r\\nmodels the price historical data of a set of inter-related stocks (or markets) along with the observed degrees of co\\x02relations among them as a graph and creates beneficial aggregation for each node by considering its related neighbor\\r\\nstocks. GCNET uses a deep graphical semi-supervised learning algorithm to generate a model for predicting the next\\r\\nprice (or index) fluctuation for each stock (or market).\\r\\nThe main characteristics and contributions of the introduced framework, GCNET, can be summarized as follows:\\r\\n• We introduce a novel framework to formulate the prediction of stock price fluctuations as a label prediction task\\r\\nover a graphical model of inter-related financial entities.\\r\\n• Our suggested framework can exploit any set of history-based predictors to generate the initial labels required\\r\\nby its semi-supervised learning process and so it is applicable in settings where there is no already known\\r\\nlabel/class for any node/stock.\\r\\n• We introduce a novel approach for modeling the relations among stocks as a so-called influence network that\\r\\nimproves its overall prediction performance.\\r\\n• A comprehensive set of experiments on two sets of hundred stocks shows that GCNET predicts the next price\\r\\nfluctuations for the stocks in the graph with a significantly better performance compared to the state of the art\\r\\nbaselines.\\r\\nIn the next section, we will review the related work. Our network construction method and prediction algorithm\\r\\nare explained in Section 3. Section 4 presents detailed information about the dataset used in our experiments, the\\r\\n2\\nexperimental setup, the results, and a discussion about the observations. Finally, we conclude the paper in Section 5.\\r\\n2. Related Works\\r\\nMany different machine learning techniques have been used for the prediction of stock markets in the past decades.\\r\\nMost of these techniques rely on analyzing the historical data of a single stock data to extract useful patterns for\\r\\npredicting its future behavior. Kara et al. (2011) used ANN and SVM to predict the direction of movement of the\\r\\nIstanbul Stock Exchange (ISE) National 100 Index and showed that ANN predicts the index significantly better than\\r\\nSVM. Zhong & Enke (2017) used ANN to predict S&P500 in their model and showed that using PCA to construct new\\r\\nrepresentations for the initial feature vectors can improve the prediction accuracy of the ANN-based model. Arevalo ´\\r\\net al. (2016) examined artificial neural networks with different structures and observed that a deep neural network with\\r\\nfive hidden layers achieves the highest accuracy in predicting the direction of movement for a single stock’s price.\\r\\nLong Short-Term Memory (LSTM) is one of the most popular models used for time series prediction. Nelson et al.\\r\\n(2017) used LSTM and MLP for forecasting the Brazilian stock market. The results showed the superiority of LSTM\\r\\nover MLP. In a more recent study, LSTM has been combined with an attention mechanism to predict the market shares\\r\\nbased on the price information from the past few days (Feng et al., 2019a). Although only one stock data is used in this\\r\\nmodel to predict, in more recent studies, researchers have emphasized the importance of considering related stocks by\\r\\ntrying to use other stocks in their model training using simple techniques. Shah et al. (2021) analyzed historical data\\r\\nof different stocks by Bi-directional Long Short-Term Memory and predict stock price trends. They also presented a\\r\\nframework for depth and time calculation learning faster than the one-directional approach.\\r\\nConvolution neural network, CNN, is another class of deep learning algorithms used in stock market forecasting\\r\\nwhile its ability to extract high-level features from price data has been studied by several researchers (Gunduz et al.,\\r\\n2017; Hoseinzade & Haratizadeh, 2019; Di Persio & Honchar, 2016). In one study, Di Persio & Honchar (2016)\\r\\ndifferent kinds of deep neural networks including ANN, LSTM, and CNN were used to predict the S&P500 index,\\r\\nand the results showed that CNN outperforms other deep models in prediction. Gunduz et al. (2017) used CNN to\\r\\npredict the market index of the Istanbul Stock Exchange.They used a handmade set of stocks to input their model to\\r\\nconsider related stocks. Hoseinzade & Haratizadeh (2019) predicted well-known stock market indices in the United\\r\\nStates using CNN. They showed that 2D and 3D CNNs could be applied to combine information from a fixed set\\r\\nof related time series in order to successfully predict their future behavior. Although the above models have yielded\\r\\nacceptable results, most of them do not use stock relationships as a valuable source of information and the rest are\\r\\nlimited to using the handful of stocks that the authors guess are relevant.\\r\\nAnother class of algorithms for financial time series prediction are the Graph-based methods Yin et al. (2022), Kia\\r\\net al. (2020), Long et al. (2020), Kia et al. (2018), Kim & Sayama (2017), and Shin et al. (2013). The majority of\\r\\nthese methods model the relations of the entities as correlation graphs and use models to predict that can not be easily\\r\\ngeneralized to other sets of stocks or markets. Park & Shin (2013) created a network of nodes representing the stocks\\r\\nin the Korean Stock Exchange and some commodity price time series based on the similarity of their feature vectors.\\r\\nThey assigned the known labels based on the amount of variation in the 5-day moving average of stock prices and\\r\\npredicted the direction of movement for the Korean Stock Exchange index using a label propagation algorithm. Kim\\r\\n& Sayama (2017) used network analysis and showed that graph structures can reveal important information about\\r\\nthe future of stock prices. Taking into account the parameters of the network, they developed an ARIMA model and\\r\\npredicted the value of the S&p500 index. In Kia et al. (2018) a graph structure is introduced in which the nodes\\r\\nrepresent the markets, and the edges represent strong correlations between the market index time series. The nodes\\r\\nwhose corresponding indices are already known, are labeled while the nodes whose corresponding indices are still\\r\\nunknown, due to the time differences between different time zones, are predicted using a label spreading algorithm.\\r\\nLong et al. (2020) creates a knowledge graph representing different companies and apply the node2vec algorithm\\r\\nto select the relevant stocks of the target for constructing helpful embedding (Grover & Leskovec, 2016). They use\\r\\nthe similarities between the resulted embeddings as a quantitative measure of the co-relation between stocks. They\\r\\nweight each stock using these co-relation values and train a LSTM model for prediction without exploiting the power\\r\\nof network analysis in the forecasting process.\\r\\nIn Kia et al. (2020) an association rule mining technique is used for defining the weights of edges in a graph of\\r\\nmarkets, and to predict the unknown labels in the graph, a variation of PageRank algorithm is used. As we mentioned\\r\\nbefore, the simple label prediction algorithms used in this class of methods, not only rely on the existence of already\\r\\n3\\nknown labels but also they fail to model possibly complex relations among the internal states of the nodes, their\\r\\ninter-relations, and their labels. Also, despite the fact that the performance of these algorithms is highly dependent\\r\\non the structure of the underlying graph, the graph construction process needs to be subject to further studies in this\\r\\ndomain. Like most of the mentioned models, Yin et al. (2022) uses the correlation graph to predict the stock price\\r\\nof the Chinese stock market. They create new representations by attention mechanism and then use these vectors to\\r\\ntrain and predict the LSTM model. This class of models relies entirely on underlying graph structures, that are mostly\\r\\ndefined based on historical price correlations among stocks, while other possible graph structures have received less\\r\\nattention.\\r\\nAs a modern node embedding and label prediction algorithm, Graph Convolutional Network, GCN, has been\\r\\nrecently introduced. GCN is an extension to CNN that is able to handle graphical data Kipf & Welling (2016). It\\r\\nis a Graph Neural Network (GNN) that takes the graph structure and node features as inputs and aggregates and\\r\\ntransforms information from neighbors of each node to create a new representation for that node which is more\\r\\ninformative for the prediction of a target variable like the label or class of the node. In this context, nodes can be\\r\\nstocks and edges indicate the mutual importance of stocks. GCN can construct representations of related stocks by\\r\\nanalyzing the underlying graph while considering both stock relationships and the initial feature vector of each stock.\\r\\nGCN has recently received extensive attention because of its outstanding performance on node classification tasks\\r\\nin various fields, including recommender systems Hekmatfar et al. (2021), traffic prediction Zhao et al. (2019) and,\\r\\ncancer diagnosis Zhou et al. (2019). In the field of stock market prediction, GNNs are limited to a few studies due to\\r\\ntheir recent emergence (Kim et al., 2019; Hu et al., 2018). Kim et al. (2019) used a graph attention network to forecast\\r\\nstocks. For creating their graph, they used a classic dataset of Wikidata in previous work (Vrandeciˇ c & Kr ´ otzsch ¨ ,\\r\\n2014), and their infrastructure network was very simple. GNN has been used in a few studies for representational\\r\\nlearning from textual data available in social media, news articles, and blogs (Hu et al., 2018) but they have not been\\r\\nwidely applied in the field of graph-based financial prediction to analyze financial graphical models yet.\\r\\nIn the next section, we will introduce GCENT, as a novel framework for graph-based prediction of the direction of\\r\\nstocks price movements. GCNET resolves the shortcomings of the existing methods by introducing a novel approach\\r\\nto modeling the prediction of the price fluctuations for a set of inter-connected stocks in a market as a semi-supervised\\r\\nlabel prediction process using GCN. It also presents a new technique for constructing the graphical model of the data\\r\\nand introduces an effective method for labeling a subset of the graph nodes using reliable history-based predictions as\\r\\nwell.\\r\\n3. Model description\\r\\nIn this section, we are going to explain our suggested semi-supervised prediction algorithm called GCNET. As\\r\\nsummarized in Algorithm 1, GCNET first models the stock market as a complex network structure of the stocks’\\r\\nhistorical data. Using the structure of the created graph, for each day of the stock market separately, we create a graph\\r\\nof nodes, which represent stocks and we have introduced a novel technique for connecting and weighting graph edges.\\r\\nAlso, each node contains a feature vector of technical indicators from the specified day of corresponding stock. In the\\r\\nnext step, we assign a set of initial labels to a portion of the nodes of test day using a set of reliable predictions made\\r\\nby a so-called PLD method. Each label in this step represents a first guess about the possible next price fluctuation\\r\\nfor the corresponding stock. The algorithm then uses a GCN technique to process the resulted partially labeled graph,\\r\\nin order to refine the initial labels and also predict the labels for the unlabeled subset of the nodes. The final labels\\r\\nform the algorithm’s predictions for the next-day fluctuations of the stocks. In the following subsections, we explain\\r\\nthe details of the GCNET steps.\\r\\n4\\nAlgorithm 1 GCNET algorithm\\r\\nInput :\\r\\nDataset (Price history for m stocks)\\r\\nOutput :\\r\\nList of predicted labels\\r\\nG ← Generate Influence Graph(Dataset)\\r\\nG ← PLD(Dataset,G) . label a subset of the nodes\\r\\nG ← Add node feature vectors to G\\r\\nPredictor ←Train GCN on G\\r\\nL ← Predictor(G) . predict labels using the trained GCN\\r\\nreturn (L)\\r\\n3.1. Network Construction\\r\\nIn this section, we introduce a graph generation algorithm for constructing a novel so-called influence network,\\r\\nwhich is designed to serve the needs of the subsequent label prediction mechanism used in the GCNET framework.\\r\\nThis algorithm tries to generate a network containing paths through which sharing information among nodes may\\r\\nimprove the performance of the target prediction task for each node. In the following sub-sections, we explain the\\r\\ndetails of the graph structure and the algorithm used for generating it.\\r\\nIn our graph the nodes represent stocks. If there are m stocks in the dataset: S tocks = {s1, s2, ..., sm}, we denote\\r\\nthe graph as G = (V, E), in which V = {vs1, vs2, ..., vsm} is the set of nodes and the set E is the set of edges. Each edge\\r\\nei, j connects the pair of nodes vsiand vsj, where i , j. The edges are weighted and we denote the weight of the edge\\r\\nei, j as wi, j.\\r\\nTo add edges to the graph, GCNET first calculates an influence score for each pair of nodes. This score esti\\x02mates how useful is to combine the historical information from those stocks for the prediction of each one’s future\\r\\nfluctuation. If this aggregation of information has a positive influence on the prediction performance, then an edge\\r\\nconnecting that pair of nodes is added to the graph, with a weight reflecting the amount of that influence.\\r\\nGCNET needs to build several simple prediction models to construct the influence graph with n nodes: For each\\r\\ntarget stock, one model is trained using that stock’s historical data and n−1 models are trained, each using the historical\\r\\ndata from the target stock and one other stock in the graph. To achieve a reasonable computation time for training the\\r\\nrequired prediction models we use Quadratic Discriminant Analysis that is a fast and effective history based learning\\r\\nalgorithm. Quadratic discriminant analysis (QDA) is a well-known statistical classification technique that is a variant\\r\\nof linear discriminant analysis that allows for non-linear separation of data. QDA classifier is attractive because it has\\r\\nclosed-form solutions that can be easily computed, work well in practice, and have no hyper-parameters to tune. QDA\\r\\nfinds an efficient decision rule when a linear discriminant procedure is not sufficient to separate the groups under study\\r\\nor when the covariance matrices are not equal among groups (James et al., 2013).\\r\\nQDA assumes multivariate data, following a normal distribution, and uses the Bayes theorem to calculate the\\r\\nprobability a sample x belongs to class k. However, a separate covariance matrix Σk, is assumed for each class,\\r\\nk = 1, 2, ..., kn (n = number of classes), yielding the quadratic discriminant function as:\\r\\nδk(x) = −\\r\\n1\\r\\n2\\r\\nlog|Σk| − 1\\r\\n2\\r\\n(x − µk)\\r\\nTΣ−1\\r\\nk\\r\\n(x − µk) + logπk (1)\\r\\nwhere δk is the score value of discriminant for class k, Σk is the covariance matrix and πk is the prior probability\\r\\nof the k-th population. Class Cl f , that sample x belongs to is then predicted as:\\r\\nCl f(x) = arg max\\r\\nk\\r\\nδk(x) (2)\\r\\nTo train the models, for each stock si a feature vector viis extracted from its historical data based on the set\\r\\nof features summarized in Table 1. Also, for each pair of stock si and sj an average feature vector is defined as\\r\\nvi j = vji = average(vi, vj). Then GCNET uses QDA to train four prediction models Pi, Pj, Pi j and Pji for each pair\\r\\nsi, sj of stocks. Pi denotes the model predicting the next price movement for stock S i using vi as input, while Pi j uses\\r\\nvi j as the input of the model to predict the next price movement for stock i ( Similar definitions hold for Pj, Pji).\\r\\n5\\nSuppose that Acci\\r\\n, Acci j, denote the prediction accuracy of Pi, Pi j for stock i while Accj and Accji represent\\r\\nthe accuracy of Pj, Pji in prediction of the next price movement for sj (all measured on the validation data). The\\r\\nin f luence{i, j}is then defined as follows:\\r\\nin f luence{i, j} =\\r\\n(Acci j − Acci) + (Accji − Accj)\\r\\n2\\r\\n(3)\\r\\nClearly, for a pair of stocks si and sj, in f luence{i, j} reflects the average improvement in prediction performance\\r\\nachieved for each stock by using the other stock’s information as well in the prediction model. Based on this measure\\r\\nthen the weight of the edge connecting the corresponding nodes in the graph is calculated as follows:\\r\\nwi, j = wj,i = max(in f luence{i, j}, 0) (4)\\r\\nIn other words, the weight of the edge between two stock nodes represents how much the aggregation of infor\\x02mation between those two nodes we expect to boost the performance of the resulting predictions for each stock. The\\r\\nintuition of using this technique in GCNET comes from the behavior of the aggregation step in the GNN algorithms\\r\\nsuch as GCN. Since this graph is going to be ultimately used by GCN, the edges form the paths over which informa\\x02tion from neighboring nodes are aggregated by GCN before being used for the final label prediction. The explained\\r\\nheuristic tries to directly estimate the usefulness of such an aggregation for the target prediction task, for each pair\\r\\nof nodes. If the aggregation seems to be useful then an edge is added with an appropriate weight, while the edge is\\r\\ndropped otherwise.\\r\\nIn the final step, GCNET removes the edges with the lowest weight values one by one, and continues to do so until\\r\\nremoving the next edge will make the graph disconnected. This sparsification process keeps the strong connections\\r\\nin the graph and removes the unreliable low-weight edges that may introduce noise to the later steps of the algorithm.\\r\\nIn addition, using a sparser graph improves the efficiency of the GCNET in learning the final label prediction model.\\r\\nThe steps of the influence graph construction procedure is summarized in Algorithm 2 and Fig. 1.\\r\\nFigure 1: Graph construction steps\\r\\nThe explained influence network structure can be used to represent each single day of the market. Basically the\\r\\nembedding and prediction steps of the GCNET can also be applied on the network model of the target day, we decided\\r\\nto use a set of five networks to represent each day. In this approach we construct five structurally similar networks, one\\r\\nfor the target day and four others for the four days before that. The adjacency matrix of all these networks are similar\\r\\nand the same as the constructed influence network, each stock has a corresponding node in each of those graphs with a\\r\\ndifferent attribute vector belonging to a different day (Figure 2). Using the network representations of the recent days,\\r\\nin addition to the target day, enriches the input data of the embedding and prediction step and allows it to consider the\\r\\nrecent past states and movements of the stocks as well to generate more general embeddings and hopefully a more\\r\\nstable prediction model.\\r\\n6\\nFigure 2: A sample influence graph generated from the dataset used in this paper to represent the stock relations\\r\\n(left). This structure is used by GCNET to form a graph representation of the five recent days (right) that undergoes a\\r\\nseparate labeling process for each target day t.\\r\\nAlgorithm 2 GenerateInfluenceGraph\\r\\nInput :\\r\\nDataset (Price history for m stocks)\\r\\nOutput :\\r\\nGraph G(V,E)\\r\\nV ← {}, E ← {}\\r\\nfor each stock si do\\r\\nV ← V\\r\\nS\\r\\n{ni}\\r\\nX\\r\\nT\\r\\nsi\\r\\n, X\\r\\nV\\r\\nsi ← split data(matrix of features of si) . Set Train and Validation part of data\\r\\nF\\r\\nT\\r\\nsi\\r\\n, F\\r\\nV\\r\\nsi ← split data(vector of past price fluctuations of si)\\r\\nend for\\r\\nfor each pair of stocks (si, sj) do\\r\\nAcci ← QDAi. f it(X\\r\\nT\\r\\nsi\\r\\n, F\\r\\nT\\r\\nsi\\r\\n).score(X\\r\\nV\\r\\nsi\\r\\n, F\\r\\nV\\r\\nsi\\r\\n)\\r\\nAccj ← QDAj. f it(X\\r\\nT\\r\\nsj\\r\\n, F\\r\\nT\\r\\nsj\\r\\n).score(X\\r\\nV\\r\\nsj\\r\\n, F\\r\\nV\\r\\nsj\\r\\n)\\r\\nXProcessed ← 1/2(Xsi + Xsj) . Averaging the two feature vectors to combine information\\r\\nX\\r\\nT\\r\\nProcessed, X\\r\\nV\\r\\nProcessed ← split data(XProcessed)\\r\\nAcci j ← QDAPi j . f it(X\\r\\nT\\r\\nProcessed, F\\r\\nT\\r\\nsi\\r\\n).score(X\\r\\nV\\r\\nProcessed, F\\r\\nV\\r\\nsi\\r\\n)\\r\\nAccji ← QDAPji . f it(X\\r\\nT\\r\\nProcessed, F\\r\\nT\\r\\nsj\\r\\n).score(X\\r\\nV\\r\\nProcessed, F\\r\\nV\\r\\nsj\\r\\n)\\r\\nIn f luencei, j = 1/2((Acci j − Acci) + (Accji − Accj)) . Average prediction improvement for the two stocks\\r\\nif In f luencei, j > 0 then\\r\\nE ← E\\r\\nS\\r\\n{ei, j}\\r\\nwi, j ← In f luencei, j . Set the weight of ei, j\\r\\nend if\\r\\nend for\\r\\nE ← sortasc(E) . Sort the edges in ascending order of weights\\r\\nfor each edge (ei, j) in E do . Edges with smaller weights are processed first\\r\\nif G is connected then\\r\\nE ← E − {ei, j}\\r\\nelast ← ei, j\\r\\nelse\\r\\nE ← E\\r\\nS\\r\\n{elast} . Add edge that makes graph connected\\r\\nreturn (G(V, E))\\r\\nend if\\r\\nend for\\r\\n7\\n3.2. Label assignment process\\r\\nAs mentioned earlier, GCNET uses a semi-supervised style of learning to generate a label prediction model, that\\r\\nuses a partially labeled graphical representation of data as input. In this setting, the label of a graph node is supposed\\r\\nto represent the next price movement for its corresponding stock, while the label prediction’s task is to predict the\\r\\nnext price movement for stocks whose labels are unknown. The graph structure that represents each training instance\\r\\nin GCNET, as we explained in the last section, is composed of five structurally identical networks modeling five\\r\\ndifferent days of the market. For the days before the target test day, the next day’s price of each stock is already\\r\\nknown and is used to label the nodes in those four graphs. However, for the last network, that models the last day\\r\\nof the market, the stocks’ next price movements are unknown and need to be predicted by the GCENT. However,\\r\\nbefore passing the set of five networks to the next step, GCNET assigns initial labels to some of the nodes of this\\r\\ntest-day network as well, by making reliable predictions about the future movements of their corresponding stocks\\r\\nusing a so-called PLD mechanism. Although those initial labels can be refined and changed by the algorithm in the\\r\\nnext steps, assigning those initial labels injects some intuition about the possible future of the market into the graph\\r\\nto improve the performance of the subsequent embedding and prediction steps of GCNET.\\r\\n3.2.1. Plausible Label Discovery(PLD)\\r\\nHere we introduce the PLD algorithm that assigns some initial plausible predicted labels to a subset of the nodes\\r\\nin the test-day graph. PLD trains a set PA of agile basic history-based algorithms to predict the labels for the nodes of\\r\\nthe graph on a daily basis.\\r\\nSuppose that for the prediction of the rise/fall in stock prices for a target day t we have a set of labeled instances\\r\\nfor each stock si for a period of T days before t in historical data. PLD splits the dataset into training and validation\\r\\nsets, such that the validation set VS sicontains samples belonging to a period of d days before t and the training set\\r\\nT S si\\r\\ncontains older samples. For each stock si, a prediction model is generated by each of the basic algorithms using\\r\\nthe training set T S si\\r\\n.\\r\\nPLD then scores the predictors of each stock by evaluating their prediction performance over the validation set\\r\\nof that stock. The reason for selecting the most recent labeled instances as the validation set is to make the scores\\r\\nassigned to the predictors more reliable by using the instances that are closer to the target day for evaluation. Also,\\r\\nto put more emphasis on predictions closer to the target day t when evaluating the predictors over the validation set,\\r\\nwe use a weighted scoring method that gives higher weights to the accuracy of predictions made for the more recent\\r\\ndays. The scoring method has been defined in equation (5):\\r\\nscore\\r\\nj\\r\\nsi =\\r\\nX\\r\\nd−1\\r\\ni=0\\r\\nat−1−i × (1 − c)\\r\\ni\\r\\nwhere\\r\\nal =\\r\\n\\uf8f1\\r\\n\\uf8f4\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f4\\uf8f3\\r\\n1 if predictor j predicts the class of instance insl\\r\\nsi\\r\\ncorrectly\\r\\n0 otherwise\\r\\nand\\r\\n0 < c \\x1c 1\\r\\n(5)\\r\\nOnce all the predictors are scored, we select the predictor with the highest score, bestsi, for each stock si. GCNET\\r\\nassumes that bestsi will have the most reliable prediction, among predictors of stock si, for the price movement of si\\r\\non the target day t. Here we call this score the predictability score of the stock node si (6):\\r\\npredictabilitysi = score\\r\\nbests\\r\\ni\\r\\nsi\\r\\n(6)\\r\\nTo select a node for initial labeling, in addition to its predictability score, GCNET also considers how dense the\\r\\ngraph is in the neighborhood of that node. Intuitively, nodes that are located in denser neighborhoods are possibly\\r\\nbetter candidates for initial labeling as their label information can be shared through several paths with many other\\r\\nnodes in the later embedding steps of the algorithm.\\r\\n8\\nTo evaluate the neighborhood density of each node, we use the weighted local clustering coefficient (Saramaki ¨\\r\\net al., 2007) to define the density score for a node si as:\\r\\ndensitysi = Ci =\\r\\n1\\r\\ndeg(i)(deg(i) − 1)\\r\\nX\\r\\nj,k\\r\\n( ˆwi, jwˆi,kwˆ j,k)\\r\\n1/3\\r\\n(7)\\r\\nWhere the edge weights are normalized by the maximum weight in the network, ˆwi, j = wi, j/max(w), and the contri\\x02bution of each triangle depends on all of its edge weights. This measure assigns high scores to the nodes located at\\r\\nthe center of dense neighborhoods and lower scores to the nodes in whose neighborhood strong edges are rare. The\\r\\nfinal score assigned to each node is the defined by the product of its predictability and density score as in (8):\\r\\nprivilegesi = predictabilitysi\\r\\n∗ densitysi\\r\\n(8)\\r\\nIn the final step, n% of the nodes with the highest node privilege are selected to be assigned initial labels. The\\r\\nassigned label to each selected node is the prediction of its corresponding stock’s best predictor,bestsi\\r\\nfor the target\\r\\nday: if the stock node siis among the top n% nodes with the highest privilege values and its best predictor predicts\\r\\na rise/fall in its price it will be labeled with +1/-1. However, if the node’s privilege score is not among the top n%\\r\\nscores then it will be left unlabeled. The main steps of the PLD procedure are presented in Algorithm 4.\\r\\nAlgorithm 3 PLD procedure for the target day t\\r\\nInput :\\r\\nTraining Data (training instances for all stocks)\\r\\nGraph G(V,E)\\r\\nOutput :\\r\\nPartially labeled graph G(V,E)\\r\\nPA ← {pa1, pa2, ..., pak} . Prediction algorithms\\r\\nfor each stock si do\\r\\nVS si ← {inst−1\\r\\nsi\\r\\n, inst−2\\r\\nsi\\r\\n, ..., inst−d\\r\\nsi\\r\\n} . d instances for validation\\r\\nT S si ← {inst−d−1\\r\\nsi\\r\\n, inst−d−2\\r\\nsi\\r\\n, ..., inst−T\\r\\nsi\\r\\n} . The rest for train\\r\\nfor each algorithm pajin PA do\\r\\nscore\\r\\nj\\r\\nsi ← paj\\r\\n.train(T S si\\r\\n).evaluate(VS si\\r\\n) . use pajto train and evaluate a predictor for stock si\\r\\nend for\\r\\nbestsi ← The best trained predictor for si . The one with the highest score\\r\\npredictabilitysi ← score\\r\\nbestsi\\r\\nsi\\r\\ndensitysi ← weighted local clustering coe f f icient(G, nodesi\\r\\n)\\r\\nprivilegesi ← predictabilitysi∗ densitysi\\r\\nend for\\r\\nT P ← Top n% stocks si with the highest privilegesi\\r\\n. The selected nodes to be assigned initial labels\\r\\nfor each stock sk in T P do\\r\\nlablelsk ← price fluctuation predicted by predictor bestsk\\r\\nfor the target day t of stock sk\\r\\nend for\\r\\nreturn (G(V,E)) . Partially labeled Graph\\r\\nThe set of the agile basic algorithms we have used in PLD includes neural network, Linear Discriminant Anal\\x02ysis, Decision Tree, Naive Bayes, and Quadratic Discriminant Analysis, Random forest. The input features used to\\r\\nrepresent the instances for training these models are summarized in Table 1.\\r\\n9\\nIndicator Description Indicator Description\\r\\nOP Open price RSI-S RSI indicator signal\\r\\nHP High price BB-S Bollinger bands indicator signal\\r\\nLP Low price MACD-S MACD indicator signal\\r\\nCP Close price SAR-S SAR indicator signal\\r\\nVolume Trading volume ADX-S ADX indicator signal\\r\\nCCI Commodity Channel Index S-S Stochastic indicator signal\\r\\nSAR Stop and reverse index MFI-S MFI indicator signal\\r\\nADX Average directional movement CCI-S CCI indicator signal\\r\\nMFI Money flow index V-S Sign(Volume -Avg(last 5 days))\\r\\nRSI Relative strength Index CPOP-S Sign(CP-OP)\\r\\nSK Slow stochastic %K CPCPY-S Sign(CP-Closing price yesterday)\\r\\nSD Slow stochastic %D\\r\\nTable 1: Technical indicators and price information used by basic prediction algorithms in PLD\\r\\nPlease note that the explained model training and evaluation process is repeated for each target day, so, the nodes\\r\\nthat are labeled by PLD, are not necessarily the same for different target days.\\r\\n3.3. Price Movement Prediction\\r\\nIn this phase, node features are added to the partially labeled graph of stocks generated in previous steps and the\\r\\nresulted graph is used to train a Graph Convolutional Network model for prediction of the stock price movements as\\r\\nfinal node labels of the graph. We first present a brief background about the Graph Convolutional Network model and\\r\\nthen we will explain the details of model training procedure.\\r\\n3.3.1. GCN Background\\r\\nGraph Convolutional Network (GCN) is a framework for representation learning in graphs and is derived from\\r\\ngraph signal processing theory. GCN can be applied directly on graph structured data to extract informative represen\\x02tations for each node by aggregating information from its neighbors in depth d. GCN can be considered as a Laplacian\\r\\nsmoothing operator for node features over graph structures. The architecture of GCN consists of a series of convolu\\x02tional layers. Each layer of GCN integrates information from the neighbors of each node to update its representation\\r\\naccording to the structure of all nodes and edges in the graph by the Laplacian matrix.\\r\\nIn spectral theory, the convolution operation is defined in the Fourier domain by computing the eigendecomposi\\x02tion of the graph Laplacian. The operation can be defined as the multiplication of a signal x ∈ R\\r\\nN\\r\\n(a scalar for each\\r\\nnode) with a filter gθ = diag(θ) parameterized by θ ∈ R\\r\\nN\\r\\n:\\r\\ngθ ? x = Ugθ(Λ)U\\r\\nT\\r\\nx (9)\\r\\nwhere U is the matrix of eigenvectors of the normalized graph Laplacian (L), with a diagonal matrix of its eigen\\x02values Λ. Equation (9) describes a spectral convolution filter gθ used for graph data x. This operation results in\\r\\npotentially intense computations. Hammond et al. (2011) approximated spectral filters in terms of Chebyshev poly\\x02nomials and Kipf & Welling (2016) limit the layer-wise convolution operation to K = 1 to alleviate the problem of\\r\\noverfitting on local neighborhood structures for graphs with very wide node degree distributions and Introduce the\\r\\nGraph Convolution Network. Also, GCN simply transform Equation (9) as a fully connected layer with a built-in\\r\\nconvolution filter, which is written as the following equation:\\r\\nH\\r\\nl+1 = ReLU(AHb lWl\\r\\n) (10)\\r\\nwhere Ab = D˜ −\\r\\n1\\r\\n2 A˜D˜ −\\r\\n1\\r\\n2 . Here, A˜ = A + IN, in which A˜ is the adjacency matrix of the graph G, with added self\\x02connections, IN is the identity matrix, D˜ =\\r\\nP\\r\\nj A˜\\r\\ni j and Wl\\r\\nis a layer-specific train able weight matrix.\\r\\n10\\n3.3.2. Training and predicting Procedure\\r\\nEach node of the graphs is represented by an initial feature vector that includes a set of technical indicator signals\\r\\nand the stock’s last day price information as features. The complete set of features used for representing each node has\\r\\nbeen summarized in Table 2. To train the label prediction model on this graph, we use a GCN with three convolution\\r\\nlayers. The first hidden layer H\\r\\n0\\r\\nreceives the original node features (feature matrix X). Our model runs 3 iterations of\\r\\nupdates according to Equation (10) to generate the final output node embeddings and the overall process of generating\\r\\nthe output can be defined as:\\r\\nY = so f tmax(AReLU b (AReLU b (AXW b 0)W1)W2) (11)\\r\\nwhere W0 ∈ R\\r\\n|X|×C0\\r\\nis an input-to-hidden weight matrix for a hidden layer with C0 feature maps and W1 ∈ R\\r\\nC0×C1\\r\\nis a hidden-to-hidden weight matrix with C1 feature maps and W2 ∈ R\\r\\nC1×C2\\r\\nis a hidden-to-output weight matrix. The\\r\\nvalue of C2 for prediction of the two classes (rise and fall) is 2. We train the network using cross-entropy loss over\\r\\npredictions for all stocks modeled in the network:\\r\\nLoss = −\\r\\nX\\r\\ni∈ZL\\r\\nX\\r\\nF\\r\\nf=1\\r\\nZi f lnYi f (12)\\r\\nwhere ZL is the set of stocks with plausible labels, F is the class of stock price movement direction.\\r\\nConsidering the number of features and training examples, we used 4 channels for the first and second layers and\\r\\n2 channels to predict 2 classes in the third layer. The activation function for the first and second layers is ReLU and\\r\\nfor the third layer is Sigmoid.\\r\\nIndicator Description Indicator Description\\r\\nRSI-S RSI indicator signal MFI-S MFI indicator signal\\r\\nBB-S Bollinger bands indicator signal CCI-S CCI indicator signal\\r\\nMACD-S MACD indicator signal V-S Sign(Volume -Avg(last 5 days))\\r\\nSAR-S SAR indicator signal CPOP-S Sign(CP-OP)\\r\\nADX-S ADX indicator signal CPCPY-S Sign(CP-Closing price yesterday)\\r\\nS-S Stochastic indicator signal\\r\\nTable 2: Features used to represent nodes in the graph\\r\\nThe GCN model is trained separately for each test day t during the test period. As we explained before, five graphs\\r\\nrepresenting days t − 4 to t are used for training the prediction model for the target day t, among which graphs for\\r\\ndays t −4 to t −1 are fully labeled and the graph representing the target day t is partially labeled. Each of these graphs\\r\\nis used in turn, in chronological order, to train the model for n epochs, i.e. GCNET first trains the GCN model using\\r\\nthe graph of the day t − 4 for n epochs, then it continues the training process for another n epochs using the graph of\\r\\nthe day t − 3 and so on (the value of n is set using the validation data). So, GCNET tries to train an initial prediction\\r\\nmodel using the fully-labeled graphs of the previous days. The labels of those graphs are reliable as they represent the\\r\\nactual price fluctuations observed in the corresponding days, and by using them GCNET is able to train a stable initial\\r\\nmodel that is consistent with the behavior of the market in past few days. That initial model is then adjusted in the\\r\\nlast n training epochs using the partially labeled graph of the target day t that represents the most recent state of the\\r\\nmarket as well as some initial predictions about its future. By processing the graphs in chronological order GCNET\\r\\npractically puts an emphasis on the more recent information, as they are used in the late steps of training in which the\\r\\nGCN model is fine-tuned and finalized.\\r\\nUsing the explained process, GCNET trains a GCN model to predict the labels for all the nodes of the day t\\r\\ngraph, whether they have initial labels or not. The prediction of the GCNET about the next price movements of each\\r\\nstock in the market can then be inferred from the final label predicted for its corresponding node in the day t graph.\\r\\nGCNET repeats these model training and label prediction steps for each prediction day in the data. Figure 3 shows\\r\\nthe component architecture and steps of GCNET.\\r\\n11\\nFigure 3: Architecture of the components of the GCNET model\\r\\n3.4. Complexity analysis\\r\\nGCNET algorithm is comprised of three phases: creating the network, running PLD, and training GCN. In the\\r\\nfollowing, we use m and n to denote the number of stocks/nodes and the number of training data samples for each\\r\\nstock.\\r\\nTo create the influence network, first, the influence value between each pair of stocks is calculated by training\\r\\nthe required QDA prediction models as we explained in section 3.1. QDA is a variation of LDA algorithms with the\\r\\ntime complexity of O(kn f + k\\r\\n3\\r\\n) where f is the number of features, and k = min(f, n). When the number of training\\r\\nsamples and the number of features is small, as they are in this work, this algorithm runs fast. In our implementations,\\r\\nwe use a small set of features and f < n, so the time complexity is O(n f 2 + f\\r\\n3\\r\\n). The number of models that must\\r\\nbe trained to construct the entire graph is O(m\\r\\n2\\r\\n) and the total computation complexity for generating the complete\\r\\ninfluence network is O(m\\r\\n2\\r\\n(n f 2 + f\\r\\n3\\r\\n)). The computational cost of sparsifying each graph is O(m\\r\\n2\\r\\n).\\r\\nThe computational time complexity of the PLD phase depends on the set of basic algorithms we select to use.\\r\\nWe have applied simple algorithms among which random forest has the highest time complexity that dominates the\\r\\nrunning time of this module. The Time complexity for building a complete decision tree is O(f n log n), where f\\r\\nis the number of features used by the model. So, the computational complexity of the random forest algorithm is\\r\\nO(k f n log n), where k is the number of trees generated by the model (Hassine et al., 2019). Since the PLD algorithm\\r\\ntrains its models for each stock separately, its model training time complexity for the configuration used in this study\\r\\nwould be O(k f m n log n). One can decide to train the basic algorithms for every prediction day, or instead, use the\\r\\ntrained models for a few days before retraining or tuning them. In this study, we use a set of simple algorithms to keep\\r\\nthe computational time low, and retrain them for each prediction day.\\r\\nThe time complexity of GCN is O(L||Ab||0 f +L N f 2), where ||Ab||0 is the number of nonzero values of the adjacency\\r\\nmatrix and f\\r\\n0\\r\\n, L and n denote the size of feature vectors used by GCN, the number of GCN layers and the number of\\r\\nthe nodes(i.e. stocks) respectively (Wu et al., 2020). The GCN training step is repeated for each prediction day while\\r\\nthe trained model is used to predict the labels for all stock nodes in the graph.\\r\\nRegarding the fact that the underlying graph used by GCNET is rather small and sparse, the running time for\\r\\nits periodical construction and daily analysis is low. So in practice, the training time of GCNET is almost linearly\\r\\nproportional to the running time of the most time-consuming algorithm used in PLD. If, as we did in this study,\\r\\n12\\nsimple and efficient algorithms are used in PLD, the GCNET algorithm will be more time-efficient than most of the\\r\\nstate-of-the-art deep models used by single stock prediction systems.\\r\\n4. Experiments\\r\\nIn this section, we first explain the data and the pre-processing step, and then we present the evaluation process\\r\\nand the results of the GCNET model.\\r\\n4.1. Data description\\r\\nTo evaluate the performance of GCNET and the baseline algorithms we used a large dataset of the famous stocks\\r\\nin the Nasdaq market. Our dataset contains the historical data for 100 stocks with the largest market capitalization in\\r\\nthe Nasdaq index from 01/01/2011 to 01/01/2021. The price data has been obtained from Yahoo Finance. Each daily\\r\\nrecord of data for each stock contains open, low, high, close, adjusted close, and volume values. The historical data\\r\\nbefore each test day is used to train the model. The PLD models and the semi-supervised label prediction mechanisms\\r\\nare retrained for every prediction day. The data in the 09/15/2020 to 09/29/2020 interval is used for validation and to\\r\\nset the hyper-parameters. The test period is from 09/30/2020 to 12/30/2020 which includes 5952 records.\\r\\n4.2. Implementation and Parameters of GCNET\\r\\nTo keep the graph structure up to date, the graph can be rebuilt in t-day intervals. If the training and validation\\r\\ndataset using which the edge weights are calculated are large enough, the resulted graph is expected to perform well\\r\\nfor a long period of time, however, in our experiments, we reproduce the graph every 30 days. Fig. 2 shows a sample\\r\\ngraph that is generated for the dataset used in this paper. We apply Adam optimizer (Da, 2014) to train the GCN\\r\\nand to avoid over-fitting we applied L2 regularization and dropout techniques. The weights of the GCN have been\\r\\ninitialized by Glorot uniform initializer (Glorot & Bengio, 2010). For GCN implementations we used Keras (Chollet\\r\\net al., 2015) and Spektral (Grattarola & Alippi, 2020) libraries. The number of top %n nodes that are labeled by the\\r\\nbasic models is set using the validation data. Other parameters like learning rate and dropout rate have been set to\\r\\nwidely-used values in the literature.\\r\\n4.3. Evaluation Metrics\\r\\nFollowing previous work in stock prediction domain (Wu et al., 2022; Kia et al., 2018), we adopt the standard\\r\\nmeasure of accuracy and Matthews Correlation Coefficient (MCC) as evaluation metrics Given the confusion matrix\\r\\n \\r\\ntp f n\\r\\nf p tn !\\r\\ncontaining the number of samples classified as true positive, false positive, true negative and false negative,\\r\\nAccuracy and MCC are defined as:\\r\\nACC =\\r\\ntp + tn\\r\\ntp + tn + f p + f n\\r\\n(13)\\r\\nMCC =\\r\\ntp × tn − f p × f n\\r\\np\\r\\n(tp + f p)(tp + f n)(tn + f p)(tn + f n)\\r\\n(14)\\r\\n4.4. Baselines\\r\\nWe compare our model with the following baselines, which include network-based models, deep learning models,\\r\\nartificial neural network models, and models that use feature selection:\\r\\n• ALSTM: This is a deep model, which uses a dual-stage attention-based recurrent neural network. In the first\\r\\nstage, the attention mechanism is applied to input features and is used to extract the relevant representation at\\r\\neach time step by referring to the previous hidden state. In the second stage, a temporal attention mechanism is\\r\\napplied to select relevant encoder hidden states across all time steps. Finally, an LSTM neural network is used\\r\\nto predict stock movement. (Qin et al., 2017).\\r\\n13\\n• STOCKNET: A variational auto-encoder that combines price and text information. Price features are modeled\\r\\nsequentially and The text is encrypted using a hierarchical attention mechanism in a multi-day sequence (Xu &\\r\\nCohen, 2018).\\r\\n• HATS: A hierarchical graph attention method that uses graph modeling to weigh different relationships between\\r\\nstocks. they use a relational modeling module with initialized node representations. Their model selectively\\r\\naggregates information on different relation types and adds the information to the representations of each com\\x02pany. they use only historical price data. (Kim et al., 2019).\\r\\n• CNN-pred: A deep CNN model that uses a diverse set of financial variables, including technical indicators,\\r\\nstock indices, futures contracts, etc. Their model constructs three and two-dimensional input tensors. Then,\\r\\nthe input tensors are fed into a specified CNN model to make predictions. They use a fixed combination of\\r\\nthe target market’s time series as well as a few related time series as input to the model to predict the future of\\r\\nthe target time series. In our experiments, we used data from three closest neighbors of the target stock in the\\r\\ncorrelation graph as the related time series whose data is fed to the model(Hoseinzade & Haratizadeh, 2019).\\r\\n• Adv-LSTM: In this work, the authors introduced a deep model using an Adversarial Attentive LSTM mech\\x02anism, which leverages adversarial training to simulate the stochasticity during model training. They advised\\r\\nemploying adversarial training to improve the generalization of a neural network prediction model because the\\r\\ninput features to stock prediction are typically based on stock price, which is essentially a stochastic variable\\r\\nand continuously changed with time by nature. Adversarial learning trains a classification model to defense\\r\\nadversarial examples, which are intentionally generated to perturb the model. They added perturbations to their\\r\\nstock data and train the model to work well under small yet intentional perturbations. In our experiment, we\\r\\nhave tried to tune the hyperparameters, the number of hidden units, lag size, and weight of the regularization\\r\\nterm (Feng et al., 2019a).\\r\\n• exLift+DiMexRank+(PLD): exLift + DiMexRank is a network-based model for prediction of market indices\\r\\nthat encrypts relationships between markets’ indices using Association rule learning and predicts the direction\\r\\nof market index movement using PageRank algorithm. The initial labeling step of the original algorithm is de\\x02signed for a specific market prediction task and cannot be applied in the domain of this study. Therefore, to use it\\r\\nas a baseline algorithm, we apply our suggested PLD method for the initial labeling step in exLift+DimexRank,\\r\\nand keep the rest of the model intact. (Kia et al., 2020).\\r\\n• Price graphs: a graph-based model that transforms time series into visibility graphs, then, structural informa\\x02tion and referring to temporal point associations are extracted from the mapped graphs by a graph embedding\\r\\nmethod. Price graphs use the attention-based layers and a fully connected layer for stock trend prediction (Wu\\r\\net al., 2022).\\r\\n4.5. Results and discussion\\r\\nIn this section, we compare our model to the baseline models mentioned in the previous section. Our evaluations\\r\\nwere performed on a large set of stocks in the Nasdaq market, and Tables 3 summarizes the average performance of\\r\\nGCNET and baseline algorithms on 10 independent experiments.\\r\\nAs it can be seen, GCNET outperforms all baselines, while HATS, Price graphs, and exLift+ DimexRank+(PLD)\\r\\nachieve the highest prediction performances among baselines. The significant superiority of GCNET over baseline\\r\\nalgorithms most of which use different sources of information and diverse sets of features as well as sophisticated\\r\\nprediction techniques demonstrates the power of the proposed graph-based prediction algorithm. As another important\\r\\nobservation, graph-based models, especially GCNET, achieve significantly higher values of MCC compared to other\\r\\nalgorithms. MCC, or Matthew’s Correlation Coefficient, is designed to summarize the confusion matrix. A high value\\r\\nin the MCC measure reflects the power of a model in successfully predicting both ascent and descent classes. One\\r\\nreason for this superiority can be the fact that graph-based techniques exploit the graph structure to infer a consistent\\r\\nset of predicted final labels. In such a setting, if an initial prediction is mistakenly biased towards one of the classes\\r\\nfor a certain stock, it can be corrected by the subsequent network-based information-aggregation steps.\\r\\nThe satisfactory performance of exLi-ft+DimexRank+(PLD), as a simple graph-based algorithm, affirms the use\\x02fulness of the graphical modeling in this domain, even if a simple label prediction mechanism is applied in that\\r\\n14\\nalgorithm. Also, the superiority of our models over exLift+DiMexRank+(PLD) (that uses random walk analysis\\r\\nto assign labels to the nodes), shows the positive effect of formulating the price movement prediction problem as\\r\\na model-based label prediction task. As we mentioned before, discovering and combining latent information from\\r\\ndifferent stocks, whose relations are modeled as a graph structure, is the core ability of GCN that we exploited in\\r\\nour suggested algorithm, and as the experimental observations show, it can lead to significant improvement of the\\r\\nprediction performance.\\r\\nAlso, Price graphs, which is a powerful and recently introduced graph-based deep model, has achieved an accuracy\\r\\nof 55.21, which is significantly lower than the performance of GCNET. The Price graphs, unlike GCNET, uses the\\r\\nvisibility graphs to model the relations between samples of a single stock over time, and the superiority of GCNET\\r\\nshows that our introduced approach for graph-based analysis of different stocks relations seems to be a more reliable\\r\\nand effective technique in the prediction of stock price fluctuations.\\r\\nModel Accuracy MCC\\r\\nALSTM (Qin et al., 2017) 52.63 ±0.04 -0.002 ±0.001\\r\\nSTOCKNET (Xu & Cohen, 2018) 54.74 ±0.27 0.0375 ±0.009\\r\\nHATS (Kim et al., 2019) 55.12 ±0.21 0.0618 ±0.007\\r\\nCNN-pred (Hoseinzade & Haratizadeh, 2019) 53.42 ±0.62 0.0366 ±0.015\\r\\nAdv-LSTM (Feng et al., 2019a) 54.66 ±0.45 0.0412 ±0.011\\r\\nexLift+ DiMexRank+(PLD)(Kia et al., 2020) 54.03 ±0.68 0.0744 ±0.013\\r\\nPrice graphs (Wu et al., 2022) 55.21 ±0.32 0.0821 ±0.008\\r\\nGCNET (our model) 56.71 ±0.49 0.1013 ±0.011\\r\\nTable 3: Comparison of GCNET model result with baselines\\r\\n4.5.1. Model components analysis\\r\\nIn this subsection, we are going to discuss the importance of model components separately and provide more\\r\\nexperiments for each component. First, we investigate the role of the influence network and then we discuss the node\\r\\nselection mechanism and GCN.\\r\\nNetwork The influence network discovers about 2300 edges during the weighting process, and after pruning the\\r\\nedges with low weight, the graph retains the effect of about 800 edges, which achieved an average of 3.67% increase\\r\\nin forecast accuracy in network validation data.\\r\\nTo see how GCNET performs when using another approach for modeling the stock relations, we replaced the\\r\\ninfluence network with a correlation network, that, as we explained in the literature review section, is the most com\\x02monly used network structure for predicting stocks. We created a network of stocks in which the edges represent the\\r\\ncorrelations among stocks past prices. We then used the same pruning technique applied for the influence network so\\r\\nthat the edges connecting the strongly correlated stock nodes remain in the network and the pruned network is still\\r\\nconnected. We kept the rest of the GCNET algorithm intact and evaluated its performance in prediction of the stock\\r\\nprice movements. The results are summarized in the Table 4:\\r\\nmethod Accuracy\\r\\nGCNET with Correlation graph 55.79\\r\\nGCNET with Influence graph 56.71\\r\\nTable 4: Comparison table of correlation and influence network performance.\\r\\nAs the results show, the original version of GCNET outperforms the one using the well known and widely used\\r\\n15\\ncorrelation graph with a significant improvement. Another interesting observation is that even the correlation based\\r\\nversion of GCNET is still superior to other baselines as reported in Table 3, that demonstrates the effectiveness of\\r\\nthe introduced graph-based framework compared to other state of the art approaches for the prediction of stock price\\r\\nmovements.\\r\\nSemi-supervised training As the main part of our model, GCN predicts the next day by combining neighbors’\\r\\ninformation and considering labeled nodes for stock movement. To demonstrate the importance of using the GCN\\r\\nalgorithm in a semi-supervised approach, we compared our model with two different supervised versions in both of\\r\\nwhich the model is trained only with the graphs from previous days whose nodes have known labels (This means that\\r\\nthe last day’s graph whose labels are unknown and in the original setting, was partially labeled using the predictions\\r\\nof the PLD mechanism is no longer available to the GCN model). In the first supervised model, GCN model trained\\r\\nusing the fully-labeled networks of the past days is directly used to predict the target day’s price movements while in\\r\\nthe second model, we fed the created embeddings by the GCN to an LSTM model for prediction of the target day’s\\r\\nprice movements. The results are shown in table 5:\\r\\nmethod Accuracy\\r\\nSupervised GCN 55.08\\r\\nSupervised GCN+LSTM 55.46\\r\\nGCNET (Semi-Supervised) (our model) 56.71\\r\\nTable 5: Comparison table of GCN in supervised framework and GCNET.\\r\\nThe table 5 shows that the use of GCN in a supervised framework reduces the performance of the model, and\\r\\nconfirms that our suggested semi-supervised approach for extracting embeddings leads to a superior label-prediction\\r\\nperformance. The reason is that in the suggested semi-supervised approach for embedding, the most recent informa\\x02tion about the state of each stock is considered during the GCN weight training and learning process over the last-day’s\\r\\npartially labeled graph (Kipf & Welling, 2016).\\r\\nLabel assignment As we explained before, the algorithm uses a pool of basic agile predictors and by network\\r\\nanalysis techniques, selects the most effective nodes for labeling as well as the best available predictions to be used\\r\\nas their label. To see the effect of using the pool of predictors mechanism for assigning accurate initial labels, we\\r\\nevaluated the performance of the model when using each single member of the pool, instead of the original ensemble\\r\\ntechnique, for label assignment. The results are summarized in Table 6 and it can be seen that using the original\\r\\nensemble labeling technique has led to a significant improvement in the GCNET’s overall performance. It can also be\\r\\nseen that among the basic algorithms used by PLD, Random Forest has the highest prediction performance, which is\\r\\nan interesting observation, as PLD and Random Forest are both ensemble methods.\\r\\n16\\nThe method used for initial label assignment Overall accuracy of GCNET\\r\\nSupport Vector Machine 52.76\\r\\nFisher Discriminant Analysis 53.62\\r\\nDecision Tree 53.89\\r\\nANN 54.03\\r\\nLSTM 54.62\\r\\nLinear Discriminant Analysis 54.94\\r\\nRandom Forest 55.11\\r\\nPlausible Label Discovery (PLD) 56.71\\r\\nTable 6: Comparison of different techniques for assigning labels to supervised nodes in test day in GCNET\\r\\nAs we know, the GCNET assigns initial labels to a subset of the nodes in the graph of the target day (as an initial\\r\\nguess about the future price movements of some stocks) and leaves the rest of the nodes unlabeled. After processing\\r\\nthis partially labeled graph, GCNET generates its final predictions for every stock, including both initially labeled and\\r\\nunlabeled ones. To see how the GCNET prediction mechanism improves the initial labels, please see Table 7. As it can\\r\\nbe seen, on average, only 52.47 of the initial labels have been correct, while GCNET improves the accuracy of those\\r\\nlabels in its final prediction by about 3.5%. It also achieves even a higher overall prediction accuracy, considering\\r\\nboth initially labeled and unlabeled nodes. This is an important observation that shows the performance of GCNET\\r\\nexceeds the prediction performance of PLD, even for the nodes that have initially been labeled by PLD. The reason\\r\\nis that GCNET combines initial predictions with the information reflecting the graph structure and the local node\\r\\nstates to refine the initial guesses and make consistent and reliable final predictions. This observation confirms the\\r\\nusefulness of graph construction and analysis techniques used by GCNET.\\r\\nData Accuracy\\r\\nPLD on initially labeled nodes 52.47\\r\\nGCNET on initially labeled nodes 55.98\\r\\nGCNET on all nodes 56.71\\r\\nTable 7: The performance of GCNET in improving the accuracy of its initial predictions\\r\\nAlso, GCNET uses a procedure to select some nodes for labeling by considering the density of each part of the\\r\\ngraph. To see the effect of node selection procedure, we evaluated the performance of GCNET version in which a\\r\\nrandom subset of nodes are selected and then the label produced by their best predictors are assigned to them as initial\\r\\nlabels. The results are represented in table 8. It can be seen that our original node selection procedure has a critical\\r\\nrole in the overall GCNET performance improvement.\\r\\nData Accuracy\\r\\nGCNET with random nodes selected for initial label assignment 54.98\\r\\nGCNET 56.71\\r\\nTable 8: The effect of GCNET’s node selection procedure on its overall performance\\r\\nTo report the effect of n, the portion of nodes that are initially labeled by PLD, on the performance of the algorithm,\\r\\nwe presented the prediction’s performance for different values of this parameter in Figure 4. One advantage of GCN is\\r\\nits outstanding performance when training the model with a rather small set of labeled nodes (Kipf & Welling, 2016).\\r\\n17\\nHowever, as shown in this figure, using a very small value for n, reduces the accuracy of the prediction possibly\\r\\nbecause in such a situation, the information provided by the labeled nodes would not be enough for extracting reliable\\r\\npatterns for prediction. On the other hand, labeling too many nodes by the basic predictors reduces the performance\\r\\nof the algorithm. These observations confirm the positive role of network analysis in boosting the performance of\\r\\nthe prediction: Injecting too much information from the history based models into the graph, not only can introduce\\r\\nsome noise to the model but also limits the effect of GCN in the label prediction process, that apparently reduces the\\r\\nperformance of the algorithm.\\r\\nFigure 4: Prediction accuracy chart based on different label thresholds on validation part of the dataset used in this\\r\\npaper\\r\\n5. Conclusion\\r\\nIn this paper, we presented a novel GCN-based framework for the prediction of stock price movements. Our\\r\\nsuggested framework models the relations among stock prices as a novel graph structure called influence network.\\r\\nThe algorithm uses a pool of supervised history-based prediction models to label a subset of nodes using the plausible\\r\\npredictions of those models. The partially labeled network is then analyzed by a GCN to extract new representations\\r\\nfor the nodes and predict the next direction of price movement for all the stocks. Our experiments showed that the\\r\\nsuggested graph-based semi-supervised prediction model significantly outperforms state of the art baseline single\\x02stock and graph-based prediction algorithms. They also show that the introduced algorithm for constructing the\\r\\ninfluence network as well the initial labeling assignment technique and the final label prediction mechanism, each has\\r\\na contribution to the high performance achieved by GCNET framework.\\r\\nThe only source of information used by the algorithm was the price history for the stocks. One direction for future\\r\\nresearch is to consider other sources of information, such as textual information available on the web, to enhance\\r\\nthe quality of the graphical modeling of the stocks possible interactions. Another suggestion would be to study the\\r\\nprocess of assigning initial labels to the nodes, especially by considering the graphical features of the nodes and the\\r\\namount of initial information that is injected into different parts of the graph.\\r\\nReferences\\r\\nArevalo, A., Ni ´ no, J., Hern ˜ andez, G., & Sandoval, J. (2016). High-frequency trading strategy based on deep neural ´\\r\\nnetworks. In International conference on intelligent computing (pp. 424–436). Springer.\\r\\nBustos, O., & Pomares-Quimbaya, A. (2020). Stock market movement forecast: A systematic review. Expert Systems\\r\\nwith Applications, 156, 113464.\\r\\nChollet, F. et al. (2015). keras.\\r\\nDa, K. (2014). A method for stochastic optimization. arXiv preprint arXiv:1412.6980, .\\r\\n18\\nDi Persio, L., & Honchar, O. (2016). Artificial neural networks architectures for stock price prediction: Comparisons\\r\\nand applications. International journal of circuits, systems and signal processing, 10, 403–413.\\r\\nFeng, F., Chen, H., He, X., Ding, J., Sun, M., & Chua, T.-S. (2019a). Enhancing stock movement prediction with\\r\\nadversarial training. arXiv preprint arXiv:1810.09936, .\\r\\nGlorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In\\r\\nProceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249–256).\\r\\nJMLR Workshop and Conference Proceedings.\\r\\nGrattarola, D., & Alippi, C. (2020). Graph neural networks in tensorflow and keras with spektral. arXiv preprint\\r\\narXiv:2006.12138, .\\r\\nGrover, A., & Leskovec, J. (2016). node2vec: Scalable feature learning for networks. In Proceedings of the 22nd\\r\\nACM SIGKDD international conference on Knowledge discovery and data mining (pp. 855–864).\\r\\nGunduz, H., Yaslan, Y., & Cataltepe, Z. (2017). Intraday prediction of borsa istanbul using convolutional neural\\r\\nnetworks and feature correlations. Knowledge-Based Systems, 137, 138–148.\\r\\nHammond, D. K., Vandergheynst, P., & Gribonval, R. (2011). Wavelets on graphs via spectral graph theory. Applied\\r\\nand Computational Harmonic Analysis, 30, 129–150.\\r\\nHassine, K., Erbad, A., & Hamila, R. (2019). Important complexity reduction of random forest in multi-classification\\r\\nproblem. In 2019 15th International Wireless Communications & Mobile Computing Conference (IWCMC) (pp.\\r\\n226–231). IEEE.\\r\\nHekmatfar, T., Haratizadeh, S., & Goliaei, S. (2021). Embedding ranking-oriented recommender system graphs.\\r\\nExpert Systems with Applications, 181, 115108.\\r\\nHoseinzade, E., & Haratizadeh, S. (2019). Cnnpred: Cnn-based stock market prediction using a diverse set of vari\\x02ables. Expert Systems with Applications, 129, 273–285.\\r\\nHu, Z., Liu, W., Bian, J., Liu, X., & Liu, T.-Y. (2018). Listening to chaotic whispers: A deep learning framework\\r\\nfor news-oriented stock trend prediction. In Proceedings of the eleventh ACM international conference on web\\r\\nsearch and data mining (pp. 261–269).\\r\\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning volume 112. Springer.\\r\\nKara, Y., Boyacioglu, M. A., & Baykan, O. K. (2011). Predicting direction of stock price index movement using ¨\\r\\nartificial neural networks and support vector machines: The sample of the istanbul stock exchange. Expert\\r\\nsystems with Applications, 38, 5311–5319.\\r\\nKia, A. N., Haratizadeh, S., & Shouraki, S. B. (2018). A hybrid supervised semi-supervised graph-based model to pre\\x02dict one-day ahead movement of global stock markets and commodity prices. Expert Systems with Applications,\\r\\n105, 159–173.\\r\\nKia, A. N., Haratizadeh, S., & Shouraki, S. B. (2020). Network-based direction of movement prediction in financial\\r\\nmarkets. Engineering Applications of Artificial Intelligence, 88, 103340.\\r\\nKim, M., & Sayama, H. (2017). Predicting stock market movements using network science: An information theoretic\\r\\napproach. Applied Network Science, 2, 1–14.\\r\\nKim, R., So, C. H., Jeong, M., Lee, S., Kim, J., & Kang, J. (2019). Hats: A hierarchical graph attention network for\\r\\nstock movement prediction. arXiv preprint arXiv:1908.07999, .\\r\\nKipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint\\r\\narXiv:1609.02907, .\\r\\n19\\nLi, W., & Liao, J. (2017). A comparative study on trend forecasting approach for stock price time series. In 2017 11th\\r\\nIEEE International Conference on Anti-counterfeiting, Security, and Identification (ASID) (pp. 74–78). IEEE.\\r\\nLong, J., Chen, Z., He, W., Wu, T., & Ren, J. (2020). An integrated framework of deep learning and knowledge graph\\r\\nfor prediction of stock price trend: An application in chinese stock exchange market. Applied Soft Computing,\\r\\n91, 106205.\\r\\nNelson, D. M., Pereira, A. C., & de Oliveira, R. A. (2017). Stock market’s price movement prediction with lstm neural\\r\\nnetworks. In 2017 International joint conference on neural networks (IJCNN) (pp. 1419–1426). IEEE.\\r\\nPark, K., & Shin, H. (2013). Stock price prediction based on a complex interrelation network of economic factors.\\r\\nEngineering Applications of Artificial Intelligence, 26, 1550–1561.\\r\\nPatel, J., Shah, S., Thakkar, P., & Kotecha, K. (2015). Predicting stock and stock price index movement using trend\\r\\ndeterministic data preparation and machine learning techniques. Expert systems with applications, 42, 259–268.\\r\\nQin, Y., Song, D., Chen, H., Cheng, W., Jiang, G., & Cottrell, G. (2017). A dual-stage attention-based recurrent neural\\r\\nnetwork for time series prediction. arXiv preprint arXiv:1704.02971, .\\r\\nRather, A. M., Sastry, V., & Agarwal, A. (2017). Stock market prediction and portfolio selection models: a survey.\\r\\nOpsearch, 54, 558–579.\\r\\nSaramaki, J., Kivel ¨ a, M., Onnela, J.-P., Kaski, K., & Kertesz, J. (2007). Generalizations of the clustering coe ¨ fficient\\r\\nto weighted complex networks. Physical Review E, 75, 027105.\\r\\nShah, J., Jain, R., Jolly, V., & Godbole, A. (2021). Stock market prediction using bi-directional lstm. In 2021\\r\\nInternational Conference on Communication information and Computing Technology (ICCICT) (pp. 1–5). IEEE.\\r\\nShin, H., Hou, T., Park, K., Park, C.-K., & Choi, S. (2013). Prediction of movement direction in crude oil prices based\\r\\non semi-supervised learning. Decision Support Systems, 55, 348–358.\\r\\nSoni, S. (2011). Applications of anns in stock market prediction: a survey. International Journal of Computer Science\\r\\n& Engineering Technology, 2, 71–83.\\r\\nVrandeciˇ c, D., & Kr ´ otzsch, M. (2014). Wikidata: a free collaborative knowledgebase. ¨ Communications of the ACM,\\r\\n57, 78–85.\\r\\nWu, J., Xu, K., Chen, X., Li, S., & Zhao, J. (2022). Price graphs: Utilizing the structural information of financial time\\r\\nseries for stock prediction. Information Sciences, 588, 405–424.\\r\\nWu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y. (2020). A comprehensive survey on graph neural\\r\\nnetworks. IEEE transactions on neural networks and learning systems, 32, 4–24.\\r\\nXu, Y., & Cohen, S. B. (2018). Stock movement prediction from tweets and historical prices. In Proceedings of\\r\\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1970–\\r\\n1979).\\r\\nYin, T., Liu, C., Ding, F., Feng, Z., Yuan, B., & Zhang, N. (2022). Graph-based stock correlation and prediction for\\r\\nhigh-frequency trading systems. Pattern Recognition, 122, 108209.\\r\\nZhao, L., Song, Y., Zhang, C., Liu, Y., Wang, P., Lin, T., Deng, M., & Li, H. (2019). T-gcn: A temporal graph\\r\\nconvolutional network for traffic prediction. IEEE Transactions on Intelligent Transportation Systems, 21, 3848–\\r\\n3858.\\r\\nZhong, X., & Enke, D. (2017). Forecasting daily stock market return using dimensionality reduction. Expert Systems\\r\\nwith Applications, 67, 126–139.\\r\\nZhou, Y., Graham, S., Alemi Koohbanani, N., Shaban, M., Heng, P.-A., & Rajpoot, N. (2019). Cgc-net: Cell graph\\r\\nconvolutional network for grading of colorectal cancer histology images. In Proceedings of the IEEE/CVF Inter\\x02national Conference on Computer Vision Workshops (pp. 0–0).\\r\\n20'},\n",
       " {'name': '2304.11533v1.pdf',\n",
       "  'content': 'Bi-Level Attention Graph Neural Networks\\r\\nRoshni G. Iyer\\r\\nUniversity of California, Los Angeles\\r\\nroshnigiyer@cs.ucla.edu\\r\\nWei Wang\\r\\nUniversity of California, Los Angeles\\r\\nweiwang@cs.ucla.edu\\r\\nYizhou Sun\\r\\nUniversity of California, Los Angeles\\r\\nyzsun@cs.ucla.edu\\r\\nAbstract—Recent graph neural networks (GNNs) with the\\r\\nattention mechanism have historically been limited to small\\x02scale homogeneous graphs (HoGs). However, GNNs handling\\r\\nheterogeneous graphs (HeGs), which contain several entity and\\r\\nrelation types, all have shortcomings in handling attention. Most\\r\\nGNNs that learn graph attention for HeGs learn either node\\x02level or relation-level attention, but not both, limiting their\\r\\nability to predict both important entities and relations in the\\r\\nHeG. Even the best existing method that learns both levels\\r\\nof attention has the limitation of assuming graph relations\\r\\nare independent and that its learned attention disregards this\\r\\ndependency association. To effectively model both multi-relational\\r\\nand multi-entity large-scale HeGs, we present Bi-Level Attention\\r\\nGraph Neural Networks (BA-GNN), scalable neural networks\\r\\n(NNs) that use a novel bi-level graph attention mechanism. BA\\x02GNN models both node-node and relation-relation interactions\\r\\nin a personalized way, by hierarchically attending to both types\\r\\nof information from local neighborhood contexts instead of the\\r\\nglobal graph context. Rigorous experiments on seven real-world\\r\\nHeGs show BA-GNN consistently outperforms all baselines, and\\r\\ndemonstrate quality and transferability of its learned relation\\x02level attention to improve performance of other GNNs.\\r\\nIndex Terms—graph neural networks, representation learning\\r\\nI. INTRODUCTION\\r\\nHighly multi-relational data are characteristic of real-world\\r\\nHeGs. Relational data in HeGs are defined as triples of form\\r\\n(h:head entity, r:relation, t:tail entity), indicating that two entities\\r\\nare connected by a specific relation type. Figure 1 shows a\\r\\nHeG formed by such triples. However, even comprehensive\\r\\nHeGs [1] remain incomplete. Regarding HeGs completion,\\r\\ndespite the recent years’ research progress in developing GNNs\\r\\nfor representation learning in various domains [7], [8], [14] and\\r\\nadapting the successful attention mechanism [17], [18], most\\r\\nGNNs face several challenges. They either are ill-equipped to\\r\\nhandle HeGs [9], [18], or do handle HeGs but do not learn\\r\\ngraph attention [4], [6], [13], [23], or learn inaccurate graph\\r\\nattention [5], [12], [19], [22].\\r\\nFig. 1: Partial HeG of AIFB dataset.\\r\\nConsidering the GNNs that learn graph attention, their\\r\\narchitectures are limited to only one level of attention, either\\r\\nfor nodes or relations, but rarely for both, shown in Table I.\\r\\nThis is problematic for modeling HeGs which contain several\\r\\ndifferent entity and relation types. Bi-level attention is more\\r\\npowerful in learning compared to uni-level attention, where only\\r\\none level of attention is learned by the model. Bi-level attention\\r\\nlearns attention at different levels of granularity in HeGs which\\r\\ncaptures more information about graph components than a uni\\x02level attention mechanism is capable of. HAN, one of the few\\r\\nmodels that attempts to use bi-level attention, unsurprisingly\\r\\nfalls short of capturing the associations between the node\\r\\nand relation levels in the HeG. First, HAN places unnatural\\r\\nassumptions on the data because it treats graph relations as\\r\\nindependent from each other, omitting most relation-relation\\r\\ninteractions in HeGs. Second, it requires manually chosen\\r\\nmeta paths that force many node-node and node-relation\\r\\ninteractions to also be left out, and requires domain specific\\r\\nknowledge to compute. Third, HAN lacks a general framework\\r\\nfor systematically studying bi-level attention.\\r\\nTo address the above challenges, in this paper, we present Bi\\x02Level Attention Graph Neural Networks (BA-GNN) for HeGs.\\r\\nTo summarize, our work makes the following contributions:\\r\\n(1) We design a general framework for bi-level attention, and\\r\\nidentify challenges of state-of-art NNs for HeGs.\\r\\n(2) We propose BA-GNN to model both multi-relational and\\r\\nmulti-entity large-scale HeGs. BA-GNN avoids manually\\r\\nchosen meta paths, and learns personalized graph prop\\x02erties by integrating graph entity/relation types, graph\\r\\nstructure, and graph attention using local graph neighbor\\x02hoods instead of global graph context. BA-GNN improves\\r\\naccuracy of state-of-art GNNs and scales to million\\x02node/edge graphs, like the AM archaeological dataset.\\r\\n(3) To our knowledge, we are the first to propose efficient\\r\\nbi-level attention GNNs that learn from dependency\\r\\ninteractions of both nodes/relations and without meta paths.\\r\\n(4) We rigorously experiment on seven real-world HeGs\\r\\nshowing BA-GNN consistently outperforms major state\\x02of-art NN groups, and also demonstrate quality and\\r\\ntransferability of BA-GNN’s attention-induced change in\\r\\ngraph structure to enrich other GNNs.\\r\\nThe remainder of this paper is organized as follows.\\r\\nSection II examines preliminaries and related work. Section III\\r\\npresents a general framework for computing bi-level attention,\\r\\nand describes BA-GNN’s architecture. Section IV presents\\r\\nexperiment results, ablation studies, and case studies of BA\\x02arXiv:2304.11533v1 [cs.LG] 23 Apr 2023\\nTABLE I: Properties of GNN and attention-based models, with ✓\\r\\nas advantages, and ✗ as disadvantages.\\r\\nType Model [A] [B] [C] [D] †\\r\\n[E] [F] [G]\\r\\n– TRANSFORMER [17] ✓ ✓ ✓ ✗ ✓ ✓ –\\r\\nTRANSE [2] ✓ ✓ ✗ ✗ ✗ ✓ ✗\\r\\n(1) HOLE [10] ✓ ✓ ✗ ✗ ✗ ✓ ✗\\r\\nDISTMULT [21] ✓ ✓ ✗ ✗ ✗ ✓ ✗\\r\\nCOMPLEX [16] ✓ ✓ ✗ ✗ ✗ ✓ ✗\\r\\n(2) GCN [9] ✗ ✓ ✗ ✗ ✗ ✓ ✗\\r\\nGAT∗[18] ✗ ✓ ✓ ✗ ✓ ✓ ✗\\r\\nMETAPATH2VEC [4] ✓ ✓ ✗ ✗ ✗ ✗ ✗\\r\\nHEREC [14] ✓ ✓ ✗ ✗ ✗ ✗ ✗\\r\\nHIN2VEC [6] ✓ ✓ ✗ ✗ ✗ ✗ ✗\\r\\n(3A) HEGAN [7] ✓ ✓ ✗ ✗ ✗ ✓ ✗\\r\\nTEMPORALGAT [5] ✗ ✗ ✓ ✗ ✓ ✓ ✗\\r\\nHETGNN [23] ✓ ✓ ✗ ✗ ✗ ✓ ✗\\r\\nR-GCN∗[13] ✓ ✓ ✗ ✗ ✗ ✓ ✗\\r\\nHAN∗[19] ✓ ✓ ✓ ✓ ✗ ✗ ✗\\r\\nDYSAT [12] ✓ ✗ ✓ ✗ ✓ ✓ ✓\\r\\n(3B) TGAT [20] ✗ ✗ ✓ ✗ ✓ ✓ ✓\\r\\nHGT [8] ✓ ✓ ✓ ✗ ✓ ✓ ✓\\r\\nGTN [22] ✓ ✓ ✓ ✗ ✓ ✗ ✓\\r\\n– BA-GNN (ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓\\r\\n† Personalized graph attention learns attention using the local graph neighborhood\\r\\ninstead of the global graph context.\\r\\n*Primary baseline models\\r\\nNotation: [A]: Does not require HoGs; [B]: Does not require a dynamic graph; [C]:\\r\\nLearns attention; [D]: Bi-level attention; [E] Personalized attention; [F]: Does not\\r\\nrequire meta paths; [G]: Transformer-inspired; (1): Non-GNN-based KGE models\\r\\nfor HeGs; (2): GNNs for HoGs; (3A): Non-TRANSFORMER-based GNNs for HeGs;\\r\\n(3B): TRANSFORMER-based GNNs for HeGs\\r\\nGNN models, and section V concludes.\\r\\nII. PRELIMINARY AND RELATED WORK\\r\\nHere, we introduce HeG concepts and discuss the achieve\\x02ment of various state-of-art NNs, summarized in Table I.\\r\\nDefinition 1: Heterogeneous Graph: We define HeGs\\r\\nas G = (V, E) with nodes vi ∈ V, and edges ei,j ∈ E\\r\\nconnecting source vi and target vj . Nodes in the HeG are\\r\\nassociated with entity types through an entity mapping function\\r\\nΛ(vi) : V −→ B, B = {b|b ∈ B}, for entity type b. Edges in the\\r\\nHeG are associated with relation types through a relation map\\x02ping function Γ(ei,j ) : E −→ R, R = {r|r ∈ R} for relation\\r\\ntype r. For efficiency in our model, we compute entity and\\r\\nrelation mapping functions for node and relation neighborhoods\\r\\nrather than globally such that Λ(vi) : Vi −→ Bi, Bi ⊂ B and\\r\\nΓ(ei,j ) : Ei −→ Ri, Ri ⊂ R. In this paper, we consider local\\r\\nHeG neighborhoods of a node vi ∈ V consisting of both one\\x02hop nodes, {vj |vj ∈ Vi} and one-hop relations, {r|r ∈ Ri}.\\r\\nDefinition 2: Meta Relation: The meta relation for ei,j\\r\\nbetween source vi and target vj is (Λ(vi), Γ(ei,j ),Λ(vj )), and\\r\\nΓ(ei,j )\\r\\n−1 = Γ(ej,i) is the inverse of Γ(ei,j ). In this paper, we\\r\\nloosely use the term relation to denote meta relation. Traditional\\r\\nmeta paths are a sequence of such meta relations.\\r\\nDefinition 3: Graph Attention: Graph attention en\\x02ables NNs to learn useful graph representations by selectively\\r\\nattending to different nodes and relations. Multiplicative\\r\\nand additive attention are state-of-art attention mechanisms\\r\\nused in NNs [17], [18], both of which operate on encoder\\r\\nstates. Multiplicative attention uses an inner product or cosine\\r\\nsimilarity of encoder states while additive attention is a linear\\r\\ncombination or concatenation of encoder states.\\r\\nA. GNNs for Homogeneous Graphs\\r\\nSuccessful models in this category, like GAT [18], use\\r\\nattention-based neural architectures for learning representations.\\r\\nGraph Attention Networks: GAT [18] are additive\\r\\nattention-based GNNs that effectively leverage graph structure\\r\\nand sparsity to compute a node’s attention. GAT models,\\r\\nhowever, are limited to HoGs and cannot handle HeGs which\\r\\ncontain different relations that may have varying levels of\\r\\nimportance for different nodes.\\r\\nB. GNNs for Heterogeneous Graphs\\r\\nSuccessful models (1) leverage different graph relations,\\r\\nlike R-GCN, (2) learn bi-level attention, like HAN, and (3)\\r\\nlearn multiplicative attention, like TRANSFORMER-based NNs.\\r\\nRelational Graph Convolutional Networks: R\\x02GCNs [13] extend GCNs and GAT, which operate on local\\r\\ngraph neighborhoods of HoGs, to operate on multi-relational\\r\\ngraphs by distinguishing nodes by relation type. R-GCNs,\\r\\nhowever, treat all relation-specific nodes as equally important.\\r\\nFurther, R-GCNs do not utilize graph attention as they are\\r\\nlimited to directly learning from weight parameters.\\r\\nHeterogeneous Graph Attention Networks: To address\\r\\nlimitations of the above models, HAN integrates bi-level\\r\\nattention, which learns node- and relation-level attention, with\\r\\nGNNs to learn node embeddings. However, HAN uses a\\r\\nglobal learnable weight vector lacking local inter-relation\\r\\ncomparison. Besides, HAN uses pre-defined metapaths which\\r\\nare computationally expensive to design and compute, and\\r\\nresult in sub-optimal graph components learned by the model.\\r\\nTransformer: TRANSFORMER models [17], although\\r\\nsuccessful in natural language processing for small text\\r\\nsequences, have limitations for multi-relational and multi-entity\\r\\nHeGs. This is because TRANSFORMER attends to all other\\r\\ntokens in the sequence, making it infeasible for large-scale\\r\\ninput. While recent works extend TRANSFORMER-like attention\\r\\nto other graph domains, they have limitations, shown in Table I.\\r\\nIII. BA-GNN ARCHITECTURE\\r\\nWe design a general bi-level attention framework for\\r\\ncomputing hierarchical attention and then discuss BA-GNN’s\\r\\narchitecture. Source code and data are at: https://github.com/\\r\\nroshnigiyer/BA-GNN. READ.md details dataset properties,\\r\\ndata splits, and hyperparameters of BA-GNN models.\\r\\nA. General Bi-Level Attention Framework\\r\\nBi-level attention in HeGs incorporates interactions between\\r\\nrelation-specific nodes for learning lower level attention which\\r\\ninforms the higher level attention that captures inter-relation\\r\\ninteractions. In this way, bi-level attention jointly attends to\\r\\nnode-node, relation-relation and node-relation interactions to\\r\\ncollectively produce a representative node embedding. Uni\\x02level attention models omit these critical graph interactions and\\r\\nability for the two-levels of attention to jointly inform each\\r\\nother. Eq. 1 describes the general bi-level attention framework\\r\\nto compute embeddings for each viin the HeG:\\r\\nhe(l+1)\\r\\ni = HigherAtt\\r\\nLowerAtt(·), R, he(l)\\r\\n\\x01\\r\\n(1)\\r\\n= AGG \\x1afψ\\r\\n\\x10\\x08\\r\\nLowerAtt(·)\\r\\n\\x0c\\r\\n\\x0cr ∈ R, he(l)\\r\\n\\t\\x11\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\nr ∈ R\\x1b!\\r\\n,\\nLowerAtt(·) = AGG\\x10\\x08\\r\\ngγ(ei,j |r, he(l))\\r\\n\\x0c\\r\\n\\x0cvj ∈ Nr\\r\\ni\\r\\n\\t\\x11\\r\\n, (2)\\r\\nR = {Γ(ei,j )|vj ∈ V}, (3)\\r\\nwhere hi and hei are the initial and projected node features\\r\\nrespectively, Riis the relation set on the edge of vi, and gγ(·)\\r\\nis a vector-output function of the node-level attention, γ, that\\r\\nprovides a relation-specific embedding summary using learned\\r\\nnode representations from the previous layer, he(l), which is\\r\\naggregated, AGG(·), over edges ei,j in the relation-specific\\r\\nneighborhood context of vj ∈ Nr\\r\\ni\\r\\n. fψ(·) is a vector-output\\r\\nfunction of the relation-level attention, ψ, that are attended\\r\\nrelation-specific local context embeddings, LowerAtt(·),\\r\\nwhich are aggregated over relations in the neighborhood context\\r\\nto form the layer’s final node embedding, he(l+1)\\r\\ni\\r\\n.\\r\\nIn Sections III-B and III-C, we propose a novel semi\\x02supervised attention-based GCN model, BA-GNN, for multi\\x02relational and multi-entity HeGs. BA-GNN performs attention\\r\\naggregation on node and relation levels, rather than nodes\\r\\nand edges. For node-level attention, attention is placed on\\r\\nthe edges of neighbor nodes. For relation-level attention,\\r\\nattention is placed on the relations, which are formed by\\r\\ngrouping edges by relation type. In this way, our model uses\\r\\na hierarchical attention mechanism. The higher-order graph\\r\\nconsiders relation-type-specific edge groups, and the lower\\x02order graph considers nodes in their local contexts. Figure 2\\r\\nsummarizes BA-GNN’s attention mechanism. BA-GNN models\\r\\nuse L stacked layers, each of which is defined through Eq. 1,\\r\\nand for further efficiency, all nodes and relations are restricted\\r\\nto their neighborhoods. Model input can be chosen as pre\\x02defined features or as a unique one-hot vector for each node.\\r\\nFig. 2: Bi-level attention visualization. (a) Node-level aggregating: A\\r\\nnode’s features is a weighted combination of its prior layer’s relation\\x02specific embeddings, z\\r\\nr\\r\\ni\\r\\n. (b) Relation-level aggregating: Relation-level\\r\\nattention is learned via multiplicative attention using neighborhood\\r\\nrelational similarity to determine relative relation importance.\\r\\nB. Node-level Attention\\r\\nNode-level attention distinguishes different roles of nodes\\r\\nin the neighborhood context for learning relation-specific node\\r\\nembeddings. As node-level attentions are target-node-specific,\\r\\nthey are different for different target nodes. Our model learns\\r\\nnode embeddings such that it intrinsically captures graph\\r\\nattributes and structure in its neighborhood. In HeGs, neighbor\\r\\nnodes may belong to different feature spaces due to different\\r\\nnode types, so we utilize an entity-specific type transformation\\r\\nmatrix, T Λ(vi), to project all node features to the same space\\r\\nthrough hei = T Λ(vi)·hi. T Λ(vi) ∈ R\\r\\nd×|Ri|\\r\\n, hi ∈ R\\r\\n|Ri|\\r\\nif the\\r\\ninitial features are chosen to be a one-hot vector of dimension d,\\r\\nwhere heiis continuously updated to learn the final embedding.\\r\\nBA-GNN’s node-level attention uses additive attention\\r\\ninspired by GAT, discussed in Section II, but overcomes GAT’s\\r\\nlimitation by extending the attention to HeGs. GAT performs\\r\\nprojections that do not consider the different relation types\\r\\nin the HeG. We address this by using a learnable relation\\x02specific attention vector, a\\r\\n(l)\\r\\nr ∈ R\\r\\n2d\\r\\n. For a specific relation r,\\r\\nthe attention is shared for all node pairs, so that each node is\\r\\ninfluenced by its neighborhood context. The attention is also\\r\\nasymmetric since the importance of vj to vi may be different\\r\\nfrom the importance of vito vj . We compute relation-specific\\r\\nnode-level attention at layer l as follows, with a softmax(·)\\r\\nactivation applied to normalize each node-pair attention weight\\r\\nand where vj , vk ∈ Nr\\r\\ni\\r\\nand x\\r\\nT\\r\\n(l)\\r\\nis transpose of x at layer\\r\\nl: γ\\r\\n(l),r\\r\\ni,j =\\r\\nexp\\x10LeakyReLUa\\r\\nT (l)\\r\\nr\\r\\nh\\r\\nhe(l)\\r\\ni\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0che(l)\\r\\nj\\r\\ni\\x01\\r\\n\\x11\\r\\nP\\r\\nvk∈Nr\\r\\ni\\r\\nexp\\x10LeakyReLUaT (l)\\r\\nr\\r\\nh\\r\\nhe(l)\\r\\ni\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0che(l)\\r\\nk\\r\\ni\\x01\\r\\n\\x11,\\r\\nwhere a\\r\\n(l)\\r\\nr attends over the concatenated, ||, node features of vi\\r\\nand vj with applied LeakyReLU(·) and softmax(·) activations.\\r\\nBy restricting the attention to within the relation-specific local\\r\\ncontext of nodes, sparsity structural information is injected into\\r\\nthe model through adjacency-masked attention layers.\\r\\nNode vi’s relation-specific embedding, z\\r\\n(l),r\\r\\ni\\r\\n, can then be\\r\\nlearned with AGG(·) from Eq. 1 being a weighted summation\\r\\nof the neighbor’s projected features as follows:\\r\\nz\\r\\n(l),r\\r\\ni = LowerAtt(·) (4)\\r\\n= AGG\\x10\\x08\\r\\ngγ(ei,j |r, he(l))\\r\\n\\x0c\\r\\n\\x0cvj ∈ Nr\\r\\ni\\r\\n\\t\\x11\\r\\n=\\r\\nP\\r\\nvj∈Nr\\r\\ni\\r\\nh\\r\\ngγ(ei,j |r, he(l))\\r\\ni\\r\\n=\\r\\nP\\r\\nvj∈Nr\\r\\ni\\r\\nh\\r\\nγ\\r\\n(l),r\\r\\ni,j he(l)j\\r\\ni\\r\\n,\\r\\nwhere z\\r\\n(l),r\\r\\ni\\r\\nprovides a summary of relation r for vi at layer l.\\r\\nWe also add skip-connections for corresponding LowerAtt(·)\\r\\nfrom the previous layer, l − 1, to preserve learned node-level\\r\\nrepresentations as the depth of the NN is extended.\\r\\nC. Relation-level Attention\\r\\nRelation-level attention distinguishes roles of different\\r\\nrelations in the neighborhood context for learning more\\r\\ncomprehensive node embeddings. In HeGs, different relations\\r\\nmay play different roles of importance for vi, in addition to vi’s\\r\\nrelation-specific neighbor nodes. So, we learn relation-level\\r\\nattention to better fuse vi’s relation-specific node embeddings.\\r\\nOne could design a simple node-relation attention mechanism\\r\\nto encode the effect of relations between nodes, but this would\\r\\nfail to capture relation-relation dependencies hidden in HeGs.\\r\\nWe address TRANSFORMER’s inefficiency for large-scale\\r\\nHeGs through an approximation technique by sampling relation\\x02specific node embeddings from the local graph context. Further,\\r\\ninstead of using the same single set of projections for all words,\\r\\nwe enable each relation-specific embedding to learn a distinct\\r\\nset of personalized projection weights, while maximizing\\r\\nparameter sharing. This technique captures unique relation\\x02dependent characteristics such that each relation-specific node\\r\\nembedding is also influenced by its local relation context.\\nNode vi’s relation-specific TRANSFORMER-based query q\\r\\n(l)\\r\\nr,i,\\r\\nkey k\\r\\n(l)\\r\\nr,i, and value v\\r\\n(l)\\r\\nr,i vectors are computed as follows:\\r\\nq\\r\\n(l)\\r\\nr,i; k\\r\\n(l)\\r\\nr,i; v\\r\\n(l)\\r\\nr,i = W1,rz\\r\\n(l),r\\r\\ni\\r\\n; W2,rz\\r\\n(l),r\\r\\ni\\r\\n; W3,rz\\r\\n(l),r\\r\\ni\\r\\n, such that\\r\\nz\\r\\n(l),r\\r\\ni\\r\\nis projected onto the learnable weight matrices\\r\\nW1,r, W2,r, W3,r ∈ R\\r\\nd×d\\r\\n. The relation-level attention for\\r\\nrelations (r, r0) are computed by iterating over all possible\\r\\nrelation pairs in the neighborhood context, r, r0 ∈ Ri, where\\r\\nRi = {Γ(ei,j )|vj ∈ Vi}. The importance of relation r\\r\\n0 of node\\r\\nviis as follows, with relation similarity being captured through\\r\\nψ\\r\\n(l),r,r0\\r\\ni = softmax(q\\r\\nT\\r\\n(l)\\r\\nr,i k\\r\\n(l)\\r\\nr\\r\\n0\\r\\n,i), where the more similar r\\r\\n0\\r\\nis\\r\\nto r, the greater the attention weights of r\\r\\n0\\r\\n, which results in\\r\\nmore contribution of r\\r\\n0\\r\\n’s embedding to vi’s final embedding. A\\r\\nsoftmax(·) activation is then applied to normalize each relation\\r\\npair’s attention weight.\\r\\nA node’s relation-specific embedding is then informed by\\r\\na weighted summation of its similarity to other local context\\r\\nrelations, ψ\\r\\n(l),r,r0\\r\\ni\\r\\n. To reduce information loss, we add a self\\x02connection of a special relation type per node, which is\\r\\nprojected onto Wi, and aggregated to the attended relation\\x02specific embedding, ψ\\r\\n(l),r,r0\\r\\ni v\\r\\n(l)\\r\\nr\\r\\n0\\r\\n,i. Lastly, a ReLU(·) activation\\r\\nis applied to get the overall relation-specific embedding,\\r\\nδ\\r\\n(l),r\\r\\ni = ReLU(P\\r\\nr\\r\\n0∈Ri\\r\\nψ\\r\\n(l),r,r0\\r\\ni v\\r\\n(l)\\r\\nr\\r\\n0\\r\\n,i + Wihe(l)\\r\\ni\\r\\n).\\r\\nNode vi’s final embedding is learned with AGG(·) in Eq. 1\\r\\nbeing a summation of all attended relation-specific embeddings\\r\\nthrough iteration of neighborhood relations, r ∈ Ri. We also\\r\\napply multi-head attention of S = {1, ..., K} heads to allow\\r\\nBA-GNN to jointly attend to different representation subspaces\\r\\nof nodes/relations, with aggregation, AGG(·), via averaging:\\r\\nhe(l+1)\\r\\ni = HigherAtt\\r\\nLowerAtt(·), R, he(l)\\r\\n\\x01\\r\\n(5)\\r\\n=\\r\\n1\\r\\nK\\r\\nPK\\r\\nk=1\\r\\nP\\r\\nr∈R \\x14\\r\\nfψ\\r\\n\\x10\\x08\\r\\nLowerAtt(·)\\r\\n\\x0c\\r\\n\\x0cr ∈ R, he(l)\\r\\n\\t\\x11\\r\\n\\x15\\r\\n=\\r\\n1\\r\\nK\\r\\nPK\\r\\nk=1\\r\\nP\\r\\nr∈Ri\\r\\n\\x14\\r\\nfψ\\r\\n\\x10\\x08\\r\\nLowerAtt(·)\\r\\n\\x0c\\r\\n\\x0cr ∈ Ri\\r\\n, he(l)\\r\\n\\t\\x11\\r\\n\\x15\\r\\n= AGG\\x10\\x08P\\r\\nr∈Ri\\r\\nh\\r\\nδ\\r\\n(l),r\\r\\ni\\r\\ni\\x0c\\r\\n\\x0ck ∈ S}\\r\\n\\x11\\r\\n.\\r\\nWe also add skip-connections for corresponding HigherAtt(·)\\r\\nfrom the previous layer, l − 1, to preserve learned higher-order\\r\\nrelation-level representations as depth of the NN is extended.\\r\\nThe final representation of a node at layer (l + 1) is:\\r\\nhe(l+1)\\r\\ni = AGG\\x12nP\\r\\nr∈Ri ReLU\\r\\nP\\r\\nr\\r\\n0∈Ri\\r\\nsoftmax(q\\r\\nT\\r\\n(l)\\r\\nr,i k\\r\\n(l)\\r\\nr\\r\\n0\\r\\n,i)v\\r\\n(l)\\r\\nr\\r\\n0\\r\\n,i + Wihe(l)\\r\\ni\\r\\n\\x01\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\nk ∈ S\\r\\no\\x13\\r\\n. (6)\\r\\nD. Analysis of Proposed Attention\\r\\nWe use multiplicative attention at the relation level, instead\\r\\nof additive attention, because learning attention through a\\r\\nconcatenation of features does not compute feature similarity\\r\\nwhich inner product operations capture. Since relation features\\r\\nare characterized by single attribute relation types, a relation’s\\r\\nscaling can be directly determined by its feature similarity to\\r\\nother relations in its neighborhood. This is unlike node-level\\r\\nattention, where node features may have several attributes,\\r\\nmaking it more difficult to learn latent similarity of nodes\\r\\nthrough direct feature comparison such as through inner product\\r\\ncomputation. Rigorous evaluation of mixed combinations of\\r\\nadditive/multiplicative attention shown in experiments further\\r\\nsupport our bi-level attention combination choice.\\r\\nIV. EXPERIMENTS\\r\\nIn this section, we evaluate BA-GNN on seven large-scale\\r\\nheterogeneous datasets (HDs). We conduct experiments on node\\r\\nclassification and link prediction using Pytorch Geometric and\\r\\nDeep Graph Library frameworks on an Nvidia Tesla V100\\r\\nGPU cluster, and report model test accuracies.\\r\\nDatasets: We evaluate on benchmark Resource Descrip\\x02tion Framework (RDF) format datasets [11] for node classi\\x02fication: AIFB, MUTAG, BGS, and AM. For link prediction,\\r\\nwe evaluate on FB15k [15], WN18 [3], and FB15k-237 [15].\\r\\nA. Node Classification\\r\\nNode classification is the semi-supervised classification of\\r\\nnodes to entity types. For evaluation consistency against pri\\x02mary baseline models, we implement BA-GNN with L = 2 and\\r\\nwhere the output of the final layer uses a softmax(·) activation\\r\\nper node. Our model follows the same node classification\\r\\nevaluation procedure as [13], using cross-entropy loss with\\r\\nparameters learned from the Adam Optimizer.\\r\\nBaselines: Table I summarizes our baselines. To adapt\\r\\nthe models to our problem setting of multi-relational, static\\r\\nHeGs, we made the following modifications. For GAT [18]\\r\\nand GCN [9], we omit HeG relations. For TEMPORALGAT [5],\\r\\nwe omit the temporal convolutional network used for temporal\\r\\ninteractions. For HETGNN [23], we consider the neighbors\\r\\nto be the entire set of neighbor nodes and relations. For\\r\\nDYSAT [12], we omit temporal attention. For TGAT [20], we\\r\\nomit functional time encoding. For HGT [8], we omit relative\\r\\ntemporal encoding.\\r\\nResults: Experiment results are in Table II. Results\\r\\nshow BA-GNN significantly and consistently outperforms all\\r\\nbaselines for all tasks on all datasets. For example, on AIFB,\\r\\nMUTAG, BGS, and AM, against the most competitive NNs per\\r\\ncategory, BA-GNN achieves relative performance gains of up to\\r\\n22%, 25%, 20%, and 24% respectively, and overall performance\\r\\ngains of up to 32%, 33%, 36%, and 35% respectively. Further,\\r\\nthe Welch t-test of unequal variance shows that BA-GNN’s\\r\\nrelative performance compared to each model per dataset is\\r\\nstatistically significant to be greater, with p-value < 0.001.\\r\\nAblation Studies: To further analyze BA-GNN’s bi\\x02level attention, we design the following BA-GNN variant\\r\\nmodels: BA-GNN-NODE, a uni-level attention model using\\r\\nBA-GNN’s node-level attention; BA-GNN-RELATION, a uni\\x02level attention model using BA-GNN’s relation-level attention;\\r\\nBA-GNN(NODE)+HAN(REL.), a hybrid model using BA-GNN’s\\r\\nnode-level attention and HAN’s relation-level attention.\\r\\nThe bi-level attention models outperform the uni-level\\r\\nattention models, shown in Table II. Further, simply comparing\\r\\nBA-GNN’s higher-order relation-level attention with HAN’s,\\r\\nshows a significant relative performance gain on all datasets. For\\r\\nexample on MUTAG, the performance gain is nearly 11.30%\\r\\nwith the replacement of only the relation-level attention. This\\nTABLE II: Node classification averaged accuracy (%) over 10 model trains on four datasets. Best results per model group are bold-faced,\\r\\nwith overall best results per dataset underscored. We compute test accuracy improvement of BA-GNN over best NNs, all NNs, and per NN\\r\\nby the difference of BA-GNN’s average accuracy and NNs per dataset column (e.g., rows 24-25), or the difference of BA-GNN’s average\\r\\naccuracy and each NN across datasets (col. 7). ANODE, REL./MNODE, REL. are BA-GNN additive/mutliplicative attention at node/relation levels.\\r\\nType Model AIFB MUTAG BGS AM BA-GNN (% improve)\\r\\nTRANSE [2] 66.89 ± 1.26 54.41 ± 0.73 58.46 ± 0.38 61.23 ± 0.57 +[32.05, 36.29]\\r\\n(1) Non-GNN-based HOLE [10] 67.98 ± 0.42 63.17 ± 0.26 72.74 ± 0.22 65.25 ± 0.38 +[22.01, 31.43]\\r\\nKGE models for HeGs DISTMULT [21] 73.64 ± 0.10 62.06 ± 0.18 68.19 ± 0.13 69.12 ± 0.24 +[25.3, 27.56]\\r\\nCOMPLEX [16] 77.24 ± 0.15 62.38 ± 0.22 74.72 ± 0.18 72.39 ± 0.16 +[20.03, 25.43]\\r\\n(2) GNNs for GCN [9] 91.99 ± 0.21 67.02 ± 0.08 78.74 ± 0.16 86.82 ± 0.60 +[6.95, 20.79]\\r\\nHoGs GAT [18] 92.50 ± 0.29 66.18 ± 0.00 77.93 ± 0.17 88.52 ± 1.65 +[6.44, 21.63]\\r\\nMETAPATH2VEC [4] 89.52 ± 0.12 66.04 ± 0.27 78.34 ± 0.13 85.48 ± 0.11 +[9.42, 21.77]\\r\\nHEREC [14] 91.03 ± 0.15 66.96 ± 0.18 79.36 ± 0.25 85.98 ± 0.07 +[7.91, 20.85]\\r\\nHIN2VEC [6] 91.63 ± 0.17 66.29 ± 0.14 79.01 ± 0.12 86.22 ± 0.21 +[7.31, 21.52]\\r\\n(3A) Non-TRANSFORMER- HEGAN [7] 92.33 ± 0.13 68.07 ± 0.08 81.60 ± 0.27 86.79 ± 0.14 +[6.61, 19.74]\\r\\nbased GNNs for HeGs TEMPORALGAT [5] 93.42 ± 0.11 66.88 ± 0.24 79.14 ± 0.13 89.10 ± 0.13 +[5.52, 20.93]\\r\\nHETGNN [23] 95.18 ± 0.16 75.64 ± 0.09 82.05 ± 0.25 89.67 ± 0.05 +[3.76, 12.70]\\r\\nR-GCN [13] 95.31 ± 0.62 73.23 ± 0.48 83.10 ± 0.80 89.29 ± 0.35 +[3.63, 14.58]\\r\\nHAN [19] 96.25 ± 0.12 76.46 ± 0.07 86.84 ± 0.21 90.68 ± 0.23 +[2.69, 11.35]\\r\\nDYSAT [12] 92.64 ± 0.21 66.57 ± 0.05 78.02 ± 0.19 88.90 ± 1.05 +[6.30, 21.24]\\r\\n(3B) TRANSFORMER-based TGAT [20] 92.84 ± 0.14 67.19 ± 0.21 78.35 ± 0.15 89.43 ± 0.28 +[6.10, 20.62]\\r\\nGNNs for HeGs HGT [8] 95.97 ± 0.15 76.84 ± 0.12 86.01 ± 0.18 90.33 ± 0.13 +[2.97, 10.97]\\r\\nGTN [22] 96.04 ± 0.17 76.32 ± 0.12 85.38 ± 0.24 90.56 ± 0.10 +[2.90, 11.49]\\r\\nBA-GNN-NODE 95.46 ± 0.13 73.19 ± 0.25 84.23 ± 0.22 89.45 ± 0.02 –\\r\\nBA-GNN variants BA-GNN-RELATION 95.28 ± 0.23 76.17 ± 0.22 85.43 ± 0.34 90.52 ± 0.18 –\\r\\nBA-GNN(NODE)+HAN(REL.) 96.30 ± 0.09 76.48 ± 0.04 86.80 ± 0.24 90.67 ± 0.35 –\\r\\nBA-GNN (MNODE /AREL.) 96.02 ± 0.13 76.20 ± 0.09 86.90 ± 0.17 90.54 ± 0.11 –\\r\\nBA-GNN (ANODE /AREL.) 96.38 ± 0.14 76.61 ± 0.22 87.00 ± 0.09 90.73 ± 0.05 –\\r\\nBA-GNN (MNODE /MREL.) 96.44 ± 0.16 77.01 ± 0.07 86.92 ± 0.12 90.78 ± 0.08 –\\r\\nBA-GNN (ours) 98.94 ± 0.13 87.81 ± 0.11 94.75 ± 0.08 96.68 ± 0.14 –\\r\\nBA-GNN (% improve) against best NNs, group 1-3 +[2.69, 21.70] +[10.97, 24.64] +[7.91, 20.03] +[6.00, 24.29] –\\r\\nagainst all NNs, group 1-3 +[2.69, 32.05] +[10.97, 33.40] +[7.91, 36.29] +[6.00, 35.45] –\\r\\nTABLE III: Link prediction results for mean reciprocal rank (MRR), and Hits @ n metrics. For each group of models, the best results are\\r\\nbold-faced. The overall best results on each dataset are underscored.\\r\\nFB15k WN18 FB15k-237\\r\\nMRR Hits @ MRR Hits @ MRR Hits @\\r\\nModel Raw Filtered 1 3 10 Raw Filtered 1 3 10 Raw Filtered 1 3 10\\r\\nR-GCN 0.251 0.651 0.541 0.736 0.825 0.553 0.814 0.686 0.928 0.955 0.158 0.248 0.153 0.258 0.414\\r\\nBA-GNN 0.261 0.702 0.601 0.778 0.857 0.590 0.820 0.698 0.945 0.959 0.195 0.260 0.160 0.268 0.468\\r\\nTRANSE 0.221 0.380 0.231 0.472 0.641 0.335 0.454 0.089 0.823 0.934 0.144 0.233 0.147 0.263 0.398\\r\\nR-GCNT 0.252 0.651 0.543 0.738 0.828 0.554 0.815 0.681 0.928 0.956 0.161 0.258 0.159 0.274 0.421\\r\\nBA-GNNT 0.264 0.700 0.649 0.781 0.858 0.593 0.822 0.692 0.943 0.960 0.200 0.268 0.168 0.275 0.493\\r\\nHOLE 0.232 0.524 0.402 0.613 0.739 0.616 0.938 0.930 0.945 0.949 0.124 0.222 0.133 0.253 0.391\\r\\nR-GCNH 0.257 0.659 0.556 0.744 0.839 0.667 0.937 0.935 0.951 0.966 0.159 0.257 0.156 0.272 0.420\\r\\nBA-GNNH 0.268 0.720 0.670 0.787 0.860 0.670 0.940 0.942 0.955 0.979 0.194 0.266 0.161 0.272 0.488\\r\\nDISTMULT 0.248 0.634 0.522 0.718 0.814 0.526 0.813 0.701 0.921 0.943 0.100 0.191 0.106 0.207 0.376\\r\\nR-GCND 0.262 0.696 0.601 0.760 0.842 0.561 0.819 0.697 0.929 0.964 0.156 0.249 0.151 0.264 0.417\\r\\nBA-GNND 0.272 0.745 0.688 0.792 0.868 0.600 0.825 0.705 0.934 0.977 0.190 0.251 0.155 0.268 0.483\\r\\nCOMPLEX 0.242 0.692 0.599 0.759 0.840 0.587 0.941 0.936 0.945 0.947 0.109 0.201 0.112 0.213 0.388\\r\\nR-GCNC 0.260 0.712 0.629 0.771 0.845 0.615 0.953 0.937 0.947 0.965 0.158 0.255 0.152 0.268 0.419\\r\\nBA-GNNC 0.278 0.788 0.731 0.867 0.880 0.671 0.978 0.981 0.988 0.992 0.170 0.262 0.159 0.270 0.485\\r\\nindicates that BA-GNN’s relation-level attention is more effec\\x02tive across the different data domains and that its personalized\\r\\nattention to local graph contexts yields performance gain.\\r\\nB. Link Prediction\\r\\nLink prediction involves assigning confidence scores to HeG\\r\\ntriples to determine how likely predicted edges belong to true\\r\\nrelations. Our models follow the same evaluation framework as\\r\\n[2] and [13] using negative sampling and cross-entropy loss\\r\\nwith parameters learned from the Adam Optimizer. We use\\r\\nevaluation metrics of mean reciprocal rank (MRR) and Hits @\\r\\nn, in raw and filtered settings. The same number of negative\\r\\nsamples, w = 1, are used to make datasets comparable.\\r\\nBaselines: We evaluate standalone GNNs (BA-GNN, R\\x02GCN), KGE models, and GNN+KGE autoencoder models using\\r\\nthe same setup procedure as [2] and [13]. The autoencoder\\r\\nmodels include: BA-GNNx and R-GCNy where x, y are\\r\\nTRANSE (T), HOLE (H), DISTMULT (D), and COMPLEX (C).\\r\\nResults and Ablation Studies: Experiment results are\\r\\nin Table III. Results show that the best BA-GNN models\\r\\noutperform R-GCN models on all datasets for all metrics of\\r\\nboth tasks of MRR and Hits @ n = 1, 3, 10. We observe that BA\\x02GNN outperforms R-GCN when comparing standalone models.\\r\\nFurther, results show that autoencoder models outperform each\\r\\nof GNN and KGE standalone models, showing that GNNs\\nand KGE models can each be benefited by their joint learning.\\r\\nResults also show that BA-GNN autoencoders outperform R\\x02GCN autoencoders on all datasets for all tasks and metrics.\\r\\nC. Case Study\\r\\nWe conduct experiments to determine the quality of relation\\x02level attention and graph-structure of BA-GNN. We modify\\r\\nthe AM dataset to contain the following relation types, each\\r\\nwith cummulative 10% splits: (1) relations randomly selected,\\r\\n(2) relations with the highest relation-level attention weights\\r\\nfrom BA-GNN, and (3) relations with the lowest relation-level\\r\\nattention weights from BA-GNN. Experiment figures on node\\r\\nclassification for HAN and BA-GNN models are in Figure 3(a).\\r\\n(2)’s graph structure yields the highest test accuracy on all\\r\\nsplits of AM compared to (1) or (3), while (3) yields the lowest\\r\\ntest accuracy. (1) is as expected in between test accuracies of\\r\\n(2) and (3). Models that do not learn relation-level attention\\r\\n(BA-GNN-NODE) still benefit from the graph structure identified\\r\\nby (2). This suggests that BA-GNN’s relation-level attention\\r\\ncan selectively identify important graph components and that\\r\\nits learning of graph structure can enhance other leading GNNs.\\r\\n(a) BA-GNN’s attention-induced graph structure for HAN, BA-GNN-NODE,\\r\\nBA-GNN-RELATION, BA-GNN\\r\\n(b) pp1 heat map: BA-GNN’s\\r\\nrelation-level attention\\r\\n(c) pp2 heat map: BA-GNN’s\\r\\nrelation-level attention\\r\\nFig. 3: Experiments evaluating learned relation-level attention and\\r\\ngraph structure of BA-GNN.\\r\\nD. Attention Visualization\\r\\nWe randomly sample two nodes belonging to entity Persons\\r\\non AIFB and plot its learned relation-level attention weights\\r\\nfrom layer l = L using heat maps, seen in Figures 3(b) and\\r\\n(c). The corresponding partial graphs of person 1 and person\\r\\n2 are in Figure 1. In Figure 3(b), author−1and member−1\\r\\nhave high attention to is_worked_on_by because a person is\\r\\nlikely to have publications and research affiliations in their\\r\\nresearch area. name_of has high attention to homepage_of,\\r\\nobserved in both Figures 3(b) and (c), because a homepage may\\r\\ndirectly contain personal identifying information. In Figure 3(c),\\r\\nhead−1and member−1 have high attention to each other, since\\r\\nhead of a research group is a member. Further, members\\r\\nof the research group are likely to work on the group’s\\r\\nprojects and focus on a particular research domain, explaining\\r\\nwhy works_at_project−1 has higher attention to head−1and\\r\\nmember−1, and works_at_project−1and members of the\\r\\nresearch group also have higher attention to is_worked_on_by.\\r\\nV. CONCLUSION\\r\\nWe propose Bi-Level Attention Graph Neural Networks (BA\\x02GNN) for modeling multi-entity and multi-relational large-scale\\r\\nheterogeneous graphs (HeGs), via entity type and meta relation\\r\\ninformation to learn graph structure and properties. Further,\\r\\nBA-GNN distinguishes nodes and relations using a novel smart\\x02sampling bi-level attention mechanism to guide the model\\r\\nwhen aggregating features in graph neighborhoods. We conduct\\r\\nextensive experiments on seven real-world heterogeneous\\r\\ndatasets, and show BA-GNN learns effective and efficient\\r\\nembeddings. We observe that BA-GNN outperforms all state\\x02of-art GNN baselines on various information recovery tasks.\\r\\nVI. ACKNOWLEDGEMENTS\\r\\nThis work was supported by NSF 1705169, 1829071,\\r\\n1937599, 2031187, 2106859; NIH R35-HL135772, NIBIB\\r\\nR01-EB027650; DARPA HR00112090027; Okawa Foundation\\r\\nGrant; and Amazon Research Awards. We also thank Kai-Wei\\r\\nChang and Yunsheng Bai for helpful discussions.\\r\\nREFERENCES\\r\\n[1] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a\\r\\ncollaboratively created graph database for structuring human knowledge.\\r\\nIn In SIGMOD Conference, pages 1247–1250, 2008.\\r\\n[2] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko.\\r\\nTranslating embeddings for modeling multi-relational data. NIPS 2013.\\r\\n[3] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel. Convolutional 2d\\r\\nknowledge graph embeddings. AAAI 2018.\\r\\n[4] Y. Dong, N. V. Chawla, and A. Swami. metapath2vec: Scalable\\r\\nrepresentation learning for heterogeneous networks. KDD 2017.\\r\\n[5] A. Fathy and K. Li. TemporalGAT: Attention-based dynamic graph\\r\\nrepresentation learning. PAKDD 2020.\\r\\n[6] T. Fu, W. Lee, and Z. Lei. HIN2Vec: Explore meta-paths in heterogeneous\\r\\ninformation networks for representation learning. CIKM 2017.\\r\\n[7] B. Hu, Y. Fang, and C. Shi. Adversarial learning on heterogeneous\\r\\ninformation networks. KDD 2019.\\r\\n[8] Z. Hu, Y. Dong, K. Wang, and Y. Sun. Heterogeneous graph transformer.\\r\\nWWW 2020.\\r\\n[9] Q. Li, Z. Han, and X. ming Wu. Deeper insights into graph convolutional\\r\\nnetworks for semi-supervised learning. AAAI 2018.\\r\\n[10] M. Nickel, L. Rosasco, and T. Poggio. Holographic embeddings of\\r\\nknowledge graphs. https://arxiv.org/abs/1510.04935, 2015.\\r\\n[11] P. Ristoski and H. Paulheim. Rdf2vec: Rdf graph embeddings for data\\r\\nmining. In International Semantic Web Conference. Springer, 2016.\\r\\n[12] A. Sankar, Y. Wu, L. Gou, W. Zhang, and H. Yang. Dynamic graph\\r\\nrepresentation learning via self-attention networks. ICLR 2019.\\r\\n[13] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and\\r\\nM. Welling. Modeling relational data with graph convolutional networks.\\r\\nhttps://arxiv.org/abs/1703.06103, 2017.\\r\\n[14] C. Shi, B. Hu, and W. X. Zhao. Heterogeneous information network\\r\\nembedding for recommendation. IEEE 2018.\\r\\n[15] K. Toutanova and D. Chen. Observed versus latent features for knowledge\\r\\nbase and text inference. In Proceedings of the 3rd Workshop on\\r\\nContinuous Vector Space Models and their Compositionality, 2015.\\r\\n[16] T. Trouillon, J. Welbl, S. Riedel, E. Gaussier, and G. Bouchard. Complex\\r\\nembeddings for simple link prediction. ICML 2016.\\r\\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\r\\nŁukasz Kaiser, and I. Polosukhin. Attention is all you need. NIPS 2017.\\r\\n[18] P. Velickovi ˇ c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and ´\\r\\nY. Bengio. Graph attention networks. ICLR 2018.\\r\\n[19] X. Wang, H. Ji, C. Shi, B. Wang, P. Cui, P. Yu, and Y. Ye. Heterogeneous\\r\\ngraph attention network. WWW 2019.\\r\\n[20] D. Xu, C. Ruan, E. Korpeoglu, S. Kumar, and K. Achan. Inductive\\r\\nrepresentation learning on temporal graphs. ICLR 2020.\\r\\n[21] B. Yang, W. Yih, X. He, J. Gao, and L. Deng. Embedding Entities and\\r\\nRelations for Learning and Inference in Knowledge Bases. ICLR 2015.\\r\\n[22] S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim. Graph Transformer\\r\\nNetworks. NeurIPS 2019.\\r\\n[23] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla. HetGNN:\\r\\nHeterogeneous Graph Neural Network. KDD 2019.'},\n",
       " {'name': '2211.07400v2.pdf',\n",
       "  'content': 'Efficient Integration of Multi-Order Dynamics and Internal\\r\\nDynamics in Stock Movement Prediction\\r\\nThanh Trung Huynh\\r\\nEcole Polytechnique Federale de\\r\\nLausanne, Switzerland\\r\\nMinh Hieu Nguyen\\r\\nHanoi University of Science and\\r\\nTechnology, Vietnam\\r\\nThanh Tam Nguyen∗\\r\\nGriffith University, Australia\\r\\nPhi Le Nguyen\\r\\nHanoi University of Science and\\r\\nTechnology, Vietnam\\r\\nMatthias Weidlich\\r\\nHumboldt-Universität zu Berlin,\\r\\nGermany\\r\\nQuoc Viet Hung Nguyen\\r\\nGriffith University, Australia\\r\\nKarl Aberer\\r\\nEcole Polytechnique Federale de\\r\\nLausanne, Switzerland\\r\\nABSTRACT\\r\\nAdvances in deep neural network (DNN) architectures have en\\x02abled new prediction techniques for stock market data. Unlike\\r\\nother multivariate time-series data, stock markets show two unique\\r\\ncharacteristics: (i) multi-order dynamics, as stock prices are affected\\r\\nby strong non-pairwise correlations (e.g., within the same industry);\\r\\nand (ii) internal dynamics, as each individual stock shows some par\\x02ticular behaviour. Recent DNN-based methods capture multi-order\\r\\ndynamics using hypergraphs, but rely on the Fourier basis in the\\r\\nconvolution, which is both inefficient and ineffective. In addition,\\r\\nthey largely ignore internal dynamics by adopting the same model\\r\\nfor each stock, which implies a severe information loss.\\r\\nIn this paper, we propose a framework for stock movement pre\\x02diction to overcome the above issues. Specifically, the framework\\r\\nincludes temporal generative filters that implement a memory\\x02based mechanism onto an LSTM network in an attempt to learn\\r\\nindividual patterns per stock. Moreover, we employ hypergraph\\r\\nattentions to capture the non-pairwise correlations. Here, using\\r\\nthe wavelet basis instead of the Fourier basis, enables us to sim\\x02plify the message passing and focus on the localized convolu\\x02tion. Experiments with US market data over six years show that\\r\\nour framework outperforms state-of-the-art methods in terms of\\r\\nprofit and stability. Our source code and data are available at\\r\\nhttps://github.com/thanhtrunghuynh93/estimate.\\r\\nCCS CONCEPTS\\r\\n• Computing methodologies → Neural networks;\\r\\nKEYWORDS\\r\\nhypergraph embedding, stock market, temporal generative filters\\r\\nACM Reference Format:\\r\\nThanh Trung Huynh, Minh Hieu Nguyen, Thanh Tam Nguyen, Phi Le\\r\\nNguyen, Matthias Weidlich, Quoc Viet Hung Nguyen, and Karl Aberer.\\r\\n2023. Efficient Integration of Multi-Order Dynamics and Internal Dynamics\\r\\n∗Corresponding author.\\r\\nWSDM ’23, February 27-March 3, 2023, Singapore, Singapore\\r\\n2023. ACM ISBN 978-1-4503-9407-9/23/02. . . $15.00\\r\\nhttps://doi.org/10.1145/3539597.3570427\\r\\nin Stock Movement Prediction. In Proceedings of the Sixteenth ACM Inter\\x02national Conference on Web Search and Data Mining (WSDM ’23), February\\r\\n27-March 3, 2023, Singapore, Singapore. ACM, New York, NY, USA, 12 pages.\\r\\nhttps://doi.org/10.1145/3539597.3570427\\r\\n1 INTRODUCTION\\r\\nThe stock market denotes a financial ecosystem where the stock\\r\\nshares that represents the ownership of businesses are held and\\r\\ntraded among the investors, with a market capitalization of more\\r\\nthan 93.7$ trillion globally at the end of 2020 [48]. In recent years,\\r\\napproaches for automated trading emerged that are driven by artifi\\x02cial intelligence (AI) models. They continuously analyze the market\\r\\nbehaviour and predict the short-term trends in stock prices. While\\r\\nthese methods struggle to understand the complex rationales be\\x02hind such trends (e.g., macroeconomic factors, crowd behaviour,\\r\\nand companies’ intrinsic values), they have been shown to yield\\r\\naccurate predictions. Moreover, they track market changes in real\\x02time, by observing massive volumes of trading data and indicators,\\r\\nand hence, enable quick responses to events, such as a market crash.\\r\\nAlso, they are relatively robust against emotional effects (greed,\\r\\nfear) that tend to influence human traders [36].\\r\\nStock market analysis has received much attention in the past.\\r\\nEarly work relies on handcrafted features, a.k.a technical indicators,\\r\\nto model the stock movement. For example, RIMA [37], a popular\\r\\ntime-series statistics model, may be applied to moving averages of\\r\\nstock prices to derive price predictions [3]. However, handcrafted\\r\\nfeatures tend to lag behind the actual price movements. Therefore,\\r\\nrecent approaches adopt deep learning to model the market based\\r\\non historic data. Specifically, recurrent neural networks (RNN) [7]\\r\\nhave been employed to learn temporal patterns from the historic\\r\\ndata and, based thereon, efficiently derive short-term price predic\\x02tions using regression [25] or classification [53].\\r\\nHowever, stock market analysis based on deep learning faces\\r\\ntwo important requirements. First, multi-order dynamics of stock\\r\\nmovements need to be incorporated. Price movements are often\\r\\ncorrelated within a specific group of stocks, e.g., companies of the\\r\\nsame industry sector that are affected by the same government\\r\\npolicies, laws, and tax rates. For instance, as shown in Fig. 1, in\\r\\nearly 2022, prices for US technology stocks (APPL (Apple), META\\r\\n(Facebook), GOOG (Google), NFLX (Netflix)) went down due to the\\r\\narXiv:2211.07400v2 [q-fin.ST] 24 Nov 2022\\nWSDM ’23, February 27-March 3, 2023, Singapore, Singapore Huynh et al.\\r\\ngeneral economic trend (inflation, increased interest rates), whereas\\r\\nstocks in the energy sector, like MPC, OKE, or OXY, experienced\\r\\nupward trends due to oil shortages caused by the Russia-Ukraine\\r\\nwar. Second, the internal dynamics per stock need to be incorpo\\x02rated. In practice, even when considering highly correlated stocks,\\r\\nthere is commonly still some individual behaviour. For example, in\\r\\nFig. 1, APPL and GOOG stocks decrease less severely than META\\r\\nand NFLX, as the former companies (Apple, Google) maintain a\\r\\nwider and more sustainable portfolio compared to the latter two\\r\\n(Facebook, Netflix) [26].\\r\\nExisting work provides only limited support for these require\\x02ments. First, to incorporate multi-order dynamics of stock markets,\\r\\nRNNs can be combined with graph neural networks (GNNs) [18].\\r\\nHere, state-of-the-art solutions adopt hypergraphs, in which an\\r\\nedge captures the correlation of multiple stocks [40, 41]. Yet, these\\r\\napproaches rely on the Fourier basis in the convolution, which\\r\\nimplies costly matrix operations and does not maintain the local\\x02ization well. This raises the question of how to achieve an efficient\\r\\nand effective convolution process for hypergraphs (Challenge 1).\\r\\nMoreover, state-of-the-art approaches apply a single RNN to all\\r\\nstocks, thereby ignoring their individual behaviour. The reason\\r\\nbeing that maintaining a separate model per stock would be in\\x02tractable with existing techniques. This raises the question of how\\r\\nto model the internal dynamics of stocks efficiently (Challenge 2).\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\n2021-10-01\\r\\n2021-11-01\\r\\n2021-12-01\\r\\n2022-01-01\\r\\n2022-02-01\\r\\n2022-03-01\\r\\n2022-04-01\\r\\n2022-05-01\\r\\nDate\\r\\nReturn\\r\\nAAPL\\r\\nGOOG\\r\\nMETA\\r\\nNFLX\\r\\nMPC\\r\\nOKE\\r\\nOXY\\r\\nEnergy\\r\\nTechnology\\r\\nStocks\\r\\nFigure 1: Illustration of complex stock price correlation\\r\\nIn this work, we address the above challenges by proposing\\r\\nEfficient Stock Integration with Temporal Generative Filters and\\r\\nWavelet Hypergraph Attentions (ESTIMATE), a profit-driven frame\\x02work for quantitative trading. Based on the aforementioned idea\\r\\nof adopting hypergraphs to capture non-pairwise correlations be\\x02tween stocks, the framework includes two main contributions:\\r\\n• We present temporal generative filters that implement a hy\\x02brid attention-based LSTM architecture to capture the stocks’\\r\\nindividual behavioural patterns (Challenge 2). These pat\\x02terns are then fed to hypergraph convolution layers to obtain\\r\\nspatio-temporal embeddings that are optimized with respect\\r\\nto the potential of the stocks for short-term profit.\\r\\n• We propose a mechanism that combines the temporal pat\\x02terns of stocks with spatial convolutions through hypergraph\\r\\nattention, thereby integrating the internal dynamics and the\\r\\nmulti-order dynamics. Our convolution process uses the\\r\\nwavelet basis, which is efficient and also effective in terms\\r\\nof maintaining the localization (Challenge 1).\\r\\nTo evaluate our approach, we report on backtesting experiments\\r\\nfor the US market. Here, we try to simulate the real trading actions\\r\\nwith a strategy for portfolio management and risk control. The\\r\\nresults demonstrate the robustness of our technique compared to\\r\\nexisting approaches in terms of stability and return. Our source\\r\\ncode and data are available [11].\\r\\nThe remainder of the paper is organised as follows. §2 introduces\\r\\nthe problem statement and gives an overview of our approach. We\\r\\npresent our new techniques, the temporal generative filters and\\r\\nwavelet hypergraph attentions, in §3 and §4. §5 presents experi\\x02ments, §6 reviews related works, and §7 concludes the paper.\\r\\n2 MODEL AND APPROACH\\r\\n2.1 Problem Formulation\\r\\nIn this section, we formulate the problem of predicting the trend of\\r\\na stock in the short term. We start with some basic notions.\\r\\nOHCLV data.At timestep 𝑡, the open-high-low-close-volume (OHLCV)\\r\\nrecord for a stock 𝑠 is a vector 𝑥\\r\\n𝑡\\r\\n𝑠 = [𝑜\\r\\n𝑡\\r\\n𝑠\\r\\n, ℎ𝑡\\r\\n𝑠\\r\\n,𝑙𝑡\\r\\n𝑠\\r\\n, 𝑐𝑡\\r\\n𝑠\\r\\n, 𝑣𝑡\\r\\n𝑠\\r\\n]. It denotes the\\r\\nopen, high, low, and close price, and the volume of shares that have\\r\\nbeen traded within that timestep, respectively.\\r\\nRelative price change. We denote the relative close price change\\r\\nbetween two timesteps𝑡1 < 𝑡2 of stock 𝑠 by 𝑑\\r\\n(𝑡1,𝑡2)\\r\\n𝑠 = (𝑐\\r\\n𝑡2\\r\\n𝑠 − 𝑐\\r\\n𝑡1\\r\\n𝑠\\r\\n)/𝑐\\r\\n𝑡1\\r\\n𝑠\\r\\n.\\r\\nThe relative price change normalizes the market price variety be\\x02tween different stocks in comparison to the absolute price change.\\r\\nFollowing existing work on stock market analysis [9, 41], we\\r\\nfocus on the prediction of the change in price rather than the\\r\\nabsolute value. The reason being that the timeseries of stock prices\\r\\nare non-stationary, whereas their changes are stationary [19]. Also,\\r\\nthis avoids the problem that forecasts often lag behind the actual\\r\\nvalue [14, 18]. We thus define the addressed problem as follows:\\r\\nProblem 1 (Stock Movement Prediction). Given a set 𝑆 of\\r\\nstocks and a lookback window of 𝑘 trading days of historic OHLCV\\r\\nrecords 𝑥\\r\\n(𝑡−𝑘−1)...𝑡\\r\\n𝑠\\r\\nfor each stock 𝑠 ∈ 𝑆, the problem of Stock Move\\x02ment Prediction is to predict the relative price change 𝑑\\r\\n(𝑡,𝑡+𝑤)\\r\\n𝑠\\r\\nfor\\r\\neach stock in a short-term lookahead window 𝑤.\\r\\nWe formulate the problem as a short-term regression for sev\\x02eral reasons. First, we consider a lookahead window over next-day\\r\\nprediction to be robust against random market fluctuations [53].\\r\\nSecond, we opt for short-term prediction, as an estimation of the\\r\\nlong-term trend is commonly considered infeasible without the inte\\x02gration of expert knowledge on the intrinsic value of companies and\\r\\non macroeconomic effects. Third, we focus on a regression problem\\r\\ninstead of a classification problem to incorporate the magnitude of\\r\\na stock’s trend, which is important for interpretation [12].\\r\\n2.2 Design Principles\\r\\nWe argue that any solution to the above problem shall satisfy the\\r\\nfollowing requirements:\\r\\n• R1: Multi-dimensional data integration: Stock market\\r\\ndata is multivariate, covering multiple stocks and multiple\\r\\nfeatures per stock. A solution shall integrate these data di\\x02mensions and support the construction of additional indica\\x02tors from basic OHCLV data.\\nJoint Stock Movement Prediction with Multi-Order Deep Learning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore\\r\\n…\\r\\nFeatures\\r\\n…\\r\\n1D-CNN Distinct Filter\\r\\nGeneration Network\\r\\n\\x01 , U\\r\\n\\x012, U2\\r\\n\\x01 , Un\\r\\nLSTM\\r\\n\\x03\\r\\n…\\r\\nTime \\r\\nsteps\\r\\n…\\r\\n\\x04\\r\\n\\x042\\r\\n\\x04\\x02\\r\\n…\\r\\nTemporal \\r\\nAttention\\r\\nStocks\\r\\nHypergraph\\r\\nConvolution\\r\\nusing Wavelets\\r\\n…\\r\\n…\\r\\n… …… Flatten\\r\\nFC\\r\\n…\\r\\n\\x05 Groundtruth\\r\\n…\\r\\n\\x052\\r\\n\\x05\\x02\\r\\n…\\r\\n\\x05 Prediction\\r\\n…\\r\\n\\x052\\r\\n\\x05\\x02\\r\\nLoss\\r\\nSector \\r\\nrelationship\\r\\nHypergraph snapshots\\r\\nStocks hypergraph\\r\\nLead-lag \\r\\nrelationship\\r\\n…\\r\\nNode\\r\\nrepresentation\\r\\nFigure 2: Overview of our framework for stock movement prediction.\\r\\n• R2: Non-stationary awareness: The stock market is driven\\r\\nby various factors, such as socio-economic effects or supply\\x02demand changes. Therefore, a solution shall be robust against\\r\\nnon-predictable behaviour of the market.\\r\\n• R3: Analysis of multi-order dynamics: The relations be\\x02tween stocks are complex (e.g., companies may both, coop\\x02erate and compete) and may evolve over time. A solution\\r\\nthus needs to analyse the multi-order dynamics in a market.\\r\\n• R4: Analysis of internal dynamics: Each stock also shows\\r\\nsome individual behaviour, beyond the multi-order corre\\x02lations induced by market segments. A solution therefore\\r\\nneeds to analyse and integrate such behaviour for each stock.\\r\\n2.3 Approach Overview\\r\\nTo address the problem of stock movement prediction in the light\\r\\nof the above design principles, we propose the framework shown\\r\\nin Fig. 2. It takes historic data in the form of OHCLV records and\\r\\nderives a model for short-term prediction of price changes per stock.\\r\\nOur framework incorporates requirement R1 by first extracting\\r\\nthe historic patterns per stock using a temporal attention LSTM.\\r\\nHere, the attention mechanism is used along with a 1D-CNN to\\r\\nassess the impact of the previous timesteps. In addition to the\\r\\nOHCLV data, we employ technical indicators to mitigate the impact\\r\\nof noisy market behaviour, thereby addressing requirement R2.\\r\\nMoreover, we go beyond the state of the art by associating the core\\r\\nLSTM parameters with a learnable vector for each stock. It serves as\\r\\na memory that stores its individual information (requirement R4)\\r\\nand results in a system of temporal generative filters. We explain\\r\\nthe details of these filters in §3.\\r\\nTo handle multi-order dynamics (requirement R3), we model\\r\\nthe market with an industry-based hypergraph, which naturally\\r\\npresents non-pairwise relationships. We then develop a wavelet con\\x02volution mechanism, which leverages the wavelet basis to achieve\\r\\na simpler convolution process than existing approaches. We ap\\x02ply a regression loss to steer the model to predict the short-term\\r\\ntrend of each stock price. The details of our proposed hypergraph\\r\\nconvolution process are given in §4.\\r\\n3 TEMPORAL GENERATIVE FILTERS\\r\\nThis section describes our temporal generative filters used to cap\\x02ture the internal dynamics of stocks.\\r\\nTechnical indicators. We first compute various technical indica\\x02tors from the input data in order to enrich the data and capture the\\r\\nhistorical context of each stock. These indicators, summarized in\\r\\nTable 1, are widely used in finance. For each stock, we concatenate\\r\\nthese indicators to form a stock price feature vector 𝑥𝑡 on day 𝑡.\\r\\nThis vector is then forwarded through a multi-layer perceptron\\r\\n(MLP) layer to modulate the input size.\\r\\nTable 1: Summary of technical indicators used.\\r\\nType Indicators\\r\\nTrend Indicators Arithmetic ratio, Close Ratio, Close SMA,\\r\\nVolume SMA, Close EMA, Volume EMA, ADX\\r\\nOscillator Indicators RSI, MACD, Stochastics, MFI\\r\\nVolatility Indicators ATR, Bollinger Band, OBV\\r\\nLocal trends. To capture local trends in stock patterns, we employ\\r\\nconvolutional neural networks (CNN). By compressing the length\\r\\nof the series of stock features, they help to mitigate the issue of long\\x02term dependencies. As each feature is a one-dimensional timeseries,\\nWSDM ’23, February 27-March 3, 2023, Singapore, Singapore Huynh et al.\\r\\nwe apply one-dimensional filters (1D-CNN) over all timesteps:\\r\\n𝑥\\r\\n𝑘\\r\\n𝑙\\r\\n= 𝑏\\r\\n𝑙\\r\\n𝑘\\r\\n+ 𝑐𝑜𝑛𝑣1𝐷(𝑤\\r\\n𝑙−1\\r\\n𝑖𝑘 , 𝑠𝑙−1\\r\\n𝑖\\r\\n) (1)\\r\\nwhere 𝑥\\r\\n𝑘\\r\\n𝑙\\r\\nrepresent the input feature at the 𝑘\\r\\n𝑡ℎ neuron of layer 𝑙;\\r\\n𝑏\\r\\n𝑙\\r\\n𝑘\\r\\nis the corresponding bias; 𝑤\\r\\n𝑙−1\\r\\n𝑖𝑘 is the kernel from the 𝑖\\r\\n𝑡ℎ neuron\\r\\nat layer 𝑙 − 1 to the 𝑘\\r\\n𝑡ℎ neuron at layer 𝑙; and 𝑠𝑙−1\\r\\n𝑖\\r\\nis the output of\\r\\nthe 𝑖\\r\\n𝑡ℎ neuron at layer 𝑙 − 1.\\r\\nTemporal LSTM extractor with Distinct Generative Filter. Af\\x02ter forwarding the features through the CNNs, we use an LSTM to\\r\\ncapture the temporal dependencies, exploiting its ability to memo\\x02rize long-term information. Given the concatenated feature 𝑞𝑡 of\\r\\nthe stocks at time 𝑡, we feed the feature through the LSTM layer:\\r\\nℎ𝑘 = 𝐿𝑆𝑇𝑀(𝑥𝑘, ℎ𝑘−1), 𝑡 −𝑇 ≤ 𝑘 ≤ 𝑡 − 1 (2)\\r\\nwhere ℎ𝑘 ∈ R\\r\\n𝑑\\r\\nis the hidden state for day 𝑙 and d is the hidden state\\r\\ndimension. The specific computation in each LSTM unit includes:\\r\\n𝑖𝑡 = 𝜎(𝑊𝑥𝑖𝑥𝑡 + 𝑈ℎ𝑖ℎ𝑡−1 + 𝑏𝑖), 𝑓𝑡 = 𝜎(𝑊𝑥 𝑓 𝑥𝑡 + 𝑈ℎ 𝑓 ℎ𝑡−1 + 𝑏𝑓),\\r\\n𝑔𝑡 = tanh(𝑊𝑥𝑔𝑥𝑡 + 𝑈ℎ𝑔ℎ𝑡−1 + 𝑏𝑔), 𝑜𝑡 = 𝜎(𝑊𝑥𝑜𝑥𝑡 + 𝑈ℎ𝑜ℎ𝑡−1 + 𝑏𝑜 ),\\r\\n𝑐𝑡 = 𝑓𝑡 ⊙ 𝑐𝑡−1 + 𝑖𝑡 ⊙ 𝑔𝑡, ℎ𝑡 = 𝑜𝑡 ⊙ tanh(𝑐𝑡).\\r\\nAs mentioned, existing approaches apply the same LSTM to\\r\\nthe historical data of different stocks, which results in the learned\\r\\nset of filters (W = {𝑊𝑥𝑖,𝑊𝑥 𝑓 ,𝑊𝑥𝑔,𝑊𝑥𝑜 }, U = {𝑈𝑥𝑖,𝑈𝑥 𝑓 ,𝑈𝑥𝑔,𝑈𝑥𝑜 })\\r\\nrepresenting the average temporal dynamics. This is insufficient to\\r\\ncapture each stock’s distinct behaviour (Challenge 2).\\r\\nA straightforward solution would be to learn and store a set of\\r\\nLSTM filters, one for each stock. Yet, such an approach quickly\\r\\nbecomes intractable, especially when the number of stocks is large.\\r\\nIn our model, we overcome this issue by proposing a memory\\x02based mechanism onto the LSTM network to learn the individual\\r\\npatterns per stock, while not expanding the core LSTM. Specifically,\\r\\nwe first assign to each stock 𝑖 a memory 𝑀𝑖in the form of a learnable\\r\\nm-dimensional vector, 𝑀𝑖 ∈ R\\r\\n𝑚. Then, for each entity, we feed the\\r\\nmemory through a Distinct Generative Filter, denoted by DGF, to\\r\\nobtain the weights (W𝑖, U\\r\\n𝑖\\r\\n) of the LSTM network for each stock:\\r\\nW𝑖, U\\r\\n𝑖 = 𝐷𝐺𝐹 (𝑀𝑖\\r\\n) (3)\\r\\nNote that DGF can be any neural network architecture, such as a\\r\\nCNN or an MLP. In our work, we choose a 2-layer MLP as DGF, as\\r\\nit is simple yet effective. As the DGF is required to generate a set of\\r\\neight filters {𝑊 𝑖\\r\\n𝑥𝑖,𝑊 𝑖\\r\\n𝑥 𝑓 ,𝑊 𝑖\\r\\n𝑥𝑔,𝑊 𝑖𝑥𝑜,𝑈𝑖\\r\\n𝑥𝑖,𝑈𝑖\\r\\n𝑥 𝑓 ,𝑈𝑖\\r\\n𝑥𝑔,𝑈𝑖𝑥𝑜 } from 𝑀𝑖\\r\\n, we\\r\\ngenerate a concatenation of the filters and then obtain the results\\r\\nby splitting. Finally, replacing the common filters by the specific\\r\\nones for each stock in Eq. 2, we have:\\r\\nℎ\\r\\n𝑖\\r\\n𝑘\\r\\n= 𝐿𝑆𝑇𝑀(𝑥\\r\\n𝑖\\r\\n𝑘\\r\\n, ℎ𝑖\\r\\n𝑘−1\\r\\n| W𝑖, U\\r\\n𝑖\\r\\n), 𝑡 −𝑇 ≤ 𝑘 ≤ 𝑡 − 1 (4)\\r\\nwhere ℎ\\r\\n𝑖\\r\\n𝑘\\r\\nis the hidden feature of each stock 𝑖.\\r\\nTo increase the efficiency of the LSTM, we apply a temporal at\\x02tention mechanism to guide the learning process towards important\\r\\nhistorical features. The attention mechanism attempts to aggregate\\r\\ntemporal hidden states ˆℎ\\r\\n𝑖\\r\\n𝑘\\r\\n= [ℎ\\r\\n𝑖\\r\\n𝑡−𝑇\\r\\n, . . . , ℎ𝑖\\r\\n𝑡−1\\r\\n] from previous days\\r\\ninto an overall representation using learned attention weights:\\r\\n𝜇(\\r\\nˆℎ\\r\\n𝑖\\r\\n𝑡\\r\\n) =\\r\\n∑︁𝑡−1\\r\\n𝑘=(𝑡−𝑇 )\\r\\n𝛼𝑘ℎ\\r\\n𝑖\\r\\n𝑘\\r\\n=\\r\\n∑︁𝑡−1\\r\\n𝑘=(𝑡−𝑇 )\\r\\nexp(ℎ\\r\\n𝑖\\r\\n𝑇\\r\\n𝑘\\r\\n)𝑊 ˆℎ\\r\\n𝑖\\r\\n𝑡\\r\\nÍ\\r\\n𝑘\\r\\nexp(ℎ\\r\\n𝑖\\r\\n𝑇\\r\\n𝑘\\r\\n)𝑊 ˆℎ\\r\\n𝑖\\r\\n𝑡\\r\\nℎ\\r\\n𝑖\\r\\n𝑘\\r\\n(5)\\r\\nwhere𝑊 is a linear transformation, 𝛼𝑘 =\\r\\nexp(ℎ\\r\\n𝑖\\r\\n𝑇\\r\\n𝑘\\r\\n)𝑊ℎˆ𝑖\\r\\n𝑡\\r\\nÍ\\r\\n𝑘\\r\\nexp(ℎ\\r\\n𝑖𝑇\\r\\n𝑘\\r\\n)𝑊ℎˆ𝑖\\r\\n𝑡\\r\\nℎ\\r\\n𝑖\\r\\n𝑘\\r\\nare the\\r\\nattention weights using softmax. To handle the non-stationary na\\x02ture of the stock market, we leverage the Hawkes process [4], as sug\\x02gested for financial timeseries in [40], to enhance the temporal at\\x02tention mechanism in Eq. 5. The Hawkes process is a “self-exciting”\\r\\ntemporal point process, where some random event “excites” the\\r\\nprocess and increases the chance of a subsequent other random\\r\\nevent (e.g., a crises or policy change). To realize the Hawke process,\\r\\nthe attention mechanism also learns an excitation parameter 𝜖𝑘 of\\r\\nthe day 𝑘 and a corresponding decay parameter 𝛾:\\r\\n𝜇ˆ(\\r\\nˆℎ\\r\\n𝑖\\r\\n𝑡\\r\\n) =\\r\\n∑︁𝑡−1\\r\\n𝑘=(𝑡−𝑇 )\\r\\n𝛼𝑘ℎ\\r\\n𝑖\\r\\n𝑘\\r\\n+ 𝜖𝑘max(𝛼𝑘ℎ\\r\\n𝑖\\r\\n𝑘\\r\\n, 0)exp(−𝛾𝛼𝑘ℎ\\r\\n𝑖\\r\\n𝑘\\r\\n) (6)\\r\\nFinally, we concatenate the extracted temporal feature 𝑧𝑖 = 𝜇ˆ(\\r\\nˆℎ\\r\\n𝑖\\r\\n𝑡\\r\\n)\\r\\nof each stock to form ZT ∈ R\\r\\n𝑛×𝑑\\r\\n, where 𝑛 is the number of stocks\\r\\nand 𝑑 is the embedding dimension.\\r\\n4 HIGH-ORDER MARKET LEARNING WITH\\r\\nWAVELET HYPERGRAPH ATTENTIONS\\r\\nTo model the groupwise relations between stocks, we aggregate\\r\\nthe learned temporal patterns of each stock over a hypergraph that\\r\\nrepresents multi-order relations of the market.\\r\\nIndustry hypergraph. To model the interdependence between\\r\\nstocks, we first initialize a hypergraph based on the industry of the\\r\\nrespective companies. Mathematically, the industry hypergraph is\\r\\ndenoted as G𝑖 = (𝑆, 𝐸𝑖,𝑤𝑖), where 𝑆 is the set of stocks and 𝐸𝑖is\\r\\nthe set of hyperedges; each hyperedge 𝑒𝑖 ∈ 𝐸𝑖 connects the stocks\\r\\nthat belong to the same industry. The hyperedge 𝑒𝑖is also assigned\\r\\na weight 𝑤𝑖 that reflects the importance of the industry, which we\\r\\nderive from the market capital of all related stocks.\\r\\nPrice correlation augmentation. Following the Efficient Market\\r\\nHypothesis [23], fundamentally correlated stocks maintain similar\\r\\nprice patterns, which can be used to reveal the missing endogenous\\r\\nrelations in addition to the industry assignment. To this end, for\\r\\nthe start of each training and testing period, we calculate the price\\r\\ncorrelation between the stocks using the historical price of the last\\r\\n1-year period. We employ the lead-lag correlation and the clustering\\r\\nmethod proposed in [6] to simulate the lag of the stock market,\\r\\nwhere a leading stock affects the trend of the rests. Then, we form\\r\\nhyperedges from the resulting clusters and add them to 𝐸𝑖. The\\r\\nhyperedge weight is, again, derived from the total market capital\\r\\nof the related stocks. We denote the augmented hypergraph by\\r\\nG = (A, W), with A and W being the hypergraph incidence matrix\\r\\nand the hyperedge weights, respectively.\\r\\nWavelet Hypergraph Convolution. To aggregate the extracted\\r\\ntemporal information of the individual stocks, we develop a hy\\x02pergraph convolution mechanism on the obtained hypergraph G,\\r\\nwhich consists of multiple convolution layers. At each layer 𝑙, the\\r\\nlatent representations of the stocks in the previous layer 𝑋\\r\\n(𝑙−1)\\r\\nare\\r\\naggregated by a convolution operator HConv(·) using the topology\\r\\nof G = (A, W) to generate the current layer representations 𝑋\\r\\n𝑙\\r\\n:\\r\\nZ\\r\\n(l) = HConv(Z(l−1)\\r\\n, A, W, P) (7)\\r\\nwhere X\\r\\nl ∈ R𝑛×𝑑\\r\\n𝑙\\r\\nand X\\r\\nl−1 ∈ R𝑛×𝑑\\r\\n𝑙−1\\r\\nwith 𝑛 being the number\\r\\nof stocks and 𝑑\\r\\n𝑙−1\\r\\n, 𝑑𝑙as the dimension of the layer-wise latent\\nJoint Stock Movement Prediction with Multi-Order Deep Learning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore\\r\\nFigure 3: Dataset arrangement for backtesting.\\r\\nfeature; P is a learnable weight matrix for the layer. Following [51],\\r\\nthe convolution process requires the calculation of the hypergraph\\r\\nLaplacian ∆, which serves as a normalized presentation of G:\\r\\n∆ = I − Dv\\r\\n1\\r\\n2 AWDe\\r\\n−1A𝑇 Dv−1/2\\r\\n(8)\\r\\nwhere Dv and Ds are the diagonal matrices containing the ver\\x02tex and hyperedge degrees, respectively. For later usage, we de\\x02note Dv\\r\\n1\\r\\n2 AWDe\\r\\n−1A𝑇 Dv−1/2 by Θ. As ∆ is a R𝑛×𝑛 positive semi\\x02definite matrix, it can be diagonalized as: ∆ = UΛUT, where Λ =\\r\\ndiag(𝜆0, . . . , 𝜆𝑘) is the diagonal matrix of non-negative eigenvalues\\r\\nand U is the set of orthonormal eigenvectors.\\r\\nExisting work leverages the Fourier basis [51] for this factoriza\\x02tion process. However, using the Fourier basis has two disadvan\\x02tages: (i) the localization during convolution process is not well\\x02maintained [49], and (ii) it requires the direct eigen-decomposition\\r\\nof the Laplacian matrix, which is costly for a complex hypergraph,\\r\\nsuch as those faced when modelling stock markets (Challenge 1).\\r\\nWe thus opt to rely on the wavelet basis [49], for two reasons: (i)\\r\\nthe wavelet basis represents the information diffusion process [42],\\r\\nwhich naturally implements localized convolutions of the vertex\\r\\nat each layer, and (ii) the wavelet basis is much sparser than the\\r\\nFourier basis, which enables more efficient computation.\\r\\nApplying the wavelet basis, let Ψs = UsΛsU\\r\\nT\\r\\ns\\r\\nbe a set of wavelets\\r\\nwith scaling parameter −𝑠. Then, we have Λs = diag(𝑒\\r\\n−𝜆0𝑠\\r\\n, . . . , 𝑒−𝜆𝑘𝑠)\\r\\nas the heat kernel matrix. The hypergraph convolution process for\\r\\neach vertex 𝑡 is computed by:\\r\\n𝐻𝐶𝑜𝑛𝑣(𝑥𝑡, 𝑦) = (Ψs (Ψs)\\r\\n−1\\r\\n) ⊙ (Ψs)\\r\\n−1\\r\\n𝑦) = ΨsΛs (Ψs)\\r\\n−1\\r\\n𝑥𝑡(9)\\r\\nwhere 𝑦 is the filter and (Ψs)\\r\\n−1𝑦 is its corresponding spectral trans\\x02formation. Based on the Stone-Weierstrass theorem [49], the graph\\r\\nwavelet (Ψs) can be polynomially approximated by:\\r\\nΨs ≈\\r\\n∑︁\\r\\n𝐾\\r\\n𝑘=0\\r\\n𝛼𝑘(∆)\\r\\n𝑘 =\\r\\n∑︁\\r\\n𝐾\\r\\n𝑘=0\\r\\n𝜃𝑘(Θ)\\r\\n𝑘\\r\\n(10)\\r\\nwhere 𝐾 is the polynomial order of the approximation.\\r\\nThe approximation facilitates the calculation of Ψs without the\\r\\neigen-decomposition of ∆. Applying it to Eq. 10 and Eq. 7 and\\r\\nchoosing LeakyReLU [2] as the activation function, we have:\\r\\nZ\\r\\n(l)\\r\\nH\\r\\n= 𝐿𝑅𝑒𝐿𝑈 ∑︁\\r\\n𝐾\\r\\n𝑘=0\\r\\n( (Dv\\r\\n1\\r\\n2 AWDe\\r\\n−1A𝑇 Dv−1/2\\r\\n)\\r\\n𝑘Z\\r\\n(l−1)\\r\\nH\\r\\nP) (11)\\r\\nTo capture the varying degree of influence each relation between\\r\\nstocks on the temporal price evolution of each stock, we also employ\\r\\nan attention mechanism [40]. This mechanism learns to adaptively\\r\\nweight each hyperedge associated with a stock based on its temporal\\r\\nfeatures. For each node 𝑣𝑖 ∈ 𝑆 and its associated hyperedge 𝑒𝑗 ∈ 𝐸,\\r\\nwe compute an attention coefficient Aˆ\\r\\n𝑖𝑗 using the stock’s temporal\\r\\nfeature 𝑥𝑖 and the aggregated hyperedge features 𝑥𝑗, quantifying\\r\\nhow important the corresponding relation 𝑒𝑗is to the stock 𝑣𝑖:\\r\\nAˆ\\r\\n𝑖𝑗 =\\r\\nexp(𝐿𝑅𝑒𝐿𝑈 (𝑎ˆ[P𝑥𝑖∥ P𝑥𝑗]))\\r\\nÍ\\r\\n𝑘 ∈𝑁𝑖\\r\\nexp(𝐿𝑅𝑒𝐿𝑈 (𝑎ˆ[P𝑥𝑖∥ P𝑥𝑗])) (12)\\r\\nwhere 𝑎ˆ is a single-layer feed forward network, ∥ is concatenation\\r\\noperator and P represents a learned linear transform. 𝑁𝑖is the\\r\\nneighbourhood set of the stock 𝑥𝑖, which is derived from the con\\x02structed hypergraph G. The attention-based learned hypergraph\\r\\nincidence matrix Aˆ is then used instead of the original A in Eq. 11 to\\r\\nlearn intermediate representations of the stocks. The representation\\r\\nof the hypergraph is denoted by ZH, which is concatenated with the\\r\\ntemporal feature ZT to maintain the stock individual characteristic\\r\\n(Challenge 2), which then goes through the MLP for dimension\\r\\nreduction to obtain the final prediction:\\r\\nZ = 𝑀𝐿𝑃 (ZT ∥ ZH) (13)\\r\\nFinally, we use the popular root mean squared error (RMSE) to\\r\\ndirectly encourage the output X to capture the actual relative price\\r\\nchange in the short term 𝑑\\r\\n(𝑡,𝑡+𝑤)\\r\\n𝑠 of each stock 𝑠, with 𝑤 being the\\r\\nlookahead window size (with a default value of five).\\r\\n5 EMPIRICAL EVALUATION\\r\\nIn this section, we empirically evaluate our framework based on\\r\\nfour research questions, as follows:\\r\\n(RQ1) Does our model outperform the baseline methods?\\r\\n(RQ2) What is the influence of each model component?\\r\\n(RQ3) Can our model be interpreted in a qualitative sense?\\r\\n(RQ4) Is our model sensitive to hyperparameters?\\r\\nBelow, we first describe the experimental setting (§5.1). We then\\r\\npresent our empirical evaluations, including an end-to-end com\\x02parison (§5.2), a qualitative study (§5.4), an ablation test (§5.3), and\\r\\nan examination of the hyperparameter sensitivity (§5.5).\\r\\n5.1 Setting\\r\\nDatasets. We evaluate our approach based on the US stock market.\\r\\nWe gathered historic price data and the information about industries\\r\\nin the S&P 500 index from the Yahoo Finance database [52], covering\\r\\n2016/01/01 to 2022/05/01 (1593 trading days). Overall, while the\\r\\nmarket witnessed an upward trend in this period, it also experienced\\r\\nsome considerable correction in 2018, 2020, and 2022. We split\\r\\nthe data of this period into 12 phases with varying degrees of\\nWSDM ’23, February 27-March 3, 2023, Singapore, Singapore Huynh et al.\\r\\nTable 2: Rolling backtesting from 2017-01-01 to 2022-05-01 on the SP500.\\r\\nModel Phase #1 Phase #2 Phase #3 Phase #4 Phase #5 Phase #6 Phase #7 Phase #8 Phase #9 Phase #10 Phase #11 Phase #12 Mean\\r\\nReturn\\r\\nLSTM 0.064 0.057 0.028 0.058 0.036 -0.032 0.059 -0.139 0.125 0.100 0.062 0.008 0.036\\r\\nALSTM 0.056 0.043 0.022 0.053 0.009 -0.068 0.036 -0.121 0.115 0.097 0.066 0.009 0.026\\r\\nHATS 0.102 0.031 -0.003 0.042 0.062 0.067 0.092 -0.074 0.188 0.132 0.063 -0.059 0.054\\r\\nLSTM-RGCN 0.089 0.051 0.005 -0.006 0.077 0.019 0.088 -0.121 0.155 0.107 0.038 -0.032 0.039\\r\\nRSR 0.065 0.043 -0.009 -0.016 0.014 -0.007 0.059 -0.113 0.089 0.056 0.038 -0.052 0.014\\r\\nSTHAN-SR 0.108 0.074 0.024 0.016 0.052 0.085 0.090 -0.105 0.158 0.107 0.058 -0.008 0.055\\r\\nHIST 0.080 0.020 -0.020 -0.030 -0.050 0.010 -0.030 0.020 0.200 0.100 0.020 -0.050 0.022\\r\\nESTIMATE 0.109 0.080 0.025 0.105 0.051 0.135 0.149 0.124 0.173 0.065 0.147 0.057 0.102\\r\\nIC\\r\\nLSTM -0.014 -0.030 -0.016 0.006 0.020 -0.034 -0.006 0.014 -0.002 -0.039 0.022 -0.023 -0.009\\r\\nALSTM -0.024 -0.025 0.025 -0.009 0.029 -0.018 -0.033 -0.024 0.045 -0.046 0.016 -0.015 -0.007\\r\\nHATS 0.013 -0.011 -0.006 -0.005 -0.018 0.029 0.027 -0.002 0.010 -0.017 -0.028 -0.012 -0.002\\r\\nLSTM-RGCN -0.019 0.020 0.024 0.021 -0.005 0.021 0.032 0.035 -0.086 0.043 -0.005 0.030 0.009\\r\\nRSR 0.008 -0.009 -0.003 -0.017 -0.009 0.018 0.011 -0.005 -0.036 0.018 -0.058 0.003 -0.007\\r\\nSTHAN-SR 0.025 -0.015 -0.016 -0.029 0.000 0.018 0.022 0.000 -0.010 0.009 0.007 -0.013 0.000\\r\\nHIST 0.003 0.000 0.005 -0.010 0.006 0.008 0.005 -0.017 0.006 0.009 0.011 0.006 0.003\\r\\nESTIMATE 0.037 0.080 0.153 0.010 0.076 0.080 0.080 0.011 0.127 0.166 0.010 0.131 0.080\\r\\nRank_IC\\r\\nLSTM -0.151 -0.356 -0.289 0.089 0.186 -1.091 -0.151 0.201 -0.019 -0.496 0.259 -0.397 -0.185\\r\\nALSTM -0.211 -0.266 0.409 -0.099 0.182 -0.289 -0.476 -0.243 0.242 -0.323 0.094 -0.174 -0.096\\r\\nHATS 0.169 -0.156 -0.139 -0.063 -0.408 0.517 0.333 -0.032 0.085 -0.344 -0.547 -0.135 -0.060\\r\\nLSTM-RGCN -0.271 0.210 0.223 0.152 -0.035 0.279 0.261 0.273 -0.416 0.329 -0.036 0.354 0.110\\r\\nRSR 0.151 -0.159 -0.051 -0.213 -0.107 0.282 0.135 -0.090 -0.292 0.175 -0.541 0.040 -0.056\\r\\nSTHAN-SR 0.690 -0.357 -0.365 -0.714 0.008 0.369 0.523 0.005 -0.265 0.169 0.141 -0.276 -0.006\\r\\nHIST 0.085 -0.008 0.125 -0.225 0.192 0.204 0.107 -0.328 0.174 0.256 0.215 0.157 0.080\\r\\nESTIMATE 0.386 0.507 1.613 0.059 0.284 0.585 0.412 0.062 0.704 0.936 0.054 0.595 0.516\\r\\nICIR\\r\\nLSTM -0.010 -0.036 -0.007 0.010 0.016 -0.038 0.004 0.010 0.011 -0.041 0.021 -0.010 -0.006\\r\\nALSTM -0.057 -0.041 0.030 -0.012 0.033 -0.015 -0.028 -0.034 0.057 -0.053 0.009 -0.002 -0.009\\r\\nHATS 0.023 -0.014 0.010 0.001 -0.016 0.033 0.035 0.020 0.023 -0.005 -0.042 -0.034 0.003\\r\\nLSTM-RGCN -0.033 0.012 0.022 0.027 -0.009 0.028 0.047 0.051 -0.085 0.054 -0.006 0.039 0.012\\r\\nRSR 0.031 -0.018 -0.005 -0.033 -0.009 0.029 0.001 -0.007 -0 .019 0.017 -0.072 -0.031 -0.010\\r\\nSTHAN-SR 0.018 -0.011 -0.016 -0.021 0.005 0.016 0.023 0.008 -0.003 0.004 0.009 -0.007 0.002\\r\\nHIST 0.004 -0.002 -0.006 -0.001 0.007 0.001 0.005 -0.014 0.009 0.009 0.021 0.010 0.004\\r\\nESTIMATE 0.033 0.081 0.148 0.032 0.076 0.103 0.064 0.058 0.103 0.142 0.020 0.098 0.080\\r\\nRank_ICIR\\r\\nLSTM -0.100 -0.364 -0.110 0.140 0.141 -0.984 0.094 0.168 0.117 -0.525 0.227 -0.172 -0.114\\r\\nALSTM -0.423 -0.344 0.415 -0.134 0.202 -0.192 -0.313 -0.343 0.306 -0.377 0.047 -0.019 -0.098\\r\\nHATS 0.234 -0.155 0.179 0.013 -0.221 0.511 0.340 0.270 0.194 -0.076 -0.525 -0.372 0.033\\r\\nLSTM-RGCN -0.387 0.111 0.197 0.170 -0.058 0.313 0.284 0.326 -0.354 0.318 -0.034 0.427 0.109\\r\\nRSR 0.378 -0.353 -0.053 -0.285 -0.065 0.326 0.010 -0.081 -0.114 0.131 -0.428 -0.288 -0.068\\r\\nSTHAN-SR 0.435 -0.211 -0.310 -0.573 0.107 0.386 0.478 0.271 -0.068 0.075 0.230 -0.149 0.056\\r\\nHIST 0.079 -0.044 -0.144 -0.020 0.205 0.018 0.122 -0.289 0.236 0.229 0.356 0.209 0.080\\r\\nESTIMATE 0.315 0.446 1.344 0.178 0.307 0.587 0.329 0.311 0.541 0.885 0.100 0.488 0.486\\r\\nPrec@N\\r\\nLSTM 0.542 0.553 0.581 0.471 0.456 0.569 0.440 0.547 0.588 0.615 0.554 0.608 0.544\\r\\nALSTM 0.583 0.585 0.550 0.471 0.514 0.575 0.431 0.556 0.650 0.518 0.497 0.627 0.546\\r\\nHATS 0.624 0.651 0.597 0.495 0.551 0.642 0.532 0.619 0.542 0.550 0.529 0.690 0.585\\r\\nLSTM-RGCN 0.565 0.589 0.600 0.505 0.505 0.628 0.522 0.583 0.517 0.538 0.566 0.592 0.559\\r\\nRSR 0.587 0.531 0.608 0.455 0.465 0.619 0.473 0.608 0.553 0.618 0.590 0.676 0.565\\r\\nSTHAN-SR 0.554 0.614 0.573 0.463 0.562 0.611 0.530 0.575 0.553 0.626 0.534 0.510 0.559\\r\\nHIST 0.691 0.625 0.476 0.512 0.452 0.561 0.548 0.634 0.463 0.615 0.395 0.605 0.548\\r\\nESTIMATE 0.619 0.631 0.673 0.524 0.540 0.739 0.568 0.669 0.611 0.679 0.547 0.724 0.627\\r\\nvolatility, with the period between two consecutive phases being\\r\\n163 days. Each phase contains 10 month of training data, 2 month\\r\\nof validation data, and 6 month of testing data (see Fig. 3).\\r\\nMetrics. We adopt the following evaluation metrics: To thoroughly\\r\\nevaluate the performance of the techniques, we employ the follow\\x02ing metrics:\\r\\n• Return: is the estimated profit/loss ratio that the portfolio\\r\\nachieves after a specific period, calculated by 𝑁𝑉𝑒/𝑁𝑉𝑠 − 1,\\r\\nwith 𝑁𝑉𝑠 and 𝑁𝑉𝑒 being the net asset value of the portfolio\\r\\nbefore and after the period.\\r\\n• Information Coefficient (IC): is a coefficient that shows how\\r\\nclose the prediction is to the actual result, computed by the\\r\\naverage Pearson correlation coefficient.\\r\\n• Information ratio based IC (ICIR): The information ratio of\\r\\nthe IC metric, calculated by 𝐼𝐶𝐼𝑅 = 𝑚𝑒𝑎𝑛(𝐼𝐶)/𝑠𝑡𝑑 (𝐼𝐶)\\r\\n• Rank Information Coefficient (Rank_IC): is the coefficient\\r\\nbased on the ranking of the stocks’ short-term profit poten\\x02tial, computed by the average Spearman coefficient [24].\\r\\n• Rank_ICIR: Information ratio based Rank_IC (ICIR): The in\\x02formation ratio of the Rank_IC metric, calculated by:\\r\\n𝑟𝑎𝑛𝑘_𝐼𝐶𝐼𝑅 = 𝑚𝑒𝑎𝑛(𝑟𝑎𝑛𝑘_𝐼𝐶)/𝑠𝑡𝑑 (𝑟𝑎𝑛𝑘_𝐼𝐶)\\r\\n• Prec@N: evaluates the precision of the top N short-term\\r\\nprofit predictions from the model. This way, we assess the\\r\\ncapability of the techniques to support investment decisions.\\r\\nBaselines. We compared the performance of our technique with\\r\\nthat of several state-of-the-art baselines, as follows:\\r\\n• LSTM: [13] is a traditional baseline which leverages a vanilla\\r\\nLSTM on temporal price data.\\r\\n• ALSTM: [9] is a stock movement prediction framework that\\r\\nintegrates the adversarial training and stochasticity simula\\x02tion in an LSTM to better learn the market dynamics.\\nJoint Stock Movement Prediction with Multi-Order Deep Learning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore\\r\\n• HATS: [18] is a stock prediction framework that models the\\r\\nmarket as a classic heterogeneous graph and propose a hier\\x02archical graph attention network to learn a stock represen\\x02tation to classify next-day movements.\\r\\n• LSTM-RGCN: [19] is a graph-based prediction framework\\r\\nthat constructs the connection among stocks with their price\\r\\ncorrelation matrix and learns the spatio-temporal relations\\r\\nusing a GCN-based encoder-decoder architecture.\\r\\n• RSR: [10] is a stock prediction framework that combines\\r\\nTemporal Graph Convolution with LSTM to learn the stocks’\\r\\nrelations in a time-sensitive manner.\\r\\n• HIST: [50] is a graph-based stock trend forecasting frame\\x02work that follows the encoder-decoder paradigm in attempt\\r\\nto capture the shared information between stocks from both\\r\\npredefined concepts as well as revealing hidden concepts.\\r\\n• STHAN-SR: [40] is a deep learning-based framework that\\r\\nalso models the complex relation of the stock market as a\\r\\nhypergraph and employs vanilla hypergraph convolution to\\r\\nlearn directly the stock short-term profit ranking.\\r\\nTrading simulation. We simulate a trading portfolio using the\\r\\noutput prediction of the techniques. At each timestep, the portfolio\\r\\nallocates an equal portion of money for k stocks, as determined by\\r\\nthe prediction. We simulate the risk control by applying a trailing\\r\\nstop level of 7% and profit taking level of 20% for all positions. We\\r\\nran the simulation 1000 times per phase and report average results.\\r\\nReproducibility environment. All experiments were conducted\\r\\non an AMD Ryzen ThreadRipper 3.8 GHz system with 128 GB of\\r\\nmain memory and four RTX 3080 graphic cards. We used Pytorch\\r\\nfor the implementation and Adam as gradient optimizer.\\r\\n5.2 End-to-end comparisons\\r\\nTo answer research question RQ1, we report in Table 2 an end-to\\x02end comparison of our approach (ESTIMATE) against the baseline\\r\\nmethods. We also visualize the average accumulated return of the\\r\\nbaselines and the S&P 500 index during all 12 phases in Fig. 4.\\r\\nIn general, our model outperforms all baseline methods across all\\r\\ndatasets in terms of Return, IC, Rank_IC and Prec@10. Our technique\\r\\nconsistently achieves a positive return and an average Prec@10 of\\r\\n0.627 over all 12 phases; and performs significant better than the\\r\\nS&P 500 index with higher overall return. STHAN-SR is the best\\r\\nmethod among the baselines, yielding a high Return in some phases\\r\\n(#1, #2, #5, #10). This is because STHAN-SR, similar to our approach,\\r\\nmodels the multi-order relations between the stocks using a hyper\\x02graph. However, our technique still outperforms STHAN-SR by a\\r\\nconsiderable margin for the other periods, including the ranking\\r\\nmetric Rank_IC even though our technique does not aim to learn\\r\\ndirectly the stock rank, like STHAN-SR.\\r\\nAmong the other baseline, models with a relational basis like\\r\\nRSR, HATS, and LSTM-RGCN outperform vanilla LSTM and AL\\x02STM models. However, the gap between classic graph-based tech\\x02niques like HATS and LSTM-RGCN is small. This indicates that the\\r\\ncomplex relations in a stock market shall be modelled. An inter\\x02esting finding is that all the performance of the techniques drops\\r\\nsignificantly during phase #3 and phase #4, even though the market\\r\\nmoves sideways. This observation highlights issues of prediction\\r\\nalgorithm when there is no clear trend for the market.\\r\\nThe training times for these techniques are shown in Fig. 5, where\\r\\nwe consider the size of training set ranging from 40 to 200 days. As\\r\\nexpected, the graph-based techniques (HATS, LSTM-RGCN, RSR,\\r\\nSTHAN-R, HIST and ESTIMATE) are slower than the rest, due to\\r\\nthe trade-off between accuracy and computation time. Among the\\r\\ngraph-based techniques, there is no significant difference between\\r\\nthe techniques using classic graphs (HATS, LSTM-RGCN) and those\\r\\nusing hypergraphs (ESTIMATE, STHAN-R). Our technique ESTI\\x02MATE is faster than STHAN-R by a considerable margin and is one\\r\\nof the fastest among the graph-based baselines, which highlights\\r\\nthe efficiency of our wavelet convolution scheme compared to the\\r\\ntraditional Fourier basis.\\r\\n5.3 Ablation Study\\r\\nTo answer question RQ2, we evaluated the importance of individ\\x02ual components of our model by creating four variants: (EST-1)\\r\\nThis variant does not employ the hypergraph convolution, but\\r\\ndirectly uses the extracted temporal features to predict the short\\x02term trend of stock prices. (EST-2) This variant does not employ\\r\\nthe generative filters, but relies on a common attention LSTM by\\r\\nexisting approaches. (EST-3) This variant does not apply the price\\x02correlation based augmentation, as described in §4. It employs\\r\\nsolely the industry-based hypergraph as input. (EST-4) This variant\\r\\ndoes not employ the wavelet basis for hypergraph convolution, as\\r\\nintroduced in §4. Rather, the traditional Fourier basis is applied.\\r\\nTable 3: Ablation test\\r\\nMetric ESTIMATE EST-1 EST-2 EST-3 EST-4\\r\\nReturn 0.102 0.024 0.043 0.047 0.052\\r\\nIC 0.080 0.013 0.020 0.033 0.020\\r\\nRankIC 0.516 0.121 0.152 0.339 0.199\\r\\nPrec@N 0.627 0.526 0.583 0.603 0.556\\r\\nTable 3 presents the results for several evaluation metrics, av\\x02eraged over all phases due to space constraints. We observe that\\r\\nour full model ESTIMATE outperforms the other variants, which\\r\\nprovides evidence for the the positive impact of each of its com\\x02ponents. In particular, it is unsurprising that the removal of the\\r\\nrelations between stocks leads to a significant degradation of the\\r\\nfinal result (approximately 75% of the average return) in EST-1. A\\r\\nsimilar drop of the average return can be seen for EST-2 and EST-3,\\r\\nwhich highlights the benefits of using generati +ve filters over a tra\\x02ditional single LSTM temporal extractor (EST-2); and of the proper\\r\\nconstruction of the representative hypergraph (EST-3). Also, the\\r\\nfull model outperforms the variant EST-4 by a large margin in every\\r\\nmetric. This underlines the robustness of the convolution with the\\r\\nwavelet basis used in ESTIMATE over the traditional Fourier basis\\r\\nthat is used in existing work.\\r\\n5.4 Qualitative Study\\r\\nWe answer research question RQ3 by visualizing in Fig. 6 the predic\\x02tion results of our technique ESTIMATE for the technology stocks\\r\\nAPPL and META from 01/10/2021 to 01/05/2022. We also compare\\r\\nESTIMATE’s performance to the variant that does not consider rela\\x02tions between stocks (EST-1) and the variant that does not employ\\r\\ntemporal generative filters (EST-2). This way, we illustrate how our\\r\\ntechnique is able to handle Challenge 1 and Challenge 2.\\nWSDM ’23, February 27-March 3, 2023, Singapore, Singapore Huynh et al.\\r\\nFigure 4: Cumulative return.\\r\\nFigure 5: Training performance.\\r\\nThe results indicate that modelling the complex multi-order\\r\\ndynamics of a stock market (Challenge 1) helps ESTIMATE and\\r\\nEST-2 to correctly predict the downward trend of technology stocks\\r\\naround the start of 2022; while the prediction of EST-1, which uses\\r\\nthe temporal patterns of each stock, suffers from a significant delay.\\r\\nAlso, the awareness of internal dynamics of ESTIMATE due to the\\r\\nusage of generative filters helps our technique to differentiate the\\r\\ntrend observed for APPL from the one of META, especially at the\\r\\nstart of the correction period in January 2022.\\r\\n5.5 Hyperparameter sensitivity\\r\\nThis experiment addresses question RQ4 on the hyperparameter\\r\\nsensitivity. Due to space limitations, we focus on the most important\\r\\nhyperparameters. The backtesting period of this experiment is set\\r\\nfrom 01/07/2021 to 01/05/2022 for the same reason.\\r\\nLookback window length T. We analyze the prediction perfor\\x02mance of ESTIMATE when varying the length T of the lookback\\r\\nwindow in Fig. 7. It can be observed that the best window length is\\r\\n20, which coincides with an important length commonly used by\\r\\nprofessional analyses strategies [1]. The performance drops quickly\\r\\nwhen the window length is less or equal than 10, due to the lack\\r\\nof information. On the other hand, the performance also degrades\\r\\nwhen the window length increases above 25. This shows that even\\r\\nwhen using an LSTM to mitigate the vanishing gradient issue, the\\r\\nmodel cannot handle very long sequences.\\r\\nLookahead window length w. We consider different lengths w\\r\\nof the lookahead window (Fig. 7) and observe that ESTIMATE\\r\\nachieves the best performance for a window of length 5. The results\\r\\ndegrade significantly when w exceeds 10. This shows that our\\r\\nmodel performs well for short-term prediction, it faces issues when\\r\\nconsidering a long-term view.\\r\\nNumber of selected top-k stocks. We analyze the variance in the\\r\\nprofitability depending on the number of selected top-k stocks from\\r\\nthe ranking in Fig. 7. We find that ESTIMATE performs generally\\r\\nwell, while the best results are obtained for k = 10.\\r\\n6 RELATED WORK\\r\\nTraditional Stock Modelling. Traditional techniques often focus\\r\\non numerical features [21, 39], referred to as technical indicators,\\r\\nsuch as the Moving Average (MA) or the Relative Strength Index\\r\\n(RSI). The features are combined with classical timeseries models,\\r\\nsuch as ARIMA [37], to model the stock movement [3]. However,\\r\\nsuch techniques often require the careful engineering by experts to\\r\\nidentify effective indicator combinations and thresholds. Yet, these\\r\\nconfigurations are often not robust against market changes.\\r\\nFor individual traders, they often engineer rules based on a spe\\x02cific set of technical indicators (which indicate the trading momen\\x02tum) to find the buying signals. For instance, a popular strategy is\\r\\nto buy when the moving average of length 5 (MA5) cross above\\r\\nthe moving average of length 20 (MA20), and the Moving Average\\r\\nConvergence Divergence (MACD) is positive. However, the engi\\x02neering of the specific features requires the extensive expertise and\\r\\nexperiment of the traders in the market. Also, the traders must con\\x02tinuously tune and backtest the strategy, as the optimized strategy\\r\\nis not only different from the markets but also keeps changing to\\r\\nthe evolving of the market. Last but not least, it is exhaustive for the\\r\\ntrader to keep track of a large number of indicators of a stock, as\\r\\nwell as the movement of multiple stocks in real time. Due to these\\r\\ndrawbacks, quantitative trading with the aid of the AI emerges in\\r\\nrecent years, especially with the advances of deep neural network\\r\\n(DNN).\\r\\nDNN-based Stock Modelling. Recent techniques leverage ad\\x02vances in deep learning to capture the non-linear temporal dy\\x02namics of stock prices through high-level latent features [5, 17, 38].\\r\\nEarlier techniques following this paradigm employ recurrent neural\\r\\nnetworks (RNN) [25] or convolutional neural networks (CNN) [47]\\r\\nto model a single stock price and predict its short-term trend. Other\\r\\nworks employ deep reinforcement learning (DRL), the combination\\nJoint Stock Movement Prediction with Multi-Order Deep Learning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\n1.25\\r\\n2021-10-01\\r\\n2021-11-01\\r\\n2021-12-01\\r\\n2022-01-01\\r\\n2022-02-012022-03-01\\r\\n2022-04-01\\r\\n2022-05-01\\r\\nDate\\r\\nClose price \\r\\n(Scaled)\\r\\nStocks\\r\\nAAPL (gt)\\r\\nAAPL (ESTIMATE)\\r\\nMETA (gt)\\r\\nMETA (ESTIMATE)\\r\\nNFLX (gt)\\r\\nNFLX (ESTIMATE)\\r\\n(a) ESTIMATE\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\n1.25\\r\\n2021-10-01\\r\\n2021-11-01\\r\\n2021-12-01\\r\\n2022-01-01\\r\\n2022-02-012022-03-01\\r\\n2022-04-01\\r\\n2022-05-01\\r\\nDate\\r\\nClose price \\r\\n(Scaled)\\r\\nStocks\\r\\nAAPL (gt)\\r\\nAAPL (ESTIMATE-1)\\r\\nMETA (gt)\\r\\nMETA (ESTIMATE-1)\\r\\nNFLX (gt)\\r\\nNFLX (ESTIMATE-1)\\r\\n(b) ESTIMATE-1\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\n1.25\\r\\n2021-10-01\\r\\n2021-11-01\\r\\n2021-12-01\\r\\n2022-01-01\\r\\n2022-02-01\\r\\n2022-03-01\\r\\n2022-04-01\\r\\n2022-05-01\\r\\nDate\\r\\nClose price \\r\\n(Scaled)\\r\\nStocks\\r\\nAAPL (gt)\\r\\nAAPL (ESTIMATE-2)\\r\\nMETA (gt)\\r\\nMETA (ESTIMATE-2)\\r\\nNFLX (gt)\\r\\nNFLX (ESTIMATE-2)\\r\\n(c) ESTIMATE-2\\r\\nFigure 6: Trend prediction\\r\\n2.00\\r\\n2.25\\r\\n2.50\\r\\n2.75\\r\\n3.00\\r\\n3.25\\r\\n3.50\\r\\n2017-01-012017-06-132017-11-232018-05-05\\r\\n2018-10-152019-03-272019-09-062020-02-162020-07-28\\r\\n2021-01-072021-06-192021-11-192022-05-01\\r\\nDate\\r\\nReturn\\r\\nLookback window\\r\\n10 20 30\\r\\n(a) Lookback window\\r\\n2.00\\r\\n2.25\\r\\n2.50\\r\\n2.75\\r\\n3.00\\r\\n3.25\\r\\n3.50\\r\\n2017-01-012017-06-132017-11-232018-05-05\\r\\n2018-10-152019-03-272019-09-062020-02-162020-07-28\\r\\n2021-01-072021-06-192021-11-192022-05-01\\r\\nDate\\r\\nReturn\\r\\nLookforward window\\r\\n(b) Lookforward window\\r\\n2.00\\r\\n2.25\\r\\n2.50\\r\\n2.75\\r\\n3.00\\r\\n3.25\\r\\n3.50\\r\\n2017-01-012017-06-13\\r\\n2017-11-232018-05-05\\r\\n2018-10-152019-03-272019-09-06\\r\\n2020-02-162020-07-282021-01-072021-06-19\\r\\n2021-11-192022-05-01\\r\\nDate\\r\\nReturn\\r\\nNum stocks\\r\\n5 15\\r\\n(c) Num stocks\\r\\nFigure 7: Hyperparameters sensitivity\\r\\nof deep learning with reinforcement learning (RL), a subfield of\\r\\nsequential decision-making. For instance, the quantitative trading\\r\\nproblem can be formulated as a Markov decision process [22] and be\\r\\naddressed by well-known DRL algorithms (e.g. DQN, DDPG [20]).\\r\\nHowever, these techniques treat the stocks independently and lack\\r\\na proper scheme to consider the complex relations between stocks\\r\\nin the market.\\nWSDM ’23, February 27-March 3, 2023, Singapore, Singapore Huynh et al.\\r\\nGraph-based Stock Modelling. Some state-of-the-art techniques\\r\\naddress the problem of correlations between stocks and propose\\r\\ngraph-based solutions to capture the inter-stock relations. For in\\x02stance, the market may be modelled as a heterogeneous graph with\\r\\ndifferent type of pairwise relations [18], which is then used in an\\r\\nattention-based graph convolution network (GCN) [8, 29, 32, 33, 43,\\r\\n46] to predict the stock price and market index movement. Similarly,\\r\\na market graph may be constructed and an augmented GCN with\\r\\ntemporal convolutions can be employed to learn, at the same time,\\r\\nthe stock movement and stock relation evolution [19]. The most\\r\\nrecent techniques [10, 40] are based on the argument that the stock\\r\\nmarket includes multi-order relations, so that the market should\\r\\nbe modelled using hypergraphs. Specifically, external knowledge\\r\\nfrom knowledge graphs enables the construction of a market hyper\\x02graph [40], which is used in a spatiotemporal attention hypergraph\\r\\nnetwork to learn interdependences of stocks and their evolution.\\r\\nThen, a ranking of stocks based on short-term profit is derived.\\r\\nDifferent from previous work, we propose a market analysis\\r\\nframework that learns the complex multi-order correlation of stocks\\r\\nderived from a hypergraph representation. We go beyond the state\\r\\nof the art ([10, 40]) by proposing temporal generative filters that\\r\\nimplement a memory-based mechanism to recognize better the indi\\x02vidual characteristics of each stock, while not over-parameterizing\\r\\nthe core LSTM model. Also, we propose a new hypergraph attention\\r\\nconvolution scheme that leverages the wavelet basis to mitigate\\r\\nthe high complexity and dispersed localization faced in previous\\r\\nhypergraph-based approaches.\\r\\n7 CONCLUSION\\r\\nIn this paper, we address two unique characteristics of the stock\\r\\nmarket prediction problem: (i) multi-order dynamics which implies\\r\\nstrong non-pairwise correlations between the price movement of\\r\\ndifferent stocks, and (ii) internal dynamics where each stock main\\x02tains its own dedicated behaviour. We propose ESTIMATE, a stock\\r\\nrecommendation framework that supports learning of the multi\\x02order correlation of the stocks (i) and their individual temporal\\r\\npatterns (ii), which are then encoded in node embeddings derived\\r\\nfrom hypergraph representations. The framework provides two\\r\\nnovel mechanisms: First, temporal generative filters are incorpo\\x02rated as a memory-based shared parameter LSTM network that\\r\\nfacilitates learning of temporal patterns per stock. Second, we pre\\x02sented attention hypergraph convolutional layers using the wavelet\\r\\nbasis, i.e., a convolution paradigm that relies on the polynomial\\r\\nwavelet basis to simplify the message passing and focus on the\\r\\nlocalized convolution.\\r\\nExtensive experiments on real-world data illustrate the effec\\x02tiveness of our techniques and highlight its applicability in trading\\r\\nrecommendation. Yet, the experiments also illustrate the impact\\r\\nof concept drift, when the market characteristics change from the\\r\\ntraining to the testing period. In future work, we plan to tackle\\r\\nthis issue by exploring time-evolving hypergraphs with the ability\\r\\nto memorize distinct periods of past data and by incorporating\\r\\nexternal data sources such as earning calls, fundamental indicators,\\r\\nnews data [28, 34, 35], social networks [30, 31, 44, 45], and crowd\\r\\nsignals [15, 16, 27].\\r\\nREFERENCES\\r\\n[1] Klaus Adam, Albert Marcet, and Juan Pablo Nicolini. 2016. Stock market volatility\\r\\nand learning. The Journal of finance 71, 1 (2016), 33–82.\\r\\n[2] Abien Fred Agarap. 2018. Deep learning using rectified linear units (relu). arXiv\\r\\npreprint arXiv:1803.08375 (2018).\\r\\n[3] Adebiyi A Ariyo, Adewumi O Adewumi, and Charles K Ayo. 2014. Stock price\\r\\nprediction using the ARIMA model. In UKSIM. 106–112.\\r\\n[4] Emmanuel Bacry, Iacopo Mastromatteo, and Jean-François Muzy. 2015. Hawkes\\r\\nprocesses in finance. Market Microstructure and Liquidity 1, 01 (2015), 1550005.\\r\\n[5] Mangesh Bendre, Mahashweta Das, Fei Wang, and Hao Yang. 2021. GPR: Global\\r\\nPersonalized Restaurant Recommender System Leveraging Billions of Financial\\r\\nTransactions. In WSDM. 914–917.\\r\\n[6] Stefanos Bennett, Mihai Cucuringu, and Gesine Reinert. 2021. Detection and\\r\\nclustering of lead-lag networks for multivariate time series with an application\\r\\nto financial markets. In MiLeTS. 1–12.\\r\\n[7] Kai Chen, Yi Zhou, and Fangyan Dai. 2015. A LSTM-based method for stock\\r\\nreturns prediction: A case study of China stock market. In Big Data. 2823–2824.\\r\\n[8] Chi Thang Duong, Thanh Tam Nguyen, Trung-Dung Hoang, Hongzhi Yin,\\r\\nMatthias Weidlich, and Quoc Viet Hung Nguyen. 2023. Deep MinCut: Learning\\r\\nNode Embeddings from Detecting Communities. Pattern Recognition 133 (2023),\\r\\n1–12.\\r\\n[9] Fuli Feng, Huimin Chen, Xiangnan He, Ji Ding, Maosong Sun, and Tat-Seng Chua.\\r\\n2019. Enhancing Stock Movement Prediction with Adversarial Training. In IJCAI.\\r\\n5843–5849.\\r\\n[10] Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua.\\r\\n2019. Temporal Relational Ranking for Stock Prediction. ACM Trans. Inf. Syst.\\r\\n37, 2 (2019).\\r\\n[11] Github. 2022. https://github.com/thanhtrunghuynh93/estimate\\r\\n[12] Yuechun Gu, Da Yan, Sibo Yan, and Zhe Jiang. 2020. Price forecast with high\\x02frequency finance data: An autoregressive recurrent neural network model with\\r\\ntechnical indicators. In CIKM. 2485–2492.\\r\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-term Memory. Neural\\r\\ncomputation 9 (1997), 1735–80.\\r\\n[14] Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, and Tie-Yan Liu. 2018. Listening\\r\\nto chaotic whispers: A deep learning framework for news-oriented stock trend\\r\\nprediction. In WSDM. 261–269.\\r\\n[15] Nguyen Quoc Viet Hung, Nguyen Thanh Tam, Lam Ngoc Tran, and Karl Aberer.\\r\\n2013. An evaluation of aggregation techniques in crowdsourcing. In WISE. 1–15.\\r\\n[16] Nguyen Quoc Viet Hung, Huynh Huu Viet, Nguyen Thanh Tam, Matthias Wei\\x02dlich, Hongzhi Yin, and Xiaofang Zhou. 2017. Computing crowd consensus with\\r\\npartial agreement. IEEE Transactions on Knowledge and Data Engineering 30, 1\\r\\n(2017), 1–14.\\r\\n[17] Thanh Trung Huynh, Chi Thang Duong, Tam Thanh Nguyen, Vinh Van Tong, Ab\\x02dul Sattar, Hongzhi Yin, and Quoc Viet Hung Nguyen. 2021. Network alignment\\r\\nwith holistic embeddings. TKDE (2021).\\r\\n[18] Raehyun Kim, Chan Ho So, Minbyul Jeong, Sanghoon Lee, Jinkyu Kim, and\\r\\nJaewoo Kang. 2019. Hats: A hierarchical graph attention network for stock\\r\\nmovement prediction. arXiv preprint arXiv:1908.07999 (2019).\\r\\n[19] Wei Li, Ruihan Bao, Keiko Harimoto, Deli Chen, Jingjing Xu, and Qi Su. 2020.\\r\\nModeling the Stock Relation with Graph Network for Overnight Stock Movement\\r\\nPrediction. In IJCAI. 4541–4547.\\r\\n[20] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\\r\\nYuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with\\r\\ndeep reinforcement learning. ICLR (2016).\\r\\n[21] Yi-Ting Liou, Chung-Chi Chen, Tsun-Hsien Tang, Hen-Hsen Huang, and Hsin\\x02Hsi Chen. 2021. FinSense: an assistant system for financial journalists and\\r\\ninvestors. In WSDM. 882–885.\\r\\n[22] Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. 2021.\\r\\nFinRL: deep reinforcement learning framework to automate trading in quantita\\x02tive finance. In ICAIF. 1–9.\\r\\n[23] Burton G Malkiel. 1989. Efficient market hypothesis. In Finance. Springer, 127–\\r\\n134.\\r\\n[24] Leann Myers and Maria J Sirois. 2004. Spearman correlation coefficients, differ\\x02ences between. Encyclopedia of statistical sciences 12 (2004).\\r\\n[25] David MQ Nelson, Adriano CM Pereira, and Renato A De Oliveira. 2017. Stock\\r\\nmarket’s price movement prediction with LSTM neural networks. In IJCNN.\\r\\n1419–1426.\\r\\n[26] New York Times. 2022. https://www.nytimes.com/2022/04/26/business/stock\\x02market-today.html\\r\\n[27] Quoc Viet Hung Nguyen, Thanh Tam Nguyen, Ngoc Tran Lam, and Karl Aberer.\\r\\n2013. Batc: a benchmark for aggregation techniques in crowdsourcing. In SIGIR.\\r\\n1079–1080.\\r\\n[28] Thanh Tam Nguyen, Quoc Viet Hung Nguyen, Matthias Weidlich, and Karl\\r\\nAberer. 2015. Result selection and summarization for Web Table search. In 2015\\r\\nIEEE 31st International Conference on Data Engineering. 231–242.\\r\\n[29] Tam Thanh Nguyen, Thanh Trung Huynh, Hongzhi Yin, Vinh Van Tong, Darnbi\\r\\nSakong, Bolong Zheng, and Quoc Viet Hung Nguyen. 2020. Entity alignment for\\nJoint Stock Movement Prediction with Multi-Order Deep Learning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore\\r\\nknowledge graphs with multi-order convolutional networks. IEEE Transactions\\r\\non Knowledge and Data Engineering (2020).\\r\\n[30] Thanh Tam Nguyen, Thanh Trung Huynh, Hongzhi Yin, Matthias Weidlich,\\r\\nThanh Thi Nguyen, Thai Son Mai, and Quoc Viet Hung Nguyen. 2022. Detecting\\r\\nrumours with latency guarantees using massive streaming data. The VLDB\\r\\nJournal (2022), 1–19.\\r\\n[31] Thanh Toan Nguyen, Thanh Tam Nguyen, Thanh Thi Nguyen, Bay Vo, Jun Jo, and\\r\\nQuoc Viet Hung Nguyen. 2021. Judo: Just-in-time rumour detection in streaming\\r\\nsocial platforms. Information Sciences 570 (2021), 70–93.\\r\\n[32] Thanh Toan Nguyen, Minh Tam Pham, Thanh Tam Nguyen, Thanh Trung Huynh,\\r\\nQuoc Viet Hung Nguyen, Thanh Tho Quan, et al. 2021. Structural representation\\r\\nlearning for network alignment with self-supervised anchor links. Expert Systems\\r\\nwith Applications 165 (2021), 113857.\\r\\n[33] Thanh Tam Nguyen, Thanh Cong Phan, Minh Hieu Nguyen, Matthias Weidlich,\\r\\nHongzhi Yin, Jun Jo, and Quoc Viet Hung Nguyen. 2022. Model-agnostic and\\r\\ndiverse explanations for streaming rumour graphs. Knowledge-Based Systems\\r\\n253 (2022), 109438.\\r\\n[34] Thanh Tam Nguyen, Thanh Cong Phan, Quoc Viet Hung Nguyen, Karl Aberer,\\r\\nand Bela Stantic. 2019. Maximal fusion of facts on the web with credibility\\r\\nguarantee. Information Fusion 48 (2019), 55–66.\\r\\n[35] Thanh Tam Nguyen, Matthias Weidlich, Hongzhi Yin, Bolong Zheng, Quoc\\r\\nViet Hung Nguyen, and Bela Stantic. 2019. User guidance for efficient fact\\r\\nchecking. PVLDB 12, 8 (2019), 850–863.\\r\\n[36] Armineh Nourbakhsh, Mohammad M Ghassemi, and Steven Pomerville. 2020.\\r\\nSpread: Automated financial metric extraction and spreading tool from earnings\\r\\nreports. In WSDM. 853–856.\\r\\n[37] Domenico Piccolo. 1990. A distance measure for classifying ARIMA models.\\r\\nJournal of time series analysis 11, 2 (1990), 153–164.\\r\\n[38] Ke Ren and Avinash Malik. 2019. Investment recommendation system for low\\x02liquidity online peer to peer lending (P2PL) marketplaces. In WSDM. 510–518.\\r\\n[39] Eduardo J Ruiz, Vagelis Hristidis, Carlos Castillo, Aristides Gionis, and Alejandro\\r\\nJaimes. 2012. Correlating financial time series with micro-blogging activity. In\\r\\nWSDM. 513–522.\\r\\n[40] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, Tyler Derr, and Rajiv Ratn\\r\\nShah. 2021. Stock selection via spatiotemporal hypergraph attention network: A\\r\\nlearning to rank approach. In AAAI. 497–504.\\r\\n[41] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Ratn Shah. 2020.\\r\\nSpatiotemporal hypergraph convolution network for stock movement forecasting.\\r\\nIn ICDM. 482–491.\\r\\n[42] Xiangguo Sun, Hongzhi Yin, Bo Liu, Hongxu Chen, Jiuxin Cao, Yingxia Shao,\\r\\nand Nguyen Quoc Viet Hung. 2021. Heterogeneous hypergraph embedding for\\r\\ngraph classification. In WSDM. 725–733.\\r\\n[43] Nguyen Thanh Tam, Huynh Thanh Trung, Hongzhi Yin, Tong Van Vinh, Darnbi\\r\\nSakong, Bolong Zheng, and Nguyen Quoc Viet Hung. 2021. Entity alignment for\\r\\nknowledge graphs with multi-order convolutional networks. In ICDE. 2323–2324.\\r\\n[44] Nguyen Thanh Tam, Matthias Weidlich, Duong Chi Thang, Hongzhi Yin, and\\r\\nNguyen Quoc Viet Hung. 2017. Retaining Data from Streams of Social Platforms\\r\\nwith Minimal Regret. In IJCAI. 2850–2856.\\r\\n[45] Nguyen Thanh Tam, Matthias Weidlich, Bolong Zheng, Hongzhi Yin, Nguyen\\r\\nQuoc Viet Hung, and Bela Stantic. 2019. From anomaly detection to rumour\\r\\ndetection using data streams of social platforms. PVLDB 12, 9 (2019), 1016–1029.\\r\\n[46] Huynh Thanh Trung, Tong Van Vinh, Nguyen Thanh Tam, Hongzhi Yin, Matthias\\r\\nWeidlich, and Nguyen Quoc Viet Hung. 2020. Adaptive network alignment with\\r\\nunsupervised and multi-order convolutional networks. In ICDE. 85–96.\\r\\n[47] Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Mon\\x02cef Gabbouj, and Alexandros Iosifidis. 2017. Forecasting stock prices from the\\r\\nlimit order book using convolutional neural networks. In CBI. 7–12.\\r\\n[48] Guifeng Wang, Longbing Cao, Hongke Zhao, Qi Liu, and Enhong Chen. 2021.\\r\\nCoupling macro-sector-micro financial indicators for learning stock representa\\x02tions with less uncertainty. In AAAI. 4418–4426.\\r\\n[49] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. 2019. Graph\\r\\nwavelet neural network. ICLR (2019).\\r\\n[50] Wentao Xu, Weiqing Liu, Lewen Wang, Yingce Xia, Jiang Bian, Jian Yin, and Tie\\x02Yan Liu. 2021. HIST: A Graph-based Framework for Stock Trend Forecasting via\\r\\nMining Concept-Oriented Shared Information. arXiv preprint arXiv:2110.13716\\r\\n(2021).\\r\\n[51] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand\\r\\nLouis, and Partha Talukdar. 2019. HyperGCN: A New Method For Training Graph\\r\\nConvolutional Networks on Hypergraphs. In NIPS. 1–12.\\r\\n[52] Yahoo Finance. 2022. https://finance.yahoo.com/\\r\\n[53] Liheng Zhang, Charu Aggarwal, and Guo-Jun Qi. 2017. Stock price prediction\\r\\nvia discovering multi-frequency trading patterns. In KDD. 2141–2149.\\r\\nA TECHNICAL INDICATORS FORMULATION\\r\\nIn this section, we express equations to formulate indicators de\\x02scribed in Section 3: Temporal Generative Filters. Let 𝑡 denote\\r\\nthe 𝑡-th time step and 𝑂𝑡, 𝐻𝑡, 𝐿𝑡,𝐶𝑡,𝑉𝑡 represents the open price,\\r\\nhigh price, low price, close price, and trading volume at 𝑡-th time\\r\\nstep, respectively. Since most of the indicators are calculated within\\r\\na certain period, we denote 𝑛 as that time window.\\r\\n• Arithmetic ratio (AR): The open, high, and low price ratio\\r\\nover the close price.\\r\\n𝐴𝑅𝑂 =\\r\\n𝑂𝑡\\r\\n𝐶𝑡\\r\\n, 𝐴𝑅𝐻 =\\r\\n𝐻𝑡\\r\\n𝐶𝑡\\r\\n, 𝐴𝑅𝐿 =\\r\\n𝐿𝑡\\r\\n𝐶𝑡\\r\\n(14)\\r\\n• Close Price Ratio: The ratio of close price over the highest\\r\\nand the lowest close price within a time window.\\r\\n𝑅𝐶𝑚𝑖𝑛𝑡 =\\r\\n𝐶𝑡\\r\\nmin( [𝐶𝑡,𝐶𝑡−1, ...,𝐶𝑡−𝑛])\\r\\n𝑅𝐶𝑚𝑎𝑥𝑡 =\\r\\n𝐶𝑡\\r\\nmax( [𝐶𝑡,𝐶𝑡−1, ...,𝐶𝑡−𝑛])\\r\\n(15)\\r\\n• Close SMA: The simple moving average of the close price\\r\\nover a time window\\r\\n𝑆𝑀𝐴𝐶𝑡=\\r\\n𝐶𝑡 + 𝐶𝑡−1 + ... + 𝐶𝑡−𝑛\\r\\n𝑛\\r\\n(16)\\r\\n• Close EMA: The exponential moving average of the close\\r\\nprice over a time window\\r\\n𝐸𝑀𝐴𝐶𝑡= 𝐶𝑡 ∗ 𝑘 + 𝐸𝑀𝐴𝐶𝑡−1∗ (1 − 𝑘) (17)\\r\\nwhere: 𝑘 =\\r\\n2\\r\\n𝑛+1\\r\\n• Volume SMA: The simple moving average of the volume\\r\\nover a time window\\r\\n𝑆𝑀𝐴𝑉𝑡=\\r\\n𝑉𝑡 + 𝑉𝑡−1 + ... + 𝑉𝑡−𝑛\\r\\n𝑛\\r\\n(18)\\r\\n• Volume EMA: The exponential moving average of the close\\r\\nprice over a time window\\r\\n𝐸𝑀𝐴𝑉𝑡= 𝑉𝑡 ∗ 𝑘 + 𝐸𝑀𝐴𝑉𝑡−1∗ (1 − 𝑘) (19)\\r\\nwhere: 𝑘 =\\r\\n2\\r\\n𝑛+1\\r\\n• Average Directional Index (ADX): ADX is used to quantify\\r\\ntrend strength. ADX calculations are based on a moving\\r\\naverage of price range expansion over a given period of\\r\\ntime.\\r\\n𝐷𝐼+\\r\\n𝑡 =\\r\\nSmoothed 𝐷𝑀+\\r\\n𝐴𝑇 𝑅𝑡\\r\\n∗ 100\\r\\n𝐷𝐼−\\r\\n𝑡 =\\r\\nSmoothed 𝐷𝑀−\\r\\n𝐴𝑇 𝑅𝑡\\r\\n∗ 100\\r\\n𝐷𝑋𝑡 =\\r\\n𝐷𝐼+\\r\\n𝑡 − 𝐷𝐼−𝑡\\r\\n𝐷𝐼+\\r\\n𝑡\\r\\n+ 𝐷𝐼−\\r\\n𝑡\\r\\n∗ 100\\r\\n𝐴𝐷𝑋𝑡 =\\r\\n𝐴𝐷𝑋𝑡−1 ∗ (𝑛 − 1) + 𝐷𝑋𝑡\\r\\n𝑛\\r\\n(20)\\r\\nwhere:\\r\\n– 𝐷𝑀+ = 𝐻𝑡 − 𝐻𝑡−1\\r\\n– 𝐷𝑀− = 𝐿𝑡−1 − 𝐿𝑡\\r\\n– Smoothed 𝐷𝑀+/− = 𝐷𝑀𝑡−1 −\\r\\n𝐷𝑀𝑡−1\\r\\n𝑛\\r\\n+ 𝐷𝑀𝑡\\r\\n– ATR: Average True Range\\r\\n• Relative Strength Index (RSI): measures the magnitude of\\r\\nrecent price changes to evaluate overbought or oversold\\r\\nconditions in the price of a stock or other asset. It is the\\r\\nnormalized ration of the average gain over the average loss.\\nWSDM ’23, February 27-March 3, 2023, Singapore, Singapore Huynh et al.\\r\\n𝑎𝑣𝑔𝐺𝑎𝑖𝑛𝑡 =\\r\\n(𝑛 − 1) ∗ 𝑎𝑣𝑔𝐺𝑎𝑖𝑛𝑡−1 + 𝑔𝑎𝑖𝑛𝑡\\r\\n𝑛\\r\\n𝑎𝑣𝑔𝐿𝑜𝑠𝑠𝑡 =\\r\\n(𝑛 − 1) ∗ 𝑎𝑣𝑔𝐿𝑜𝑠𝑠𝑡−1 + 𝑙𝑜𝑠𝑠𝑡\\r\\n𝑛\\r\\n𝑅𝑆𝐼𝑡 = 100 −\\r\\n100\\r\\n1 +\\r\\n𝑎𝑣𝑔𝐺𝑎𝑖𝑛𝑡\\r\\n𝑎𝑣𝑔𝐿𝑜𝑠𝑠𝑡\\r\\n(21)\\r\\nwhere:\\r\\n– 𝑔𝑎𝑖𝑛𝑡 =\\r\\n(\\r\\n𝐶𝑡 − 𝐶𝑡−1, if 𝐶𝑡 > 𝐶𝑡−1\\r\\n0, otherwise\\r\\n– 𝑙𝑜𝑠𝑠𝑡 =\\r\\n(\\r\\n0, if 𝐶𝑡 > 𝐶𝑡−1\\r\\n𝐶𝑡−1 − 𝐶𝑡, otherwise\\r\\n• Moving average convergence divergence (MACD): shows\\r\\nthe relationship between two moving averages of a stock’s\\r\\nprice. It is calculated by the subtraction of the long-term\\r\\nEMA from the short-term EMA.\\r\\n𝑀𝐴𝐶𝐷𝑡 = 𝐸𝑀𝐴𝑡 (𝑛 = 12) − 𝐸𝑀𝐴𝑡 (𝑛 = 26) (22)\\r\\nwhere:\\r\\n– 𝐸𝑀𝐴𝑡 (𝑛 = 12): The exponential moving average at 𝑡-th\\r\\ntime step of the close price over 12-time steps.\\r\\n– 𝐸𝑀𝐴𝑡 (𝑛 = 26): The exponential moving average at 𝑡-th\\r\\ntime step of the close price over 26-time steps.\\r\\n• Stochastics: an oscillator indicator that points to buying or\\r\\nselling opportunities based on momentum\\r\\n𝑆𝑡𝑜𝑐ℎ𝑎𝑠𝑡𝑖𝑐𝑡 = 100 ∗\\r\\n𝐶𝑢𝑟𝑡 − 𝐿𝑡−𝑛\\r\\n𝐻𝑡−𝑛 − 𝐿𝑡−𝑛\\r\\n(23)\\r\\nwhere:\\r\\n– 𝑇 𝑃𝑡 =\\r\\n𝐻𝑡 +𝐿𝑡 +𝐶𝑡\\r\\n3\\r\\n– 𝑚𝑜𝑛𝑒𝑦𝐹𝑙𝑜𝑤𝑡 = 𝑇 𝑃𝑡 ∗ 𝑉𝑜𝑙𝑢𝑚𝑒𝑡\\r\\n– 𝑝𝑜𝑠𝑀𝐹𝑡 = 𝑝𝑜𝑠𝑀𝐹𝑡−1 + 𝑚𝑜𝑛𝑒𝑦𝐹𝑙𝑜𝑤𝑡if 𝑇 𝑃𝑡 > 𝑇 𝑃𝑡−1\\r\\n– 𝑛𝑒𝑔𝑀𝐹𝑡 = 𝑛𝑒𝑔𝑀𝐹𝑡−1 + 𝑚𝑜𝑛𝑒𝑦𝐹𝑙𝑜𝑤𝑡if 𝑇 𝑃𝑡 ≤ 𝑇 𝑃𝑡−1\\r\\n• Money Flow Index (MFI): an oscillator measures the flow of\\r\\nmoney into and out over a specified period of time. The MFI\\r\\nis the normalized ratio of accumulating positive money flow\\r\\n(upticks) over negative money flow values (downticks).\\r\\n𝑀𝐹𝐼𝑡 = 100 −\\r\\n100\\r\\n1 +\\r\\nÍ𝑡\\r\\n𝑡−𝑛 𝑝𝑜𝑠𝑀𝐹\\r\\nÍ𝑡\\r\\n𝑡−𝑛 𝑛𝑒𝑔𝑀𝐹\\r\\n(24)\\r\\nwhere:\\r\\n– 𝑇 𝑃𝑡 =\\r\\n𝐻𝑡 +𝐿𝑡 +𝐶𝑡\\r\\n3\\r\\n– 𝑚𝑜𝑛𝑒𝑦𝐹𝑙𝑜𝑤𝑡 = 𝑇 𝑃𝑡 ∗ 𝑉𝑜𝑙𝑢𝑚𝑒𝑡\\r\\n– 𝑝𝑜𝑠𝑀𝐹𝑡 = 𝑝𝑜𝑠𝑀𝐹𝑡−1 + 𝑚𝑜𝑛𝑒𝑦𝐹𝑙𝑜𝑤𝑡if 𝑇 𝑃𝑡 > 𝑇 𝑃𝑡−1\\r\\n– 𝑛𝑒𝑔𝑀𝐹𝑡 = 𝑛𝑒𝑔𝑀𝐹𝑡−1 + 𝑚𝑜𝑛𝑒𝑦𝐹𝑙𝑜𝑤𝑡if 𝑇 𝑃𝑡 ≤ 𝑇 𝑃𝑡−1\\r\\n• Average of True Ranges (ATR): The simple moving average of\\r\\na series of true range indicators. True range indicators show\\r\\nthe max range between (High - Low), (High-Previous_Close),\\r\\nand (Previous_Close - Low).\\r\\n𝑇 𝑅𝑡 = Max(𝐻𝑡 − 𝐿𝑡, |𝐻𝑡 − 𝐶𝑡−1 |, |𝐿𝑡 − 𝐶𝑡−1 |)\\r\\n𝐴𝑇 𝑅𝑡 =\\r\\n1\\r\\n𝑛\\r\\n∑︁𝑡\\r\\n𝑖=𝑡−𝑛\\r\\n𝑇 𝑅𝑖\\r\\n(25)\\r\\n• Bollinger Band (BB): a set of trendlines plotted two standard\\r\\ndeviations (positively and negatively) away from a simple\\r\\nmoving average (SMA) of a stock’s price.\\r\\n𝐵𝑂𝐿𝑈𝑡 = 𝑀𝐴𝑡 (𝑇 𝑃𝑡,𝑤) + 𝑚 ∗ 𝜎 [𝑇 𝑃𝑡,𝑤]\\r\\n𝐵𝑂𝐿𝐷𝑡 = 𝑀𝐴𝑡 (𝑇 𝑃𝑡,𝑤) − 𝑚 ∗ 𝜎 [𝑇 𝑃𝑡,𝑤]\\r\\n(26)\\r\\nwhere:\\r\\n– 𝐵𝑂𝐿𝑈𝑡: Upper Bollinger Band at 𝑡-th time step\\r\\n– 𝐵𝑂𝐿𝐷𝑡: Lower Bollinger Band at 𝑡-th time step\\r\\n– 𝑀𝐴𝑡: Moving average at 𝑡-th time step\\r\\n– 𝑇 𝑃𝑡 =\\r\\n𝐻𝑡 +𝐿𝑡 +𝐶𝑡\\r\\n3\\r\\n– 𝑚: number of standard deviations (typically 2)\\r\\n– 𝜎 [𝑇 𝑃, 𝑛]: Standard deviation over last w periods of 𝑇 𝑃\\r\\n• On-Balance Volume (OBV): measures buying and selling\\r\\npressure as a cumulative indicator that adds volume on up\\r\\ndays and subtracts volume on down days.\\r\\n𝑂𝐵𝑉𝑡 = 𝑂𝐵𝑉𝑡−1 +\\r\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f4\\uf8f4\\r\\n\\uf8f3\\r\\n𝑉𝑡, if 𝐶𝑡 > 𝐶𝑡−1\\r\\n0, if 𝐶𝑡 = 𝐶𝑡−1\\r\\n−𝑉𝑡, if 𝐶𝑡 < 𝐶𝑡−1\\r\\n(27)'},\n",
       " {'name': '2108.01393v1.pdf',\n",
       "  'content': 'Electrical peak demand forecasting– A review\\r\\nShuang Daia, Fanlin Menga,∗, Hongsheng Daia, Qian Wangband Xizhong Chenc\\r\\naDepartment of Mathematical Sciences, University of Essex, Colchester, UK\\r\\nbDepartment of Computer Science, Durham University, Durham, UK\\r\\ncSchool of Engineering, University College Cork, Cork, Ireland\\r\\nA R T I C L E I N F O\\r\\nKeywords:\\r\\npeak demand management\\r\\npeak load forecast\\r\\ntime series\\r\\nmachine learning\\r\\ndeep learning\\r\\nsmart grid\\r\\nA B S T R A C T\\r\\nThe power system is undergoing rapid evolution with the roll-out of advanced metering infras\\x02tructure and local energy applications (e.g. electric vehicles) as well as the increasing penetration\\r\\nof intermittent renewable energy at both transmission and distribution level, which characterizes\\r\\nthe peak load demand with stronger randomness and less predictability and therefore poses a\\r\\nthreat to the power grid security. Since storing large quantities of electricity to satisfy load de\\x02mand is neither economically nor environmentally friendly, effective peak demand management\\r\\nstrategies and reliable peak load forecast methods become essential for optimizing the power\\r\\nsystem operations. To this end, this paper provides a timely and comprehensive overview of\\r\\npeak load demand forecast methods in the literature. To our best knowledge, this is the first\\r\\ncomprehensive review on such topic. In this paper we first give a precise and unified problem\\r\\ndefinition of peak load demand forecast. Second, 139 papers on peak load forecast methods were\\r\\nsystematically reviewed where methods were classified into different stages based on the time\\x02line. Thirdly, a comparative analysis of peak load forecast methods are summarized and different\\r\\noptimizing methods to improve the forecast performance are discussed. The paper ends with a\\r\\ncomprehensive summary of the reviewed papers and a discussion of potential future research\\r\\ndirections.\\r\\n1. Introduction\\r\\nWith the roll-out of advanced metering infrastructure (AMI) [1], the power system is undergoing rapid evolution.\\r\\nThe Office for Gas and Electricity Markets (Ofgem) has announced that the UK plans to install more than 50 million\\r\\nsmart meters by 2020 [2]. On the one hand, the installation of smart meters enables real-time information exchange\\r\\nbetween power suppliers and end-users and therefore increases the efficiency of the electric power supply and encour\\x02ages different smart energy applications such as demand response and demand side management [3]. On the other\\r\\nhand, the high temporal resolution energy consumption data coupled with intermittent energy resources such as wind\\r\\nenergy make electricity demand present unprecedented diversity and complexity.\\r\\nDifferent electricity generation units have been adopted in the power plants to meet the specific electrical demand/\\r\\nload types. Among all the units, peak load units have the lowest efficiency and the highest cost. It is estimated that a\\r\\n5%-15% reduction in peak load would bring substantial benefits in saving resources and decreasing real-time electricity\\r\\ntariffs [4], which calls for effective peak load management strategies.\\r\\nTo realize that, being able to accurately predict the magnitude and occurring time of peak load/ demand, which\\r\\ncan not only give the power plants sufficient start-up time to avoid grid congestion but also is fundamental in ensuring\\r\\nthe economic benefits and the security and stability of the power grid. With the increasing penetration of large-scale\\r\\nintermittent energy such as wind and solar as well as energy storage power station, it has given rise to new characteristics\\r\\nof peak loads and a more challenging task for peak load/ demand forecast.\\r\\nIn such a context, it is evident that accurate peak load demand forecast becomes essential element to the power\\r\\ngrid operations [5]. Although optimizing smart grid operation based on standard load forecast has been a long-held\\r\\nprinciple [6], the new digital and smart grid era calls for more attention to build flexible peak load forecast frameworks\\r\\nto adapt to the rapid development of the power system.\\r\\n∗Corresponding author\\r\\nsd19628@essex.ac.uk (S. Dai); fanlin.meng@essex.ac.uk (F. Meng); hdaia@essex.ac.uk (H. Dai);\\r\\nqian.wang173@hotmail.com (Q. Wang); xizhong.chen@ucc.ie (X. Chen)\\r\\nORCID(s): 0000-0002-4866-0011 (F. Meng)\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 1 of 34\\r\\narXiv:2108.01393v1 [eess.SY] 3 Aug 2021\\nElectrical peak demand forecasting– A review\\r\\nTable 1\\r\\nThe importance of peak load demand forecast for electricity stakeholders\\r\\nElectricity market stakeholders Importance of peak load demand forecast\\r\\nGrid operators\\r\\n∙ Improve the utilization rate of power generation equipment\\r\\n∙ Reduce the cost of power generation and investment in power facilities\\r\\n∙ Alleviate the supply pressure of the grid during peak hours\\r\\nElectricity retailers ∙ Make reasonable tariff schemes so as to maximize profits\\r\\n∙ Offer energy-efficiency rebates to encourage customers to reduce peak load demand\\r\\nEnd-users Commercial and industrial ∙ Improve the economic benefits and save production resources\\r\\n∙ Alleviate environmental pollution by distributing emissions concentrated in the peak hours\\r\\nResidential Save electricity bills and improve their living standard\\r\\nGovernment ∙ Enable a reliable power supply system\\r\\n∙ Ensure economic growth and social welfare\\r\\n1.1. Motivation and contributions of this review\\r\\nInstead of the continuous and stable power generation, peak load power plants only run for a short time over\\r\\na year, which is neither economical nor environmentally friendly. Peak load management strategies were therefore\\r\\nproposed to reduce peak load generation costs based on the incentive and punishment mechanism and programs, such\\r\\nas interruptible load control, demand-side bidding, and emergency demand response [7] [8]. Moreover, it is important\\r\\nto know reasonably well the future peak load demand in order to plan and trigger relevant peak demand management\\r\\nstrategies and mechanisms. Therefore, accurate and reliable peak load forecast is crucial for materializing any peak\\r\\ndemand management strategy.\\r\\nIn general, a mature peak load forecast can help the system operator to manage the peak load demand effectively in\\r\\nadvance, and thus to achieve demand response to help reduce greenhouse gas emissions and decrease non-renewable\\r\\nfuel reliance. Moreover, the ultimate goal of peak demand management and forecast is to balance electricity supply\\r\\nand demand to maximize the benefits of system. Therefore, to further highlight the motivation of the peak demand\\r\\nforecast, TABLE 1 lists key stakeholders (grid operators, electricity retailers, electricity end-users, government) and\\r\\nthe impact of peak load demand forecast on them [9] [10] [11].\\r\\nBased on the above analysis, the objective of this review is to provide a clear and comprehensive overview of the\\r\\npeak load demand forecast methods in the literature. To the best of our knowledge, this should be the significant review\\r\\non the topic of peak load forecast. In particular, the key contributions of this review are described as follows.\\r\\n• We give precise definitions of key factors relevant to the peak load forecast framework, which could serve as a\\r\\nuseful standardization and guidance for future research in this area.\\r\\n• We conduct a thorough review of peak load demand forecast methods and explores hybrid forecast models from\\r\\na historical and systematic point of view.\\r\\n• We provide a comparative analysis based on existing studies and discuss potential improving methods for peak\\r\\nload demand forecast. A comprehensive summary regarding the application scopes of the reviewed methods is\\r\\nalso presented, which could provide useful insights for future research directions.\\r\\n1.2. Literature retrieval strategy\\r\\nBefore a detailed overview of peak load demand forecast methods, a necessary initial step is to follow the standard\\r\\ncriteria and protocols to select highly related and high-quality sources and publications.\\r\\nThe literature retrieval databases selected are ScienceDirect (SD) and the Institution of Electrical and Electronics\\r\\nEngineers (IEEE). SD is a famous academic database provided by Elsevier, in which more than a billion articles\\r\\nare downloaded every year, making it the most downloaded academic search platform among academic databases\\r\\n[12]. IEEE publishes a wide range of peer-reviewed journals, and the criteria defined are of recognized authoritative\\r\\ninfluence in the field of electrical power analysis [13].\\r\\nThe following key phrases are used during the literature retrieval process (searching range of the year: 1872-2020):\\r\\n• Peak load forecasting/estimation/prediction\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 2 of 34\\nElectrical peak demand forecasting– A review\\r\\n• Peak load demand forecast/estimation/prediction\\r\\n• Maximum load forecasting/estimation/prediction\\r\\nThe keywords in each key phrase utilize the Boolean operator ’AND’; each key phrase is connected with the Boolean\\r\\noperator ’OR’.\\r\\nAfter excluding low-relevance articles without key phrases in the title and abstract, a total of 139 highly related\\r\\nand high-quality papers form the basis of this review through a preliminary analysis. The obtained studies consist of\\r\\n67 journal papers and 72 conference papers. The subsequent discussions of peak load demand forecast are all based\\r\\non the literature obtained in this section.\\r\\n1.3. Systematic overview of literature based on time line\\r\\nTo understand the historical development trend of peak load demand forecast, important to follow the timeline to\\r\\nconduct a systematic review. Figure. 1 shows the the number of total publications and journal publications published\\r\\nevery year from 1956 to 2020.\\r\\nFigure 1: The number of papers published every year from 1956.\\r\\nBuilt on the exponential trend of obtained publications, the exploration of peak load forecast can be roughly cate\\x02gorized into three stages following the timeline: the initial stage, the developing stage, and the developed stage. The\\r\\ninitial stage was from 1956 to 1990, during which the research on peak load demand forecast was in its infancy with a\\r\\nsmall number of publications. The strengthening phase was from 1991 to 2003, during which the number of publica\\x02tions began to increase gradually, with three or four articles published every year. The developed period is from 2004\\r\\nto 2020 with a large number of publications on peak load forecast.\\r\\nThe large number of publications over the past decade reveal that there are increasing interests on peak demand\\r\\nforecast. This could be explained by the fact that with the economic development, there are increasing electricity\\r\\nconsumption. As a result, peak load forecast becomes increasingly important for safe and reliable energy systems\\r\\noperation. Moreover, considering increasing integration of modern and clean energy technologies such as electric\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 3 of 34\\nElectrical peak demand forecasting– A review\\r\\nvehicles (EVs) and wind energy, it would be become more challenging for the peak demand forecast and the research\\r\\ninterests on the topic will continue to grow in the future.\\r\\nIt should be noted that the number of journal publications on peak load forecast each year over the last decade is\\r\\nusually within the range of 1 to 4, which may indicate there still lacks sufficient efforts from the researchers but on\\r\\nthe other hand indicate more research opportunities. This paper will provide a timely review on the important topic\\r\\nof peak load forecast with a clear definition of the research problem, comprehensive review of existing methods and a\\r\\ncomparative and forward-looking analysis of future research directions.\\r\\n1.4. Structure of the review\\r\\nThe remainder of this paper is organized as follows: Section 2 provides comprehensive summaries and precise\\r\\ndefinitions for the peak load forecast problem including the forecast period, influential variables, general outputs, and\\r\\nevaluation metrics. Section 3 describes peak load demand forecast methods following the timeline by dividing them\\r\\ninto manual/human expert stage, classic peak load demand forecast stage, and advanced peak load demand forecast\\r\\nstage. Section 4 firstly gives a comparative analysis and explores possible improving methods for the peak load demand\\r\\nforecast framework. Then, a comprehensive summary of existing studies on the peak load forecast will be presented.\\r\\nIn Section 5, a conclusion is given with possible future research directions discussed.\\r\\n2. Peak load demand forecast problem definition\\r\\nA general peak load demand forecast framework is shown in Figure 2. Intuitively, the general framework for peak\\r\\nload demand forecast is similar to standard load forecast. However, peak load demand forecast has its particularity\\r\\nwhen it comes to specific sub-processes, such as input variables and output results. To our best knowledge, many\\r\\nterms that have been defined in the standard load forecast have not been well defined in the peak load demand forecast,\\r\\nwhich leads to different understanding of the same terms in different studies. To this end, for the first time, this paper\\r\\nwill provide an unification of relevant terms to accurately define peak load demand forecast methods and to provide\\r\\ngeneralized guidance for future research on this topic.\\r\\nFigure 2: A general peak load demand forecast framework.\\r\\nThe following subsections will first summarize the commonly used time horizon for short-term, medium-term, and\\r\\nlong-term peak load demand forecast according to the reviewed literature. Secondly, influential variables used in peak\\r\\nload demand forecast models will be discussed. Thirdly, the outputs of the forecasting model will be summarised.\\r\\nFinally, special evaluation indicators for peak load demand forecast results are presented.\\r\\n2.1. Peak load demand forecast time period\\r\\nAlthough the forecast horizon of standard load forecast has been well known, there is no such summary for peak\\r\\nload demand forecast.\\r\\nTherefore, through analysing the reviewed literature, we classify the time horizon of peak load demand forecast\\r\\ninto following categories:\\r\\n• Short-term peak load demand forecast (STPLF), to forecast peak load from several hours to days (days<7) [14–\\r\\n17].\\r\\n• Medium-term peak load demand forecast (MTPLF), to forecast peak load from per week to months (months <\\r\\n12) [17–19].\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 4 of 34\\nElectrical peak demand forecasting– A review\\r\\nTable 2\\r\\nPopular exogenous variables for peak load demand forecast models\\r\\nDetail\\r\\nWeather variables\\r\\n[22]\\r\\nMaximum dry-bulb temperature, average dry-bulb temperature,\\r\\nminimum dry-bulb temperature, average relative humidity, average pressure,\\r\\naverage amount of cloud, rainfall volume, duration of bright sunshine,\\r\\ndaily global solar radiation, average wind speed\\r\\nCalendar variables\\r\\n[23]\\r\\nTime of the day, day of the week, week of the month, month of the year,\\r\\nseason, year number, holidays, special events\\r\\nEconomic variables\\r\\n[24],[22]\\r\\nGross National Product (GNP), Gross Domestic Product (GDP),\\r\\npopulation growth rate, consumer growth rate, tariff structure, electricity price\\r\\nOther variables\\r\\n[25],[26]\\r\\nCustomer type (commercial, residential, industrial, etc.)\\r\\n• Long-term peak load demand forecast (LTPLF), to forecast peak load from more than a year ahead [16–19].\\r\\nIt is worth noting that based on the reviewed papers: 1) daily peak load demand forecast is mainly studied among\\r\\nSTPLF; 2) weekly and monthly peak load demand forecast is mainly studied among MTPLF; and 3) annual peak load\\r\\ndemand forecast is mainly studied for LTPLF.\\r\\n2.2. Influential variables of peak load demand forecast\\r\\n2.2.1. Endogenous variables\\r\\nThe endogenous variables used in peak load demand forecast differ from those used in standard load forecast. For\\r\\nexample, assume that the training data are hourly load consumption for one year. A standard load demand forecast\\r\\nmodel will use the hourly load data, i.e. 24 × 365 data points, as the endogenous variables. However, a peak load\\r\\ndemand forecast model will use the daily peak load value (sometimes also with the daily peak time), i.e. 1 × 365 data\\r\\npoints (1 × 365 data pairs if with the daily peak time), as the endogenous variables.\\r\\nThe endogenous variables used by peak load demand forecast models are generally peak load data in similar days,\\r\\nwhich can often reflect the internal structure similar to the peak load in the forecast period, making it easier for the\\r\\nalgorithm to capture the characteristics of the predicted target. [20] proposed novel algorithms to identify the recent\\r\\ndays that are similar to the days before the forecast, and the peak load close to the predicted date is then deduced as\\r\\nhistorical training data by analogy with the rule of thumb to improve the prediction accuracy.\\r\\nFurthermore, since the input data only need the peak load in a specific period, the input variable dimension of\\r\\npeak load demand forecast is much lower than that of standard load forecast. The advantage of this is reflected in the\\r\\nhigh computational efficiency of peak load demand forecast model. [21] compared both the number of input features\\r\\nand the computation time of hourly load forecast and daily peak load forecast based on the same historical data. The\\r\\nstatistical analysis showed that only six input features were needed for the daily peak load demand forecast, while 171\\r\\ninput features were necessary for the hourly load forecast.\\r\\n2.2.2. Exogenous variables\\r\\nTABLE 2 summarizes the exogenous variables that are frequently used in peak load demand forecast models.\\r\\nThe selection of exogenous variables of peak load demand forecast models is similar to yet different from standard\\r\\nload forecast. According to the table, it can be seen that the input variables of peak load are similar to load prediction\\r\\non the macro level, namely temperature, humidity, etc. Moreover, the selection of input variables of peak load is\\r\\nclosely related to the forecasting period, which is also similar to that of the standard load forecast. The commonly\\r\\nused variables of STPLF are weather and calendar factors. For MTPLF and LTPLF, in addition to the weather and\\r\\ncalendar variables, it is necessary to capture socio-economic development and population growth trends. Besides, the\\r\\nacquisition method and accuracy of long-term weather data are also thorny problems that must be considered wisely\\r\\nfor MTPLF and LTPLE.\\r\\nOn the other hand, the difference between peak load demand forecast and standard load forecast is that, since\\r\\nthe prediction target is a series of extreme values under most conditions, the variables that most closely related to\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 5 of 34\\nElectrical peak demand forecasting– A review\\r\\na peak load demand forecast model are the extreme variables with the ability to indicate the change degrees of the\\r\\nweather, such as the maximum and minimum temperature. In addition, some weather variables are internally related\\r\\nand can affect each other. For example, [27] pointed out that high relative humidity in months with apparentseasonality\\r\\n(summer/winter) would lead to an increase in demand for refrigeration or heating, thus affecting the forecast accuracy\\r\\nof peak load demand. Therefore, in their study, relative humidity was quantified as temperature change to correct the\\r\\ninaccurate input variables, which significantly improved the forecasting accuracy of peak load demand.\\r\\nCalendar variables have a significant influence on areas with rare special events and regular holidays. In [23], the\\r\\ninfluence of lunar calendar festivals in Egypt on peak load was considered, and the influence of Ramadan is quantified as\\r\\na weight factor and input into the expert system. The prediction results showed that models considering special festivals\\r\\nhad better performance than others. Moreover, electricity consumption in the weekend and holidays of commercial\\r\\nand industrial sectors is considerably changed from that of working days, and the peak load may not even occur in\\r\\nthese sectors during non-working days for the most time. Therefore, some of the reviewed works modeled historical\\r\\ndata separately based on these calendar factors to improve the forecast performance. [28] trained models separately for\\r\\neach hour of a day, and the weekend and weekdays were also considered as criteria for model training, which resulted\\r\\nin 48 independent models to predict morning peak and afternoon peak in a day. This time-division modeling method\\r\\ndistinguishes between working days and non-working days, which significantly overcome the defect of the traditional\\r\\nmodel in predicting peak load on weekends.\\r\\n2.3. Outputs of peak load demand forecast\\r\\nThe main difference between peak load demand forecast and standard load forecast is that the output is usually\\r\\none value or a pair of values (e.g. a peak load value with its occurring time/date) whereas the output of standard load\\r\\nforecast is generally a set of load values (time series). Existing studies did not make an unified definition for the output\\r\\nof the peak load demand forecast. By reviewing relevant literature, the output of peak load forecast are summarized\\r\\nas follows:\\r\\n• Forecast the total peak consumption on a given peak day [29]\\r\\n• Forecast load usage pattern during a given peak period [30]\\r\\n• Forecast peak time [31].\\r\\n• Forecast peak value [32].\\r\\n• Forecast peak value and its occurring time simultaneously [33].\\r\\n• Forecast peak value and forecast its occurring time separately [34].\\r\\n• Forecast peak (or together with valley) value as an additional input to produce load profile [35].\\r\\n2.4. Evaluation indicators for peak load demand forecast\\r\\nThe evaluation indicators of peak load demand forecast models are partly the same as those of standard load forecast\\r\\nmodels, such as mean absolute error (MAE), mean square error (MSE), root mean square error (RMSE), mean absolute\\r\\npercentage error (MAPE), etc. Since these indicators are well known in standard load forecast, this section will not\\r\\ndescribe them in detail. Instead, to highlight the particularity of peak load demand forecast accuracy metrics compared\\r\\nwith standard load forecast, this section will list some special evaluation indicators used in the existing studies. In\\r\\naddition, the following evaluation indicators that are specific to peak load forecast are given below.\\r\\n• Evaluation indicators for peak value\\r\\nAssuming that ̂𝑦 is the predicted peak value, 𝑦 is the actual peak value, 𝑛 is the number of the training samples:\\r\\n̂𝑦 =\\r\\n{\\r\\n̂𝑦1, ̂𝑦2, ..., ̂𝑦𝑛\\r\\n}\\r\\n(1)\\r\\n𝑦 =\\r\\n{\\r\\n𝑦1, 𝑦2, ..., 𝑦𝑛\\r\\n}\\r\\n(2)\\r\\nShuang Dai et al.: Preprint submitted to Elsevier\\nElectrical peak demand forecasting– A review\\r\\nPeak absolute percentage error (𝑃 𝐴𝑃 𝐸) [33] is defined as:\\r\\n𝑃 𝐴𝑃 𝐸 =\\r\\n100%\\r\\n𝑛\\r\\n∑𝑛\\r\\n𝑖=1\\r\\n∣\\r\\n̂𝑦𝑖 − 𝑦𝑖\\r\\n𝑦𝑖\\r\\n∣ (3)\\r\\nThe range of 𝑃 𝐴𝑃 𝐸 is [0,+∞). 𝑃 𝐴𝑃 𝐸 equals 0% represents a perfect trained model, while 𝑃 𝐴𝑃 𝐸 greater\\r\\nthan 100% indicates an unacceptable model.\\r\\n• Evaluation indicators for peak occurring time\\r\\nAssuming that ̂𝑡 is the predicted peak occurring time, 𝑡 is the actual peak occurring time, 𝑛 is the number of the\\r\\ntraining samples:\\r\\n̂𝑡 =\\r\\n{\\r\\n̂𝑡1\\r\\n,̂𝑡2, ...,̂𝑡𝑛\\r\\n}\\r\\n(4)\\r\\n𝑡 =\\r\\n{\\r\\n𝑡1, 𝑡2, ..., 𝑡𝑛\\r\\n}\\r\\n(5)\\r\\nBy using 𝜖 to represent the tolerance residual for the peak occurring time, and ℎ as a flag to represent whether\\r\\nthe predicted occurring time hits the tolerance interval [𝑡\\r\\n𝑖 − 𝜖, 𝑡𝑖 + 𝜖], then we have, the Hit rate (HR) [33] is\\r\\ndefined as::\\r\\n𝐻𝑅 =\\r\\n100%\\r\\n𝑛\\r\\n∑𝑛\\r\\n𝑖=1\\r\\nℎ𝑖(6)\\r\\nwhere\\r\\nℎ𝑖 =\\r\\n{\\r\\n1 ̂𝑡\\r\\n𝑖 ∈ [𝑡𝑖 − 𝜖, 𝑡𝑖 + 𝜖]\\r\\n0 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒\\r\\n(7)\\r\\nThe peak time forecasting is usually measured by 𝐻𝑅 [33][34], which specifies a period before and after the\\r\\npeak load occurring as the forecast error tolerance range. The prediction is considered to be correct as long as\\r\\nthe predicted time falls within the tolerance interval.\\r\\n3. Peak load demand forecast methods\\r\\nTraditional forecasting methods can be roughly divided into qualitative and quantitative analysis [36]. Qualitative\\r\\nanalysis refers to the use of expert opinions to develop theoretical insights for prediction, such as curve fitting and\\r\\nextrapolation techniques, on the premise that historical data are not available or technical experiments are not feasible\\r\\n[36]. Quantitative analysis, on the other hand, presumes historical data are available and the future development of\\r\\ndata still follows the changing trend of historical data within a reasonable range. In quantitative analysis, mathematical\\r\\nor statistical methods are usually used.\\r\\nFollowing the literature obtained in Section 1.2, the number of publications for each method is summarized in\\r\\nFigure 3 while Figure 4 shows the timeline of each method being first used for peak load forecast. Finally, the peak\\r\\nload demand forecast methods are categorized according to the development timeline and types in Figure 5.\\r\\nAccording to Figures 3, 4 and 5, the development of peak load demand forecast methods can be broadly classified\\r\\ninto three stages: the manual/human expert stage starting in the late 1950s, the classic stage starting in the early\\r\\n1970s, and the advanced stage starting in the early 1990s. There are few studies in the manual/human expert stage.\\r\\nIn the classic stage, regression is the most popular method for peak load demand forecast, followed by time series\\r\\ndecomposition, stochastic time series models, and exponential smoothing. In the advanced stage, artificial neural\\r\\nnetwork (ANN) based methods are the most favorable choice.\\r\\nIn the following we will provide a detailed review of methods in e\\nElectrical peak demand forecasting– A review\\r\\nFigure 3: The number of publications for each type of methods.\\r\\nFigure 4: Time line of the first time each method was used for peak load demand forecast.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 8 of 34\\nElectrical peak demand forecasting– A review\\r\\nFigure 5: Three stages of peak load demand forecast methods.\\r\\n3.1. Manual/Human expert peak load demand forecast stage\\r\\nA manual peak load demand forecast method to transform the forecasted weather into daylight illumination param\\x02eters was proposed in [37]. The obtained results were then combined with the peak load demand table to calculate peak\\r\\nload increment, which was then applied to load curves to forecast daily peak load demand one day ahead. However,\\r\\nmost of the studies were based on the simple calculation of variable relations or human experts’ opinions to estimate\\r\\npeak load demand before 1971. Most of the results predicted by over-relying on calculation and human experience\\r\\nwere unsatisfying, due to the special characteristics of peak load demand.\\r\\n3.2. Classic peak load demand forecast stage\\r\\n3.2.1. Regression\\r\\nRegression analysis decides model parameters by function expression based on the historical data, thus obtains the\\r\\ncausal relationship between 𝑚 explanatory variables 𝐱 =\\r\\n[\\r\\n𝑥1, ..., 𝑥𝑚\\r\\n]\\r\\nand response variables 𝑦 (peak loads) to produce\\r\\npeak load demand forecast [38]. Mathematically, a general regression model can be represented by Eq (8).\\r\\n𝑦 = 𝑓(𝐱, 𝛽) + 𝑒 (8)\\r\\nwhere 𝛽 represent the model coefficients to be learned from data and 𝑒 denotes the residual left unexplained by this\\r\\nmodel.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier\\nElectrical peak demand forecasting– A review\\r\\nThe above model is said to be univariate regression when 𝑦 describes a univariate random variable, otherwise, it\\r\\ncan be described as multivariate regression. Besides, if there is only one explanatory variable (i.e., 𝑚 = 1), the model\\r\\nis simple regression, otherwise, it is multiple regression. A parametric regression model is based on the knowing form\\r\\nof 𝑓(·), in which parameters 𝛽 are needed to be estimated. When there is a linear relationship between parameters 𝛽\\r\\nand the explanatory variables 𝐱, the model is said linear, otherwise, the model is known as non-linear [38].\\r\\nMost of the obtained papers used univariate regression and selected multiple explanatory variables to get precise\\r\\nforecast results. [39] proposed multiple regression-based approaches that took calendar effects into account to forecast\\r\\nshort-term system load. The approach produced forecasts using four models: an initial peak forecast regression model,\\r\\nan initial hourly forecast regression model, an adjusted peak model, and an adjusted hourly model. All four models\\r\\nwere based on regression, and the forecasted initial peak load and the maximum hourly load were combined with past\\r\\nerrors in the adjusted peak model to produce the new peak forecast. Then the new peak forecast was used as a constraint\\r\\nin the adjusted hourly model to produce the final hourly forecasts. The proposed model was more flexible in handling\\r\\nthe effects of special days and avoided causing iterative residuals for multiple-day forecasts since past forecast errors\\r\\nwere considered as one of the influential variables. A regression-based model was proposed in [40], which considered\\r\\neconometric effects such as GDP, consumer price index, and population as extra explanatory variables to perform\\r\\nLTPLF for Zimbabwe. [41] included a new variable, average wind chill, for the winter season, and considered the\\r\\nholidays’ effects by using transformation and reflection techniques to produce better daily peak forecast. [42] adopted\\r\\na multiple regression model to linearise the load trend for Tokyo Electricity Power Company. The model is simple and\\r\\nhas a promising peak load demand forecast performance with a MAPE of 1.43%. However, the performance of the\\r\\nmodel on a dataset with more fluctuated load patterns was not satisfactory.\\r\\nThere are a few papers utilizing probabilistic forecasts in the regression framework for peak load demand fore\\x02cast. Instead of providing an estimate of the peak load demand, the probabilistic forecast is capable of predicting\\r\\nthe distribution intervals, in which the uncertainty inherent of the demand could be quantified. [16] presented a new\\r\\nmethodology to forecast annual and weekly peak load demand. This method adopted semi-parametric additive models\\r\\nto estimate the correlations between predictors and the peak load demand. Same as [40], weather, calendar, economic,\\r\\nand population variables were also considered in the paper, and the results showed a remarkable improvement in the\\r\\nforecast accuracy. [43] modeled monthly peak load demand as a function by considering the monthly peak time and\\r\\nthe monthly peak load as two key variables. In this paper, the Gaussian process was used to forecast peak load demand.\\r\\nIt also proposed a method to optimize the hyper-parameters in the kernel function, which was vital to improve forecast\\r\\naccuracy. [44] utilized Alternating Conditional Expectation (ACE) to model hourly peak load during a month. Un\\x02like most multiple linear regression models, the seasonal and trend components in this model did not require a priori\\r\\ndecomposition, and the non-parametric transformed functions could be obtained through ACE. In the model, weather\\r\\nvariables such as temperature and humidity were analyzed and used as input to perform a probability density peak load\\r\\ndemand forecast.\\r\\nRegression models are also often found being used combined with other techniques to improve forecast accuracy.\\r\\n[45] extended the model in [42] by using trend cancellation and estimation techniques to minimize the effects of tran\\x02sitional seasons. [46] performed monthly peak load demand forecast for the central region of Saudi Arabia where\\r\\nthree time-series methods (Census-II multiplicative decomposition, seasonal auto-regressive integrated moving aver\\x02age (SARIMA), Winters’ seasonal smoothing) were combined with the regression model. [32] considered intra-daily\\r\\nseasonality effect, and proposed a new method to forecast daily peak load based on functional data analysis. They\\r\\nfirstly introduced functional clustering to obtain groups that contains similar load usage patterns. Then each group\\r\\nwas assigned a specialized functional regression model. Finally, functional linear discriminant analysis was applied to\\r\\nassign new curves to the classified groups to perform peak load demand forecast. The proposed method demonstrated\\r\\npromising performance.\\r\\nSome representative regression methods for peak load forecast are summarized in TABLE 3. As aforementioned,\\r\\nthe advantage of regression analysis lies in the model is usually simple and easy to understand, with fewer parameters\\r\\nand higher forecast efficiency. However, regression analysis usually makes assumptions on the historical data and did\\r\\nnot consider the correlation between different time trends, which could limit its applications in some cases. In the\\r\\nfollowing sections, we will review time series models for peak demand forecast to consider the potential correlation\\r\\nbetween historical data at different time points.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 10\\nElectrical peak demand forecasting– A review\\r\\nTable 3\\r\\nRegression analysis for peak load demand forecast\\r\\nReference Model detail Input Variable(s) Forecast horizon Geographic scope Forecast output(s) Performance\\r\\n[47]\\r\\nA composite\\r\\nmultiregression\\x02decomposition\\r\\nmodel\\r\\nDaily peak load,\\r\\nweather variables\\r\\n(base ambient temperature,\\r\\nmean value of maximum and\\r\\nminimum ambient temperatures),\\r\\ncalendar variables\\r\\n(holiday, day of the week) (9 years)\\r\\nMonthly peak\\r\\ndemand forecast\\r\\n(MTPLF)\\r\\nRegion\\r\\n(Consolidated Electric\\r\\nCompany of Central\\r\\nRegion of Kingdom\\r\\nof Saudi Arabia)\\r\\nLoad values\\r\\n(monthly peak\\r\\nload value)\\r\\nMAPE:\\r\\n7.88%\\r\\n[39]\\r\\nModeled temperature,\\r\\nholiday and other special\\r\\neffects using binary variables\\r\\nto build a time-independent\\r\\npeak load demand forecast model\\r\\ndaily peak load,\\r\\nweather variables\\r\\n(maximum temperature),\\r\\ncalendar variables\\r\\n(holiday, day of week) (6 years)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(Pacific Gas and\\r\\nElectric Company)\\r\\nLoad values\\r\\n(next day’s peak\\r\\nload value,\\r\\nhourly loads)\\r\\nMPE:\\r\\n0.51%-0.04%\\r\\n[42]\\r\\nProposed a transformation\\r\\nfunction with translation and\\r\\nreflection methods to deal with\\r\\nthe seasonal load change,\\r\\nannual load growth and\\r\\nthe latest daily load change\\r\\npeak load of weekdays\\r\\n(except holidays),\\r\\nweather variables\\r\\n(daily maximum temperature,\\r\\nminimum temperature and humidity)\\r\\n(4 years)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(Tokyo Electric\\r\\nPower Company\\r\\n(TEPCO))\\r\\nLoad values\\r\\n(next day’s peak\\r\\nload value)\\r\\nMPE:\\r\\n1.50%\\r\\n[28] Multiple linear regression\\r\\nFall&winter historical load,\\r\\nweather variables\\r\\n(daily maximum temperature,\\r\\nseven-day moving average\\r\\nof past-midnight temperature),\\r\\ncalendar variables\\r\\n(day of week,\\r\\nmonth of the year, year)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(Puget Sound\\r\\nPower and\\r\\nLight Company)\\r\\nLoad values\\r\\n(next day’s hourly\\r\\nload value,\\r\\nnext day’s\\r\\npeak load)\\r\\nMAPE:\\r\\n2.45%-6.40%\\r\\n[48]\\r\\nCombine fuzzy system\\r\\nwith MLR, weekdays\\r\\nand weekend days\\r\\nwere assigned with\\r\\ndifferent models\\r\\nDaily 15-minutes peak load,\\r\\ndaily energy consumption\\r\\n(4 months)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion (substations) Load values\\r\\n(daily peak load)\\r\\nAbsolute Error\\r\\nrange:\\r\\nwork days:\\r\\n0.33%-14.05%\\r\\nweekend days:\\r\\n0.30%-18.98%\\r\\n[32]\\r\\nBuilt model based on\\r\\nfunctional data analysis.\\r\\nFunctional clustering,\\r\\nfunctional linear regression\\r\\nand functional linear\\r\\ndiscriminant analysis were used\\r\\nDaily load curve (hourly)\\r\\nfor heating demand\\r\\n(four discrete periods\\r\\ncovering four years)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nBuildings\\r\\n(using\\r\\ndistrict-heating\\r\\nsystem)\\r\\nLoad values\\r\\n(daily peak load)\\r\\nMAPE:\\r\\n3.61%-32.68%\\r\\n3.2.2. Time series decomposition\\r\\nThere are different time series decomposition methods, such as Fourier series analysis, wavelet methods and em\\x02pirical mode decomposition (EMD).\\r\\nA general time-series decomposition model usually adopts the addition or multiplication model to split the original\\r\\ntime series into four sub-parts: Secular trend (T), Seasonal Variation (S), Cyclical Variation (C), Irregular Variation,\\r\\n(I). In the context of peak load demand forecast:\\r\\n• The secular trend refers to the continuous change of peak load demand in a long period.\\r\\n• The seasonal variation refers to the regular seasonal change of peak load demand.\\r\\n• The cyclical variation is the periodic change in peak load demand over years.\\r\\n• The irregular variation refers to the unexpected change of the peak load demand caused by many random factors.\\r\\nWhen predicting the future peak load, each component is calculated separately first, and then the forecasted value\\r\\nof for each sub-part is passed to the addition model or the multiplication model to obtain the final prediction.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 11 of 34\\nElectrical peak demand forecasting– A review\\r\\nThe addition model of time series decomposition is defined as:\\r\\n𝑦𝑡 = 𝑇𝑡 + 𝑆𝑡 + 𝐶𝑡 + 𝐼𝑡(9)\\r\\nwhere the four components in the addition model are independent of each other, all of which are expressed in absolute\\r\\nquantities and of the same order of magnitude.\\r\\nOn the other hand, the multiplication model of time series decomposition is:\\r\\n𝑦𝑡 = 𝑇𝑡 × 𝑆𝑡 × 𝐶𝑡 × 𝐼𝑡(10)\\r\\nDifferent from the addition model, the four components of the multiplication model are dependent on each other. In\\r\\ngeneral, the secular trend in the multiplication model is expressed in absolute quantity, while other components are\\r\\nexpressed in relative quantity.\\r\\nWhen 𝑇𝑡, 𝑆𝑡, 𝐶𝑡do not change over time, addition model is usually selected. Otherwise, the multiplication model\\r\\ncould be a better choice. It should, however, be noted that, there is a convertible relationship between the addition and\\r\\nmultiplication models where log function is one of the effective converting methods [36].\\r\\nIn [49], the addition model is utilized to produce monthly peak load demand probabilistic forecast. It also uti\\x02lized Fourier transformation to reduce the non-stationary time series to a stationary series. Moreover, Monte Carlo\\r\\nsimulation was adopted to simplify the computation process. [50] designed a comparative experiment to compare the\\r\\ntime-series model using Fourier series with the auto-regressive model. Weekly peak load demand forecasts for one\\r\\nyear ahead were produced, which showed that although over-forecasts exist, the model using Fourier expansion could\\r\\ntrack the dynamic behavior of peak load demand and produce better results than the auto-regressive model.\\r\\n[51] utilized a multiplicative model to forecast the monthly peak load on a regional power grid. [38] utilized the\\r\\ndecomposition method to develop a multi regression-decomposition model. The proposed method aims to forecast\\r\\nmonthly peak loads for one year ahead, and the result was promising with a MAPE of 7.88%. The advantage of this\\r\\nmethod is that it related the historical load trend with diverse influencing factors, and could simulate additional cyclic\\r\\neffects. [52] used the real-world load data from Korea Electric Power Corporation to perform one day ahead daily\\r\\npeak load demand forecast. In this paper, Fourier transformation was adopted to identify the chaotic characteristics\\r\\nof the time series, and the optimal and embedding dimension and delay time were determined to be used as inputs to\\r\\ntrain an artificial neural network (ANN) model. MAPE for daily peak load demand forecast of the proposed model\\r\\nwas close to 1.4%. As aforementioned, wavelet transformation is also a traditional time series decomposition method,\\r\\nand it can transform the information from the time domain to frequency domain, and thus capture the low-frequency\\r\\nand high-frequency components of the peak load signal. In [53], wavelet decomposition was introduced to combine\\r\\nwith other advanced methods to build a hybrid model. Daily peak load demand forecast for Iran National Grid was\\r\\nconducted based on the proposed model.\\r\\nEmpirical mode decomposition as a more recent time series decomposition method is also used in peak demand\\r\\nforecast. In [54], empirical mode decomposition was proposed to capture long-run seasonality, short-run effects, and\\r\\ntrend effects for the daily peak load. The load decomposition results given by EMD contain physical meaning related\\r\\nto time series characterizes, and thus can improve the forecast accuracy.\\r\\nNote that time series decomposition uses the deterministic function to extract information. Therefore, it often\\r\\nignores the stochastic factors of the original time series, resulting in insufficient information extraction, which can be\\r\\ncompensated by the stochastic time series models, as we will discuss in the next subsection.\\r\\n3.2.3. Stochastic time series models\\r\\nStochastic time series models can be generally divided into: the auto-regression (AR(𝑝)) model where 𝑝 denotes\\r\\nthe order of auto-regression; the moving average (MA(𝑞)) model where 𝑞 is the order of moving average; the auto\\x02regression moving average (ARMA(𝑝, 𝑞)) model; the auto-regression integrated moving average (ARIMA(𝑝, 𝑑, 𝑞))\\r\\nmodel where 𝑑 denotes the order of integration; and the seasonal auto-regression integrated moving average (SARIMA(𝑝, 𝑑, 𝑞)\\r\\n(𝑃 , 𝐷, 𝑄)\\r\\n𝑠\\r\\n) model where 𝑃 , 𝐷, 𝑄 are the seasonal parts of the model corresponding to 𝑝, 𝑑, 𝑞.\\r\\nA SARIMA model may be written as [55]:\\r\\n𝜙𝑝(𝐵)𝜓𝑃(𝐵\\r\\n𝑆\\r\\n)𝑌𝑡 = 𝜃𝑞(𝐵)𝜏𝑄(𝐵\\r\\n𝑆\\r\\n)𝛼𝑡(11)\\r\\nwhere: 𝑌𝑡 = ▿\\r\\n𝑑▿\\r\\n𝐷\\r\\n𝑆\\r\\n𝑦𝑡, 𝑦𝑡is the peak load demand observed at time 𝑡. ▿\\r\\n𝑑\\r\\nand ▿\\r\\n𝐷\\r\\n𝑆\\r\\ndenote the non-seasonal and seasonal\\r\\ndifference (𝑆 is the seasonal length) operators respectively, which transform 𝑦𝑡into stationary tim\\nElectrical peak demand forecasting– A review\\r\\nTable 4\\r\\nStochastic time series models for peak load demand forecast\\r\\nReference Model detail Input variable Forecast horizon Geographic scope Forecast output Performance\\r\\n[21]\\r\\nModified ARIMA\\r\\n(incorporate ARIMA\\r\\nwith human experience)\\r\\nHourly loads\\r\\n(in the range of\\r\\npeak times),\\r\\nweather variables\\r\\n(current temperature),\\r\\ncalendar variables\\r\\n(day of year, which\\r\\nwas distinguished based\\r\\non average temperature),\\r\\nhuman operator’s estimation\\r\\nof current peak load\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nCountry\\r\\n(Iran’s power\\r\\nnetwork)\\r\\nLoad values\\r\\n(next day’s peak\\r\\nload value, hourly\\r\\nloads)\\r\\nHourly loads:\\r\\nMAPE:\\r\\n1.45%- 1.99%;\\r\\nDaily peak loads:\\r\\nME:\\r\\n1.01%-1.97%\\r\\n[57]\\r\\nARMA(6,5), use\\r\\nSARIMA(1,1,2)x(1,1,2)12\\r\\nto extrapolated maximum\\r\\ntemperature for\\r\\nforecasted periods\\r\\nMonthly peak load demand,\\r\\nweather variables\\r\\n(maximum temperature),\\r\\ncalendar variables\\r\\n(time, months)\\r\\nMonthly peak\\r\\ndemand forecast\\r\\n(MTPLF)\\r\\nRegion\\r\\n(Saudi Consolidated\\r\\nElectric Company)\\r\\nLoad values\\r\\n(monthly peak load demands\\r\\nfor next year)\\r\\nMost months\\r\\nwere observed to\\r\\nfall between the\\r\\nupper and lower\\r\\nlimits of\\r\\nthe forecasts\\r\\n[60]\\r\\nSARIMA, SARIMA with\\r\\ngeneralized autoregressive\\r\\nconditional heteroskedastic\\r\\nerrors (SARIMA-GARCH),\\r\\nregression-SARIMA\\x02GARCH\\r\\n(Reg-SARIMA-GARCH)\\r\\nDaily peak load demand\\r\\n(the maximum hourly\\r\\ndemand in a 24-hour period),\\r\\nweather variables\\r\\n(maximum temperature),\\r\\ncalendar variables\\r\\n(day of week, holiday,\\r\\nmonth of a year)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(industrial commercial\\r\\nand domestic sectors\\r\\nof South Africa)\\r\\nLoad values\\r\\n(monthly peak\\r\\ndemands for\\r\\nnext year)\\r\\nMAPE:\\r\\n1.42%\\r\\n[61]\\r\\nSARIMA models\\r\\nwith different length\\r\\nof historical data\\r\\nDaily peak load demand\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(New South Wales,\\r\\nAustralia)\\r\\nLoad values\\r\\n(daily peak load demands)\\r\\nMAPE:\\r\\n2.921%-7.946%\\r\\n(one to seven\\r\\ndays ahead)\\r\\nbackshift operator, which is used to represent the backshift of time. When 𝐵 is used for 𝑦𝑡, it means to reverse by one\\r\\nunit of time (𝐵𝑦𝑡 = 𝑦𝑡 − 1). For monthly data, 𝐵12𝑦𝑡 = 𝑦𝑡 − 12 represents data of the same month of the last year.\\r\\n𝜙𝑝(𝐵) and 𝜓𝑃(𝐵\\r\\n𝑆) are the non-seasonal and seasonal auto-regression operators, respectively. 𝜃𝑞\\r\\n(𝐵) and 𝜏𝑄(𝐵\\r\\n𝑆) are\\r\\nthe non-seasonal and seasonal moving average operators, respectively. 𝑝, 𝑃 , 𝑞, 𝑄 denote the maximum backshift order\\r\\nfor the non-seasonal, seasonal, auto-regression, and moving average operators, respectively. 𝛼𝑡is the white noise at\\r\\ntime 𝑡. The above model can be represented by SARIMA(𝑝, 𝑑, 𝑞)(𝑃 , 𝐷, 𝑄)\\r\\n𝑠 where 𝑠 = 4 represents the seasonal time\\r\\nseries, and 𝑠 = 12 represents the monthly time series. When 𝑃 = 𝐷 = 𝑄 = 0, the SARIMA model degenerates into\\r\\nan ARIMA model, and when 𝑃 = 𝐷 = 𝑄 = 𝑝 = 𝑑 = 𝑞 = 0, the SARIMA model degenerates into the white noise\\r\\nprocess.\\r\\nIt is worth pointing out that AR, MA, and ARMA models are suitable for weak/wide stationary time series. ARIMA\\r\\nis used for non-stationary time series, and SARIMA can deal with non-stationary and cyclical time series.\\r\\n[56] developed two models to forecast weekly peak load one year ahead, where MA model for the seasonal-cyclic\\r\\ncomponent is utilized. In [57], ARMA was used for monthly peak load demand forecast by considering the seasonal\\r\\npatterns and load fluctuations. Some hybrid methods combining ARMA with other methods such as regression models\\r\\n[58] and ANN [59] have been seen in the peak demand forecast.\\r\\nWhen practical conditions are considered such as economic and cultural factors, it often lead to nonstationary time\\r\\nseries problems. For such cases, ARIMA and SARIMA, are widely used. [21] utilized ARIMA models to forecast\\r\\nhourly loads and daily peak load demand, incorporating human experience within the model. The proposed approach\\r\\nadopted ARIMA to produce a raw output as the initial input of the modified model, and also took temperature into\\r\\naccount to distinguish hot and cold days to perform a regression-based analysis. The results of the proposed models\\r\\nwere compared with ANN, standard ARIMA, and human operators, and the accuracy of the modified ARIMA models\\r\\noutperformed other models. [62] used load data from Dubai to build models based on ARIMA and dynamic regres\\x02sion, to forecast monthly peak load where the 𝑅2 of method is 0.997. [63] developed a model based on SARIMA to\\r\\nforecast monthly peak load for Sulaimany Governorate in northern Iraq. The adequate SARIMA model they found was\\r\\nShuang Dai\\nElectrical peak demand forecasting– A review\\r\\n(1, 1, 0)(0, 2, 1)12, and the forecast results gave better opportunities for the power planners to determine the maximum\\r\\ngenerating capacity for peak load demand. The paper also pointed out that ARIMA was suitable for short-term forecast\\x02ing since it prioritized the closer time series. [64] utilized SARIMA to forecast monthly peak load demand for India.\\r\\nThe SARIMA model outperformed the official load forecasting provided by the Central Electricity Authority (CEA)\\r\\nfor both static and dynamic horizons in all five regional grids in India. [65] compared SARIMA with Holt-Winters\\r\\nmultiplicative exponential smoothing, and it showed that the SARIMA model produced better forecast results.\\r\\nAlthough ARIMA and SARIMA models have promising forecast results such as in [21][63][66][67][68], they usu\\x02ally do not take into account trend fluctuations in the data. Following this, variations based on ARIMA and SARIMA\\r\\nhave been proposed in some studies to overcome the limitation and to further improve the forecast accuracy. [69] com\\x02bine generalized autoregressive conditional heteroskedastic errors (GARCH) with ARIMA to define the maximum\\r\\npeak load demand level by considering the unexpected randomness of the load series. [60] presented a regression\\x02SARIMA model with generalized autoregressive conditional heteroskedastic errors (Reg-SARIMA-GARCH), which\\r\\ncould accommodate the volatility of the daily peak load demand and the multiple seasonality of the mid and long term\\r\\npeak demand. The proposed model was used to conduct daily peak forecast for South Africa, and the comparative\\r\\nexperiment showed that the proposed model produced better prediction accuracy than the piecewise linear regression\\r\\nmodel, the SARIMA model, and the SARIMA-GARCH model.\\r\\nSome representative stochastic time series methods for peak load forecast are summarized in TABLE 4.\\r\\nAs aforementioned, the historical load data for peak load demand forecast are characterized by its high randomness.\\r\\nTherefore, the stochastic time series model is widely used in peak load demand forecast as an effective method to deal\\r\\nwith random sequences. However, there are some limitations in the stochastic time series methods. For instance, for\\r\\nMA model, it gives the same weight to all the time series data, which does not necessarily reflect the actual situation.\\r\\nAs such, a more flexible weights assignment of the data needs to be considered. In the next subsection, we will discuss\\r\\nthe exponential smoothing, which can overcome the above limitation.\\r\\n3.2.4. Exponential smoothing\\r\\nExponential smoothing is a time series analysis method developed based on MA. Exponential smoothing predicts\\r\\nthe future peak load according to the weighted average of the historical time series. The recent data are given a larger\\r\\nweight whereas the previous data are given a smaller weight. This is based on the principle that the influence of a\\r\\ncertain variable on subsequent behavior is gradually attenuating [70].\\r\\nA general exponential smoothing model for peak load demand forecast can be written as [36]:\\r\\n̂𝑦𝑡+1 = 𝛼𝑦𝑡 + 𝛼(1 − 𝛼)𝑦𝑡−1 + 𝛼(1 − 𝛼)\\r\\n2\\r\\n𝑦𝑡−2 + ⋅ ⋅ ⋅ (12)\\r\\nwhere ̂𝑦𝑡+1 is the forecasted peak load demand at time 𝑡 + 1, and 𝛼 ∈ [0, 1] is the smoothing parameter that controls\\r\\nthe weights decrease (exponentially).\\r\\nExponential smoothing can be divided into several different forms. [71] provides a comprehensive review of ex\\x02ponential smoothing methods, in which 17 basic methods and some extensions based on these methods are described\\r\\nin detail. In general, single exponential smoothing is applied to sequences without trends or seasonality, and second\\r\\nexponential smoothing is applied to time series that only have trends. The triple exponential smoothing (also known as\\r\\nthe Holt-Winters) targets sequences with both trends and seasonality. When modeling the seasonal data, a Holt-Winters\\r\\nmodel consists of three smoothing equations each having its smoothing parameters: trend, level, and seasonality com\\x02ponents [72]. When the seasonal variations are constant and uncorrelated with time series, the additive Holt-Winters\\r\\nmodel can be hired. However, if the seasonal variables change proportionally with time series, the multiplication\\r\\nmodel can be chosen to predict the seasonal data.\\r\\nThe triple exponential smoothing is the most commonly used method in the reviewed papers. [39] used exponential\\r\\nsmoothing to correct the forecast values that were consistently too high or too small, and the adjusted model showed\\r\\ngood capability to track the fast-changing load demand and produce hourly forecasts with higher accuracy. [73] pro\\x02posed a decision support system based on a variety of time series techniques. The near-optimal monthly peak forecast\\r\\nmodels were built by exponential smoothing, Box-Jenkins vector, and dynamic regression to perform short-term peak\\r\\nload demand forecast. Moreover, a comprehensive assessment of the models was provided by using several evalu\\x02ation indicators such as MAPE, Akaike information criterion (AIC), Bayes information criterion (BIC), and MSE.\\r\\nThe results of the proposed system showed that different models performed differently towards different regions of the\\r\\ncountry.\\r\\nShuang Dai et al.: Preprint submitted to Elsev\\nElectrical peak demand forecasting– A review\\r\\nWhen trends and seasonal variations dominate the time series, the Holt-Winters exponential smoothing usually\\r\\noutperforms the ARIMA. This was confirmed by [74], which used the Holt-Winters Smoothing to forecast peak load\\r\\ndemand for England and Wales. The model they built described the intra-daily and intra-weekly seasonal cycles, and\\r\\nthe comparative results showed that this approach achieved a better accuracy than the ARIMA model.\\r\\nExponential smoothing is often used in combination with other methods to build composite models. [39] applied\\r\\nexponential smoothing to a regression model and compared it with the regression model with ARIMA, and the exper\\x02iment revealed that most of the initial forecasts were corrected after applying the exponential smoothing. However,\\r\\nthe smoothing coefficient of exponential smoothing needs to be artificially selected, and if the time series fluctuates\\r\\nwildly (e.g., the peak load), it will produce unsatisfactory prediction results [75].\\r\\n3.2.5. Kalman filter and grey prediction\\r\\nKalman filter (KF) is a linear system state equation that can estimate the system state optimally through the input\\r\\nand output observations of the system. Since the observed data include the noise, the optimal estimation can also be\\r\\nregarded as the filtering process. KF comprises two processes: prediction and correction. During the prediction, the\\r\\nfilter makes a forecast of the current state using the estimation of the state from the previous timestep. The correction\\r\\nwas performed using observations of the current state to correct the predicted value acquired in the prediction phase\\r\\nto obtain an improved estimation. Besides, apart from being known as the recursive state estimator for linear systems,\\r\\nsome KF variants are also capable of non-linear systems [76].\\r\\n[77] presented a hybrid learning scheme that consists of unsupervised and supervised learning phases to forecast\\r\\ndaily and weekly peak/average load profiles. The KF-based learning algorithm was engaged to find the optimum\\r\\nparameters and functions in the supervised learning phase. [33] selected KF as one of the benchmarks to carry out an\\r\\nintegrated hybrid model for STPLF.\\r\\nThe grey system is between a white box model and a black box model, where it focuses on learning the internal\\r\\nstructure, parameters, and general characteristics, and tries to decipher known information as much as possible. Grey\\r\\ntime series prediction model was constructed based on the observed historical time series reflecting the predicted peak\\r\\nload characteristics.\\r\\n[78] used grey correlation theory in sensitivity analysis to select relevant meteorological variables for daily peak\\r\\nload demand forecast. [30] developed a variable weight combination forecasting model by combining the grey model\\r\\nand ARIMA model, which was used to forecast load consumption in the peak load month for MTPLF. The hybrid model\\r\\nwas proved reliable to handle the non-smooth characteristics of monthly load data and achieved satisfactory forecast\\r\\naccuracy. [34] proposed a hybrid grey model to forecast yearly peak load and its occurring date simultaneously for\\r\\nLTPLF. The model only needed a small amount of historical annual peak load data to produce the forecast results, and\\r\\nit was claimed the model was highly adaptive to dynamic changes of yearly peak load.\\r\\n3.3. Advanced peak load demand forecast stage\\r\\nWith the emergence of artificial intelligence (AI) and big data, traditional AI-based techniques such as fuzzy logic\\r\\n(FL), expert system (ES) and genetic algorithm (GA) and modern AI and machine learning based methods such as\\r\\nartificial neural network (ANN) and deep learning, support vector machines (SVMs), and ensemble models have been\\r\\nadopted for peak load demand forecast.\\r\\n3.3.1. Traditional AI-based methods\\r\\nAs a bridging stage between the classic and advanced methods, there are a few studies in the reviewed literature\\r\\nusing traditional AI-based methods for peak load forecast.\\r\\nFuzzy logic imitates the uncertainty concept of judging and reasoning of the human brain. It applies fuzzy rules\\r\\nto the reasoning of the system with the uncertain model to deal with the fuzzy information that is difficult to handle\\r\\nby conventional methods. [45] adopted separate fuzzy models to predict the peak and valley load, and the simulation\\r\\nresults showed a good prediction accuracy. [48] proposed a fuzzy regression approach to peak load estimation. The\\r\\neffectiveness of the proposed method was demonstrated by forecasting daily load consumption and daily peak load\\r\\ndemand at the distribution level.\\r\\nExpert system has also been used in peak load forecast. An ES is a computer system, which is a knowledge-based\\r\\nprogramming method that absorbs the domain knowledge and experience of experts and makes intelligent decisions\\r\\nbased on the reasoning of such knowledge and experience. A complete ES consists of the knowledge base, the reasoning\\r\\nmachine, the knowledge acquisition part, and the interpretation interface. [23] implemented a knowledge-based ES to\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 15 of 34\\nElectrical peak demand forecasting– A review\\r\\nforecast yearly peak load for both typical fast-developing system and regular developing system, and the knowledge\\r\\nbase of this system was composed of both static and dynamic variables. The results proved that the knowledge-based\\r\\nES yielded the best performance among all considered models (time series model, traditional ES model, econometric\\r\\nmodel).\\r\\nFuzzy logic has been combined with ES to produce better prediction results. [79] built an ES based on fuzzy set\\r\\ntheory to forecast hourly load in Taiwan by improving the estimation accuracy of the peak and trough loads. The\\r\\nproposed ES could handle uncertain weather variables and heuristic rules, and it could update peak and trough loads\\r\\niteratively to produce a more accurate forecast. [80] proposed a fuzzy ES to forecast morning and afternoon valley,\\r\\nnoon and evening peak based on weather information and historical load data from the Greek power system. The results\\r\\nshowed that fuzzy ES could forecast daily peak and valley loads reasonably well compared with neural networks.\\r\\nGenetic Algorithm has also been adopted in building models for peak load forecast. GA is based on natural selection\\r\\nand population genetics, which makes the population evolve to the optimal region in the searching space through\\r\\nselection, crossover, variation, evaluation, and other operations. GA can also be used to optimize the parameters of\\r\\nthe forecast models such as initial connection weights of the networks and the threshold values of nodes for neural\\r\\nnetworks. In [33], 13 years of regional data from France were utilized for training a real-valued genetic algorithm\\r\\n(RGA)-based neural network with support vector machine (NN-SVM) model. Daily load profile forecast and monthly\\r\\npeak load demand forecast were generated, and the comparative experiments showed that the proposed model was\\r\\nsuitable for forecasting long-term peak load. [81] implemented a comparative analysis for yearly peak load demand\\r\\nforecast based on the unified Egyptian network data. In the experiment, models based on GA, least-square, and least\\r\\nabsolute value filtering were trained separately. The results showed that the model developed based on GA gave the\\r\\nbest performance with the lowest forecast error of 0.70%.\\r\\n3.3.2. Artificial neural network and deep learning-based methods\\r\\nANN was proposed in 1991, and it has attracted much attention in the peak load demand forecast. Many advanced\\r\\nmethods based on ANN such as deep learning methods have since been proposed with good performance.\\r\\nArtificial neural network (ANN) is inspired by the anatomy of the human brain, and it consists of artificial neurons\\r\\nin multi-layers for information communication. An example structure of ANN is shown in Figure 6.\\r\\nFigure 6: An example structure of ANN.\\r\\nA typical ANN consists of the input layer, the hidden layer, and the output layer. Except for the input layer, each\\r\\nneuron in ANN is connected to neurons of the former layer (i.e. the input neurons), with each connection corresponding\\r\\nto a weight. The sum of the product of all input and the corresponding connection weights are passed to an active\\r\\nfunction to calculate each neuron’s final value, as is shown in Figure 7.\\r\\nThe activation function needs to be selected according to data characteristics, and the Sigmoid function is the\\r\\nmost commonly used active function of ANN models [82]. One of the well-known ANN is the backpropagation (BP)\\r\\nneural network, a multi-layer neural network with error backward propagation. BP is widely used for its satisfying\\r\\nperformance on prediction tasks. It, however, suffers from high computational cost and low computational efficiency,\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 16 of 34\\nElectrical peak demand forecasting– A review\\r\\nFigure 7: The calculation process of a neuron in ANN.\\r\\nand therefore the radial basis function network (RBFN) was brought up to deal with this. The input variables of RBFN\\r\\npass directly to the hidden layers without additional weights, and RBFN is proved to be less time-consuming than the\\r\\ntraditional multi-layer neural network [83].\\r\\n[84] collected hourly temperature and load data from Seattle to build a model based on ANN, and the trained ANN\\r\\nwas then used to forecast daily peak load and hourly load and total daily load. The mean error of the peak load forecast\\r\\nmodel was ranged between 1.55% and 2.60%. This ANN model allowed a more flexible relationship between weather\\r\\nvariables and load patterns. However, the model produced higher errors when people have specific start-up activities,\\r\\nwhich indicates that the use of additional calendar variables should yield better results. [85] utilized ANN to perform\\r\\nannual regional peak load demand forecast of Taiwan. The proposed model had three input neurons corresponding to\\r\\neconomic, demographic, and weather variables, two hidden neurons, and one output neuron representing the regional\\r\\npeak load that needs to be estimated. The effectiveness of the proposed model was demonstrated by comparing the\\r\\nforecast accuracy with a regression-based model. [86] was presented to forecast daily peak load up to seven days\\r\\nahead based on a feed-forward neural network with the steepest descent, Bayesian regularization, resilient and adaptive\\r\\nbackpropagation learning methods. [87] presented a new model based on ANN by employing Bayesian regularized\\r\\nneural network model with Levenberg-Marquart (LM) backpropagation algorithm to forecast daily peak load demand\\r\\nfor a commercial building complex. [88] forecasted annual peak load demand five years ahead for Iran based on RBF.\\r\\nThe paper selected variables related to yearly incremental growth rate and pointed out that long-term forecasting should\\r\\npay more attention to economic factors than weather conditions.\\r\\nThere are a few studies combining multiple ANNs to improve the forecast accuracy, in which the peak load is\\r\\nusually generated as a by-product to enhance the forecast performance further. [89] developed a model based on\\r\\ncascaded ANNs (CANNs) to forecast the load profile one day ahead where the daily peak, valley, and total load\\r\\nconsumption were first estimated by an ANN, and then such forecasted values were used as additional input data for\\r\\nthe next day’s load profile forecast. The results revealed that the cascaded structure of ANN could produce more\\r\\nsatisfactory forecasting results. [90] proposed a multi-stage ANN to forecast load demand in two stages. Firstly, the\\r\\ndaily peak and valley values were generated by ANN. Second, based on the peak and valley load, the whole electricity\\r\\ndemand curve was produced. This method outperformed normal ANN for significantly reduced the MAPE. Although\\r\\nmultiple ANNs can provide promising results for load forecasting, [90] revealed that the multi-stage ANN suffers from\\r\\nhigher computational complexity than a single ANN.\\r\\nIn general, ANN has apparent advantages for its adaptive learning and function approximation capabilities. Since\\r\\nANN can deal with the high randomness and uncertainty of the time series well, it is recommended for STPLF. How\\x02ever, ANN models suffer from long training time and are easy to fall into local optimum. Therefore, researchers mostly\\r\\nfocused on optimizing the neural network structure, such as combining with fuzzy logic [58][91][92], to further im\\x02prove the model training efficiency and forecasting accuracy.\\r\\nMany modern and advanced methods have emerged based on ANN, such as self-organizing map (SOM), recur\\x02rent neural network (RNN), and convolution neural network (CNN). In particular, long-short memory (LSTM) is one\\r\\nvariation of RNN [93].\\r\\nSOM is often used as improving method due to its unsupervised learning characteristics together with other forecast\\r\\nmethods to produce the final results. In [94, 95], SOM was adopted to cluster days with similar load consumption\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 17 of 34\\nElectrical peak demand forecasting– A review\\r\\npatterns. Then, based on a feed-forward multilayer neural network, daily peak load and valley load were estimated to\\r\\ncompute the desired hourly load. [96] adopted SOM to cluster load profiles, and principal components analysis (PCA)\\r\\nfor reducing the dimensions of the data. Then, separate feed-forward neural network was trained for each cluster. The\\r\\ncomparative analysis demonstrated the superiority of the proposed method.\\r\\nRNN and CNN are commonly used deep learning methods, which have more complex network structures, such as\\r\\nmore hidden layers and recurrent structures. Deep learning models can better capture the dynamic characteristics of\\r\\npeak load to provide a more accurate and stable prediction and have more robust learning and generalization ability\\r\\nthan the standard ANN, especially in the big data era. [20] proposed a method to combine RNN with dynamic time\\r\\nwarping (DTW) for short-term peak load demand forecast. The DTW was introduced to identify load curves with\\r\\nsimilar trends, and a bespoke gated RNN was trained to forecast daily peak load demand one month ahead based on\\r\\nthe half-hourly load data. The proposed method achieved a satisfactory MAPE of 1.01%. In addition, comparative\\r\\nanalysis suggested that the DTW distance had the ability to adapt to the dynamic change of non-stationary daily peak\\r\\nload series. In [97], the LSTM layer was adopted to forecast weekly peak load in Korea. In this study, input variables\\r\\nincluding weekly peak load, weekly temperature, and weekly GDP of the previous year were used. The LSTM layer\\r\\nin this paper was proved to be able to capture more useful characteristics of the load data, and results showed good\\r\\nforecast accuracy with the lowest forecast error of 2.16%.\\r\\nAbout 60 studies have employed ANN and deep learning based methods to perform peak load demand forecasting,\\r\\nrevealing its dominant popularity in the peak load forecast. Some representative references related to these methods\\r\\nare listed in TABLE 5.\\r\\n3.3.3. Support Vector Machines\\r\\nAs one popular machine learning method, support vector machines can minimize actual risk by seeking risk min\\x02imization so that to get satisfactory forecasting performance. The variation of SVMs for regression problems is repre\\x02sented as support vector regression (SVR) [100], which is efficient for large-scale regression problems [101].\\r\\nGiven a training dataset 𝑇 = (𝐱𝟏, 𝑦1), ...,(𝐱𝐢, 𝑦𝑖), ...,(𝐱𝐦, 𝑦𝑚) where 𝐱𝐢∈𝑅𝑛 denotes the 𝑖-th observation (𝑛-dimensional\\r\\ninput vector), 𝑦𝑖∈𝑅 is the output corresponding to 𝐱𝐢, and 𝑚 denotes the size of training set. For non-linear SVMs, the\\r\\nbasic idea is to introduce kernel as below:\\r\\n𝑦 = ⟨𝐰, 𝜙(𝐱)⟩ + 𝑏 (13)\\r\\nwhere 𝜙(𝐱) is the hypothetical higher dimensional feature space. Coefficients 𝐰 and 𝑏 need to be estimated based on\\r\\nthe structure risk minimization principle.\\r\\n[102] introduced local prediction based on SVM for electric daily peak load forecast. The local prediction can find\\r\\nthe approximation function in the reconstructed embedded space. The partitioned inputs were assigned with an SVM\\r\\nmodel in each sub-domains, and thus local prediction could make better forecasts than the single/global model. [103]\\r\\ndeveloped a novel online-SVM model based on the standard SVM. The proposed model was used to forecast daily\\r\\npeak load for the residential building in Surrey, and results showed that the model could be a more intelligent tool for\\r\\nsmart grid systems. [104] adopted SVM to build a model for monthly peak load prediction. Firstly, feature selection\\r\\nwas implemented based on correlation analysis. Then the training set was reconstructed by the topology network and\\r\\nrandom walk with restart (RWR) algorithm. Moreover, a feed-forward correlation was utilized to minimize the effect\\r\\nof unknown errors. Finally, the preprocessed training data was fed into SVM to train a model with higher accuracy.\\r\\nSimilar to ANN, SVMs are more suitable for STPLF and can cope with nonlinear and high dimensional data\\r\\n[103][104]. The disadvantage of SVMs is also similar to that of ANN for suffering from long training time with large\\r\\ndata sets. Besides, the hyperparameters of SVMs need to be manually selected, which is also a complex step that\\r\\nneeded to be optimized.\\r\\n3.3.4. Ensemble Learning\\r\\nEnsemble learning trains multiple learners and aggregates each learner’s predicted results to obtain the final out\\x02put through combining strategies, which generally involve averaging, voting, and stacking [105]. According to the\\r\\ndependencies between learners, one possible classification of the popular ensemble learning methods is as follows:\\r\\n(1) Learners have to be generated in sequence to satisfy the strong dependency between them (boosting).\\r\\n(2) Learners are allowed to be simultaneously generated since there is no strong dependence between them (bagging\\r\\nand random forest).\\r\\nShuang Dai et al.: Preprint su\\nElectrical peak demand forecasting– A review\\r\\nEnsemble learning has been widely used in peak load demand forecast in recent years. Ensemble models used in\\r\\nthe reviewed studies mainly are: boosting, bagging, and random forest.\\r\\nBoosting adjusts the sample distribution according to the performance of the initial learner so that samples with\\r\\nthe wrong prediction get more attention than others, and then it trains the next learner based on the adjusted sample.\\r\\nThe process is iterated until a specified number of learner clusters are generated, or the aggregated learning criteria\\r\\nreaches the stop threshold [105]. Commonly used boosting algorithms in the reviewed papers are adaptive boosting\\r\\n(AdaBoost), boosting tree, gradient boosting (GB) and Extreme gradient boosting (XGBoost). [106] adopted three\\r\\nmachine learning models (ANN with nonlinear autoregressive exogenous multivariable inputs, multivariate linear\\r\\nregression, and AdaBoost) to predict load profile one month, one season, and one year ahead at the district level. During\\r\\ntraining, datasets with different sizes were utilized for training models for different prediction intervals. This paper\\r\\nalso adopted feature extraction to select essential variables, and the results showed that the AdaBoost outperformed\\r\\nother models significantly for all prediction intervals. Moreover, for seasonal forecasting, the error range of AdaBoost\\r\\nwas relatively narrow, which indicated that the model trained based on AdaBoost was more capable of capturing the\\r\\ndynamic change of load curves. [107] conducted short-term load forecasting for southern California. In this study,\\r\\ndifferent models were adopted (multivariate linear regression, random forest, and GB) and the installed solar capacity\\r\\nwas identified to be an important feature during the forecasting. The comparative experiment results revealed two\\r\\ninsights: (1) The fact that the installed solar capacity became an important feature suggested that new and clean\\r\\nenergy resources are important components in the system that researchers need to pay more attention to; (2) Different\\r\\nforecasting accuracy in different periods indicated that being able to capture the fluctuation of load curves is important\\r\\nfor forecast. [108] combined complete ensemble empirical mode decomposition with XGBoost to predict daily load\\r\\nconsumption, daily peak load, and daily water delivery. Compared to traditional XGBoost, the hybrid model showed\\r\\na lower MAPE of 5.99% for the daily peak load demand forecast.\\r\\nBagging is based on bootstrap sampling. It carries out multiple times of put-back sampling for a given dataset\\r\\nand trains learners simultaneously based on the obtained sampling set. When bagging is applied to a regression task,\\r\\na simple mean or median can be adopted to obtain the final output [109]. [110], for the first time, utilized bagging\\r\\nto forecast monthly load demand for countries with different development stages. The paper combined bagging with\\r\\nexponential smoothing and SARIMA and then used simple mean and median to aggregate the results from single\\r\\nlearners. A new variation of bagging, Remainder Sieve Bootstrap (RSB) was also proposed to enhance the forecasting\\r\\nresults, and the result showed that the proposed method yielded the best MAPE for both developed and developing\\r\\ncountries.\\r\\nRandom forest (RF) can be seen as an extension of bagging, which further introduces random selection in con\\x02structing individual decision tree learners based on bagging.\\r\\nThe RF firstly uses bootstrap to generate its training sets, and then a decision tree is constructed for each of the\\r\\ntraining set. Features are randomly selected and an optimization criteria is used to guide the split of nodes in construct\\x02ing each decision tree learner. The prediction strategies of RF are: voting for the classification task, and averaging for\\r\\nthe regression task [105].\\r\\nAs the number of learners increases, RF generally converges to a smaller generalization error than bagging. More\\x02over, the training efficiency of RF is often superior to bagging, benefiting from the randomness in constructing single\\r\\nlearners. In [111], an ensemble method combining eight popular forecasting algorithms (multiple linear regression\\r\\n(MLR), ARIMA, SVM, RF, multi-layer perceptron, boosting tree, and multivariate adaptive regression splines) is pro\\x02posed for peak load forecast. Each model in the studies was assigned to a weight by GA. The results showed that\\r\\nSVM and RF had the largest weights, which indicates that these two algorithms contributed more potential gains for\\r\\nenhancing peak load demand forecast accuracy. [112] adopted RF to predict hourly load usage patterns for two educa\\x02tional buildings in North Central Florida, and the feature importance distribution was also produced as a by-product.\\r\\nThe proposed model was compared with the regression tree and SVM, and the results showed that RF had the best\\r\\nsuperiority among all the trained models. Moreover, the feature importance distribution also proved that the influential\\r\\nfeatures changed depending on different education periods, which indicated that the load usage behavior of educational\\r\\nbuildings is highly related to different semesters.\\r\\n3.3.5. Hybrid techniques\\r\\nMany novel hybrid models with satisfactory forecast performance have been proposed in the reviewed papers, and\\r\\nsome of the models have already been discussed in the previous section. TABLE 6 summarised papers utilizing hybrid\\r\\nmodels according to combinations of methods in different forecast methods development stages.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 19 of 34\\nElectrical peak demand forecasting– A review\\r\\nManual + Classic stage. There are a few papers that proposed hybrid models based on the combination of manual\\r\\nstage and classic stage, in which [21] was the earliest work among the obtained papers that utilized the combination\\r\\nof classic forecasting methods with human experience. In this paper, human experts’ opinions were selected as one of\\r\\nthe initial input variables for the daily peak load demand forecast. The proposed modified ARIMA was compared with\\r\\nstandard ARIMA, and the results revealed that the former had the best performance with the lowest MAPE of 1.01%\\r\\nfor predicting the daily peak load of cold Sunday to cold Wednesday.\\r\\nClassic + Advanced stage. Some papers combined methods from the classic stage with methods from the advanced\\r\\nstage. Among which, [48] combined fuzzy logic with a regression model. The fuzzy set theory is good at representing\\r\\nthe uncertainty of the data, which allows the use of additional customer information as inputs to the forecast model,\\r\\nand could achieve more accurate forecasts. [116] used the combination of PCA and MLR to forecast weekly peak\\r\\nload at the distribution level. Firstly, the correlation analysis was utilized to select the important features, and the PCA\\r\\nwas adopted to reduce the redundancy of the input dimensions. Finally, the output from PCA was applied to MLR to\\r\\nperform mid-term peak load prediction. This hybrid model was simpler than many advanced AI-based methods, yet\\r\\ncould also achieve satisfactory forecast accuracy.\\r\\nAdvanced + Advanced stage. From TABLE 6 we can see that most of the proposed hybrid models are the com\\x02binations of the advanced stage methods. Among which, [53] proposed a hybrid method to forecast daily peak load\\r\\nfor Iran. The model was built using the combination of wavelet decomposition, NN, and GA. Historical load data and\\r\\nweather variables from three different cities were used to train the model. The proposed model was also compared\\r\\nwith other advanced models, and the results showed that this model outperformed most of the models. [80], proposed\\r\\na hybrid model combining fuzzy logic with the expert system. In this study, fuzzy logic has the advantage of obtain\\x02ing the uncertain and incomplete information from the real-world data, which will be then considered as the input of\\r\\nthe expert system, such that the hybrid model can make more accurate predictions based on the acquired knowledge.\\r\\n[58] and [92] both combined fuzzy logic with neural network. The advantage of the hybrid model is that the neural\\r\\nnetwork has strong self-learning ability and can make good use of the expression provided by fuzzy logic to produce\\r\\nforecasts with higher accuracy. Moreover, the fuzzy neural network is effective when handling peak loads with strong\\r\\nfluctuations, and it is good at capturing the calendar effect than other advanced models. In [33], the real-valued genetic\\r\\nalgorithm (RGA) based neural network-SVM model was proposed. In the model, the neural network was responsible\\r\\nfor producing the growth index for the forecast target, SVM was adopted to output the deviation value, and the RGA\\r\\nwas adopted to select optimal parameters for the neural network and SVM. The experiment demonstrated that the\\r\\nproposed hybrid model had good performance on both short and mid-term load demand forecast.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 20 of 34\\nElectrical peak demand forecasting– A review\\r\\nTable 5\\r\\nANN/ANN-based methods for peak load demand forecast\\r\\nReference Model detail Input variable Forecast horizon Geographic scope Forecast output Performance\\r\\n[84]\\r\\nPeak load: 3 input neurons,\\r\\n5 hidden neurons and the\\r\\noutput peak load at a given day;\\r\\ntotal load: 3 input neurons,\\r\\n5 hidden neurons and the output\\r\\ntotal load at a given day;\\r\\nhourly load: 6 input neurons,\\r\\n10 hidden neurons, the output\\r\\nload at a given hour.\\r\\nPredicted temperatures;\\r\\npeak load value, total\\r\\nload of a day (a sum),\\r\\nhourly load (1-24 hour ahead),\\r\\nweather variables\\r\\n(average, peak and lowest\\r\\ntemperature at predicted day)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(Seattle/Tacoma)\\r\\nLoad values\\r\\n( peak load value,\\r\\ntotal load of a day\\r\\n(a sum), hourly\\r\\nload (1-24 hour\\r\\nahead))\\r\\nError:\\r\\nPeak load:\\r\\n2.60%;\\r\\nTotal load:\\r\\n3.39%;\\r\\nHourly load:\\r\\n1.64%\\r\\n[98]\\r\\nThe neural network has\\r\\n46 input nodes,\\r\\n60 hidden nodes,\\r\\nand the one output\\r\\nlayer (peak/valley load)\\r\\nHistorical peak/valley loads,\\r\\nweather variables\\r\\n(high/low temperature)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(Taiwan)\\r\\nLoad value\\r\\n(next day’s\\r\\npeak/valley\\r\\nload value)\\r\\nError:\\r\\n1.19%\\r\\n[35]\\r\\nThe lower ANNs:\\r\\n16 input neurons, 8 hidden\\r\\nneuron, 3 output neurons;\\r\\nThe upper ANNs:\\r\\n107 neurons, 2 hidden layers\\r\\neach contains 35 neurons,\\r\\n48 outputs indicating\\r\\n48 half-hourly loads.\\r\\nHistorical loads,\\r\\nweather variables\\r\\n(maximum and minimum\\r\\ntemperatures,\\r\\nmaximum humidity),\\r\\ncalendar variables\\r\\n(time of the day,\\r\\nday of the week, special\\r\\nevent and holidays)\\r\\nHalf-hourly load\\r\\nfor the next day\\r\\n(STPLF)\\r\\nCountry\\r\\n(Kuwait)\\r\\nLoad values\\r\\n(peak load value,\\r\\nvalley load value,\\r\\ndaily load, half-hourly\\r\\nload profile.\\r\\nMPE for\\r\\npeak load:\\r\\n3.18%\\r\\n[23]\\r\\nFeedforward neural\\r\\nnetwork based on:\\r\\nLevenberg-Marquardt\\r\\nback-propagation algorithm,\\r\\nQuasi-Newton\\r\\nback-propagation algorithm\\r\\n+ principle components\\r\\nanalysis (PCA)\\r\\nPeak load of previous day,\\r\\nweather variables\\r\\n(temperature of the day\\r\\n(maximum. minimum),\\r\\ngross minimum temperature.\\r\\nrainfall, evaporation per day,\\r\\nsunshine hours of\\r\\nthe previous day,\\r\\nwind speed, the dry bulb\\r\\ntemperature, the wet bulb\\r\\ntemperature. the vapour pressure,\\r\\nthe relative humidity (%) and\\r\\nthe soil temperature ),\\r\\ncalendar variables\\r\\n(seasons, year number,\\r\\nday of the year, day of the week,\\r\\nholiday)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(a 220 kV substation\\r\\nof Haryana Vidyut\\r\\nPrasaran Nigam Ltd.\\r\\n(HVPNL) )\\r\\nLoad values\\r\\n(peak load value\\r\\nof the current day/\\r\\none to seven days\\r\\nahead)\\r\\nMAPE:\\r\\nLMBP:\\r\\n2.87%,\\r\\nQuasi-Newton\\r\\nBP:\\r\\n2.38%-2.41%\\r\\n[99]\\r\\nFeedforward neural network\\r\\nbased on the conjugate\\r\\ngradient (CG) backpropagation\\r\\nalgorithm (Fletcher–Reeves\\r\\nconjugate gradient\\r\\nbackpropagation method\\r\\n(FRCGBP), Polak–Ribiere\\r\\nconjugate gradient\\r\\nbackpropagation method\\r\\n(PRCGBP), Powell–Beale\\r\\nconjugate gradient backpropagation\\r\\nmethod (PBCGBP) and scaled\\r\\nconjugate gradient backpropagation\\r\\nmethod (SCGBP)) + PCA\\r\\nPeak load of previous day,\\r\\nweather variables\\r\\n(temperature of the day\\r\\n(maximum. minimum),\\r\\ngross minimum temperature.\\r\\nrainfall, evaporation per day,\\r\\nsunshine hours of\\r\\nthe previous day,\\r\\nwind speed, the dry bulb\\r\\ntemperature, the wet bulb\\r\\ntemperature. the vapour pressure,\\r\\nthe relative humidity (%) and\\r\\nthe soil temperature ),\\r\\ncalendar variables\\r\\n(seasons, year number,\\r\\nday of the year, day of the week,\\r\\nholiday)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(a 220 kV substation\\r\\nof Haryana Vidyut\\r\\nPrasaran Nigam Ltd.\\r\\n(HVPNL) )\\r\\nLoad values\\r\\n(peak load value\\r\\nof the current day/\\r\\none to seven days\\r\\nahead)\\r\\nMAPE:\\r\\nFRCGBP:\\r\\n2.43%,\\r\\nPRCGBP:\\r\\n2.31%,\\r\\nPBCGBP:\\r\\n2.32%,\\r\\nSCGBP:\\r\\n2.40%\\r\\n[96]\\r\\nSelf-organizing map (SOM) +\\r\\nFeedforward neural network\\r\\n+ PCA\\r\\nPeak load,\\r\\nweather variables\\r\\n(temperature, wind speed,\\r\\ncloud cover, relative humidity),\\r\\ncalendar variables\\r\\n(day of the week,\\r\\nweek of the month,\\r\\nmonth of year, year number,\\r\\nholidays)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nRegion\\r\\n(Tehran Regional\\r\\nElectric Utility\\r\\nCompany)\\r\\nLoad values\\r\\n(peak load value\\r\\none day ahead)\\r\\nMAPE:\\r\\n1.5%-2.61%\\r\\n[20]\\r\\nDynamic time warping +\\r\\nBespoke gated Recurrent\\r\\nNeural Network (RNN)\\r\\nHistorical load every 30 minutes,\\r\\nweather variables\\r\\n(daily average temperature),\\r\\ncalendar variables\\r\\n(day of the week, holidays)\\r\\nForecast output:\\r\\nsingle load value (daily peak load)\\r\\nDaily peak\\r\\ndemand forecast\\r\\n(STPLF)\\r\\nCountry\\r\\n(competition data\\r\\nfrom the European\\r\\nNetwork on Intelligent\\r\\nTechnologies (EUNITE))\\r\\nLoad values\\r\\n(daily peak load)\\r\\nMAPE:\\r\\n1.01%\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 21 of 34\\nElectrical peak demand forecasting– A review\\r\\nTable 6\\r\\nSummary of papers utilizing hybrid model\\r\\nCombination of the stages Hybrid models with references\\r\\nManual/human expert stage\\r\\n+ Classic stage Human knowledge + ARIMA [21]\\r\\nClassic stage + Classic stage Decomposition + Regression [46]\\r\\nARIMA + Regression [58]\\r\\nClassic stage + Advanced stage\\r\\nRegression + FL [48][113]\\r\\nRegression + GA [114][115]\\r\\nRegression + PCA [116]\\r\\nExponential Smoothing + FL [117]\\r\\nAdvanced stage + Advanced stage\\r\\nFL + ANN [91][45][58][92][29]\\r\\nFL + ES [77][80]\\r\\nFL + SOM [27]\\r\\nFL + GA [118]\\r\\nANNs [90]\\r\\nANN + SOM [119]\\r\\nANN + SOM + PCA [96]\\r\\nANN + GA [17][53][120]\\r\\nGA + RBFN + SVM [33]\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 22 of 34\\nElectrical peak demand forecasting– A review\\r\\n4. Discussion and summary\\r\\nThis section will first give a comparative analysis of the peak demand forecast methods. Then, improving methods\\r\\nfor peak load forecast models are discussed. Finally, a comprehensive summary and discussion of the papers reviewed\\r\\nwill be presented.\\r\\n4.1. Comparative studies of different models\\r\\nEach forecasting method has its advantages and disadvantages, therefore, it is necessary to compare the perfor\\x02mance of various forecasting methods to understand their advantages and limitations. To this end, we will summarize\\r\\nthe existing comparative studies in the literature. Some representative comparative studies are listed in TABLE 7\\r\\nincluding composite/combined models or comparison analysis. A composite model could refer to inter-methods com\\x02posite models (i.e. hybrid models) and result-weighted composite models. Based on different development stages\\r\\nof forecast methods (classic stage and advanced stage), existing comparative studies could be categorized into intra\\x02comparison (e.g. methods within the classic stage) and inter-comparison (e.g. methods from both classic and advanced\\r\\nstage).\\r\\nWhen compared with human expert opinions, the methods in the classic stage showed good performance, as pre\\x02sented in [73]. In the classic stage, regression, as the most popular method in the reviewed papers, are often selected as\\r\\nthe benchmark for building hybrid models [39] [85] [115] [60]. For instance, [39] combined the exponential smooth\\x02ing with regression to forecast the daily peak load demand, and compared the results with the combination of ARIMA\\r\\nand regression. Results showed that the former model could alleviate the bias caused by the latter. [60] also used\\r\\nregression as a benchmark to compare its performance with the hybrid model (Reg-SARIMA-GARCH).\\r\\nIn comparing the methods in classic stage with methods in the advanced stage, [85] utilized ANN and regression\\r\\nto forecast annual peak load demand for Taiwan and the results showed that ANN could achieve a better performance.\\r\\nFor instance, [115] combined GA with symbolic regression to build an STPLF framework, and the results showed the\\r\\nhybrid model could achieve comparable performance to an ANN model.\\r\\nConsidering different distribution and diversity of the data and problem, hybrid methods are not always achieving\\r\\nsatisfactory performance and sometimes could be counterproductive. [58] compared ARMA with a hybrid model (FL\\r\\n+ ANN) for daily peak load demand forecast. The obtained results showed that ARMA performed better when samples\\r\\nwere trained with weekends whereas the proposed hybrid model gained better forecast accuracy when excluding the\\r\\nweekends. [33] combined real-valued GA with SVM and ANN, and the hybrid model was then used for producing daily\\r\\npeak load and its occurring time. The experiment compared the proposed model with other models (real-valued GA\\x02SVM, KF, RBFN). Under the same experimental conditions, surprisingly the real-valued GA-SVM model produced\\r\\nthe worst results.\\r\\nThere are also a few studies that conducted the comparative analysis based on methods in the advanced stage\\r\\nonly. For example, [31] formulated peak load forecast as a classification problem and compared several methods\\r\\nincluding LSTM, SVM, RF, CNN, and Adaboost. The results showed that among all the methods, LSTM had the\\r\\nbest performance following by SVM, RF, and CNN, whereas Adaboost produced unsatisfactory forecast results. [108]\\r\\nproposed a hybrid model (complete ensemble empirical mode decomposition (CEEMDAN) combined with XGBoost),\\r\\nwhich was then compared with other models such as CEEMDAN-RF, RBFN. The results revealed that the proposed\\r\\nmodel generated the best performance, whereas RBFN had the largest MAPE among these three models.\\r\\n4.2. Improving methods for peak load demand forecast models\\r\\nAs aforementioned, hybrid methods by combing different methods could be an option to improve the forecast\\r\\naccuracy. There are some other measures that could be taken to further improve the forecast performance such as\\r\\nthrough optimizing the model inputs (data normalization, feature selection and transformation), and improving the\\r\\nmodels/algorithms (e.g. by integrating clustering methods).\\r\\n4.2.1. Data\\r\\nThe magnitude difference between the data set and various variables is likely to lead to the deviation prediction\\r\\nof the training algorithm. Many training algorithms, such as SVR, require input variables of a similar order of mag\\x02nitude. Beside, in the real scenarios, load data often need to be normalized due to privacy requests [32]. Therefore,\\r\\ndata normalization is a necessary preprocessing step for training the model. Among the reviewed papers, the com\\x02monly used data normalization methods are: zero mean normalization (Z-score normalization) [96][103] and Min-Max\\r\\nnormalization [121][122].\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 23 of 34\\nElectrical peak demand forecasting– A review\\r\\nThe training data size is another important factor that could affect the output accuracy of the model in the training\\r\\nprocess. If the training size is too small, information learned by the model will be insufficient, and the performance\\r\\nwill be poor as a consequence. On the other hand, too much training data will lead to low computational efficiency.\\r\\nTherefore, a good trade-off and balance between the training size and the computation time is worth investigating.\\r\\n[87] considers training data of four different lengths (one week to four weeks) in forecasting sub-hourly load usage and\\r\\ndaily peak load. The training results showed that the larger the training size, the higher the training accuracy of the\\r\\nneural network model. In [61], different training sizes were used to predict the peak load two days to one week ahead.\\r\\nSpecifically, the training data are the hourly load from New South Wales in the past three months, six months, nine\\r\\nmonths, and one year, respectively. The results showed that the model trained with six months of historical data is the\\r\\nbest at predicting the peak load in the coming days among all the models.\\r\\n4.2.2. Feature transformation/Feature selection\\r\\nAs aforementioned, the input variables are often numerous especially in the big data era when training a forecasting\\r\\nmodel. However, many variables may have unrelated characteristics with the target/ response variable , and variables\\r\\nmay also be interdependent, which may easily lead to long training time and decreased forecast performance.\\r\\nFeature transformation and feature selection are usually adopted to address the problem [123].\\r\\nFeature transformation aims to get transformed features by creating a new feature space and the commonly used\\r\\nmethods include PCA, independent component analysis (ICA), and linear discriminant analysis (LDA). Feature selec\\x02tion [124] is to select a subset from the original feature space and commonly used methods include filtering, wrapper\\r\\nand embedding.\\r\\nMost of the reviewed studies utilizing feature selection on peak load forecast adopted filtering and wrapper [87]\\r\\nwhile those using feature transformation utilized PCA. For instance, PCA was compared with correlation analysis in\\r\\n[111], in which the original pattern matrix of the training data is 28×1095. Through correlation analysis, variables\\r\\nwith correlation factors greater than 0.95 were selected. By applying PCA, the dimension of the input matrix was\\r\\nreduced to 11×100. After combining the user-defined neural network to train the model for daily peak load demand\\r\\nforecast, the forecasting accuracy showed that the trained model using PCA was superior to correlation analysis both\\r\\nin computational time and training accuracy.\\r\\n4.2.3. Clustering methods\\r\\nWith the installation of smart meters, high resolution distributed energy consumption data (e.g. at building levels)\\r\\nbecomes available, which provides opportunities in studying different behaviours of forecast models under different\\r\\nbuildings. For instance, [125] compared performance of different forecast models on different buildings and concluded\\r\\nthat clustering buildings based on their historical load usage patterns should be considered to produce more meaning\\r\\ninsights (e.g. to improve forecast accuracy) instead of their predefined building use types. Clustering methods divide\\r\\nthe data into different clusters according to certain standards, such as distance criterion. After clustering, the data\\r\\nwithin the same cluster have great similarity, while the data belonging to different clusters have great difference [126].\\r\\nThe accuracy of peak load forecast can be improved by training different models for different clusters and then obtaining\\r\\nthe aggregated final forecasts.\\r\\nClustering methods can be divided into partition based clustering (e.g. 𝐾-means), hierarchical clustering, density\\x02based clustering (e.g. density-based spatial clustering of applications with noise (DBSCAN)), and model-based cluster\\x02ing (e.g. Gaussian mixture models) [127]. Some studies employing clustering in the peak load forecast are summarized\\r\\nin TABLE 8 where commonly used clustering algorithms in peak load forecast are 𝐾-means, hierarchical clustering,\\r\\nSOM, and fuzzy clustering (FC).\\r\\n𝐾-means is a classical algorithm of partition based clustering, which has high efficiency when handling large-scale\\r\\ndata. Some variants based on 𝐾-means, such as the entropy weighted 𝐾-means [111] have also been used in peak load\\r\\nforecast. Hierarchical clustering can be classified into aggregation hierarchical clustering and splitting hierarchical\\r\\nclustering [133]. For instance, [129] adopted hierarchical clustering to optimize the input daily data of a feed-forward\\r\\nneural network (FNN) to predict load usage during a peak period, and the results demonstrated that the FNN could\\r\\nconverge more quickly and produce more accurate results. SOM is a commonly used clustering method owing to its\\r\\nunsupervised feature. [96] firstly utilized SOM to cluster peak loads, then each cluster was trained separately by FNN\\r\\nto get a specified model. Results showed that the proposed hybrid method is effective for daily peak load forecast.\\r\\nThe above methods belong to hard clustering since each data point can only be assigned to a single cluster. Instead,\\r\\nfuzzy clustering such as Fuzzy C-means is a soft clustering method where each observation can belong to multiple\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 24 \\nElectrical peak demand forecasting– A review\\r\\nclusters with corresponding membership coefficients [134]. For instance, [131] adopted fuzzy clustering to cluster peak\\r\\nload patterns according to the working/ non-working days. [132] combined fuzzy clustering with FNN to forecast load\\r\\ncurves during peak load period.\\r\\n4.3. Summary of the reviewed studies\\r\\nTABLE 9 gives a comprehensive summary of the reviewed papers including their forecasting periods, forecast\\r\\noutputs, input variables, improving methods and geographical scope.\\r\\nIt is worth mentioning that the classification of peak load forecast methods into three stages (i.e. manual, classic\\r\\nand advanced) generally aligns with the evolving of power systems. In the manual and classic stage, the traditional\\r\\nenergy-intensive industry dominated the electricity market with a relative stable peak demand patterns. Peak load\\r\\nforecasting based on statistical methods were commonly used. With the development of smart grids and the changing\\r\\nenergy landscape at both demand side (e.g. demand side management and electric vehicles) and supply side (e.g.\\r\\nintermittent renewable energy supply at both transmission and distribution level), peak demand patterns become more\\r\\nrandom and less predictable. As such, more advanced methods that can better take advantage of big data and capture\\r\\ncomplex patterns such as deep learning and hybrid machine learning methods are preferable choices.\\r\\nIn addition, different from traditional load forecasting, the occurrence time and magnitudes of the peak demand\\r\\nare equally important in peak load forecasting. The peak load occurrence time is a field that may be more related to\\r\\nextreme value theories or quantile regression because of some rare events. Moreover, considering the uncertainty of\\r\\npeak load, it is also an effective forecasting method to take the peak load as anomalous data to quantify its occurrence\\r\\nprobability and magnitude probability [135].\\r\\nFor the input variables, historical load data, weather variables, and calendar variables were usually used in STPLF.\\r\\nOn the other hand, economic and other variables such as population growth rate were frequently used in MTPLF and\\r\\nLTPLF. Moreover, due to the high randomness of peak load, the forecast model is greatly affected by small probability\\r\\nevents such as extreme weather and accidental events. Accidental events vary among individuals/ entities, whose\\r\\nimpact is often limited to a small range and therefore difficult to forecast. Although extreme weather also belongs\\r\\nto the small probability event, its impact is usually well studied. For instance, it is necessary to focus on climatic\\r\\nfactors such as the maximum (lowest) temperature, the duration of the high (low) temperature, and the humidity. The\\r\\nmaximum (lowest) temperature determines the peak load value. The duration of high (low) temperature affects the\\r\\npeak occurring time range, and the humidity further aggravates the difference between the physical and the actual\\r\\ntemperature, thus affecting consumers’ electricity usage decisions.\\r\\nMost of the forecast methods utilizing improving techniques (e.g. clustering) belong to the advanced stage of peak\\r\\nload demand forecast, which indicates that research attention to this field is increasing. However, it is worth pointing\\r\\nout that although there are many clustering methods available, the current load curvy-based clustering heavily relies\\r\\non additional physical and social-economic information from entities/users (in other words, the domain knowledge) in\\r\\norder to properly interpret clustering results. More efforts on how to effectively incorporate domain knowledge into\\r\\nthe forecast methods and improving techniques are needed.\\r\\nAs for the forecast geographic scope, researchers usually consider peak load forecast over a wide range of regions\\r\\nor countries during the classic stage. With the development of smart grids and the installation of smart meters at\\r\\nthe local level, there are more high resolution temporal and spatial data becoming available [127]. In addition, the\\r\\nincreasing penetration of distributed energy resources (e.g. electric vehicles and microgrids) coupled with distributed\\r\\nintelligence and local energy applications [50] brings the operation and maintenance of power systems into a new era\\r\\nof disaggregated environment. From the perspective of peak load forecast , the highly random human activities will\\r\\nhave higher impact on the forecast performance in small geographic areas (e.g. community level) than aggregated level\\r\\n(e.g. region/country) [174]. Therefore, more research efforts on the interaction between electricity usage decisions of\\r\\nend users and disaggregated load forecasting are needed in the future.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 25 of 34\\nElectrical peak demand forecasting– A review\\r\\nTable 7\\r\\nPapers with composite model/comparative analysis\\r\\nReference Model/Experiment type Detail Forecast contents Performance\\r\\n[73] Composite model\\r\\nBuilt an decision support\\r\\nsystem (DSS) to compare\\r\\nARIMA, exponential smoothing\\r\\nand human expert suggestions\\r\\nMonthly peak load demand\\r\\nfor UAE\\r\\nDifferent models suited for\\r\\ndifferent areas (Abu-Dhabi:\\r\\nARIMA(1,1,1)(0,1,2),\\r\\nDubai: Exponential smoothing,\\r\\nSharjah: ARIMA(1,0,2)(0,1,3));\\r\\nDSS outperformed human experts\\r\\n[85] Comparative analysis ANN vs Regression\\r\\nAnnual peak load demand\\r\\nfor 4 regions of Taiwan\\r\\n(Region of China)\\r\\nMAPE of ANN vs Regression:\\r\\nNorthern: 1.06% vs 2.45%\\r\\nCentral: 1.73% vs 8.52%\\r\\nSouthern: 2.48% vs 8.29%\\r\\nEastern: 3.62% vs 4.10%\\r\\n[39]\\r\\nComposite model +\\r\\ncomparative analysis\\r\\nApplied exponential smoothing\\r\\nto regression model.\\r\\nRegression + ARIMA vs\\r\\nRegression + exponential smoothing\\r\\nHourly load, daily peak\\r\\ndemand for PG&E\\r\\nRegression + exponential\\r\\nsmoothing could almost eliminate\\r\\nthe negative bias caused by\\r\\nregression + ARIMA\\r\\n[115]\\r\\nComposite model +\\r\\ncomparative analysis\\r\\nGP + symbolic regression\\r\\nvs MLP (ANN)\\r\\nDaily peak load demand for\\r\\na distribution power system\\r\\nin Romania\\r\\nMaximum APE of GP +\\r\\nsymbolic regression vs MLP:\\r\\n10% vs 11%\\r\\nMPE of GP + symbolic\\r\\nregression vs MLP:\\r\\n0.2% vs 0.1%\\r\\n[60]\\r\\nComposite model +\\r\\ncomparative analysis\\r\\nPiece-wise linear regression vs\\r\\nSARIMA vs a SARIMA with\\r\\ngeneralized autoregressive conditional\\r\\nheteroskedastic errors\\r\\n(SARIMA-GARCH) vs\\r\\nregression-SARIMA-GARCH\\r\\n(Reg-SARIMA-GARCH)\\r\\nDaily peak load demand for\\r\\nSouth Africa\\r\\nMAPE for regression vs SARIMA\\r\\nvs SARIMA-GARCH vs\\r\\nReg-SARIMA-GARCH:\\r\\n2.77% vs 1.47% vs 1.43%\\r\\nvs 1.42%\\r\\n[80]\\r\\nComposite model +\\r\\ncomparative analysis\\r\\nFuzzy logic (FL) + expert system (ES)\\r\\nvs NN with similar structure\\r\\nMorning and afternoon\\r\\nvalley, noon and evening\\r\\npeak for different seasons\\r\\nYearly ME for FL + ES vs NN:\\r\\n2.45% vs 2.56%\\r\\n[58]\\r\\nComposite model +\\r\\ncomparative analysis ANFIS (Fuzzy logic + NN) vs ARMA\\r\\nDaily peak load demand forecast\\r\\nfor a utility company\\r\\nin Malaysia\\r\\nAverage MAPE for ANFIS\\r\\nvs ARMA:\\r\\nIncluding weekends:\\r\\n7.26% vs 2.45%\\r\\nExcluding weekends:\\r\\n1.95% vs 4.67%\\r\\n[33]\\r\\nComposite model +\\r\\ncomparative analysis\\r\\nRGA-SVM (real-valued genetic\\r\\nalgorithm-SVM) vs KF vs RBF\\r\\nvs RGA based NN-SVM\\r\\n(NN thread to output yearly growth\\r\\nindex and SVM thread to output\\r\\nmoment-specific forecast )\\r\\nDaily peak load demand\\r\\nwith occurring time\\r\\nMAPE for RGA-SVM vs KF vs\\r\\nRBF vs RGA based NN-SVM:\\r\\none week:14.26% vs 6.63%\\r\\nvs 6.39% vs 3.20%\\r\\nOccurring time: Hit rate of RGA\\r\\nbased NN-SVM (one-hour time\\r\\ndeviation tolerance): 91.7%\\r\\n[31] Comparative analysis\\r\\nRegard peak hour forecast\\r\\nas a classification problem;\\r\\nNaive Bayes vs SVM vs Random\\r\\nForest (RF) vs AdaBoost vs CNN\\r\\nvs LSTM vs Stacked\\r\\nAutoEncoder (SAE)\\r\\nDaily peak hour one day ahead\\r\\nfor Ontario\\r\\nAccuracy for Naive Bayes vs\\r\\nSVM vs RF vs AdaBoost\\r\\nvs CNN vs LSTM vs SAE:\\r\\nWinter: 0.83 vs 0.97 vs 0.97\\r\\nvs 0.82 vs 0.97 vs 0.98 vs 0.97\\r\\nSummer: 0.66 vs 0.95 vs 0.94\\r\\nvs 0.63 vs 0.94 vs 0.95 vs 0.94\\r\\n[108]\\r\\nComposite model +\\r\\ncomparative analysis\\r\\nCEEMDAN(complete ensemble\\r\\nempirical mode decomposition)\\r\\n-XGBoost vs CEEMDAN-RF vs\\r\\nRBFNN vs PSO-SVM vs LSSVM\\r\\n(least squares SVM)\\r\\nDaily energy consumption\\r\\n+ daily peak power +\\r\\ndaily water delivery\\r\\nMAPE for CEEMDAN-XGBoost\\r\\nvs CEEMDAN-RF vs RBFNN vs\\r\\nPSO-SVM vs LSSVM vs XGBoost:\\r\\ndaily energy consumption: 4.85%\\r\\nvs 6.26% vs 7.67% vs 7.92 vs\\r\\n7.87% vs 8.06%\\r\\ndaily peak power: 5.99% vs\\r\\n6.40% vs 9.25% vs 8.15% vs\\r\\n8.89% vs 9.13%\\r\\ndaily water delivery: 5.09% vs\\r\\n6.31% vs 8.30% vs 8.08%\\r\\nvs 8.37% vs 8.32%\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 26 of 34\\nElectrical peak demand forecasting– A review\\r\\nTable 8\\r\\nClustering methods for improving the performance of peak load forecast\\r\\nClustering methods References\\r\\nK-means and its extensions [111],[128],[122]\\r\\nHierarchical clustering [129],[130]\\r\\nFuzzy clustering [117],[121],[131],[132],\\r\\nSOM [15],[94],[96]\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 27 of 34\\nElectrical peak demand forecasting– A review\\r\\nTable 9\\r\\nComprehensive summary for the reviewed literature FS/FT: Feature Selection/Feature\\r\\nTransformation; H: Historical load data; W: Weather variables; C: Calendar variables;\\r\\nE/O: Economic/Other variables; V: Peak values; V+T: Peak values+Occurring time; LP:\\r\\nLoad Profiles.\\r\\nMethods Forecast\\r\\nperiods\\r\\nImproving methods Input variables Geographic scope Output References\\r\\nClustering FS/FT H W C E/O Region Country City Building/\\r\\nHousehold V V+T LP\\r\\nRegression\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\r\\n[136],[46],[39],[42], [28], [45], [48],\\r\\n[41],[114],[131],[130],[32],[137],[40],\\r\\n[44],[138],[111], [122],[139], [140]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ [38],[116],[141],[142],[43]\\r\\nLTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ [89],[143],[144],[16]\\r\\nStochastic\\r\\ntime series\\r\\nmodels\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [61],[92],[68],[21],[69],[62], [65],[58],\\r\\n[59],[46][60],[66],[67]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [30],[63],[57],[13],[50],[55],[64]\\r\\nLTPLF\\r\\nTime series\\r\\ndecomposition\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [54],[53],[52],[29]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ [49],[50],[47],[51],[55]\\r\\nLTPLF\\r\\nExponential\\r\\nsmoothing\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ [39],[74],[117]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ ✓ [46],[57],[73]\\r\\nLTPLF\\r\\nKalman\\r\\nfiltering\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [77],[33]\\r\\nMTPLF\\r\\nLTPLF\\r\\nGrey\\r\\nprediction\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ [78]\\r\\nMTPLF ✓ ✓ ✓ [30]\\r\\nLTPLF ✓ ✓ ✓ ✓ [34]\\r\\nANN\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\r\\n[45],[85],[145],[90],[92],[87],[146],\\r\\n[147],[139],[148],[94],[95],[149],[84],[150],\\r\\n[98],[151],[152],[153],[132],[115],[129],[119],\\r\\n[154],[155],[120],[156],[29],[157],[158],\\r\\n[59],[159],[160],[161],[52],[91],\\r\\n[23],[162],[163],[35],[27],[96],[23]\\r\\n[116],[83],[88],[33]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ [164],[126],[24],[22],[165],[119]\\r\\nLTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ [99]\\r\\nExpert\\r\\nsystem\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ [79],[80]\\r\\nMTPLF\\r\\nLTPLF ✓ ✓ ✓ ✓ ✓ ✓ [166]\\r\\nFuzzy\\r\\nlogic\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [79],[91],[45],[48],[113],[21],[167],\\r\\n[58],[121],[92],[117],[168]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ [118]\\r\\nLTPLF\\r\\nGenetic\\r\\nalgorithm\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [114],[115],[118],[17],[33],[53],[111]\\r\\nMTPLF\\r\\nLTPLF ✓ ✓ ✓ [81]\\r\\nSVMs\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ [26],[78],[102],[103],[169],[104],[170],[31]\\r\\nMTPLF\\r\\nLTPLF\\r\\nBoosting\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ [107],[108]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ ✓ [106]\\r\\nLTPLF ✓ ✓ ✓ ✓ ✓ ✓ [106]\\r\\nBagging\\r\\nSTPLF\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ ✓ [110]\\r\\nLTPLF\\r\\nRF\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ [111],[112],[171]\\r\\nMTPLF\\r\\nLTPLF\\r\\nCNN\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ [31],[172]\\r\\nMTPLF\\r\\nLTPLF\\r\\nRNN\\r\\nSTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [20],[173],[31]\\r\\nMTPLF ✓ ✓ ✓ ✓ ✓ ✓ ✓ [93],[97]\\r\\nLTPLF\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 28 of 34\\nElectrical peak demand forecasting– A review\\r\\n5. Conclusion and future work\\r\\nMotived by the importance of peak load demand forecast from the perspectives of electricity market stakeholders,\\r\\nthis paper carries out a systematic review of the peak load demand forecast, which aims to summarize existing studies\\r\\non the topic and provide guidance for future research. First, we aim to provide an unified problem definition for peak\\r\\ndemand load forecast. Then, the peak load demand forecast methods were categorized into three stages based on\\r\\ntheir development timeline, and a thorough review of relevant methods in each stage was conducted. Moreover, a\\r\\ncomparative analysis of different forecast methods was presented, and useful improving techniques for enhancing the\\r\\nforecast performance were discussed. Finally, a comprehensive summary of reviewed papers on the peak load forecast\\r\\nframework was presented with possible future research directions.\\r\\nWith high-resolution load data (e.g. residential smart meter data) becoming increasingly available, data privacy is\\r\\nan important issue that needs to be addressed. In the new digital era, using private encryption algorithms to protect\\r\\nthe consumers’ data has become an essential task that researchers must deal with [175]. There are challenges in terms\\r\\nof electricity data transmission and storage compliance, security and privacy protection [176]. In addition, it is well\\r\\nknown that the data size and quality usually determines the training quality of machine learning models. However,\\r\\nin practice relevant data that are deemed necessary for a forecast task might be owned by different organizations. To\\r\\nmake accurate predictions, it is necessary to combine diverse data sources from different organizations in building the\\r\\nmodel. This could be achieved by aggregating all the data sources into a third party central database, however, it may\\r\\nface inevitable security risks because of the central distribution of the data [177]. Therefore, designing the forecasting\\r\\nframework under the premise of meeting data privacy, security, and regulatory requirements (e.g. through federated\\r\\nlearning [178]) is an important future research trend on peak load demand forecast.\\r\\nReferences\\r\\n[1] S. Zhou, M. A. Brown, Smart meter deployment in europe: A comparative case study on the impacts of national policy schemes, Journal of\\r\\nCleaner Production 144 (2017) 22–32.\\r\\n[2] Office of gas and electricity markets (ofgem), transition to smart meters., https://www.ofgem.gov.uk/gas/retail-market/metering/transition\\x02smart-meters/, 2020. [accessed 18 December 2020].\\r\\n[3] F. Rahimi, A. Ipakchi, Demand response as a market resource under the smart grid paradigm, IEEE Transactions on Smart Grid 1 (2010)\\r\\n82–88.\\r\\n[4] A. Sheffrin, H. Yoshimura, D. LaPlante, B. Neenan, Harnessing the power of demand, The Electricity Journal 21 (2008) 39–50.\\r\\n[5] M. J. C. Espinoza, J. U. Suykens, R. U. Belmans, B. U. J. De Moor, Electric load forecasting - using kernel based modeling for nonlinear\\r\\nsystem identification, IEEE Control Systems Magazine 27 (2007) 43–57.\\r\\n[6] I. Koutsopoulos, L. Tassiulas, Control and optimization meet the smart power grid: Scheduling of power demands for optimal energy\\r\\nmanagement, in: Proceedings of the 2nd International Conference on Energy-efficient Computing and Networking, 2011, pp. 41–50.\\r\\n[7] J. Saebi, H. Taheri, J. Mohammadi, S. S. Nayer, Demand bidding/buyback modeling and its impact on market clearing price, in: 2010 IEEE\\r\\nInternational Energy Conference, IEEE, 2010, pp. 791–796.\\r\\n[8] P. Cappers, C. Goldman, D. Kathan, Demand response in us electricity markets: Empirical evidence, Energy 35 (2010) 1526–1535.\\r\\n[9] S. L. Koh, Y. S. Lim, Evaluating the economic benefits of peak load shifting for building owners and grid operator, in: 2015 International\\r\\nConference on Smart Grid and Clean Energy Technologies (ICSGCE), IEEE, 2015, pp. 30–34.\\r\\n[10] J. Sardi, N. Mithulananthan, M. Gallagher, D. Q. Hung, Multiple community energy storage planning in distribution networks using a\\r\\ncost-benefit analysis, Applied energy 190 (2017) 453–463.\\r\\n[11] H. Mao, X.-J. Zeng, G. Leng, Y.-J. Zhai, J. A. Keane, Short-term and midterm load forecasting using a bilevel optimization model, IEEE\\r\\nTransactions on Power Systems 24 (2009) 1080–1090.\\r\\n[12] Sciencedirect (sd), https://www.sciencedirect.com/, 2020. [accessed 16 August 2020].\\r\\n[13] the institution of electrical and electronics engineers (ieee), https://www.ieee.org/, 2020. [accessed 16 August 2020].\\r\\n[14] P. E. Mcsharry, S. Bouwman, G. Bloemhof, Probabilistic forecasts of the magnitude and timing of peak electricity demand, IEEE Transactions\\r\\non Power Systems (2005) 1166–1172.\\r\\n[15] Z. Wang, Developed case-based reasoning system for short-term load forecasting, in: 2006 IEEE Power Engineering Society General\\r\\nMeeting, IEEE, 2006, pp. 6–pp.\\r\\n[16] R. J. Hyndman, S. Fan, Density forecasting for long-term peak electricity demand, IEEE Transactions on Power Systems 25 (2009) 1142–\\r\\n1153.\\r\\n[17] G. M. Khan, S. Khan, F. Ullah, Short-term daily peak load forecasting using fast learning neural network, in: 2011 11th International\\r\\nConference on Intelligent Systems Design and Applications, IEEE, 2011, pp. 843–848.\\r\\n[18] T. Yalcinoz, U. Eminoglu, Short term and medium term power distribution load forecasting by neural networks, Energy Conversion and\\r\\nManagement 46 (2005) 1393–1405.\\r\\n[19] H. G. Stoll, L. J. Garver, Least-cost electric utility planning, Wiley New York, 1989.\\r\\n[20] Z. Yu, Z. Niu, W. Tang, Q. Wu, Deep learning for daily peak load forecasting–a novel gated recurrent neural network combining dynamic\\r\\ntime warping, Ieee Access 7 (2019) 17184–17194.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 29 of 34\\nElectrical peak demand forecasting– A review\\r\\n[21] N. Amjady, Short-term hourly load forecasting using time-series modeling with peak load estimation capability, IEEE Transactions on Power\\r\\nSystems 16 (2001) 798–805.\\r\\n[22] S. Phimphachan, K. Chamnongthai, P. Kumhom, N. Jittiwarangkul, A. Sangswang, Energy and peak load forecast models using neural\\r\\nnetwork for fast developing area, in: IEEE International Symposium on Communications and Information Technology, 2004. ISCIT 2004.,\\r\\nvolume 1, IEEE, 2004, pp. 389–393.\\r\\n[23] L. Saini, M. Soni, Artificial neural network based peak load forecasting using levenberg–marquardt and quasi-newton methods, IEE\\r\\nProceedings-Generation, Transmission and Distribution 149 (2002) 578–584.\\r\\n[24] K. Nagasaka, M. Al Mamun, Long-term peak demand prediction of 9 japanese power utilities using radial basis function networks, in: IEEE\\r\\nPower Engineering Society General Meeting, 2004., IEEE, 2004, pp. 315–322.\\r\\n[25] R. P. Broadwater, A. Sargent, Estimating substation peaks from load research data, IEEE Transactions on Power Delivery 12 (1997) 451–456.\\r\\n[26] J.-Z. Wang, L. Wu, H.-Y. Lu, Special periods peak load analysis and superior forecasting method based on ls-svm, in: 2007 International\\r\\nConference on Wavelet Analysis and Pattern Recognition, volume 1, IEEE, 2007, pp. 249–253.\\r\\n[27] E. Contaxi, C. Delkis, S. Kavatza, C. Vournas, The effect of humidity in a weather-sensitive peak load forecasting model, in: 2006 IEEE\\r\\nPES Power Systems Conference and Exposition, 2006, pp. 1528–1534.\\r\\n[28] R. Ramanathan, R. Engle, C. W. J. Granger, F. Vahid-Araghi, C. Brace, Short-run forecasts of electricity loads and peaks, International\\r\\nJournal of Forecasting 13 (1997) 161–174.\\r\\n[29] S. Nuchprayoon, Forecasting of daily load curve on monthly peak day using load research data and harmonics model, in: 2016 6th IEEE\\r\\nInternational Conference on Control System, Computing and Engineering (ICCSCE), 2016, pp. 338–342.\\r\\n[30] J. Zhengyuan, Electricity consumption forecasting in peak load month based on variable weight combination forecasting model, in: 2008\\r\\nIEEE International Conference on Automation and Logistics, IEEE, 2008, pp. 1265–1269.\\r\\n[31] J. Liu, L. E. Brown, Prediction of hour of coincident daily peak load, in: 2019 IEEE Power Energy Society Innovative Smart Grid Tech\\x02nologies Conference (ISGT), 2019, pp. 1–5.\\r\\n[32] A. Goia, C. May, G. Fusai, Functional clustering and linear regression for peak load forecasting, International Journal of Forecasting 26\\r\\n(2010) 700 – 711.\\r\\n[33] J. Xue, Z. Xu, J. Watada, Building an integrated hybrid model for short-term and mid-term load forecasting with genetic optimization,\\r\\nInternational journal of innovative computing, information & control: IJICIC 8 (2012) 7381–7391.\\r\\n[34] H.-T. Yang, T.-C. Liang, K.-R. Shih, C.-L. Huang, Power system yearly peak load forecasting: a grey system modeling approach, in:\\r\\nProceedings 1995 International Conference on Energy Management and Power Delivery EMPD’95, volume 1, IEEE, 1995, pp. 261–266.\\r\\n[35] A. S. Alfuhaid, M. A. El-Sayed, M. S. Mahmoud, Cascaded artificial neural networks for short-term load forecasting, Power Systems IEEE\\r\\nTransactions on 12 (1997) 1524–1529.\\r\\n[36] R. J. Hyndman, G. Athanasopoulos, Forecasting: Principles and practice, London: Bowker-Saur. Pharo (2014).\\r\\n[37] D. K. A. Gillies, B. Bernholtz, P. J. Sandiford, A new approach to forecasting daily peak loads [includes discussion], Power Apparatus &\\r\\nSystems Part III Transactions of the American Institute of Electrical Engineers 75 (1956).\\r\\n[38] J. E. Turner, D. J. Downing, J. S. Bogard, Statistical methods in radiation physics, Wiley Online Library, 2012.\\r\\n[39] A. D. Papalexopoulos, A regression-based approach to short-term system load forecasting, IEEE Transactions on Power Systems 5 (1990)\\r\\n1535–1547.\\r\\n[40] V. Mtembo, G. A. Taylor, A. Ekwue, A novel econometric model for peak demand forecasting, in: 2014 49th International Universities\\r\\nPower Engineering Conference (UPEC), 2014, pp. 1–6.\\r\\n[41] H. Alfares, M. Nazeeruddin, Regression-based methodology for daily peak load forecasting, in: Proceedings of the 2nd International\\r\\nConference on Operations and Quantitative Management, volume 3, 1999, pp. 468–471.\\r\\n[42] T. Haida, S. Muto, Regression based peak load forecasting using a transformation technique, IEEE Transactions on Power Systems 9 (1994)\\r\\n1788–1794.\\r\\n[43] P. Atsawathawichok, P. Teekaput, T. Ploysuwan, Long term peak load forecasting in thailand using multiple kernel gaussian process, in:\\r\\n2014 11th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology\\r\\n(ECTI-CON), IEEE, 2014, pp. 1–4.\\r\\n[44] Y. K. Bichpuriya, S. Soman, A. Subramanyam, Non-parametric probability density forecast of an hourly peak load during a month, in: 2014\\r\\nPower Systems Computation Conference, IEEE, 2014, pp. 1–6.\\r\\n[45] T. Haida, S. Muto, Y. Takahashi, Y. Ishi, Peak load forecasting using multiple-year data with trend data processing techniques, Electrical\\r\\nEngineering in Japan 124 (1998) 7–16.\\r\\n[46] E. Barakat, M. Qayyum, M. Hamed, S. Al Rashed, Short-term peak demand forecasting in fast developing utility with inherit dynamic\\r\\nload characteristics. i. application of classical time-series methods. ii. improved modelling of system dynamic load characteristics, IEEE\\r\\nTransactions on Power Systems 5 (1990) 813–824.\\r\\n[47] E. Barakat, M. Eissa, Forecasting monthly peak demand in fast growing electric utility using a composite multiregression-decomposition\\r\\nmodel, in: IEE Proceedings C (Generation, Transmission and Distribution), volume 136, IET, 1989, pp. 35–41.\\r\\n[48] J. Nazarko, W. Zalewski, The fuzzy regression approach to peak load estimation in power distribution systems, IEEE Transactions on Power\\r\\nSystems 14 (1999) 809–814.\\r\\n[49] P. Gupta, A stochastic approach to peak power-demand forecasting in electric utility systems, IEEE Transactions on Power Apparatus and\\r\\nSystems (1971) 824–832.\\r\\n[50] S. Fong, H. Yang, The six technical gaps between intelligent applications and real-time data mining: a critical review, journal of emerging\\r\\ntechnologies in web intelligence 3 (2011) 63–73.\\r\\n[51] N. Dongxiao, Z. Yunyun, L. Jinpeng, The application of time series seasonal multiplicative model and garch error amending model on\\r\\nforecasting the monthly peak load, in: 2009 International Forum on Computer Science-Technology and Applications, volume 3, IEEE, 2009,\\r\\npp. 135–138.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 30 of 34\\nElectrical peak demand forecasting– A review\\r\\n[52] J.-G. Choi, J.-K. Park, K.-H. Kim, J.-C. Kim, A daily peak load forecasting system using a chaotic time series, in: Proceedings of International\\r\\nConference on Intelligent System Application to Power Systems, IEEE, 1996, pp. 283–287.\\r\\n[53] M. Moazzami, A. Khodabakhshian, R. Hooshmand, A new hybrid day-ahead peak load forecasting method for iran’s national grid, Applied\\r\\nEnergy 101 (2013) 489–501.\\r\\n[54] M. O. M. Mahmoud, F. Mhamdi, M. Jaidane-Saidane, Long term multi-scale analysis of the daily peak load based on the empirical mode\\r\\ndecomposition, in: 2009 IEEE Bucharest PowerTech, IEEE, 2009, pp. 1–6.\\r\\n[55] M. Beiraghi, A. Ranjbar, Discrete fourier transform based approach to forecast monthly peak load, in: 2011 Asia-Pacific Power and Energy\\r\\nEngineering Conference, IEEE, 2011, pp. 1–5.\\r\\n[56] Z. El-Razaz, N. Al-Mohawes, Weekly peak load forecasting for fast-developing cities, Canadian Electrical Engineering Journal 11 (1986)\\r\\n184–187.\\r\\n[57] E. Barakat, J. Al-Qassim, S. Al Rashed, New model for peak demand forecasting applied to highly complex load characteristics of a fast\\r\\ndeveloping area, in: IEE Proceedings C (Generation, Transmission and Distribution), volume 139, IET, 1992, pp. 136–140.\\r\\n[58] A. R. Fadhilah, S. Suriawati, H. H. Amir, Z. A. Izham, S. Mahendran, Malaysian day-type load forecasting, in: 2009 3rd International\\r\\nConference on Energy and Environment (ICEE), 2009, pp. 408–411.\\r\\n[59] K. Mahmud, J. Ravishankar, M. J. Hossain, Z. Y. Dong, The impact of prediction errors in the domestic peak power demand management,\\r\\nIEEE Transactions on Industrial Informatics 16 (2020) 4567–4579.\\r\\n[60] C. Sigauke, D. Chikobvu, Prediction of daily peak electricity demand in south africa using volatility forecasting models, Energy Economics\\r\\n33 (2011) 882–888.\\r\\n[61] M. As’ ad, Finding the best arima model to forecast daily peak electricity demand, in: Proceedings of the Fifth Annual ASEARC Conference\\r\\n- Looking to the future - Programme and Proceedings, University of Wollongong, 2012, pp. 1–4.\\r\\n[62] R. M. Roken, M. A. Badri, Time series models for forecasting monthly electricity peak-load for dubai, Chancellor’s Undergraduate Research\\r\\nAward (2006) 1–14.\\r\\n[63] Y. Kareem, A. R. Majeed, Monthly peak-load demand forecasting for sulaimany governorate using sarima., in: 2006 IEEE/PES Transmission\\r\\n& Distribution Conference and Exposition: Latin America, IEEE, 2006, pp. 1–5.\\r\\n[64] S. R. Rallapalli, S. Ghosh, Forecasting monthly peak demand of electricity in india—a critique, Energy Policy 45 (2012) 516–520.\\r\\n[65] S. Ghosh, Univariate time-series forecasting of monthly peak demand of electricity in northern india, International Journal of Indian Culture\\r\\nand Business Management 1 (2008) 466–474.\\r\\n[66] F. A. Razak, A. H. Hashim, I. Z. Abidin, M. Shitan, Moving holidays’ effects on the malaysian peak daily load, in: 2010 IEEE International\\r\\nConference on Power and Energy, 2010, pp. 906–910.\\r\\n[67] C. Heylman, Y. G. Kim, J. Wang, Forecasting energy trends and peak usage at the university of virginia, in: 2015 Systems and Information\\r\\nEngineering Design Symposium, 2015, pp. 362–368.\\r\\n[68] A. Ananthasingam, A. Atputharajah, Forecast daily night peak electric power demand in sri lankan power system, in: 2015 IEEE 10th\\r\\nInternational Conference on Industrial and Information Systems (ICIIS), 2015, pp. 238–243.\\r\\n[69] C. Hor, S. J. Watson, S. Majithia, Daily load forecasting and maximum demand estimation using arima and garch, in: 2006 International\\r\\nConference on Probabilistic Methods Applied to Power Systems, 2006, pp. 1–6.\\r\\n[70] E. Ostertagová, O. Ostertag, The simple exponential smoothing model, in: The 4th International Conference on Modelling of Mechanical\\r\\nand Mechatronic Systems, Technical University of Košice, Slovak Republic, Proceedings of conference, 2011, pp. 380–384.\\r\\n[71] E. S. Gardner Jr, Exponential smoothing: The state of the art, Journal of forecasting 4 (1985) 1–28.\\r\\n[72] R. A. Yaffee, M. McGee, An introduction to time series analysis and forecasting: with applications of SAS® and SPSS®, Elsevier, 2000.\\r\\n[73] M. A. Badri, A. Al-Mutawa, D. Davis, D. Davis, Edssf: A decision support system (dss) for electricity peak-load forecasting, Energy 22\\r\\n(1997) 579–589.\\r\\n[74] J. W. Taylor, Short-term electricity demand forecasting using double seasonal exponential smoothing, Journal of the Operational Research\\r\\nSociety 54 (2003) 799–805.\\r\\n[75] B. Billah, M. L. King, R. D. Snyder, A. B. Koehler, Exponential smoothing model selection for forecasting, International journal of\\r\\nforecasting 22 (2006) 239–247.\\r\\n[76] S. J. Julier, J. K. Uhlmann, New extension of the kalman filter to nonlinear systems, in: Signal processing, sensor fusion, and target\\r\\nrecognition VI, volume 3068, International Society for Optics and Photonics, 1997, pp. 182–193.\\r\\n[77] P. Dash, H. Satpathy, S. Rahman, Short term daily average and peak load predications using a hybrid intelligent approach, in: Proceedings\\r\\n1995 International Conference on Energy Management and Power Delivery EMPD’95, volume 2, IEEE, 1995, pp. 565–570.\\r\\n[78] L. Ran, G. Chaoyun, The relevance analysis between electrical day peak load and meteorological index based on wavelet denoising and\\r\\nsvm, in: 2008 Third International Conference on Electric Utility Deregulation and Restructuring and Power Technologies, IEEE, 2008, pp.\\r\\n788–793.\\r\\n[79] Y.-Y. Hsu, K.-L. Ho, Fuzzy expert systems: an application to short-term load forecasting, in: IEE Proceedings C (Generation, Transmission\\r\\nand Distribution), volume 139, IET, 1992, pp. 471–477.\\r\\n[80] S. Kiartzis, A. Bakirtzis, J. Theocharis, G. Tsagas, A fuzzy expert system for peak load forecasting. application to the greek power system,\\r\\nin: 2000 10th Mediterranean Electrotechnical Conference. Information Technology and Electrotechnology for the Mediterranean Countries.\\r\\nProceedings. MeleCon 2000 (Cat. No. 00CH37099), volume 3, IEEE, 2000, pp. 1097–1100.\\r\\n[81] K. M. El-naggar, K. A. Al-rumaih, Electric load forecasting using genetic based algorithm, optimal filter estimator and least error squares\\r\\ntechnique: Comparative study, World Academy of Science, Engineering and Technology 6 (2005) 138–142.\\r\\n[82] S. Agatonovic-Kustrin, R. Beresford, Basic concepts of artificial neural network (ann) modeling and its application in pharmaceutical\\r\\nresearch, Journal of Pharmaceutical & Biomedical Analysis 22 (2000) 717–727.\\r\\n[83] M. Ghomi, M. Goodarzi, M. Goodarzi, Peak load forecasting of electric utilities for west province of iran by using neural network without\\r\\nweather information, in: 2010 12th International Conference on Computer Modelling and Simulation, IEEE, 2010, pp. 28–32.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 31 of 34\\nElectrical peak demand forecasting– A review\\r\\n[84] S. Saeed Madani, Electric load forecasting using an artificial neural network, IEEE Transactions on Power Systems 6 (1991) 442–449.\\r\\n[85] C. C. Hsu, C. Y. Chen, Regional load forecasting in taiwan––applications of artificial neural networks, Energy Conversion & Management\\r\\n44 (2003) 1941–1949.\\r\\n[86] L. M. Saini, Peak load forecasting using bayesian regularization, resilient and adaptive backpropagation learning based artificial neural\\r\\nnetworks, Electric Power Systems Research 78 (2008) 1302–1310.\\r\\n[87] Y. T. Chae, R. Horesh, Y. Hwang, Y. M. Lee, Artificial neural network model for forecasting sub-hourly electricity usage in commercial\\r\\nbuildings, Energy & Buildings 111 (2016) 184–194.\\r\\n[88] L. Ghods, M. Kalantar, Long-term peak demand forecasting by using radial basis function neural networks, Iranian Journal of Electrical and\\r\\nElectronic Engineering 6 (2010) 175–182.\\r\\n[89] H. K. Temraz, K. M. El-Nagar, M. M. A. Salama, Applications of noniterative least absolute value estimation for forecasting annual peak\\r\\nelectric power demand, Canadian Journal of Electrical and Computer Engineering 23 (1998) 141–146.\\r\\n[90] L. Hernández, C. Baladón, J. Aguiar, L. Calavia, B. Carro, A. Sánchez-Esguevillas, J. Sanjuán, á. González, J. Lloret, Improved short-term\\r\\nload forecasting based on two-stage predictions with artificial neural networks in a microgrid environment, Energies 6 (2013) 4489–4507.\\r\\n[91] S. Mandal, A. Agrawal, Fuzzy-neural network based short term peak and average load forecasting (stpalf) system with network security,\\r\\nin: IECEC-97 Proceedings of the Thirty-Second Intersociety Energy Conversion Engineering Conference (Cat. No. 97CH6203), volume 3,\\r\\nIEEE, 1997, pp. 2193–2196.\\r\\n[92] Z. Tavassoli-Hojati, S. Ghaderi, H. Iranmanesh, P. Hilber, E. Shayesteh, A self-partitioning local neuro fuzzy model for short-term load\\r\\nforecasting in smart grids, Energy 199 (2020) 117514.\\r\\n[93] Ermatita, I. Pahendra, E. Darnila, M. Sadli, M. Sinambela, W. Fuadi, Peak load forecasting based on long short term memory, in: 2019\\r\\nInternational Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS), 2019, pp. 137–140.\\r\\n[94] Y.-Y. Hsu, C.-C. Yang, Design of artificial neural networks for short-term load forecasting. part 1: Self-organising feature maps for day type\\r\\nidentification, in: IEE Proceedings C (Generation, Transmission and Distribution), volume 138, IET, 1991, pp. 407–413.\\r\\n[95] Y.-Y. Hsu, C.-C. Yang, Design of artificial neural networks for short-term load forecasting. part 2: Multilayer feedforward networks for peak\\r\\nload and valley load forecasting, in: IEE Proceedings C (Generation, Transmission and Distribution), volume 138, IET, 1991, pp. 414–418.\\r\\n[96] M. Amin-Naseri, A. Soroush, Combined use of unsupervised and supervised learning for daily peak load forecasting, Energy Conversion\\r\\nand Management 49 (2008) 1302–1308.\\r\\n[97] B. Kwon, R. Park, K. Song, Weekly peak load forecasting for 104 weeks using deep learning algorithm, in: 2019 IEEE PES Asia-Pacific\\r\\nPower and Energy Engineering Conference (APPEEC), 2019, pp. 1–4.\\r\\n[98] K. Ho, Y.-Y. Hsu, C.-C. Yang, Short term load forecasting using a multilayer neural network with an adaptive learning algorithm, IEEE\\r\\nTransactions on Power Systems 7 (1992) 141–149.\\r\\n[99] L. M. Saini, M. K. Soni, Artificial neural network-based peak load forecasting using conjugate gradient methods, IEEE Transactions on\\r\\nPower Systems 17 (2002) 907–912.\\r\\n[100] H. Drucker, C. J. Burges, L. Kaufman, A. Smola, V. Vapnik, et al., Support vector regression machines, Advances in neural information\\r\\nprocessing systems 9 (1997) 155–161.\\r\\n[101] C. Coello, E. Mezura-Montes, Constraint-handling in genetic algorithms through the use of dominance-based tournament selection, Ad\\x02vanced Engineering Informatics 16 (2002) 193–203.\\r\\n[102] E. El-Attar, J. Goulermas, Q. Wu, Forecasting electric daily peak load based on local prediction, in: 2009 IEEE Power & Energy Society\\r\\nGeneral Meeting, IEEE, 2009, pp. 1–6.\\r\\n[103] J. Dhillon, S. A. Rahman, S. U. Ahmad, M. J. Hossain, Peak electricity load forecasting using online support vector regression, in: 2016\\r\\nIEEE Canadian Conference on Electrical and Computer Engineering (CCECE), IEEE, 2016, pp. 1–4.\\r\\n[104] P. Zeng, M. Jin, Peak load forecasting based on multi-source data and day-to-day topological network, IET Generation, Transmission\\r\\nDistribution 12 (2018) 1374–1381.\\r\\n[105] R. Polikar, Ensemble learning, in: Ensemble machine learning, Springer, 2012, pp. 1–34.\\r\\n[106] T. Ahmad, H. Chen, Potential of three variant machine-learning models for forecasting district level medium-term and long-term energy\\r\\ndemand in smart grid environment, Energy 160 (2018) 1008–1020.\\r\\n[107] N. Zhang, Z. Li, X. Zou, S. M. Quiring, Comparison of three short-term load forecast models in southern california, Energy 189 (2019)\\r\\n116358.\\r\\n[108] H. Lu, F. Cheng, X. Ma, G. Hu, Short-term prediction of building energy consumption employing an improved extreme gradient boosting\\r\\nmodel: a case study of an intake tower, Energy 203 (2020) 117756.\\r\\n[109] D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning Internal Representations by Error Propagation, MIT Press, 1988.\\r\\n[110] E. M. de Oliveira, F. L. Cyrino Oliveira, Forecasting mid-long term electric energy consumption through bagging arima and exponential\\r\\nsmoothing methods, Energy 144 (2018) 776–788.\\r\\n[111] C. Fan, F. Xiao, S. Wang, Development of prediction models for next-day building energy consumption and peak power demand using data\\r\\nmining techniques, Applied Energy 127 (2014) 1–10.\\r\\n[112] Z. Wang, Y. Wang, R. Zeng, R. S. Srinivasan, S. Ahrentzen, Random forest based hourly building energy prediction, Energy and Buildings\\r\\n171 (2018) 11–25.\\r\\n[113] G. Cartina, V. Alexandrescu, G. Grigoras, M. Moshe, Peak load estimation in distribution networks by fuzzy regression approach, in:\\r\\n2000 10th Mediterranean Electrotechnical Conference. Information Technology and Electrotechnology for the Mediterranean Countries.\\r\\nProceedings. MeleCon 2000 (Cat. No. 00CH37099), volume 3, IEEE, 2000, pp. 907–910.\\r\\n[114] S. Kato, K. Yukita, Y. Goto, K. Ichiyanagi, Study of daily peak load forecasting by structured representation on genetic algorithms for\\r\\nfunction fitting, in: IEEE/PES Transmission and Distribution Conference and Exhibition, volume 3, IEEE, 2002, pp. 1686–1690.\\r\\n[115] M. Gavrilas, C. V. Sfintes, O. Ivanov, Comparison of neural and evolutionary approaches to peak load estimation in distribution systems, in:\\r\\nEUROCON 2005-The International Conference on\" Computer as a Tool\", volume 2, IEEE, 2005, pp. 1461–1464.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 32 of 34\\nElectrical peak demand forecasting– A review\\r\\n[116] R. Torkzadeh, A. Mirzaei, M. M. Mirjalili, A. S. Anaraki, M. R. Sehhati, F. Behdad, Medium term load forecasting in distribution systems\\r\\nbased on multi linear regression principal component analysis: A novel approach, in: 2014 19th Conference on Electrical Power Distribution\\r\\nNetworks (EPDC), 2014, pp. 66–70.\\r\\n[117] A. Laouafi, M. Mordjaoui, F. Laouafi, T. E. Boukelia, Daily peak electricity demand forecasting based on an adaptive hybrid two-stage\\r\\nmethodology, International Journal of Electrical Power & Energy Systems 77 (2016) 136–144.\\r\\n[118] M.-R. Lee, S.-J. Wang, L. Yi-Yu, L.-I. Tai, H.-J. Shi, The maximum power demand forecasting with fuzzy theory, in: 2010 International\\r\\nSymposium on Computer, Communication, Control and Automation (3CA), volume 2, IEEE, 2010, pp. 419–422.\\r\\n[119] O. A. Carpinteiro, R. C. Leme, A. de Souza, et al., A hierarchical hybrid neural model with time integrators in long-term peak-load forecasting,\\r\\nin: Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 5, IEEE, 2005, pp. 2960–2965.\\r\\n[120] O. Ivanov, M. Gavrilaş, Multilayer perceptron architecture optimization for peak load estimation, in: 12th Symposium on Neural Network\\r\\nApplications in Electrical Engineering (NEUREL), 2014, pp. 67–72.\\r\\n[121] J. Yu, H. Lee, Y. Jeong, S. Kim, Linguistic fuzzy modeling approach for daily peak load forecasting, in: 2013 International Conference on\\r\\nFuzzy Theory and Its Applications (iFUZZY), IEEE, 2013, pp. 116–121.\\r\\n[122] J. R. G. Sarduy, K. G. D. Santo, M. A. Saidel, Linear and non-linear methods for prediction of peak load at university of são paulo,\\r\\nMeasurement 78 (2016) 187–201.\\r\\n[123] F. Escolano, P. Suau, B. Bonev, Feature selection and transformation, Information Theory in Computer Vision and Pattern Recognition\\r\\n(2009) 211–269.\\r\\n[124] M. Dash, H. Liu, Feature selection for classification, Intelligent data analysis 1 (1997) 131–156.\\r\\n[125] S. Dai, F. Meng, Energy forecasting with building characteristics analysis, in: 2020 International Joint Conference on Neural Networks\\r\\n(IJCNN), IEEE, 2020, pp. 1–7.\\r\\n[126] X. Fu, X.-J. Zeng, P. Feng, X. Cai, Clustering-based short-term load forecasting for residential electricity under the increasing-block pricing\\r\\ntariffs in china, Energy 165 (2018) 76–89.\\r\\n[127] A. K. Jain, M. N. Murty, P. J. Flynn, Data clustering: a review, ACM computing surveys (CSUR) 31 (1999) 264–323.\\r\\n[128] A. Salimi-Beni, D. Farrokhzad, M. Fotuhi-Firuzabad, S. Alemohammad, A new approach to determine base, intermediate and peak-demand\\r\\nin an electric power system, in: 2006 International Conference on Power System Technology, IEEE, 2006, pp. 1–5.\\r\\n[129] L. Jin, Y. Feng, Y. Jilai, Peak load forecasting using hierarchical clustering and rprop neural network, in: 2006 IEEE PES Power Systems\\r\\nConference and Exposition, IEEE, 2006, pp. 1535–1540.\\r\\n[130] L. Jin, Y. Jilai, L. Zhuo, Load forecast for system operation in peak load period, in: 2007 IEEE Power Engineering Society General Meeting,\\r\\nIEEE, 2007, pp. 1–6.\\r\\n[131] L. Jin, Y. J. Lai, T. X. Long, Peak load forecasting based on robust regression model, in: 2004 International Conference on Probabilistic\\r\\nMethods Applied to Power Systems, IEEE, 2004, pp. 123–128.\\r\\n[132] L. Jin, L. Ziyang, S. Jingbo, S. Xinying, An efficient method for peak load forecasting, in: 2005 International Power Engineering Conference,\\r\\nIEEE, 2005, pp. 1–52.\\r\\n[133] C. Ding, X. He, Cluster merging and splitting in hierarchical clustering algorithms, in: 2002 IEEE International Conference on Data Mining,\\r\\n2002. Proceedings., IEEE, 2002, pp. 139–146.\\r\\n[134] J. C. Bezdek, Pattern recognition with fuzzy objective function algorithms, Springer Science & Business Media, 2013.\\r\\n[135] L. Xu, S. Wang, R. Tang, Probabilistic load forecasting for buildings considering weather forecasting uncertainty and uncertain peak load,\\r\\nApplied Energy 237 (2019) 180–195.\\r\\n[136] L. E. Stetson, G. L. Stark, Peak electrical demands of individuals and groups of rural residential customers, IEEE Transactions on Industry\\r\\nApplications 24 (1988) 772–776.\\r\\n[137] J. Huang, Y. Li, Y. Liu, Summer daily peak load forecasting considering accumulation effect and abrupt change of temperature, in: 2012\\r\\nIEEE Power and Energy Society General Meeting, IEEE, 2012, pp. 1–4.\\r\\n[138] T. Ploysuwan, Spectral mixture kernel for pattern discovery and time series forecasting of electricity peak load, in: TENCON 2014-2014\\r\\nIEEE Region 10 Conference, IEEE, 2014, pp. 1–5.\\r\\n[139] K. Ogihara, S. Urano, A study of risk reduction for daily peak load demand forecasting, in: 2019 IEEE Milan PowerTech, IEEE, 2019, pp.\\r\\n1–6.\\r\\n[140] C. Tairen, J. M. Lehr, L. Olga, M.-R. Manel, Distribution feeder-level day-ahead peak load forecasting methods and comparative study, IET\\r\\nGeneration Transmission & Distribution 12 (2018) 3270–3278.\\r\\n[141] T. Ploysuwan, P. Atsawathawichok, P. Teekaput, Peak load forecasting of electricity generating authority of thailand by gaussian process, in:\\r\\n2014 International Electrical Engineering Congress (iEECON), IEEE, 2014, pp. 1–4.\\r\\n[142] M. N. S. K. Shabbir, M. Z. Ali, X. Liang, M. S. A. Chowdhury, A probabilistic approach considering contingency parameters for peak load\\r\\ndemand forecasting, Canadian Journal of Electrical and Computer Engineering 41 (2018) 224–233.\\r\\n[143] Y. K. Bichpuriya, S. Soman, A. Subramanyam, Robust probability density forecasts of yearly peak load using non-parametric model, in:\\r\\n2016 IEEE Power and Energy Society General Meeting (PESGM), IEEE, 2016, pp. 1–5.\\r\\n[144] E. Khorsheed, Long-term energy peak load forecasting models: A hybrid statistical approach, in: 2018 Advances in Science and Engineering\\r\\nTechnology International Conferences (ASET), IEEE, 2018, pp. 1–6.\\r\\n[145] Z. Wang, Y. Cao, Mutual information and non-fixed anns for daily peak load forecasting, in: 2006 IEEE PES Power Systems Conference\\r\\nand Exposition, IEEE, 2006, pp. 1523–1527.\\r\\n[146] K. Gajowniczek, R. Nafkha, T. Zabkowski, Electricity peak demand classification with artificial neural networks, in: 2017 Federated\\r\\nConference on Computer Science and Information Systems (FedCSIS), 2017, pp. 307–315.\\r\\n[147] K. Gajowniczek, R. Nafkha, T. Zabkowski, Seasonal peak demand classification with machine learning techniques, in: 2018 International\\r\\nConference on Applied Mathematics Computer Science (ICAMCS), 2018, pp. 101–1014.\\r\\n[148] S. Chemetova, P. Santos, A. J. Pires, Peak load forecasting in electrical deregulated market environment - the dynamic tariffs, in: IECON\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 33 of 34\\nElectrical peak demand forecasting– A review\\r\\n2019 - 45th Annual Conference of the IEEE Industrial Electronics Society, 2019, pp. 2227–2232.\\r\\n[149] D. C. Park, O. Mohammed, Artificial neural network based electric peak load forecasting, in: IEEE Proceedings of the SOUTHEAST\\x02CON’91, IEEE, 1991, pp. 225–228.\\r\\n[150] K. Y. Lee, Y. T. Cha, Short-term load forecasting using an artificial neural network, IEEE Transactions on Power Systems 7 (1992) 124–132.\\r\\n[151] I. Drezga, S. Rahman, Short-term load forecasting with local ann predictors, IEEE Transactions on Power Systems 14 (1999) 844–850.\\r\\n[152] M. A. Aboul-Magd, et al., An artificial neural network model for electrical daily peak load forecasting with an adjustment for holidays,\\r\\nin: LESCOPE 01. 2001 Large Engineering Systems Conference on Power Engineering. Conference Proceedings. Theme: Powering Beyond\\r\\n2001 (Cat. No. 01ex490), IEEE, 2001, pp. 105–113.\\r\\n[153] T. Matsui, T. Iizaka, Y. Fukuyama, Peak load forecasting using analyzable structured neural network, in: 2001 IEEE Power Engineering\\r\\nSociety Winter Meeting. Conference Proceedings (Cat. No. 01CH37194), volume 2, IEEE, 2001, pp. 405–410.\\r\\n[154] M. B. Tasre, P. P. Bedekar, V. N. Ghate, Daily peak load forecasting using ann, in: 2011 Nirma University International Conference on\\r\\nEngineering, IEEE, 2011, pp. 1–6.\\r\\n[155] J. Milojković, I. Litovski, V. Litovski, Ann application for the next day peak electricity load prediction, in: 11th Symposium on Neural\\r\\nNetwork Applications in Electrical Engineering, IEEE, 2012, pp. 237–241.\\r\\n[156] S. Chemetova, P. Santos, M. Ventim-Neves, Load peak forecasting in different load patterns situations, in: 2016 10th International Conference\\r\\non Compatibility, Power Electronics and Power Engineering (CPE-POWERENG), IEEE, 2016, pp. 148–151.\\r\\n[157] A. Jarndal, S. Hamdan, Forecasting of peak electricity demand using annga and ann-pso approaches, in: 2017 7th International Conference\\r\\non Modeling, Simulation, and Applied Optimization (ICMSAO), IEEE, 2017, pp. 1–5.\\r\\n[158] M. H. Leak, G. K. Venayagamoorthy, Forecasting peak daily load in distribution feeders, in: 2018 Clemson University Power Systems\\r\\nConference (PSC), IEEE, 2018, pp. 1–8.\\r\\n[159] T. Onoda, Next day’s peak load forecasting using an artificial neural network, in: [1993] Proceedings of the Second International Forum on\\r\\nApplications of Neural Networks to Power Systems, IEEE, 1992, pp. 284–289.\\r\\n[160] Y. Morioka, K. Sakurai, A. Yokoyama, Y. Sekine, Next day peak load forecasting using a multilayer neural network with an additional\\r\\nlearning, in: [1993] Proceedings of the Second International Forum on Applications of Neural Networks to Power Systems, IEEE, 1992, pp.\\r\\n60–65.\\r\\n[161] Y. Mizukami, T. Nishimori, Maximum electric power demand prediction by neural network, in: [1993] Proceedings of the Second Interna\\x02tional Forum on Applications of Neural Networks to Power Systems, IEEE, 1992, pp. 296–301.\\r\\n[162] D. Abdellah, L. Djamel, Forecasting the algerian load peak profile using time series model based on backpropagation neural networks, in:\\r\\n4th International Conference on Power Engineering, Energy and Electrical Drives, IEEE, 2013, pp. 1734–1737.\\r\\n[163] S. Takiyar, Grid reliability enhancement by peak load forecasting with a pso hybridized ann model, in: 2015 4th International Conference\\r\\non Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions), 2015, pp. 1–6.\\r\\n[164] O. A. S. Carpinteiro, R. C. Leme, A. C. Z. D. Souza, C. A. M. Pinheiro, E. M. Moreira, Long-term load forecasting via a hierarchical neural\\r\\nmodel with time integrators, Electric Power Systems Research 77 (2006) 371–378.\\r\\n[165] S. Phimphachanh, K. Chammongthai, P. Kumhom, A. Sangswang, Using neural network for long term peak load forecasting in vientiane\\r\\nmunicipality, in: 2004 IEEE Region 10 Conference TENCON 2004., volume 100, IEEE, 2004, pp. 319–322.\\r\\n[166] M. S. Kandil, S. M. El-Debeiky, N. E. Hasanien, Long-term load forecasting for fast developing utility using a knowledge-based expert\\r\\nsystem, IEEE Power Engineering Review 22 (2002) 78–78.\\r\\n[167] M. Rashidi, F. Rashidi, H. Monavar, Peak load forecasting in power systems using emotional learning based fuzzy logic, in: SMC’03\\r\\nConference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and\\r\\nAssurance (Cat. No.03CH37483), 2003, pp. 1985–1988.\\r\\n[168] J. Jamaaluddin, D. Hadidjaja, I. Sulistiyowati, E. Suprayitno, I. Anshory, S. Syahrorini, Very short term load forecasting peak load time\\r\\nusing fuzzy logic, in: IOP Conference Series: Materials Science and Engineering, volume 403, IOP Publishing, 2018, p. 012070.\\r\\n[169] I. P. Panapakidis, G. C. Christoforidis, N. Asimopoulos, A. S. Dagoumas, Combining wavelet transform and support vector regression model\\r\\nfor day-ahead peak load forecasting in the greek power system, in: 2017 IEEE International Conference on Environment and Electrical\\r\\nEngineering and 2017 IEEE Industrial and Commercial Power Systems Europe (EEEIC/I&CPS Europe), IEEE, 2017, pp. 1–6.\\r\\n[170] M. K. Azad, S. Uddin, M. Takruri, Support vector regression based electricity peak load forecasting, in: 2018 11th International Symposium\\r\\non Mechatronics and its Applications (ISMA), 2018, pp. 1–5.\\r\\n[171] A. Satre-Meloy, M. Diakonova, P. Grünewald, Cluster analysis and prediction of residential peak demand profiles using occupant activity\\r\\ndata, Applied Energy 260 (2020) 114246.\\r\\n[172] J. Liu, L. E. Brown, Effect of forecast accuracy on day ahead prediction of coincident peak days, in: 2019 IEEE Innovative Smart Grid\\r\\nTechnologies - Asia (ISGT Asia), 2019, pp. 661–666.\\r\\n[173] S. Ai, A. Chakravorty, C. Rong, Evolutionary ensemble lstm based household peak demand prediction, in: 2019 International Conference\\r\\non Artificial Intelligence in Information and Communication (ICAIIC), IEEE, 2019, pp. 1–6.\\r\\n[174] T. Chen, J. Lehr, O. Lavrova, M. Martinez-Ramonz, Distribution-level peak load prediction based on bayesian additive regression trees, in:\\r\\n2016 IEEE Power and Energy Society General Meeting (PESGM), IEEE, 2016, pp. 1–5.\\r\\n[175] J. Lim, I. Doh, K. Chae, Security system architecture for data integrity based on a virtual smart meter overlay in a smart grid system, Soft\\r\\nComputing 20 (2016) 1829–1840.\\r\\n[176] D. Banisar, S. Davies, Global trends in privacy protection: An international survey of privacy, data protection, and surveillance laws and\\r\\ndevelopments, J. Marshall J. Computer & Info. L. 18 (1999) 1.\\r\\n[177] F. Hu, M. Qiu, J. Li, T. Grant, D. Taylor, S. Mccaleb, L. Butler, R. Hamner, A review on cloud computing: Design challenges in architecture\\r\\nand security, Journal of Computing & Information Technology 19 (2011) 25–55.\\r\\n[178] Q. Yang, Y. Liu, T. Chen, Y. Tong, Federated machine learning: Concept and applications, ACM Transactions on Intelligent Systems and\\r\\nTechnology (TIST) 10 (2019) 1–19.\\r\\nShuang Dai et al.: Preprint submitted to Elsevier Page 34 of 34'},\n",
       " {'name': '2202.01381v2.pdf',\n",
       "  'content': 'ETSformer: Exponential Smoothing Transformers\\r\\nfor Time-series Forecasting\\r\\nGerald Woo1 2, Chenghao Liu1, Doyen Sahoo1, Akshat Kumar2 & Steven Hoi1\\r\\n1Salesforce Research Asia, 2Singapore Management University\\r\\n{gwoo,chenghao.liu,dsahoo,shoi}@salesforce.com\\r\\n{akshatkumar}@smu.edu.sg\\r\\nAbstract\\r\\nTransformers have been actively studied for time-series forecasting in recent years.\\r\\nWhile often showing promising results in various scenarios, traditional Transform\\x02ers are not designed to fully exploit the characteristics of time-series data and thus\\r\\nsuffer some fundamental limitations, e.g., they are generally not decomposable or\\r\\ninterpretable, and are neither effective nor efficient for long-term forecasting. In this\\r\\npaper, we propose ETSformer, a novel time-series Transformer architecture, which\\r\\nexploits the principle of exponential smoothing in improving Transformers for time\\x02series forecasting. In particular, inspired by the classical exponential smoothing\\r\\nmethods in time-series forecasting, we propose the novel exponential smoothing at\\x02tention (ESA) and frequency attention (FA) to replace the self-attention mechanism\\r\\nin vanilla Transformers, thus improving both accuracy and efficiency. Based on\\r\\nthese, we redesign the Transformer architecture with modular decomposition blocks\\r\\nsuch that it can learn to decompose the time-series data into interpretable time\\x02series components such as level, growth and seasonality. Extensive experiments on\\r\\nvarious time-series benchmarks validate the efficacy and advantages of the proposed\\r\\nmethod. Code is available at https://github.com/salesforce/ETSformer.\\r\\n1 Introduction\\r\\nTransformer models have achieved great success in the fields of NLP [8, 33] and CV [4, 9] in recent\\r\\ntimes. The success is widely attributed to its self-attention mechanism which is able to explicitly\\r\\nmodel both short and long range dependencies adaptively via the pairwise query-key interaction.\\r\\nOwing to their powerful capability to model sequential data, Transformer-based architectures [20,\\r\\n37, 38, 40, 41] have been actively explored for the time-series forecasting, especially for the more\\r\\nchallenging Long Sequence Time-series Forecasting (LSTF) task. While showing promising results,\\r\\nit is still quite challenging to extract salient temporal patterns and thus make accurate long-term\\r\\nforecasts for large-scale data. This is because time-series data is usually noisy and non-stationary.\\r\\nWithout incorporating appropriate knowledge about time-series structures [1, 13, 31], it is prone to\\r\\nlearning the spurious dependencies and lacks interpretability.\\r\\nMoreover, the use of content-based, dot-product attention in Transformers is not effective in detecting\\r\\nessential temporal dependencies for two reasons. (1) Firstly, time-series data is usually assumed\\r\\nto be generated by a conditional distribution over past observations, with the dependence between\\r\\nobservations weakening over time [17, 23]. Therefore, neighboring data points have similar values,\\r\\nand recent tokens should be given a higher weight1 when measuring their similarity [13, 14]. This\\r\\nindicates that attention measured by a relative time lag is more effective than that measured by the\\r\\n1An assumption further supported by the success of classical exponential smoothing methods and ARIMA\\r\\nmodel selection methods tending to select small lags.\\r\\nPreprint. Under review.\\r\\narXiv:2202.01381v2 [cs.LG] 20 Jun 2022\\nsimilarity of the content when modeling time-series. (2) Secondly, many real world time-series display\\r\\nstrong seasonality – patterns in time-series which repeat with a fixed period. Automatically extracting\\r\\nseasonal patterns has been proved to be critical for the success of forecasting [5, 6, 36]. However, the\\r\\nvanilla attention mechanism is unlikely able to learn these required periodic dependencies without\\r\\nany in-built prior structure.\\r\\nForecast\\r\\nHorizon\\r\\nLookback\\r\\nWindow\\r\\nTime\\r\\nLevel\\r\\nDamped\\r\\nGrowth\\r\\nTrend\\t(Level\\t+\\tGrowth)\\r\\nSeason Decomposition Composition\\r\\nData\\r\\nFigure 1: Illustration demonstrating how ETS\\x02former generates forecasts via a decomposition\\r\\n(of intermediate representations) into seasonal and\\r\\ntrend components. The seasonal component ex\\x02tracts salient periodic patterns and extrapolates\\r\\nthem. The trend component which is a combi\\x02nation of the level and growth terms, first estimates\\r\\nthe current level of the time-series, and subse\\x02quently adds a damped growth term to generate\\r\\ntrend forecasts.\\r\\nTo address these limitations, we propose ETS\\x02former, an effective and efficient Transformer\\r\\narchitecture for time-series forecasting, inspired\\r\\nby exponential smoothing methods [13] and il\\x02lustrated in Figure 1. First of all, ETSformer in\\x02corporates inductive biases of time-series struc\\x02tures by performing a layer-wise level, growth,\\r\\nand seasonal decomposition. By leveraging the\\r\\nhigh capacities of deep architectures and an ef\\x02fective residual learning scheme, ETSformer\\r\\nis able to extract a series of latent growth and\\r\\nseasonal patterns and model their complex de\\x02pendencies. Secondly, ETSformer introduces a\\r\\nnovel Exponential Smoothing Attention (ESA)\\r\\nand Frequency Attention (FA) to replace vanilla\\r\\nattention. In particular, ESA constructs attention\\r\\nscores based on the relative time lag to the query,\\r\\nand achieves O(Llog L) complexity for the\\r\\nlength-L lookback window and demonstrates\\r\\npowerful capability in modeling the growth com\\x02ponent. FA leverages the Fourier transformation\\r\\nto extract the dominating seasonal patterns by se\\x02lecting the Fourier bases with the K largest am\\x02plitudes in frequency domain, and also achieves\\r\\nO(Llog L) complexity. Finally, the predicted forecast is a composition of level, trend, and seasonal\\r\\ncomponents, which makes it human interpretable. We conduct extensive empirical analysis and show\\r\\nthat ETSformer achieves state-of-the-art performance by outperforming competing approaches over 6\\r\\nreal world datasets on both the multivariate and univariate settings, and also visualize the time-series\\r\\ncomponents to verify its interpretability.\\r\\n2 Related Work\\r\\nTransformer based deep forecasting. Inspired by the success of Transformers in CV and NLP,\\r\\nTransformer-based time-series forecasting models have been actively studied recently. LogTrans\\r\\n[20] introduces local context to Transformer models via causal convolutions in the query-key pro\\x02jection layer, and propose the LogSparse attention to reduce complexity to O(Llog L). Informer\\r\\n[41] extends the Transformer by proposing the ProbSparse attention and distillation operation to\\r\\nachieve O(Llog L) complexity. AST [38] leverages a sparse normalization transform, α-entmax, to\\r\\nimplement a sparse attention layer. It further incorporates an adversarial loss to mitigate the adverse\\r\\neffect of error accumulation in inference. Similar to our work that incorporates prior knowledge of\\r\\ntime-series structure, Autoformer [37] introduces the Auto-Correlation attention mechanism which\\r\\nfocuses on sub-series based similarity and is able to extract periodic patterns. Yet, their implemen\\x02tation of series decomposition which performs de-trending via a simple moving average over the\\r\\ninput signal without any learnable parameters is arguably a simplified assumption, insufficient to\\r\\nappropriately model complex trend patterns. ETSformer on the other hand, decomposes the series by\\r\\nde-seasonalization as seasonal patterns are more identifiable and easier to detect [7]. Furthermore,\\r\\nthe Auto-Correlation mechanism fails to attend to information from the local context (i.e. forecast\\r\\nat t + 1 is not dependent on t, t − 1, etc.) and does not separate the trend component into level and\\r\\ngrowth components, which are both crucial for modeling trend patterns. Lastly, similar to previous\\r\\nwork, their approach is highly reliant on manually designed dynamic time-dependent covariates (e.g.\\r\\nmonth-of-year, day-of-week), while ETSformer is able to automatically learn and extract seasonal\\r\\npatterns from the time-series signal directly.\\r\\n2\\nAttention Mechanism. The self-attention mechanism in Transformer models has recently received\\r\\nmuch attention, its necessity has been greatly investigated in attempts to introduce more flexibility\\r\\nand reduce computational cost. Synthesizer [29] empirically studies the importance of dot-product\\r\\ninteractions, and show that a randomly initialized, learnable attention mechanisms with or without\\r\\ntoken-token dependencies can achieve competitive performance with vanilla self-attention on various\\r\\nNLP tasks. [39] utilizes an unparameterized Gaussian distribution to replace the original attention\\r\\nscores, concluding that the attention distribution should focus on a certain local window and can\\r\\nachieve comparable performance. [25] replaces attention with fixed, non-learnable positional patterns,\\r\\nobtaining competitive performance on NMT tasks. [19] replaces self-attention with a non-learnable\\r\\nFourier Transform and verifies it to be an effective mixing mechanism. While our proposed ESA\\r\\nshares the spirit of designing attention mechanisms that are not dependent on pair-wise query-key\\r\\ninteractions, our work is inspired by exploiting the characteristics of time-series and is an early\\r\\nattempt to utilize prior knowledge of time-series for tackling the time-series forecasting tasks.\\r\\n3 Preliminaries and Background\\r\\nProblem Formulation Let xt ∈ R\\r\\nm denote an observation of a multivariate time-series at time step\\r\\nt. Given a lookback window Xt−L:t = [xt−L, . . . , xt−1], we consider the task of predicting future\\r\\nvalues over a horizon, Xt:t+H = [xt, . . . , xt+H−1]. We denote Xˆ\\r\\nt:t+H as the point forecast of\\r\\nXt:t+H. Thus, the goal is to learn a forecasting function Xˆ\\r\\nt:t+H = f(Xt−L:t) by minimizing some\\r\\nloss function L : R\\r\\nH×m × RH×m → R.\\r\\nExponential Smoothing We instantiate exponential smoothing methods [13] in the univariate fore\\x02casting setting. They assume that time-series can be decomposed into seasonal and trend components,\\r\\nand trend can be further decomposed into level and growth components. Specifically, a commonly\\r\\nused model is the additive Holt-Winters’ method [12, 35], which can be formulated as:\\r\\nLevel : et = α(xt − st−p) + (1 − α)(et−1 + bt−1)\\r\\nGrowth : bt = β(et − et−1) + (1 − β)bt−1\\r\\nSeasonal : st = γ(xt − et) + (1 − γ)st−p\\r\\nForecasting : ˆxt+h|t = et + hbt + st+h−p (1)\\r\\nwhere p is the period of seasonality, and xˆt+h|tis the h-steps ahead forecast. The above equations\\r\\nstate that the h-steps ahead forecast is composed of the last estimated level et, incrementing it by h\\r\\ntimes the last growth factor, bt, and adding the last available seasonal factor st+h−p. Specifically, the\\r\\nlevel smoothing equation is formulated as a weighted average of the seasonally adjusted observation\\r\\n(xt − st−p) and the non-seasonal forecast, obtained by summing the previous level and growth\\r\\n(et−1 + bt−1). The growth smoothing equation is implemented by a weighted average between\\r\\nthe successive difference of the (de-seasonalized) level, (et − et−1), and the previous growth, bt−1.\\r\\nFinally, the seasonal smoothing equation is a weighted average between the difference of observation\\r\\nand (de-seasonalized) level, (xt − et), and the previous seasonal index st−p. The weighted average\\r\\nof these three equations are controlled by the smoothing parameters α, β and γ, respectively.\\r\\nA widely used modification of the additive Holt-Winters’ method is to allow the damping of trends,\\r\\nwhich has been proved to produce robust multi-step forecasts [22, 28]. The forecast with damping\\r\\ntrend can be rewritten as:\\r\\nxˆt+h|t = et + (φ + φ\\r\\n2 + · · · + φh\\r\\n)bt + st+h−p, (2)\\r\\nwhere the growth is damped by a factor of φ. If φ = 1, it degenerates to the vanilla forecast. For\\r\\n0 < φ < 1, as h → ∞ this growth component approaches an asymptote given by φbt/(1 − φ).\\r\\n4 ETSformer\\r\\nIn this section, we redesign the classical Transformer architecture into an exponential smoothing\\r\\ninspired encoder-decoder architecture specialized for tackling the time-series forecasting problem.\\r\\nOur architecture design methodology relies on three key principles: (1) the architecture leverages the\\r\\nstacking of multiple layers to progressively extract a series of level, growth, and seasonal represen\\x02tations from the intermediate latent residual; (2) following the spirit of exponential smoothing, we\\r\\n3\\nInput\\r\\nEmbedding\\r\\nLookback\\tWindow:\\t!\"#:!\\r\\nGrowth\\r\\nDamping\\r\\nFrequency\\r\\nAttention\\r\\n+\\r\\n!\\r\\n%\\r\\n!\"#:!\\r\\n%\\r\\n!:!&\\'\\r\\n%\\r\\n+!:!&\\'\\r\\n%\\r\\nForecast\\tHorizon:\\tE!:!&\\'\\r\\nLayer\\t2\\r\\nLayer\\t1\\r\\n!\\r\\n\"\\r\\nEncoder\\r\\nG+S\\tStack\\t2\\r\\nG+S\\tStack\\tN\\r\\nG+S\\tStack\\t1\\r\\n+\\r\\nLevel\\tStack\\r\\n+\\r\\nLinear\\r\\nDecoder\\r\\nLayer\\tN\\r\\nLinear\\r\\nConcat\\r\\nDifference\\r\\nLinear\\r\\nExponential\\t\\r\\nSmoothing\\t\\r\\nAttention\\r\\n!\"#:!\\r\\n%\"(\\r\\n!\"#:!\\r\\n% (Growth)\\r\\nDFT\\r\\nTop-K\\t\\r\\nAmplitude\\r\\niDFT\\r\\n!\"#:!\\r\\n%\"(\\r\\n!\"#:!\\r\\n% (Season)\\r\\n+\\r\\nFeedforward\\r\\nLayerNorm\\r\\nMulti-Head\\tES\\t\\r\\nAttention\\r\\nLayerNorm\\r\\n-\\r\\nFrequency\\r\\nAttention\\r\\n-\\r\\nLevel\\r\\n!\"#:!\\r\\n%\\r\\n!\"#:!\\r\\n%\"( !\"#:!%\"(\\r\\n!\"#:!\\r\\n% (Level)\\r\\n!\"#:!\\r\\n%\\r\\n!\"#:!\\r\\n%\\r\\nFigure 2: ETSformer model architecture.\\r\\nextract the salient seasonal patterns while modeling level and growth components by assigning higher\\r\\nweight to recent observations; (3) the final forecast is a composition of level, growth, and seasonal\\r\\ncomponents making it human interpretable. We now expound how our ETSformer architecture\\r\\nencompasses these principles.\\r\\n4.1 Overall Architecture\\r\\nFigure 2 illustrates the overall encoder-decoder architecture of ETSformer. At each layer, the encoder\\r\\nis designed to iteratively extract growth and seasonal latent components from the lookback window.\\r\\nThe level is then extracted in a similar fashion to classical level smoothing in Equation (1). These\\r\\nextracted components are then fed to the decoder to further generate the final H-step ahead forecast\\r\\nvia a composition of level, growth, and seasonal forecasts, which is defined:\\r\\nXˆt:t+H = Et:t+H + Linear\\x10XN\\r\\nn=1\\r\\n(B\\r\\n(n)\\r\\nt:t+H + S\\r\\n(n)\\r\\nt:t+H)\\r\\n\\x11\\r\\n, (3)\\r\\nwhere Et:t+H ∈ R\\r\\nH×m, and B\\r\\n(n)\\r\\nt:t+H,S\\r\\n(n)\\r\\nt:t+H ∈ R\\r\\nH×d\\r\\nrepresent the level forecasts, and the growth\\r\\nand seasonal latent representations of each time step in the forecast horizon, respectively. The\\r\\nsuperscript represents the stack index, for a total of N encoder stacks. Note that Linear(·) : R\\r\\nd →\\r\\nR\\r\\nm operates element-wise along each time step, projecting the extracted growth and seasonal\\r\\nrepresentations from latent to observation space.\\r\\n4.1.1 Input Embedding\\r\\nRaw signals from the lookback window are mapped to latent space via the input embedding module,\\r\\ndefined by Z\\r\\n(0)\\r\\nt−L:t = E\\r\\n(0)\\r\\nt−L:t = Conv(Xt−L:t), where Conv is a temporal convolutional filter with\\r\\nkernel size 3, input channel m and output channel d. In contrast to prior work [20, 37, 38, 41], the\\r\\ninputs of ETSformer do not rely on any other manually designed dynamic time-dependent covariates\\r\\n(e.g. month-of-year, day-of-week) for both the lookback window and forecast horizon. This is\\r\\nbecause the proposed Frequency Attention module (details in Section 4.2.2) is able to automatically\\r\\nuncover these seasonal patterns, which renders it more applicable for challenging scenarios without\\r\\nthese discriminative covariates and reduces the need for feature engineering.\\r\\n4.1.2 Encoder\\r\\nThe encoder focuses on extracting a series of latent growth and seasonality representations in a\\r\\ncascaded manner from the lookback window. To achieve this goal, traditional methods rely on the\\r\\nassumption of additive or multiplicative seasonality which has limited capability to express complex\\r\\npatterns beyond these assumptions. Inspired by [10, 24], we leverage residual learning to build an\\r\\n4\\nexpressive, deep architecture to characterize the complex intrinsic patterns. Each layer can be inter\\x02preted as sequentially analyzing the input signals. The extracted growth and seasonal signals are then\\r\\nremoved from the residual and undergo a nonlinear transformation before moving to the next layer.\\r\\nEach encoder layer takes as input the residual from the previous encoder layer Z\\r\\n(n−1)\\r\\nt−L:t\\r\\nand emits\\r\\nZ\\r\\n(n)\\r\\nt−L:t\\r\\n, B\\r\\n(n)\\r\\nt−L:t\\r\\n,S\\r\\n(n)\\r\\nt−L:t\\r\\n, the residual, latent growth, and seasonal representations for the lookback\\r\\nwindow via the Multi-Head Exponential Smoothing Attention (MH-ESA) and Frequency Attention\\r\\n(FA) modules (detailed description in Section 4.2). The following equations formalizes the overall\\r\\npipeline in each encoder layer, and for ease of exposition, we use the notation := for a variable update.\\r\\nSeasonal: S\\r\\n(n)\\r\\nt−L:t = FAt−L:t(Z\\r\\n(n−1)\\r\\nt−L:t\\r\\n)\\r\\nZ\\r\\n(n−1)\\r\\nt−L:t\\r\\n:= Z\\r\\n(n−1)\\r\\nt−L:t − S\\r\\n(n)\\r\\nt−L:t\\r\\nGrowth: B\\r\\n(n)\\r\\nt−L:t = MH-ESA(Z\\r\\n(n−1)\\r\\nt−L:t\\r\\n)\\r\\nZ\\r\\n(n−1)\\r\\nt−L:t\\r\\n:= LN(Z\\r\\n(n−1)\\r\\nt−L:t − B\\r\\n(n)\\r\\nt−L:t\\r\\n)\\r\\nZ\\r\\n(n)\\r\\nt−L:t = LN(Z\\r\\n(n−1)\\r\\nt−L:t + FF(Z\\r\\n(n−1)\\r\\nt−L:t\\r\\n))\\r\\nLN is layer normalization [2], FF(x) = Linear(σ(Linear(x))) is a position-wise feedforward\\r\\nnetwork [33] and σ(·) is the sigmoid function.\\r\\nLevel Module Given the latent growth and seasonal representations from each layer, we extract the\\r\\nlevel at each time step t in the lookback window in a similar way as the level smoothing equation in\\r\\nEquation (1). Formally, the adjusted level is a weighted average of the current (de-seasonalized) level\\r\\nand the level-growth forecast from the previous time step t − 1. It can be formulated as:\\r\\nE\\r\\n(n)\\r\\nt = α ∗\\r\\n\\x10\\r\\nE\\r\\n(n−1)\\r\\nt − Linear(S\\r\\n(n)\\r\\nt\\r\\n)\\r\\n\\x11\\r\\n+ (1 − α) ∗\\r\\n\\x10\\r\\nE\\r\\n(n)\\r\\nt−1 + Linear(B\\r\\n(n)\\r\\nt−1\\r\\n)\\r\\n\\x11\\r\\n,\\r\\nwhere α ∈ R\\r\\nm is a learnable smoothing parameter, ∗ is an element-wise multiplication term, and\\r\\nLinear(·) : R\\r\\nd → Rm maps representations to observation space. Finally, the extracted level in the\\r\\nlast layer E\\r\\n(N)\\r\\nt−L:t\\r\\ncan be regarded as the corresponding level for the lookback window. We show in\\r\\nAppendix A.3 that this recurrent exponential smoothing equation can also be efficiently evaluated\\r\\nusing the efficient AES algorithm (Algorithm 1) with an auxiliary term.\\r\\n4.1.3 Decoder\\r\\nThe decoder is tasked with generating the H-step ahead forecasts. As shown in Equation (3),\\r\\nthe final forecast is a composition of level forecasts Et:t+H, growth representations B\\r\\n(n)\\r\\nt:t+H\\r\\nand seasonal representations S\\r\\n(n)\\r\\nt:t+H in the forecast horizon. It comprises N Growth + Sea\\x02sonal (G+S) Stacks, and a Level Stack. The G+S Stack consists of the Growth Damping\\r\\n(GD) and FA blocks, which leverage B\\r\\n(n)\\r\\nt\\r\\n, S\\r\\n(n)\\r\\nt−L:t\\r\\nto predict B\\r\\n(n)\\r\\nt:t+H, S\\r\\n(n)\\r\\nt:t+H, respectively.\\r\\nGrowth: B\\r\\n(n)\\r\\nt:t+H = TD(B\\r\\n(n)\\r\\nt\\r\\n) Seasonal: S\\r\\n(n)\\r\\nt:t+H = FAt:t+H(S\\r\\n(n)\\r\\nt−L:t\\r\\n)\\r\\nTo obtain the level in the forecast horizon, the Level Stack repeats the level in the last time step t\\r\\nalong the forecast horizon. It can be defined as Et:t+H = RepeatH(E\\r\\n(N)\\r\\nt\\r\\n) = [E\\r\\n(N)\\r\\nt\\r\\n, . . . , E\\r\\n(N)\\r\\nt\\r\\n],\\r\\nwith RepeatH(·) : R\\r\\n1×m → RH×m.\\r\\nGrowth Damping To obtain the growth representation in the forecast horizon, we follow the idea of\\r\\ntrend damping in Equation (2) to make robust multi-step forecast. Thus, the trend representations can\\r\\nbe formulated as:\\r\\nTD(B\\r\\n(n)\\r\\nt\\r\\n)j =\\r\\nX\\r\\nj\\r\\ni=1\\r\\nγ\\r\\niB\\r\\n(n)\\r\\nt\\r\\n,\\r\\nTD(B\\r\\n(n)\\r\\nt−L:t\\r\\n) = [TD(B\\r\\n(n)\\r\\nt\\r\\n)t, . . . , TD(B\\r\\n(n)\\r\\nt\\r\\n)t+H−1],\\r\\nwhere 0 < γ < 1 is the damping parameter which is learnable, and in practice, we apply a multi-head\\r\\nversion of trend damping by making use of nh damping parameters. Similar to the implementation\\r\\nfor level forecast in the Level Stack, we only use the last trend representation in the lookback window\\r\\nB\\r\\n(n)\\r\\nt\\r\\nto forecast the trend representation in the forecast horizon.\\r\\n5\\nTime\\r\\n(a) Full Attention (2017)\\r\\nTime\\r\\n(b) Sparse Attention (2020, 2021)\\r\\nTime\\r\\n(c) Log-sparse Attention (2019)\\r\\nTime\\r\\nPeriod\\t1 Period\\t2\\r\\n(d) Autocorrelation Attention\\r\\n(2021)\\r\\nTime\\r\\n(e) Exponential Smoothing Atten\\x02tion (Ours)\\r\\nTime\\r\\nHigh\\tAmplitude\\tFrequencies\\r\\nPeriod\\t1 Period\\t2\\r\\nExtrapolated\\r\\nPattern\\r\\n(f) Frequency Attention (Ours)\\r\\nFigure 3: Comparison between different attention mechanisms. (a) Full, (b) Sparse, and (c) Log\\x02sparse Attentions are adaptive mechanisms, where the green circles represent the attention weights\\r\\nadaptively calculated by a point-wise dot-product query, and depends on various factors including\\r\\nthe time-series value, additional covariates (e.g. positional encodings, time features, etc.). (d)\\r\\nAutocorrelation attention considers sliding dot-product queries to construct attention weights for\\r\\neach rolled input series. We introduce (e) Exponential Smoothing Attention (ESA) and (f) Frequency\\r\\nAttention (FA). ESA directly computes attention weights based on the relative time lag, without\\r\\nconsidering the input content, while FA attends to patterns which dominate with large magnitudes in\\r\\nthe frequency domain.\\r\\n4.2 Exponential Smoothing Attention and Frequency Attention Mechanism\\r\\nConsidering the ineffectiveness of existing attention mechanisms in handling time-series data, we\\r\\ndevelop the Exponential Smoothing Attention (ESA) and Frequency Attention (FA) mechanisms to\\r\\nextract latent growth and seasonal representations. ESA is a non-adaptive, learnable attention scheme\\r\\nwith an inductive bias to attend more strongly to recent observations by following an exponential\\r\\ndecay, while FA is a non-learnable attention scheme, that leverages Fourier transformation to select\\r\\ndominating seasonal patterns. A comparison between existing work and our proposed ESA and FA is\\r\\nillustrated in Figure 3.\\r\\n4.2.1 Exponential Smoothing Attention\\r\\nVanilla self-attention can be regarded as a weighted combination of an input sequence, where the\\r\\nweights are normalized alignment scores measuring the similarity between input contents [32].\\r\\nInspired by the exponential smoothing in Equation (1), we aim to assign a higher weight to recent\\r\\nobservations. It can be regarded as a novel form of attention whose weights are computed by\\r\\nthe relative time lag, rather than input content. Thus, the ESA mechanism can be defined as\\r\\nAES : R\\r\\nL×d → RL×d\\r\\n, where AES(V )t ∈ R\\r\\nd denotes the t-th row of the output matrix, representing\\r\\nthe token corresponding to the t-th time step. Its exponential smoothing formula can be further\\r\\nwritten as:\\r\\nAES(V )t = αVt + (1 − α)AES(V )t−1 =\\r\\nXt−1\\r\\nj=0\\r\\nα(1 − α)\\r\\njVt−j + (1 − α)tv0,\\r\\nwhere 0 < α < 1 and v0 are learnable parameters known as the smoothing parameter and initial state\\r\\nrespectively.\\r\\nEfficient AES algorithm The straightforward implementation of the ESA mechanism by constructing\\r\\nthe attention matrix, AES and performing a matrix multiplication with the input sequence (detailed\\r\\nalgorithm in Appendix A.4) results in an O(L\\r\\n2\\r\\n) computational complexity.\\r\\nAES(V ) =\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\nAES(V )1\\r\\n.\\r\\n.\\r\\n.\\r\\nAES(V )L\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fb = AES ·\\r\\n\\x14\\r\\nv\\r\\nT\\r\\n0\\r\\nV\\r\\n\\x15\\r\\n,\\r\\nYet, we are able to achieve an efficient algorithm by exploiting the unique structure of the exponential\\r\\nsmoothing attention matrix, AES, which is illustrated in Appendix A.1. Each row of the attention\\r\\n6\\nmatrix can be regarded as iteratively right shifting with padding (ignoring the first column). Thus, a\\r\\nmatrix-vector multiplication can be computed with a cross-correlation operation, which in turn has an\\r\\nefficient fast Fourier transform implementation [21]. The full algorithm is described in Algorithm 1,\\r\\nAppendix A.2, achieving an O(Llog L) complexity.\\r\\nMulti-Head Exponential Smoothing Attention (MH-ESA) We use AES as a basic building block,\\r\\nand develop the Multi-Head Exponential Smoothing Attention to extract latent growth representations.\\r\\nFormally, we obtain the growth representations by taking the successive difference of the residuals.\\r\\nZ˜\\r\\n(n)\\r\\nt−L:t = Linear(Z\\r\\n(n−1)\\r\\nt−L:t\\r\\n),\\r\\nB\\r\\n(n)\\r\\nt−L:t = MH-AES(Z˜\\r\\n(n)\\r\\nt−L:t − [Z˜\\r\\n(n)\\r\\nt−L:t−1\\r\\n, v\\r\\n(n)\\r\\n0\\r\\n]),\\r\\nB\\r\\n(n)\\r\\nt−L:t\\r\\n:= Linear(B\\r\\n(n)\\r\\nt−L:t\\r\\n),\\r\\nwhere MH-AES is a multi-head version of AES and v\\r\\n(n)\\r\\n0\\r\\nis the initial state from the ESA mechanism.\\r\\n4.2.2 Frequency Attention\\r\\nThe goal of identifying and extracting seasonal patterns from the lookback window is twofold. Firstly,\\r\\nit can be used to perform de-seasonalization on the input signals such that downstream components\\r\\nare able to focus on modeling the level and growth information. Secondly, we are able to extrapolate\\r\\nthe seasonal patterns to build representations for the forecast horizon. The main challenge is to\\r\\nautomatically identify seasonal patterns. Fortunately, the use of power spectral density estimation for\\r\\nperiodicity detection has been well studied [34]. Inspired by these methods, we leverage the discrete\\r\\nFourier transform (DFT, details in Appendix B) to develop the FA mechanism to extract dominant\\r\\nseasonal patterns.\\r\\nSpecifically, FA first decomposes input signals into their Fourier bases via a DFT along the temporal\\r\\ndimension, F(Z\\r\\n(n−1)\\r\\nt−L:t\\r\\n) ∈ C\\r\\nF ×d where F = bL/2c + 1, and selects bases with the K largest\\r\\namplitudes. An inverse DFT is then applied to obtain the seasonality pattern in time domain.\\r\\nFormally, this is given by the following equations:\\r\\nΦk,i = φ\\r\\n\\x10\\r\\nF(Z\\r\\n(n−1)\\r\\nt−L:t\\r\\n)k,i\\x11, Ak,i =\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0cF(Z\\r\\n(n−1)\\r\\nt−L:t\\r\\n)k,i\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c,\\r\\nκ\\r\\n(1)\\r\\ni\\r\\n, . . . , κ\\r\\n(K)\\r\\ni = arg Top-K\\r\\nk∈{2,...,F }\\r\\nn\\r\\nAk,io,\\r\\nS\\r\\n(n)\\r\\nj,i =\\r\\nX\\r\\nK\\r\\nk=1\\r\\nAκ\\r\\n(k)\\r\\ni\\r\\n,ih\\r\\ncos(2πf\\r\\nκ\\r\\n(k)\\r\\ni\\r\\nj + Φκ\\r\\n(k)\\r\\ni\\r\\n,i) + cos(2π ¯fκ\\r\\n(k)\\r\\ni\\r\\nj + Φ¯\\r\\nκ\\r\\n(k)\\r\\ni\\r\\n,i)\\r\\ni\\r\\n, (4)\\r\\nwhere Φk,i, Ak,i are the phase/amplitude of the k-th frequency for the i-th dimension, arg Top-K\\r\\nreturns the arguments of the top K amplitudes, K is a hyperparameter, fk is the Fourier frequency\\r\\nof the corresponding index, and ¯fk, Φ¯\\r\\nk,i are the Fourier frequency/amplitude of the corresponding\\r\\nconjugates.\\r\\nFinally, the latent seasonal representation of the i-th dimension for the lookback window is formulated\\r\\nas S\\r\\n(n)\\r\\nt−L:t,i = [S\\r\\n(n)\\r\\nt−L,i, . . . ,S\\r\\n(n)\\r\\nt−1,i]. For the forecast horizon, the FA module extrapolates beyond\\r\\nthe lookback window via, S\\r\\n(n)\\r\\nt:t+H,i = [S\\r\\n(n)\\r\\nt,i , . . . ,S\\r\\n(n)\\r\\nt+H−1,i]. Since K is a hyperparameter typically\\r\\nchosen for small values, the complexity for the FA mechanism is similarly O(Llog L).\\r\\n5 Experiments\\r\\nThis section presents extensive empirical evaluations on the LSTF task over 6 real world datasets,\\r\\nETT, ECL, Exchange, Traffic, Weather, and ILI, coming from a variety of application areas (details\\r\\nin Appendix D) for both multivariate and univariate settings. This is followed by an ablation study of\\r\\nthe various contributing components, and interpretability experiments of our proposed model. An\\r\\nadditional analysis on computational efficiency can be found in Appendix H for space. For the main\\r\\nbenchmark, datasets are split into train, validation, and test sets chronologically, following a 60/20/20\\r\\nsplit for the ETT datasets and 70/10/20 split for other datasets. Inputs are zero-mean normalized and\\r\\nwe use MSE and MAE as evaluation metrics. Further details on implementation and hyperparameters\\r\\ncan be found in Appendix C.\\r\\n7\\nTable 1: Multivariate forecasting results over various forecast horizons. Best results are bolded, and\\r\\nsecond best results are underlined.\\r\\nMethods ETSformer Autoformer Informer LogTrans Reformer LSTnet LSTM\\r\\nMetrics MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTm2\\r\\n96 0.189 0.280 0.255 0.339 0.365 0.453 0.768 0.642 0.658 0.619 3.142 1.365 2.041 1.073\\r\\n192 0.253 0.319 0.281 0.340 0.533 0.563 0.989 0.757 1.078 0.827 3.154 1.369 2.249 1.112\\r\\n336 0.314 0.357 0.339 0.372 1.363 0.887 1.334 0.872 1.549 0.972 3.160 1.369 2.568 1.238\\r\\n720 0.414 0.413 0.422 0.419 3.379 1.388 3.048 1.328 2.631 1.242 3.171 1.368 2.720 1.287\\r\\nECL\\r\\n96 0.187 0.304 0.201 0.317 0.274 0.368 0.258 0.357 0.312 0.402 0.680 0.645 0.375 0.437\\r\\n192 0.199 0.315 0.222 0.334 0.296 0.386 0.266 0.368 0.348 0.433 0.725 0.676 0.442 0.473\\r\\n336 0.212 0.329 0.231 0.338 0.300 0.394 0.280 0.380 0.350 0.433 0.828 0.727 0.439 0.473\\r\\n720 0.233 0.345 0.254 0.361 0.373 0.439 0.283 0.376 0.340 0.420 0.957 0.811 0.980 0.814\\r\\nExchange\\r\\n96 0.085 0.204 0.197 0.323 0.847 0.752 0.968 0.812 1.065 0.829 1.551 1.058 1.453 1.049\\r\\n192 0.182 0.303 0.300 0.369 1.204 0.895 1.040 0.851 1.188 0.906 1.477 1.028 1.846 1.179\\r\\n336 0.348 0.428 0.509 0.524 1.672 1.036 1.659 1.081 1.357 0.976 1.507 1.031 2.136 1.231\\r\\n720 1.025 0.774 1.447 0.941 2.478 1.310 1.941 1.127 1.510 1.016 2.285 1.243 2.984 1.427\\r\\nTraffic\\r\\n96 0.607 0.392 0.613 0.388 0.719 0.391 0.684 0.384 0.732 0.423 1.107 0.685 0.843 0.453\\r\\n192 0.621 0.399 0.616 0.382 0.696 0.379 0.685 0.390 0.733 0.420 1.157 0.706 0.847 0.453\\r\\n336 0.622 0.396 0.622 0.337 0.777 0.420 0.733 0.408 0.742 0.420 1.216 0.730 0.853 0.455\\r\\n720 0.632 0.396 0.660 0.408 0.864 0.472 0.717 0.396 0.755 0.423 1.481 0.805 1.500 0.805\\r\\nWeather\\r\\n96 0.197 0.281 0.266 0.336 0.300 0.384 0.458 0.490 0.689 0.596 0.594 0.587 0.369 0.406\\r\\n192 0.237 0.312 0.307 0.367 0.598 0.544 0.658 0.589 0.752 0.638 0.560 0.565 0.416 0.435\\r\\n336 0.298 0.353 0.359 0.359 0.578 0.523 0.797 0.652 0.639 0.596 0.597 0.587 0.455 0.454\\r\\n720 0.352 0.388 0.419 0.419 1.059 0.741 0.869 0.675 1.130 0.792 0.618 0.599 0.535 0.520\\r\\nILI\\r\\n24 2.527 1.020 3.483 1.287 5.764 1.677 4.480 1.444 4.400 1.382 6.026 1.770 5.914 1.734\\r\\n36 2.615 1.007 3.103 1.148 4.755 1.467 4.799 1.467 4.783 1.448 5.340 1.668 6.631 1.845\\r\\n48 2.359 0.972 2.669 1.085 4.763 1.469 4.800 1.468 4.832 1.465 6.080 1.787 6.736 1.857\\r\\n60 2.487 1.016 2.770 1.125 5.264 1.564 5.278 1.560 4.882 1.483 5.548 1.720 6.870 1.879\\r\\n5.1 Results\\r\\nFor the multivariate benchmark, baselines include recently proposed time-series/efficient Transform\\x02ers – Autoformer, Informer, LogTrans, and Reformer [16], and RNN variants – LSTnet [18], and\\r\\nLSTM [11]. Univariate baselines further include N-BEATS [24], DeepAR [26], ARIMA, Prophet\\r\\n[30], and AutoETS [3]. We obtain baseline results from the following papers: [37, 41], and further\\r\\nrun AutoETS from the Merlion library [3]. Table 1 summarize the results of ETSformer against top\\r\\nperforming baselines on a selection of datasets, for the multivariate setting, and Table 4 in Appendix F\\r\\nfor space. Results for ETSformer are averaged over three runs (standard deviation in Appendix G).\\r\\nOverall, ETSformer achieves state-of-the-art performance, achieving the best performance (across\\r\\nall datasets/settings, based on MSE) on 35 out of 40 settings for the multivariate case, and 17 out\\r\\nof 23 for the univariate case. Notably, on Exchange, a dataset with no obvious periodic patterns,\\r\\nETSformer demonstrates an average (over forecast horizons) improvemnt of 39.8% over the best\\r\\nperforming baseline, evidencing its strong trend forecasting capabilities. We highlight that for cases\\r\\nwhere ETSformer does not achieve the best performance, it is still highly competitive, and is always\\r\\nwithin the top 2 performing methods, based on MSE, for 40 out of 40 settings in the multivariate\\r\\nbenchmark , and 21 out of 23 settings of the univariate case.\\r\\n5.2 Ablation Study\\r\\nTable 2: Ablation study on the various components of ETSformer, on the horizon= 24 setting.\\r\\nDatasets ETTh2 ETTm2 ECL Traffic\\r\\nETSformer\\r\\nMSE 0.262 0.110 0.163 0.571\\r\\nMAE 0.337 0.222 0.287 0.373\\r\\nw/o Level\\r\\nMSE 0.434 0.464 0.275 0.649\\r\\nMAE 0.466 0.518 0.373 0.393\\r\\nw/o Season\\r\\nMSE 0.521 0.131 0.696 1.334\\r\\nMAE 0.450 0.236 0.677 0.779\\r\\nw/o Growth\\r\\nMSE 0.290 0.115 0.167 0.583\\r\\nMAE 0.359 0.226 0.288 0.383\\r\\nMH-ESA → MHA\\r\\nMSE 0.656 0.343 0.205 0.586\\r\\nMAE 0.639 0.451 0.323 0.380\\r\\n8\\n100 120 140 160 180 200 220 240\\r\\n0.75\\r\\n0.50\\r\\n0.25\\r\\n0.00\\r\\n0.25\\r\\n0.50\\r\\n0.75 Lookback\\r\\nGround Truth\\r\\nETSformer (MSE: 0.0096)\\r\\nAutoformer (MSE: 0.0115)\\r\\n0 10 20 30 40\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\n0.2 ETTh1\\r\\nGround Truth\\r\\nForecast\\r\\nTrend\\r\\nSeason\\r\\n190 200 210 220 230 240\\r\\n0.0\\r\\n0.1\\r\\n0.2\\r\\n0.3\\r\\n0.4\\r\\n0.5\\r\\nGround Truth (Trend)\\r\\nETSformer (Level+Growth, MSE: 0.0042)\\r\\nAutoformer (Trend, MSE: 0.0262)\\r\\n0 20 40 60 80\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\nECL\\r\\nGround Truth\\r\\nForecast\\r\\nTrend\\r\\nSeason\\r\\n190 200 210 220 230 240\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\nGround Truth (Season)\\r\\nETSformer (Season, MSE: 0.0129)\\r\\nAutoformer (Season, MSE: 0.0219)\\r\\n0 20 40 60 80\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\nweather\\r\\nGround Truth\\r\\nForecast\\r\\nTrend\\r\\nSeason\\r\\nFigure 4: Left: Visualization of decomposed forecasts from ETSformer and Autoformer on a synthetic\\r\\ndatset. (i) Ground truth and non-decomposed forecasts of ETSformer and Autoformer on synthetic\\r\\ndata. (ii) Trend component. (iii) Seasonal component. The data sample on which Autoformer\\r\\nobtained lowest MSE was selected for visualization. Right: Visualization of decomposed forecasts\\r\\nfrom ETSformer on real world datasets, ETTh1, ECL, and Weather. Note that season is zero-centered,\\r\\nand trend successfully tracks the level of the time-series. Due to the long sequence forecasting setting\\r\\nand with a damping, the growth component is not visually obvious, but notice for the Weather dataset,\\r\\nthe trend pattern is has a strong downward slope initially (near time step 0), and is quickly damped.\\r\\nWe study the contribution of each major component which the final forecast is composed of level,\\r\\ngrowth, and seasonality. Table 2 first presents the performance of the full model, and subsequently, the\\r\\nperformance of the resulting model by removing each component. We observe that the composition\\r\\nof level, growth, and season provides the most accurate forecasts across a variety of application areas,\\r\\nand removing any one component results in a deterioration. In particular, estimation of the level\\r\\nof the time-series is critical. We also analyse the case where MH-ESA is replaced with a vanilla\\r\\nmulti-head attention, and observe that our trend attention formulation indeed is more effective.\\r\\n5.3 Interpretability\\r\\nETSformer generates forecasts based on a composition of interpretable time-series components. This\\r\\nmeans we can visualize each component individually, and understand how seasonality and trend\\r\\naffects the forecasts. We showcase this ability in Figure 4 on both synthetic and real world data.\\r\\nExperiments with synthetic data are crucial in this case, since we are not able to obtain the ground\\r\\ntruth decomposition from real world data. ETSformer is first trained on the synthetic dataset (details\\r\\nin Appendix E) with clear (nonlinear) trend and seasonality patterns which we can control. Given\\r\\na lookback window (without noise), we visualize the forecast, as well as decomposed trend and\\r\\nseasonal forecasts. ETSformer successfully forecasts interpretable level, trend (level + growth),\\r\\nand seasonal components, as observed in the trend and seasonality components closely tracking the\\r\\nground truth patterns. Despite obtaining a low MSE, the competing decomposition based approach,\\r\\nAutoformer, struggles to disambiguate between trend and seasonality.\\r\\n6 Conclusion\\r\\nInspired by the classical exponential smoothing methods and emerging Transformer approaches\\r\\nfor time-series forecasting, we proposed ETSformer, a novel Transformer-based architecture for\\r\\ntime-series forecasting which learns level, growth, and seasonal latent representations and their\\r\\ncomplex dependencies. ETSformer leverages the novel Exponential Smoothing Attention and\\r\\nFrequency Attention mechanisms which are more effective at modeling time-series than vanilla\\r\\nself-attention mechanism, and at the same time achieves O(Llog L) complexity, where L is the\\r\\nlength of lookback window. Our extensive empirical evaluation shows that ETSformer achieves\\r\\nstate-of-the-art performance, beating competing baselines in 35 out of 40 and 17 out of 23 settings\\r\\nfor multivariate and univariate forecasting respectively. Future directions include including additional\\r\\ncovariates such as holiday indicators and other dummy variables to consider holiday effects which\\r\\ncannot be captured by the FA mechanism.\\r\\n9\\nReferences\\r\\n[1] Vassilis Assimakopoulos and Konstantinos Nikolopoulos. The theta model: a decomposition approach to\\r\\nforecasting. International journal of forecasting, 16(4):521–530, 2000.\\r\\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\\r\\n[3] Aadyot Bhatnagar, Paul Kassianik, Chenghao Liu, Tian Lan, Wenzhuo Yang, Rowan Cassius, Doyen\\r\\nSahoo, Devansh Arpit, Sri Subramanian, Gerald Woo, et al. Merlion: A machine learning library for time\\r\\nseries. arXiv preprint arXiv:2109.09265, 2021.\\r\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\\r\\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\\r\\npages 213–229. Springer, 2020.\\r\\n[5] Robert B Cleveland, William S Cleveland, Jean E McRae, and Irma Terpenning. Stl: A seasonal-trend\\r\\ndecomposition. J. Off. Stat, 6(1):3–73, 1990.\\r\\n[6] William P Cleveland and George C Tiao. Decomposition of seasonal time series: a model for the census\\r\\nx-11 program. Journal of the American statistical Association, 71(355):581–587, 1976.\\r\\n[7] Alysha M De Livera, Rob J Hyndman, and Ralph D Snyder. Forecasting time series with complex\\r\\nseasonal patterns using exponential smoothing. Journal of the American statistical association, 106(496):\\r\\n1513–1527, 2011.\\r\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec\\x02tional transformers for language understanding. ArXiv, abs/1810.04805, 2019.\\r\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\r\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\\r\\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\\r\\nInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?\\r\\nid=YicbFdNTTy.\\r\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\\r\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\\r\\n[11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\\r\\n1997.\\r\\n[12] Charles C Holt. Forecasting seasonals and trends by exponentially weighted moving averages. International\\r\\njournal of forecasting, 20(1):5–10, 2004.\\r\\n[13] Rob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. Forecasting with exponential smoothing:\\r\\nthe state space approach. Springer Science & Business Media, 2008.\\r\\n[14] Rob J Hyndman and Yeasmin Khandakar. Automatic time series forecasting: the forecast package for r.\\r\\nJournal of statistical software, 27(1):1–22, 2008.\\r\\n[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and\\r\\nYann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego,\\r\\nCA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.\\r\\n6980.\\r\\n[16] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In Interna\\x02tional Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=\\r\\nrkgNKkHtvB.\\r\\n[17] Vitaly Kuznetsov and Mehryar Mohri. Learning theory and algorithms for forecasting non-stationary time\\r\\nseries. In NIPS, pages 541–549. Citeseer, 2015.\\r\\n[18] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal\\r\\npatterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research &\\r\\nDevelopment in Information Retrieval, pages 95–104, 2018.\\r\\n[19] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with fourier\\r\\ntransforms. arXiv preprint arXiv:2105.03824, 2021.\\r\\n10\\n[20] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan.\\r\\nEnhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.\\r\\nArXiv, abs/1907.00235, 2019.\\r\\n[21] Michaël Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks through ffts.\\r\\nCoRR, abs/1312.5851, 2014.\\r\\n[22] Eddie McKenzie and Everette S Gardner Jr. Damped trend exponential smoothing: a modelling viewpoint.\\r\\nInternational Journal of Forecasting, 26(4):661–665, 2010.\\r\\n[23] Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary ϕ-mixing and β-mixing\\r\\nprocesses. Journal of Machine Learning Research, 11(2), 2010.\\r\\n[24] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion\\r\\nanalysis for interpretable time series forecasting. In International Conference on Learning Representations,\\r\\n2019.\\r\\n[25] Alessandro Raganato, Yves Scherrer, and Jörg Tiedemann. Fixed encoder self-attention patterns in\\r\\ntransformer-based machine translation. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of\\r\\nthe Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020,\\r\\nvolume EMNLP 2020 of Findings of ACL, pages 556–568. Association for Computational Linguis\\x02tics, 2020. doi: 10.18653/v1/2020.findings-emnlp.49. URL https://doi.org/10.18653/v1/2020.\\r\\nfindings-emnlp.49.\\r\\n[26] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting\\r\\nwith autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–1191, 2020. ISSN\\r\\n0169-2070. doi: https://doi.org/10.1016/j.ijforecast.2019.07.001. URL https://www.sciencedirect.\\r\\ncom/science/article/pii/S0169207019301888.\\r\\n[27] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:\\r\\nA simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):\\r\\n1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.\\r\\n[28] Ivan Svetunkov. Complex exponential smoothing. Lancaster University (United Kingdom), 2016.\\r\\n[29] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking\\r\\nself-attention for transformer models. In International Conference on Machine Learning, pages 10183–\\r\\n10192. PMLR, 2021.\\r\\n[30] Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37–45, 2018.\\r\\n[31] Marina Theodosiou. Forecasting monthly and quarterly time series using stl decomposition. International\\r\\nJournal of Forecasting, 27(4):1178–1195, 2011.\\r\\n[32] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.\\r\\nTransformer dissection: An unified understanding for transformer’s attention via the lens of kernel. In\\r\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\r\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4344–4353,\\r\\nHong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\r\\nD19-1443. URL https://aclanthology.org/D19-1443.\\r\\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\r\\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\\r\\nsystems, pages 5998–6008, 2017.\\r\\n[34] Michail Vlachos, Philip Yu, and Vittorio Castelli. On periodicity detection and structural periodic similarity.\\r\\nIn Proceedings of the 2005 SIAM international conference on data mining, pages 449–460. SIAM, 2005.\\r\\n[35] Peter R Winters. Forecasting sales by exponentially weighted moving averages. Management science, 6\\r\\n(3):324–342, 1960.\\r\\n[36] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. CoST: Contrastive learning of\\r\\ndisentangled seasonal-trend representations for time series forecasting. In International Conference on\\r\\nLearning Representations, 2022. URL https://openreview.net/forum?id=PilZY3omXV2.\\r\\n[37] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers\\r\\nwith Auto-Correlation for long-term series forecasting. In Advances in Neural Information Processing\\r\\nSystems, 2021.\\r\\n11\\n[38] Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei, and Junzhou Huang. Adversarial sparse\\r\\ntransformer for time series forecasting. In NeurIPS, 2020.\\r\\n[39] Weiqiu You, Simeng Sun, and Mohit Iyyer. Hard-coded gaussian attention for neural machine translation.\\r\\narXiv preprint arXiv:2005.00742, 2020.\\r\\n[40] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A\\r\\ntransformer-based framework for multivariate time series representation learning. In Proceedings of the\\r\\n27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2114–2124, 2021.\\r\\n[41] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\r\\nInformer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of AAAI,\\r\\n2021.\\r\\n12\\nA Exponential Smoothing Attention\\r\\nA.1 Exponential Smoothing Attention Matrix\\r\\nAES =\\r\\n\\uf8ee\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8ef\\r\\n\\uf8f0\\r\\n(1 − α)\\r\\n1 α 0 0 . . . 0\\r\\n(1 − α)\\r\\n2 α(1 − α) α 0 . . . 0\\r\\n(1 − α)\\r\\n3 α(1 − α)2 α(1 − α) α . . . 0\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n(1 − α)\\r\\nL α(1 − α)L−1\\r\\n. . . α(1 − α)\\r\\nj\\r\\n. . . α\\r\\n\\uf8f9\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fa\\r\\n\\uf8fb\\r\\nA.2 Efficient Exponential Smoothing Attention Algorithm\\r\\nAlgorithm 1 PyTorch-style pseudocode of efficient AES\\r\\nconv1d_fft: efficient convolution operation implemented with fast Fourier transform (Appendix A, Algorithm 3),\\r\\nouter: outer product\\r\\n# V: value matrix, shape: L x d\\r\\n# v0: initial state, shape: d\\r\\n# alpha: smoothing parameter, shape: 1\\r\\n# obtain exponentially decaying weights\\r\\n# and compute weighted combination\\r\\npowers = arange(L) # L\\r\\nweight = alpha ∗ (1 − alpha) ∗∗ flip(powers) # L\\r\\noutput = conv1d_fft(V, weight, dim=0) # L x d\\r\\n# compute contribution from initial state\\r\\ninit_weight = (1 − alpha) ∗∗ (powers + 1) # L\\r\\ninit_output = outer(init_weight, v0) # L x d\\r\\nreturn init_output + output\\r\\nA.3 Level Smoothing via Exponential Smoothing Attention\\r\\nE\\r\\n(n)\\r\\nt = α ∗ (E\\r\\n(n−1)\\r\\nt − S\\r\\n(n)\\r\\nt\\r\\n) + (1 − α) ∗ (E\\r\\n(n)\\r\\nt−1 + B\\r\\n(n)\\r\\nt−1\\r\\n)\\r\\n= α ∗ (E\\r\\n(n−1)\\r\\nt − S\\r\\n(n)\\r\\nt\\r\\n) + (1 − α) ∗ B\\r\\n(n)\\r\\nt−1\\r\\n+ (1 − α) ∗ [α ∗ (E\\r\\n(n−1)\\r\\nt−1 − S\\r\\n(n)\\r\\nt−1\\r\\n) + (1 − α) ∗ (E\\r\\n(n)\\r\\nt−2 + B\\r\\n(n)\\r\\nt−2\\r\\n)]\\r\\n= α ∗ (E\\r\\n(n−1)\\r\\nt − S\\r\\n(n)\\r\\nt\\r\\n) + α ∗ (1 − α) ∗ (E\\r\\n(n−1)\\r\\nt−1 − S\\r\\n(n)\\r\\nt−1\\r\\n)\\r\\n+ (1 − α) ∗ B\\r\\n(n)\\r\\nt−1 + (1 − α)\\r\\n2\\r\\n∗ B\\r\\n(n)\\r\\nt−2\\r\\n+ (1 − α)\\r\\n2\\r\\n[α ∗ (E\\r\\n(n−1)\\r\\nt−2 − S\\r\\n(n)\\r\\nt−2\\r\\n) + (1 − α) ∗ (E\\r\\n(n)\\r\\nt−3 + B\\r\\n(n)\\r\\nt−3\\r\\n)]\\r\\n.\\r\\n.\\r\\n.\\r\\n= (1 − α)\\r\\nt\\r\\n(E\\r\\n(n)\\r\\n0 − S\\r\\n(n)\\r\\n0\\r\\n) +Xt−1\\r\\nj=0\\r\\nα ∗ (1 − α)\\r\\nj\\r\\n∗ (E\\r\\n(n−1)\\r\\nt−j − S\\r\\n(n)\\r\\nt−j\\r\\n) +Xt\\r\\nk=1\\r\\n(1 − α)\\r\\nk\\r\\n∗ B\\r\\n(n)\\r\\nt−k\\r\\n= AES(E\\r\\n(n−1)\\r\\nt−L:t − S\\r\\n(n)\\r\\nt−L:t\\r\\n) +Xt\\r\\nk=1\\r\\n(1 − α)\\r\\nk\\r\\n∗ B\\r\\n(n)\\r\\nt−k\\r\\nBased on the above expansion of the level equation, we observe that E\\r\\n(t)\\r\\nn can be computed by a sum\\r\\nof two terms, the first of which is given by an AES term, and we finally, we note that the second term\\r\\ncan also be calculated using the conv1d_fft algorithm, resulting in a fast implementation of level\\r\\nsmoothing.\\r\\n13\\nA.4 Further Details on ESA Implementation\\r\\nAlgorithm 2 PyTorch-style pseudocode of naive\\r\\nAES\\r\\nmm: matrix multiplication, outer: outer product\\r\\nrepeat: einops style tensor operations,\\r\\ngather: gathers values along an axis specified by dim\\r\\n# V: value matrix, shape: L x d\\r\\n# v0: initial state, shape: d\\r\\n# alpha: smoothing parameter, shape: 1\\r\\nL, d = V.shape\\r\\n# obtain exponentially decaying weights\\r\\npowers = arange(L) # L\\r\\nweight = alpha ∗ (1 − alpha).pow(flip(powers)) #\\r\\nL\\r\\n# perform a strided roll operation\\r\\n# rolls a matrix along the columns in a strided\\r\\nmanner\\r\\n# i.e. first row is shifted right by L−1\\r\\npositions,\\r\\n# second row is shifted L−2, ..., last row is\\r\\nshifted by 0.\\r\\nweight = repeat(weight, \\'L −> T L\\', T=L) # L x L\\r\\nindices = repeat(arange(L), \\'L −> T L\\', T=L)\\r\\nindices = (indices − (arange(L) + 1).unsqueeze(1)\\r\\n) % L\\r\\nweight = gather(weight, dim=−1, index=indices)\\r\\n# triangle masking to achieve the exponential\\r\\nsmoothing attention matrix\\r\\nweight = triangle_causal_mask(weight)\\r\\noutput = mm(weight, V)\\r\\ninit_weight = (1 − alpha) ∗∗ (powers + 1)\\r\\ninit_output = outer(init_weight, v0)\\r\\nreturn init_output + output\\r\\nAlgorithm 3 PyTorch-style pseudocode of\\r\\nconv1d_fft\\r\\nnext_fast_len: find the next fast size of input data to fft,\\r\\nfor zero-padding, etc.\\r\\nrfft: compute the one-dimensional discrete Fourier Trans\\x02form for real input\\r\\nx.conj(): return the complex conjugate, element-wise\\r\\nirfft: computes the inverse of rfft\\r\\nroll: roll array elements along a given axis\\r\\nindex_select: returns a new tensor which index es the in\\x02put tensor along dimension dim using the entries in index\\r\\n# V: value matrix, shape: L x d\\r\\n# weight: exponential smoothing attention vector,\\r\\nshape: L\\r\\n# dim: dimension to perform convolution on\\r\\n# obtain lengths of sequence to perform\\r\\nconvolution on\\r\\nN = V.size(dim)\\r\\nM = weight.size(dim)\\r\\n# Fourier transform on inputs\\r\\nfast_len = next_fast_len(N + M − 1)\\r\\nF_V = rfft(V, fast_len, dim=dim)\\r\\nF_weight = rfft(weight, fast_len, dim=dim)\\r\\n# multiplication and inverse\\r\\nF_V_weight = F_V ∗ F_weight.conj()\\r\\nout = irfft(F_V_weight, fast_len, dim=dim)\\r\\nout = out.roll(−1, dim=dim)\\r\\n# select the correct indices\\r\\nidx = range(fast_len − N, fast_len)\\r\\nout = out.index_select(dim, idx)\\r\\nreturn out\\r\\nAlgorithm 2 describes the naive implementation for ESA by first constructing the exponential\\r\\nsmoothing attention matrix, AES, and performing the full matrix-vector multiplication. Efficient\\r\\nAES relies on Algorithm 3, to achieve an O(Llog L) complexity, by speeding up the matrix-vector\\r\\nmultiplication. Due to the structure lower triangular structure of AES (ignoring the first column), we\\r\\nnote that performing a matrix-vector multiplication with it is equivalent to performing a convolution\\r\\nwith the last row. Algorithm 3 describes the pseudocode for fast convolutions using fast Fourier\\r\\ntransforms.\\r\\nB Discrete Fourier Transform\\r\\nThe DFT of a sequence with regular intervals, x = (x0, x1, . . . , xN−1) is a sequence of complex\\r\\nnumbers,\\r\\nck =\\r\\nN\\r\\nX−1\\r\\nn=0\\r\\nxn · exp(−i2πkn/N),\\r\\nfor k = 0, 1, . . . , N − 1, where ck are known as the Fourier coefficients of their respective Fourier\\r\\nfrequencies. Due to the conjugate symmetry of DFT for real-valued signals, we simply consider the\\r\\nfirst bN/2c + 1 Fourier coefficients and thus we denote the DFT as F : R\\r\\nN → CbN/2c+1. The DFT\\r\\nmaps a signal to the frequency domain, where each Fourier coefficient can be uniquely represented\\r\\n14\\nby the amplitude, |ck|, and the phase, φ(ck),\\r\\n|ck| =\\r\\np\\r\\nR{ck}\\r\\n2 + I{ck}2 φ(ck) = tan−1\\r\\n\\x12\\r\\nI{ck}\\r\\nR{ck}\\r\\n\\x13\\r\\nwhere R{ck} and I{ck} are the real and imaginary components of ck respectively. Finally, the\\r\\ninverse DFT maps the frequency domain representation back to the time domain,\\r\\nxn = F\\r\\n−1\\r\\n(c)n =\\r\\n1\\r\\nN\\r\\nN\\r\\nX−1\\r\\nk=0\\r\\nck · exp(i2πkn/N),\\r\\nC Implementation Details\\r\\nC.1 Hyperparameters\\r\\nFor all experiments, we use the same hyperparameters for the encoder layers, decoder stacks, model\\r\\ndimensions, feedforward layer dimensions, number of heads in multi-head exponential smoothing\\r\\nattention, and kernel size for input embedding as listed in Table 3. We perform hyperparameter\\r\\ntuning via a grid search over the number of frequencies K, lookback window size, and learning rate,\\r\\nselecting the settings which perform the best on the validation set based on MSE (on results averaged\\r\\nover three runs). The search range is reported in Table 3, where the lookback window size search\\r\\nrange was decided to be set as the values for the horizon sizes for the respective datasets.\\r\\nTable 3: Hyperparameters used in ETSformer.\\r\\nHyperparameter Value\\r\\nEncoder layers 2\\r\\nDecoder stacks 2\\r\\nModel dimension 512\\r\\nFeedforward dimension 2048\\r\\nMulti-head ESA heads 8\\r\\nInput embedding kernel size 3\\r\\nK K ∈ {0, 1, 2, 3}\\r\\nLookback window size L ∈ {96, 192, 336, 720}\\r\\nLookback window size (ILI) L ∈ {24, 36, 48, 60}\\r\\nLearning rate lr ∈ {1e−3, 3e−4, 1e−4, 3e−5, 1e−5}\\r\\nC.2 Optimization\\r\\nWe use the Adam optimizer [15] with β1 = 0.9, β2 = 0.999, and \\x0f = 1e − 08, and a batch size of 32.\\r\\nWe schedule the learning rate with linear warmup over 3 epochs, and cosine annealing thereafter for a\\r\\ntotal of 15 training epochs for all datasets. The minimum learning rate is set to 1e-30. For smoothing\\r\\nand damping parameters, we set the learning rate to be 100 times larger and do not use learning rate\\r\\nscheduling. Training was done on an Nvidia A100 GPU.\\r\\nC.3 Regularization\\r\\nWe apply two forms of regularization during the training phase.\\r\\nData Augmentations We utilize a composition of three data augmentations, applied in the follow\\x02ing order - scale, shift, and jitter, activating with a probability of 0.5.\\r\\n1. Scale – The time-series is scaled by a single random scalar value, obtained by sampling\\r\\n\\x0f ∼ N (0, 0.2), and each time step is x˜t = \\x0fxt.\\r\\n2. Shift – The time-series is shifted by a single random scalar value, obtained by sampling\\r\\n\\x0f ∼ N (0, 0.2) and each time step is x˜t = xt + \\x0f.\\r\\n3. Jitter – I.I.D. Gaussian noise is added to each time step, from a distribution \\x0ft ∼ N (0, 0.2),\\r\\nwhere each time step is now x˜t = xt + \\x0ft.\\r\\n15\\nDropout We apply dropout [27] with a rate of p = 0.2 across the model. Dropout is applied on the\\r\\noutputs of the Input Embedding, Frequency Self-Attention and Multi-Head ES Attention blocks, in\\r\\nthe Feedforward block (after activation and before normalization), on the attention weights, as well\\r\\nas damping weights.\\r\\nD Datasets\\r\\nETT2 Electricity Transformer Temperature [41] is a multivariate time-series dataset, comprising of\\r\\nload and oil temperature data recorded every 15 minutes from electricity transformers. ETT consists\\r\\nof two variants, ETTm and ETTh, whereby ETTh is the hourly-aggregated version of ETTm, the\\r\\noriginal 15 minute level dataset.\\r\\nECL3 Electricity Consuming Load measures the electricity consumption of 321 households clients\\r\\nover two years, the original dataset was collected at the 15 minute level, but is pre-processed into an\\r\\nhourly level dataset.\\r\\nExchange4 Exchange [18] tracks the daily exchange rates of eight countries (Australia, United\\r\\nKingdom, Canada, Switzerland, China, Japan, New Zealand, and Singapore) from 1990 to 2016.\\r\\nTraffic5 Traffic is an hourly dataset from the California Department of Transportation describing\\r\\nroad occupancy rates in San Francisco Bay area freeways.\\r\\nWeather6 Weather measures 21 meteorological indicators like air temperature, humidity, etc., every\\r\\n10 minutes for the year of 2020.\\r\\nILI7Influenza-like Illness records the ratio of patients seen with ILI and the total number of patients\\r\\non a weekly basis, obtained by the Centers for Disease Control and Prevention of the United States\\r\\nbetween 2002 and 2021.\\r\\nE Synthetic Dataset\\r\\nThe synthetic dataset is constructed by a combination of trend and seasonal component. Each instance\\r\\nin the dataset has a lookack window length of 192 and forecast horizon length of 48. The trend\\r\\npattern follows a nonlinear, saturating pattern, b(t) = 1\\r\\n1+exp β0(t−β1)\\r\\n, where β0 = −0.2, β1 = 192.\\r\\nThe seasonal pattern follows a complex periodic pattern formed by a sum of sinusoids. Concretely,\\r\\ns(t) = A1 cos(2πf1t) + A2 cos(2πf2t, where f1 = 1/10, f2 = 1/13 are the frequencies, A1 =\\r\\nA2 = 0.15 are the amplitudes. During training phase, we use an additional noise component by\\r\\nadding i.i.d. gaussian noise with 0.05 standard deviation. Finally, the i-th instance of the dataset is\\r\\nxi = [xi(1), xi(2), . . . , xi(192 + 48)], where xi(t) = b(t) + s(t + i) + \\x0f.\\r\\n2\\r\\nhttps://github.com/zhouhaoyi/ETDataset\\r\\n3\\r\\nlhttps://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\r\\n4\\r\\nhttps://github.com/laiguokun/multivariate-time-series-data\\r\\n5\\r\\nhttps://pems.dot.ca.gov/\\r\\n6\\r\\nhttps://www.bgc-jena.mpg.de/wetter/\\r\\n7\\r\\nhttps://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\\r\\n16\\nF Univariate Forecasting Benchmark\\r\\nTable 4: Univariate forecasting results over various forecast horizons. Best results are bolded, and\\r\\nsecond best results are underlined.\\r\\nMethods ETSformer Autoformer Informer N-BEATS DeepAR Prophet ARIMA AutoETS\\r\\nMetrics MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTm2\\r\\n96 0.080 0.212 0.065 0.189 0.088 0.225 0.082 0.219 0.099 0.237 0.287 0.456 0.211 0.362 0.794 0.617\\r\\n192 0.150 0.302 0.118 0.256 0.132 0.283 0.120 0.268 0.154 0.310 0.312 0.483 0.261 0.406 1.078 0.740\\r\\n336 0.175 0.334 0.154 0.305 0.180 0.336 0.226 0.370 0.277 0.428 0.331 0.474 0.317 0.448 1.279 0.822\\r\\n720 0.224 0.379 0.182 0.335 0.300 0.435 0.188 0.338 0.332 0.468 0.534 0.593 0.366 0.487 1.541 0.924\\r\\nExchange\\r\\n96 0.099 0.230 0.241 0.299 0.591 0.615 0.156 0.299 0.417 0.515 0.828 0.762 0.112 0.245 0.192 0.316\\r\\n192 0.223 0.353 0.273 0.665 1.183 0.912 0.669 0.665 0.813 0.735 0.909 0.974 0.304 0.404 0.355 0.442\\r\\n336 0.421 0.497 0.508 0.605 1.367 0.984 0.611 0.605 1.331 0.962 1.304 0.988 0.736 0.598 0.577 0.578\\r\\n720 1.114 0.807 0.991 0.860 1.872 1.072 1.111 0.860 1.890 1.181 3.238 1.566 1.871 0.935 1.242 0.865\\r\\nG ETSformer Standard Deviation\\r\\nTable 5: ETSformer main benchmark results with standard deviation. Experiments are performed\\r\\nover three runs.\\r\\n(a) Multivariate benchmark.\\r\\nMetrics MSE (SD) MAE (SD) ETTm2\\r\\n96 0.189 (0.002) 0.280 (0.001)\\r\\n192 0.253 (0.002) 0.319 (0.001)\\r\\n336 0.314 (0.001) 0.357 (0.001)\\r\\n720 0.414 (0.000) 0.413 (0.001)\\r\\nECL\\r\\n96 0.187 (0.001) 0.304 (0.001)\\r\\n192 0.199 (0.001) 0.315 (0.002)\\r\\n336 0.212 (0.001) 0.329 (0.002)\\r\\n720 0.233 (0.006) 0.345 (0.006)\\r\\nExchange\\r\\n96 0.085 (0.000) 0.204 (0.001)\\r\\n192 0.182 (0.003) 0.303 (0.002)\\r\\n336 0.348 (0.004) 0.428 (0.003)\\r\\n720 1.025 (0.031) 0.774 (0.014)\\r\\nTraffic\\r\\n96 0.607 (0.005) 0.392 (0.005)\\r\\n192 0.621 (0.015) 0.399 (0.013)\\r\\n336 0.622 (0.003) 0.396 (0.003)\\r\\n720 0.632 (0.004) 0.396 (0.004)\\r\\nWeather\\r\\n96 0.197 (0.007) 0.281 (0.008)\\r\\n192 0.237 (0.005) 0.312 (0.004)\\r\\n336 0.298 (0.003) 0.353 (0.003)\\r\\n720 0.352 (0.007) 0.388 (0.002)\\r\\nILI\\r\\n24 2.527 (0.061) 1.020 (0.021)\\r\\n36 2.615 (0.103) 1.007 (0.013)\\r\\n48 2.359 (0.056) 0.972 (0.011)\\r\\n60 2.487 (0.006) 1.016 (0.007)\\r\\n(b) Univariate benchmark.\\r\\nMetrics MSE (SD) MAE (SD) ETTm2\\r\\n96 0.080 (0.001) 0.212 (0.001)\\r\\n192 0.150 (0.024) 0.302 (0.026)\\r\\n336 0.175 (0.012) 0.334 (0.014)\\r\\n720 0.224 (0.008) 0.379 (0.006)\\r\\nExchange\\r\\n96 0.099 (0.003) 0.230 (0.003)\\r\\n192 0.223 (0.015) 0.353 (0.009)\\r\\n336 0.421 (0.002) 0.497 (0.000)\\r\\n720 1.114 (0.049) 0.807 (0.016)\\r\\n17\\nH Computational Efficiency\\r\\n4896\\r\\n168336720\\r\\n144028805670\\r\\nLookback Window\\r\\n0.1\\r\\n0.2\\r\\n0.3\\r\\n0.4\\r\\n0.5\\r\\nTime (s)\\r\\nETSformer (K=1)\\r\\nETSformer (K=2)\\r\\nETSformer (K=3)\\r\\nAutoformer\\r\\nInformer\\r\\nTransformer\\r\\n4896\\r\\n168336720\\r\\n144028805670\\r\\nForecast Horizon\\r\\n0.1\\r\\n0.2\\r\\n0.3\\r\\n0.4\\r\\n0.5\\r\\nTime (s)\\r\\n(a) Runtime Efficiency Analysis\\r\\n4896\\r\\n168336720\\r\\n144028805670\\r\\nLookback Window\\r\\n0\\r\\n5\\r\\n10\\r\\n15\\r\\n20\\r\\n25\\r\\n30\\r\\nMemory (GB)\\r\\nETSformer (K=1)\\r\\nETSformer (K=2)\\r\\nETSformer (K=3)\\r\\nAutoformer\\r\\nInformer\\r\\nTransformer\\r\\n4896\\r\\n168336720\\r\\n144028805670\\r\\nForecast Horizon\\r\\n0\\r\\n5\\r\\n10\\r\\n15\\r\\n20\\r\\n25\\r\\n30\\r\\n35\\r\\nMemory (GB)\\r\\n(b) Memory Efficiency Analysis\\r\\nFigure 5: Computational Efficiency Analysis. Values reported are based on the training phase of\\r\\nETTm2 multivariate setting. Horizon is fixed to 48 for lookback window plots, and lookback is fixed\\r\\nto 48 for forecast horizon plots. For runtime efficiency, values refer to the time for one iteration. The\\r\\n“ \" marker indicates an out-of-memory error for those settings.\\r\\nIn this section, our goal is to compare ETSformer’s computational efficiency with that of competing\\r\\nTransformer-based approaches. Visualized in Figure 5, ETSformer maintains competitive efficiency\\r\\nwith compting quasilinear complexity Transformers, while obtaining state-of-the-art performance.\\r\\nFurthermore, due to ETSformer’s unique decoder architecture which relies on its Trend Damping\\r\\nand Frequency Attention modules rather than output embeddings, ETSformer maintains superior\\r\\nefficiency as forecast horizon increases.\\r\\n18'},\n",
       " {'name': '1704.02446v1.pdf',\n",
       "  'content': 'Seismic facies recognition based on prestack data using deep convolutional autoencoder\\r\\nFeng Qian1, Miao Yin2, Ming-Jun Su3, Yaojun Wang1, Guangmin Hu1\\r\\n1Center of Information Geoscience, University of Electronic Science and Technology of China,\\r\\n2School of Resource and Environment, University of Electronic Science and Technology of China,\\r\\n3PetroChina Research Institute of Petroleum Exploration and Development (RIPED) - Northwest, Lanzhou, China\\r\\nSUMMARY\\r\\nPrestack seismic data carries much useful information that can\\r\\nhelp us find more complex atypical reservoirs. Therefore, we\\r\\nare increasingly inclined to use prestack seismic data for seis\\x02mic facies recognition. However, due to the inclusion of ex\\x02cessive redundancy, effective feature extraction from prestack\\r\\nseismic data becomes critical. In this paper, we consider seis\\x02mic facies recognition based on prestack data as an image clus\\x02tering problem in computer vision (CV) by thinking of each\\r\\nprestack seismic gather as a picture. We propose a convo\\x02lutional autoencoder (CAE) network for deep feature learn\\x02ing from prestack seismic data, which is more effective than\\r\\nprincipal component analysis (PCA) in redundancy removing\\r\\nand valid information extraction. Then, using conventional\\r\\nclassification or clustering techniques (e.g. K-means or self\\x02organizing maps) on the extracted features, we can achieve\\r\\nseismic facies recognition. We applied our method to the\\r\\nprestack data from physical model and LZB region. The re\\x02sult shows that our approach is superior to the conventionals.\\r\\nINTRODUCTION\\r\\nSeismic facies recognition is an auxiliary means that we use\\r\\nmachine learning to generate facies map which reveals the\\r\\ndetails of the underlying geological features. With the im\\x02provement of computer’s computing ability, pattern recogni\\x02tion techniques can be used to handle larger seismic data and\\r\\nget more accurate results. As a result, seismic facies recogni\\x02tion becomes increasingly significant.\\r\\nSo far, almost all supervised and unsupervised classification\\r\\nalgorithms including K-means, self-organizing maps (SOM),\\r\\ngenerative topographic mapping (GTM), support vector ma\\x02chines (SVM) and artificial neural networks (ANN) etc. have\\r\\nbeen successfully used in seismic facies recognition. In fact,\\r\\nthe most important parameter in the analysis is not classifica\\x02tion method but the correct input attributes (Zhao et al., 2015).\\r\\nIt means that effective feature extraction from seismic data\\r\\nis the key to seismic facies recognition. Balz et al. (2000)\\r\\ndeveloped a methodology to generate a so-called AVO-trace\\r\\nfrom prestack seismic data which can be classified by con\\x02ventional clustering techniques. Coleou et al. (2003) extracted ´\\r\\nfeatures via constructing vectors of dip magnitude, coherence\\r\\nand reflector parallelism. de Matos et al. (2006) used wavelet\\r\\ntransforms to identify seismic trace singularities in each ge\\x02ologically oriented segment. Gao (2007) applied gray level\\r\\nco-occurrence matrix (GLCM) to generate facies map using\\r\\nSOM. Roy et al. (2013) proposed an SOM classification work\\x02flow based on multiple seismic attributes. Of the seismic facies\\r\\nrecognition methods, only a few extract features from prestack\\r\\nseismic data.\\r\\nObviously, prestack seismic data contains richer information,\\r\\nwith which we can obtain more details of atypical reservoirs\\r\\nin facies map. Nevertheless, there is also a large amount of re\\x02dundancy in the prestack data that may effect the performance\\r\\nof seismic facies recognition negatively. Principal components\\r\\nanalysis (PCA) and independent components analysis (ICA)\\r\\nare commonly used to reduce redundancy of attributes to low\\x02dimensional independent meta features (Gao, 2007). These\\r\\napproaches just map the original prestack seismic data to a\\r\\nsubspace in view of mathematics. However, our motivation\\r\\nis to develop a model, which can learn profound features from\\r\\nseismic data automatically.\\r\\nIn order to construct the model, referring to the latest technolo\\x02gies in the fields of artificial intelligence (AI) and computer\\r\\nvision (CV), we applied convolutional neural network (CNN)\\r\\nin seismic facies recognition. Convolutional neural network is\\r\\none framework of deep learning which has been used effec\\x02tively in many machine learning tasks. With the excellence of\\r\\nfeature learning, CNN has achieved particular success in the\\r\\ndomain of image classification (Krizhevsky et al., 2012), ac\\x02tion recognition (Ji et al., 2013), video classification (Karpathy\\r\\net al., 2014), speech recognition (Abdel-Hamid et al., 2014)\\r\\nand face recognition (Farfade et al., 2015). Considering the\\r\\nseismic prestack data as images, we convert seismic facies\\r\\nrecognition to image classification problem resolved by using\\r\\nCNN to extract features.\\r\\nIn this paper, we develop a convolutional autoencoder (CAE)\\r\\nnetwork with multiple layers. In each layer, higher-level fea\\x02ture maps are generated by convolving features of the lower\\x02level layer with converlutional filters which are trained by min\\x02imizing a loss function (explained in the next section). Firstly,\\r\\nwe input the seismic prestack data to train the network and ob\\x02tain extracted features. Then we utilize unsupervised pattern\\r\\nrecognition techniques to generate the facies map. We have\\r\\ncompared the facies maps depicted by our method and other\\r\\ntwo methods (PCA and using postack data) with two types of\\r\\nprestack seismic data. The result demonstrates the superiority\\r\\nof our approach.\\r\\nMETHOD\\r\\nThe workflow of our method mainly consists of the following\\r\\nfour steps:\\r\\n(1) Select a proper window of data which contains at least one\\r\\ncrest and one trough per trace in prestack seismic gathers along\\r\\nan interpreted horizon;\\r\\n(2) Input these two-dimensional data to the convolutional au\\x02arXiv:1704.02446v1 [cs.CV] 8 Apr 2017\\nSeismic facies recognition based on prestack data using deep convolutional autoencoder\\r\\nPatch representation\\r\\n(encoding)\\r\\nReconstruction\\r\\n(decoding)\\r\\nA window of prestack seismic data \\r\\n(input)\\r\\nFeature maps\\r\\nReconstructed prestack seismic data\\r\\n(output)\\r\\nConvolution Max pooling Unpooling Deconvolution\\r\\nFigure 1: Architecture of convolutional neural network we proposed to learn deep features from prestack seismic data.\\r\\ntoencoder network and train it;\\r\\n(3) Re-input these two-dimensional data to the trained net\\x02work, obtain features extracted by it;\\r\\n(4) Use unsupervised pattern recognition algorithm to cluster\\r\\nthese extracted features and generate facies map.\\r\\nConvolutional Neural Network For Feature Learning\\r\\nAn overview of our convolutional neural network is given by\\r\\nFigure 1. The input X is a window of prestack seismic data,\\r\\nwhich can be considered as an image. Our goal is to learn\\r\\nsome features that can best represent X. That means we wish to\\r\\nfind a mapping Φ, which makes the reconstruction Z as similar\\r\\nas possible to X. Feature learning from seismic prestack data\\r\\ninvolves the following three aspects.\\r\\nPatch representation. This operation extracts patches from\\r\\nthe seismic prestack window data X and represents each patch\\r\\nas a high-dimensional tensor. These tensors comprise a set of\\r\\nfeature maps, of which the number is equal to the dimension\\x02ality of the tensors. At the beginning the input data is con\\x02volved, and then a max pooling step is used to shrink the con\\x02volutional layers — taking the maximum of each 2×2 matrix\\r\\nwith a strides of 2 along both width and height. Max pooling\\r\\nreduces the spatial size of the representation and hence to also\\r\\ncontrol overfitting. Formally, for a window of prestack data as\\r\\nan input X, the latent representation of the k\\r\\nth feature map is\\r\\ngiven by\\r\\nΦk(X) = σ (Ψ(X∗Wk +bk)), (1)\\r\\nwhere W represents the filter, Ψ donates max pooling, b is the\\r\\nbias which is broadcasted to the whole map. σ is an activation\\r\\nfunction that is considered as a non-linear mapping (we use\\r\\nleaky ReLu function), and ∗ denotes the 2D convolution. Here\\r\\nW is of a size 1×n×n×k, where 1 is the number of channels\\r\\nin the input (for prestack seicmic data, the value of is always\\r\\n1), n is the spatial size of a filter, and k is the number of filters.\\r\\nIntuitively, W applies k convolutions on the input, and each\\r\\nconvolution has a kernel size n × n. The output is composed\\r\\nof k feature maps. b is an k-dimensional vector, whose each\\r\\nelement is associated with a filter. The convolution of an m×m\\r\\nmatrix with an n × n matrix may in fact result in an (m + n −\\r\\n1) × (m + n − 1) matrix (full convolution) or in an (m − n +\\r\\n1)×(m−n+1) (valid convolution).\\r\\nReconstruction. In the network, the back two layers are sym\\x02metrical with the front two layers, which are called unpooling\\r\\nlayer and deconvolution layer. The purpose of this design is to\\r\\nreconstruct the unlabeled input. The reconstruction is obtained\\r\\nusing\\r\\nZ = σ\\r\\n X\\r\\nk∈H\\r\\nΨ˜ (Yk) ∗W˜\\r\\nk +ck\\r\\n!\\r\\n, (2)\\r\\nwhere again there is one bias c per input channel. H identifies\\r\\nthe group of latent feature maps, W˜ identifies the flip operation\\r\\nover both dimensions of the weights (Masci et al., 2011), Ψ˜\\r\\ndonates unpooling operation. Generally, max pooling is non\\x02revertible, so instead we formulate a approximate solution that\\r\\ncan be used in this case. When reconstructing, a single indi\\x02vidual unpooling location is picked randomly and the pooled\\r\\nvalue is placed there, setting the other unpooling location to\\r\\nzero. This is a good approximation for the max unpooling pro\\x02cess, although it may results in some noise in the reconstructed\\r\\nmaps.\\r\\nTraining. We use denoising autoencoding to train the network.\\r\\nThe input X is transformed to some new value Xc by randomly\\r\\nsetting values zero with a very small probability and the net\\x02work is trained to reconstruct the original input. Training is\\nSeismic facies recognition based on prestack data using deep convolutional autoencoder\\r\\n(a) (b) (c)\\r\\nFigure 2: (a) The original waveform of a window of prestack seismic data. (b) The reconstructed prestack seismic data. (c) The\\r\\nextracted features using the convolutional autoencoder network.\\r\\ndone individually layer by layer and posed as an optimization\\r\\nproblem. We find weights W and biases b such that the fol\\x02lowing loss function is minimized. We use mean squared error\\r\\n(MSE) as the loss function:\\r\\nL =\\r\\n1\\r\\n2n\\r\\nXn\\r\\ni=1\\r\\n(Xi −Zi)\\r\\n2\\r\\n. (3)\\r\\nThe weights W and the biases b are initialized to small ran\\x02dom values found using the fan-in and fan-out criteria (Hinton,\\r\\n2012). As for standard networks the back-propagation algo\\x02rithm is applied to compute the gradient of the error function\\r\\nwith respect to the parameters.\\r\\nWe make use of automatic derivatives calculated by Tensor\\x02Flow with GPUs (Abadi et al., 2016) and iteratively input each\\r\\ndata taken from prestack seismic gathers while updating the\\r\\nfilters in the direction that minimizes the loss function. An\\r\\noriginal input and the reconstruction are shown in Figure 2-a\\r\\nand 2-b. Figure 2-c gives the extracted features from the gather.\\r\\nUnsupervised Pattern Recognition\\r\\nThis is the final stage of our method for generating facies map.\\r\\nThe goal is to cluster the one-dimensional vectors, which is\\r\\nderived from the middle feature maps of the convolutional au\\x02toencoder network. Clustering can be accomplished by several\\r\\nunsupervised strategies such as K-means clustering, fuzzy c\\x02means, SOM, etc. Without losing generality, we use K-means\\r\\nalgorithm to generate facies map. It is based on minimizing\\r\\nthe following function\\r\\nJm =\\r\\nX\\r\\nN\\r\\ni=1\\r\\nXc\\r\\nj=1\\r\\nU\\r\\nm\\r\\ni j\\r\\n\\r\\r\\n\\rxi −Cj\\r\\n\\r\\r\\n\\r\\r\\n2\\r\\n, (4)\\r\\nwhere m is any real number greater than one, Ui j is the de\\x02gree of membership of Xi\\r\\nin the cluster j, Xiis the i\\r\\nth of d\\x02dimensional measured data, and Cj\\r\\nis the d-dimension center\\r\\nof cluster. The cluster centroids are calculated by using the\\r\\nfollowing equation:\\r\\nCj =\\r\\nPN\\r\\ni=1\\r\\nu\\r\\nm\\r\\ni jXi\\r\\nPN\\r\\ni=1\\r\\nu\\r\\nm\\r\\ni j\\r\\n. (5)\\r\\nEXAMPLE\\r\\nTo verify the performance of our method, we applied it to two\\r\\nkinds of prestack data. One is from artificial physical model,\\r\\nanother is from LZB region which is a real work area. We se\\x02lected a interpreted horizon includes known fractures and took\\r\\na window of 48ms along the horizon to obtain data matrices.\\r\\nThe learning rate is assigned to 0.02. We set the number of\\r\\nlayers of the feature maps to 10, which are obtained by the\\r\\nconvolutional filters of size 3×3×10. On the other hand, we\\r\\nalso used poststack data and PCA (retain 90% of the principal\\r\\ncomponent) based on prestack data to generate facies maps, to\\r\\nwhich we compared the result derived by the proposed method.\\r\\nAll the results are shown in Figure 3 and Figure 4.\\r\\nPhysical Model\\r\\nAs shown in Figure 3-a, the physical model consists of three\\r\\nwater tanks and many caves. In Figure 3-b, we can clearly\\r\\nsee the smallest caves and the river (marked with red circle).\\r\\nHowever, the small caves are not presented in Figure 3-c. The\\r\\nriver in Figure 3-d, is also not as clear as in Figure 3-b.\\r\\nLZB Region\\r\\nThe facies maps of the three methods are shown in Figure 4.\\r\\nSimilarly, Figure 4-a is generated by our method, of which the\\r\\neffect is better than the other two.\\r\\nCONCLUSION\\r\\nWe have presented a novel deep learning approach for seismic\\r\\nfacies recognition based on prestack data. We show that with\\r\\nthe representation and reconstruction architecture of convolu\\x02tional neural network and layer-by-layer unsupervised training\\r\\nstrategy, reliable features can be learned directly from prestack\\nSeismic facies recognition based on prestack data using deep convolutional autoencoder\\r\\n(a)\\r\\n(b)\\r\\n(c)\\r\\n(d)\\r\\nFigure 3: Physical model and its facies maps. (a) The proto\\x02type of physical model. (b) The result using deep CAE based\\r\\non prestack data. (c) The result using poststack data. (d) The\\r\\nresult using PCA based on prestack data.\\r\\n(a)\\r\\n(b)\\r\\n(c)\\r\\nFigure 4: Facies maps of LZB region. (a) The result using deep\\r\\nCAE based on prestack data. (b) The result using poststack\\r\\ndata. (c) The result using PCA based on prestack data.\\r\\nseismic data. With the learned features we achieved superior\\r\\nperformance compared to the current method which is based\\r\\non poststack data or superficial features learned from prestack\\r\\ndata. In future work, we will focus on discovering how the pa\\x02rameters, such as the number of levels, the number of hidden\\r\\nunits, and the sparsity of active hidden units of the convolu\\x02tional autoencoders affect the performance.\\r\\nACKNOWLEDGMENTS\\r\\nThis work was supported by the Natural Science Foundation\\r\\nof China (No. U1562218). We thank the CNPC Key Labo\\x02ratory of Geophysical Prospecting under China University of\\r\\nPetroleum (Beijing) for their cooperation in providing data and\\r\\nsupport.\\nSeismic facies recognition based on prestack data using deep convolutional autoencoder\\r\\nREFERENCES\\r\\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al., 2016,\\r\\nTensorflow: Large-scale machine learning on heterogeneous distributed systems: arXiv preprint arXiv:1603.04467.\\r\\nAbdel-Hamid, O., A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, 2014, Convolutional neural networks for speech recog\\x02nition: IEEE/ACM Transactions on audio, speech, and language processing, 22, 1533–1545.\\r\\nBalz, O., F. Pivot, and D. Badolato, 2000, Fast identification of avo-anomalies using classification of prestack waveforms, in SEG\\r\\nTechnical Program Expanded Abstracts 2000: Society of Exploration Geophysicists, 106–109.\\r\\nColeou, T., M. Poupon, and K. Azbel, 2003, Unsupervised seismic facies classification: A review and comparison of techniques ´\\r\\nand implementation: The Leading Edge, 22, 942–953.\\r\\nde Matos, M. C., P. L. Osorio, and P. R. Johann, 2006, Unsupervised seismic facies analysis using wavelet transform and self\\x02organizing maps: Geophysics, 72, P9–P21.\\r\\nFarfade, S. S., M. J. Saberian, and L.-J. Li, 2015, Multi-view face detection using deep convolutional neural networks: Proceedings\\r\\nof the 5th ACM on International Conference on Multimedia Retrieval, ACM, 643–650.\\r\\nGao, D., 2007, Application of three-dimensional seismic texture analysis with special reference to deep-marine facies discrimination\\r\\nand interpretation: Offshore angola, west africa: AAPG bulletin, 91, 1665–1683.\\r\\nHinton, G. E., 2012, A practical guide to training restricted boltzmann machines, in Neural networks: Tricks of the trade: Springer,\\r\\n599–619.\\r\\nJi, S., W. Xu, M. Yang, and K. Yu, 2013, 3d convolutional neural networks for human action recognition: IEEE transactions on\\r\\npattern analysis and machine intelligence, 35, 221–231.\\r\\nKarpathy, A., G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, 2014, Large-scale video classification with convolu\\x02tional neural networks: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 1725–1732.\\r\\nKrizhevsky, A., I. Sutskever, and G. E. Hinton, 2012, Imagenet classification with deep convolutional neural networks: Advances\\r\\nin neural information processing systems, 1097–1105.\\r\\nMasci, J., U. Meier, D. Cires¸an, and J. Schmidhuber, 2011, Stacked convolutional auto-encoders for hierarchical feature extraction:\\r\\nInternational Conference on Artificial Neural Networks, Springer, 52–59.\\r\\nRoy, A., B. L. Dowdell, and K. J. Marfurt, 2013, Characterizing a mississippian tripolitic chert reservoir using 3d unsupervised and\\r\\nsupervised multiattribute seismic facies analysis: An example from osage county, oklahoma: Interpretation, 1, SB109–SB124.\\r\\nZhao, T., V. Jayaram, A. Roy, and K. J. Marfurt, 2015, A comparison of classification techniques for seismic facies recognition:\\r\\nInterpretation, 3, SAE29–SAE58.'},\n",
       " {'name': '1710.03205v1.pdf',\n",
       "  'content': 'Page | 1\\r\\nBehavioral Finance Option Pricing Formulas Consistent with Rational Dynamic \\r\\nAsset Pricing\\r\\nSvetlozar Rachev (Texas Tech University)\\r\\nStoyan Stoyanov (Stony Brook University)\\r\\nFrank J. Fabozzi (EDHEC)\\r\\nSummary: We derive behavioral finance option pricing formulas consistent with the rational \\r\\ndynamic asset pricing theory. In the existing behavioral finance option pricing formulas, the price\\r\\nprocess of the representative agent is not a semimartingale, which leads to arbitrage opportunities \\r\\nfor the option seller. In the literature on behavioral finance option pricing it is allowed the option \\r\\nbuyer and seller to have different views on the instantaneous mean return of the underlying price \\r\\nprocess, which leads to arbitrage opportunities according to Black (1972). We adjust the \\r\\nbehavioral finance option pricing formulas to be consistent with the rational dynamic asset pricing \\r\\ntheory, by introducing transaction costs on the velocity of trades which offset the gains from the\\r\\narbitrage trades.\\r\\n1.Introduction\\r\\nThere is a lot of controversy regarding the discrepancy between behavioral finance and rational \\r\\nfinance. Rubinstein (2001, p. 16) criticizes the behavioral finance approach to asset pricing. He \\r\\npoints out that as trained financial economist, he was taught that the “Prime Directive” in pricing \\r\\nis “Explain asset prices by rational models. Only if all attempts fail, resort to irrational investor \\r\\nbehavior.” In his opinion, the behavioralists literature “has lost all the constraints of this directive.” \\r\\nStatman (1995, p. 14) takes the opposite view, “standard finance is indeed so weighted down with \\r\\nanomalies that it makes much sense to continue the reconstruction of financial theory on behavioral \\nPage | 2\\r\\nlines.” There is an extensive literature that surveys the debate about pros and the cons of behavioral \\r\\nand rational approaches.1\\r\\nIn this paper, we reconcile the basic behavioral finance approach to asset pricing with the rational \\r\\nno-arbitrage pricing. We extend the Black (1972) approach to rational dynamic market with two \\r\\nrisky assets embedding transaction costs so that the behavioral market model can be viewed as a \\r\\nspecial case of the Black model. We illustrate our approach using the basic behavioral dynamic \\r\\nasset pricing model proposed by in Shefrin (2005, Section 8.1). The original Shefrin behavioral \\r\\nmodel allows for arbitrage opportunities when studied from the point of view of the rational \\r\\nfinance dynamic asset pricing theory. We demonstrate how to extend Black’s approach to \\r\\nencompass the Shefrin behavioral model.\\r\\nNext, we study the behavioral option pricing model proposed by Benninga and Mayshar \\r\\n(2000). The model assumes that the representative agent applies a discount factor, which is a \\r\\nconvex mixture of the discount factors of the two market agents who are sharing an aggregate \\r\\nconsumption process. The model allows for arbitrage opportunities. We show that with the \\r\\nintroduction of hedging and transaction costs, the Benninga-Mayshar model is arbitrage free within \\r\\nthe rational dynamic asset pricing theory.\\r\\n \\r\\n1 See, among others, Zechhauser (1986), Hirshleifer (2001), Shiller (2003), Barberis and Thaler. \\r\\n(2003, 2005), Brav, Heaton and Rosenberg (2004), Curtis and Statman (2004), Parisi and Smith \\r\\n(2005, Chapter 21), Thaler (2005), and Ricciardi (2008).\\nPage | 3\\r\\nNext, we consider markets with limited arbitrage opportunities, a topic extensively studied \\r\\nin the literature on behavioral finance (see, for example, Chandra (2016, Section 8.3).2 After \\r\\nintroducing the notion of a market agent, who we refer to as the “almost pure arbitrageur”, whose \\r\\nSharpe ratio goes to infinity, we then derive option pricing formula when the almost pure \\r\\narbitrageur is taking a short position in the option contract. We then introduce hedging costs and \\r\\nconsider the limiting case leading to option pricing in the presence of limited arbitrage. All our \\r\\nconsiderations are based on the rational finance dynamic asset pricing.\\r\\n In the BF literature the option pricing is relatively new topic3 with ShM4as a cornerstone model \\r\\nbuilt upon the Benninga and Mayshar (2000) model (shortly, BMM). ShM is an equilibrium \\r\\napproach to asset pricing, in which the representative agent views the return from the underlying \\r\\nasset as a mixture of two different normal distributed returns representing the heterogeneous views \\r\\non the asset return of the buyer and the seller of the option. Since mixture of two different log-\\r\\n \\r\\n2 Grossman and Stiglitz (1980) argue that in a perfectly efficient market (a market with no \\r\\narbitrage opportunities), traders would not have the incentive to gather information, and all \\r\\ngathered information would have been costless.\\r\\n3 Shefrin (2005), Chapter 21, Locke (2010), Pena, Alemanni and Zanotti (2011), Matsumura K. \\r\\nand Kawamoto M.(2013).\\r\\n4 See Shefrin (2005) Chapters 8 and 21\\nPage | 4\\r\\nnormal distributions is not infinitely divisible5, the price process of the representative investor is \\r\\nnot a semimartingale and thus the model allows for arbitrage opportunities6.\\r\\nFirst let us recall that lognormal distributions are infinitely divisible, but if the mixing measure is \\r\\nwith finite support and thus the mixing measure is not infinitely divisible, the mixture of log normal \\r\\ndistributions is not infinitely divisible distribution. This, unfortunately, leads to the fact that the \\r\\nprocess of the representative agent’s price process as defined in ShM (formula (8.15) on page 103, \\r\\nand formula (21.7) on page 306) is not infinitely divisible process in the limit (when the number \\r\\nof steps to the terminal time increase to infinity). As a result, ShM is not free of arbitrage \\r\\nopportunities Similar problem arises in BMM where it is written” The function f\\r\\n∗\\r\\n(Y) in (41) can \\r\\nbe considered as an average of the two agents’ density functions.” Indeed, however f\\r\\n∗\\r\\n(Y) is not \\r\\ninfinitely divisible density and that will lead to arbitrage opportunities BMM. Neither ShM nor \\r\\nBMM provide hedging strategies as those cannot be constructed.\\r\\nSecond, within the Rational Dynamic Asset Pricing Theory (RDAPT) the most important problem \\r\\nis the characterization of economically rational consistent models for financial markets7. In the \\r\\n \\r\\n5\\r\\n For extensive reviews on infinitely divisible distributions and processes we refer to Bondesson \\r\\n(1992), Sato (1999), Bose, Dasguota and Rubin (2002), Steutel and van Harn (2004)., Kyprianou \\r\\n(2006), Applebaum (2009).\\r\\n6\\r\\n Shefrin (2005) page 319, formulas (21.24), (21.25) defines a market model with two investors \\r\\nsharing two price processes with common Brownian motion as market driver, the same volatility \\r\\nparameters and different instantaneous mean returns, which leads to arbitrage opportunities\\r\\naccording to Black (1972).\\r\\n7 See for example Duffie (2001), Chapter 6.\\nPage | 5\\r\\nRDAPT, the central assumption is that of no-arbitrage: a market participant (designated as ℶ) \\r\\nshould not engage in a contract in which ℶ can lose infinite amount of money in a frictionless \\r\\nmarket. Regardless how irrational ℶ could be, ℶ should not be so misled as to be subject to infinite \\r\\nloss. There is no reason for behavioralists to object that no-arbitrage assumption as a fundamental \\r\\nnotion in finance, rational or behavioral. If this assumption is not satisfied, agents using financial \\r\\nasset pricing formulas allowing for arbitrages could suffer tremendous losses. If a trader being a \\r\\nbehaviorist decides still to apply the ShM-or BMM- option pricing while being long in the \\r\\ncontract, there will be a “rational” trader who will take the short position and apply arbitrage \\r\\nstrategy. Indeed, in ShM- and BMM--approaches there is no suggestion for a hedging strategy of \\r\\nthe option seller should use, simply because there is none.\\r\\nThe failing in the ShM-option pricing, viewed from RDAPT viewpoint, is due to the fact \\r\\nthat the price process of the representative investor is not a semimartingale. The main reasons to \\r\\nuse semimartingales in modeling the dynamics of the asset prices are the following 8:\\r\\n(1) The semimartingales are the largest possible class for price processes, when defining the gains \\r\\nfrom the trading strategies applied to price processes9;\\r\\n \\r\\n8 Black and Scholes (1973), Merton (1973), Harrison and Kreps (1979), Harison and Pliska (1981), \\r\\n(1983), Dalang, Morton and Willinger (1990), Ansel and Stricker,(1994), Delbaen and \\r\\nSchachermayer (1994),(1997), (1998), (2006), Rachev et al. (2011), Strong (2014). .\\r\\n9 We do not discuss fractional processes (see Mishura (2008) and other generalizations (see for \\r\\nexample, Frittelli (1997) and Kardaras (2010)) as they are not related to the problem we are \\r\\ndealing with. \\nPage | 6\\r\\n(2)A second reason why semimartingale models are omnipresent is the fundamental work in \\r\\nRDAPT on no-arbitrage criteria which can be summarized as Fundamental Asset Pricing Theorem \\r\\nand “No Free Lunch with Vanishing Risk” (NFLVR) condition — and the mathematical notion of \\r\\nexistence of equivalent probability measures, under which asset prices have some sort of \\r\\nmartingale property, which leads to the price process again has to be a semimartingale.\\r\\n In this paper, we suggest several approaches to adjust the ShM- BMM- option pricing \\r\\nformulas for traders having heterogeneous views on the underlying pricing process so that those \\r\\nformulas are co0nsisting with the RDAPT. Namely, we will impose trading costs (so called arb\\x02costs) which will offset the gains from the arbitrage opportunities the hedger could have. \\r\\nEquilibrium options pricing formulas when the traders have heterogeneous beliefs are well studied \\r\\nin RDART10. Our approach to option pricing in the presence of heterogeneous beliefs is different, \\r\\nwhich can be roughly explained as follows: for the hedger ℶ to realize an arbitrage strategy, ℶ must\\r\\ntrade in high speed, thus placing arb-costs on the velocity 11 of trades can offset the gains ℶ\\r\\naccumulates when applying the arbitrage trade. In regular hedging when no arbitrage occurs the \\r\\narb-costs are not significant. As a conclusion, our approach leads to the opinion that in modern \\r\\nfinancial markets, transaction cost on the velocity of trading should be imposed to remove potential \\r\\narbitrage gains. \\r\\n \\r\\n10 Buraschi and Jiltsov (2p006), Chabakauri G. (2013), He and Shi (2016). Muhle-Karbe and Nutz \\r\\n(2016) showed simple wining strategies in option contracts when the buyer and the seller of the \\r\\noption contract have heterogeneous views. \\r\\n11 See Duffie (2001), p 104, formula (3).\\nPage | 7\\r\\nThe paper is organized as follows. In Section 2 we adjust ShM to be consistent with \\r\\nRDAPT applying binomial tree model with transaction costs. In Section 3 introduce arb-costs on \\r\\nthe delta- hedge positions in order to eliminate the gains from the arbitrage opportunities, ShM \\r\\nmodel can invoke in continuous time asset pricing. In Section 4 we consider similar type arb- costs\\r\\non the trading velocity using binomial tree model. In Section 5 we consider a general model with \\r\\narb- costs on the delta- and gamma-positions of the hedge portfolio. Our concluding remarks are \\r\\nin Section 6.\\r\\n2. Shefrin’s behavioral asset pricing model with transaction costs\\r\\nWe start with the description of ShM but from the viewpoint of RDAPT. Consider a financial \\r\\nmarket with two investors sharing an aggregate consumption (AC) 𝜔(0) > 0 amount at 𝑡\\r\\n(0)\\r\\n. At \\r\\nany subsequent period 𝑡\\r\\n(𝑘+1) = (𝑘 + 1)∆𝑡, 𝑘 = 0,1, . . , 𝑛 − 1,𝑡(𝑛) = 𝑇, 𝑛 ↑ ∞, the aggregate \\r\\namount available will unfold through a binomial process, growing by 𝑢\\r\\n(∆𝑡) > 1, or 𝑑(∆𝑡) < 1. \\r\\nUnder ShM the investor ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 attaches probability 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡) for upward movement of the \\r\\nAC-process and 1 − 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡) for downturn movement, that is, the discrete AC-dynamics is given \\r\\nby\\r\\n𝜔\\r\\n(𝑗)\\r\\n(𝑡\\r\\n(𝑘+1)\\r\\n) = {\\r\\n𝜔\\r\\n(𝑗)\\r\\n(𝑡\\r\\n(𝑘)\\r\\n)𝑢\\r\\n(∆𝑡)\\r\\n, 𝑤. 𝑝. 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡)\\r\\n𝜔\\r\\n(𝑗)\\r\\n(𝑡\\r\\n(𝑘)\\r\\n)𝑑\\r\\n(∆𝑡)\\r\\n, 𝑤. 𝑝. 1 − 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡).\\r\\n (1)\\r\\n The ShM is an equilibrium model based on the heterogeneous beliefs of the two investors \\r\\nor determine the dynamics of the state-price process of the representative investor. The ShM \\r\\nderives the dynamics of the state-price process of the representative agent as a probability mixture \\r\\nof different geometric Brownian motions representing the dynamics of the state-price processes of \\nPage | 8\\r\\ntwo investors. Thus, the state-price process of the representative agent is not a semimartingale and \\r\\nShM allows for arbitrage opportunities.\\r\\n2.1. Binomial tree with heterogeneous views on the model parameters \\r\\nOur goal, is to adapt the behavioral framework in the ShM, within RDAPT. First, let us make the \\r\\nfollowing observation when extending the ShM. Suppose ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2, differ not only on their \\r\\nviews of the probabilities 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡) but also on their views of the size of the upward and downward \\r\\nmovements, that is, ℶ\\r\\n(𝑗)\\r\\nviews the AC-process as follows,\\r\\n𝜔\\r\\n(𝑗)\\r\\n(𝑡\\r\\n(𝑘+1)\\r\\n) = {\\r\\n𝜔\\r\\n(𝑗)\\r\\n(𝑡\\r\\n(𝑘)\\r\\n)𝑢\\r\\n(𝑗,∆𝑡)\\r\\n, 𝑤. 𝑝. 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡)\\r\\n𝜔\\r\\n(𝑗)\\r\\n(𝑡\\r\\n(𝑘)\\r\\n)𝑑\\r\\n(𝑗,∆𝑡)\\r\\n, 𝑤. 𝑝. 1 − 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡),𝑗 = 1,2\\r\\n (2)\\r\\nLet us now make use of Kim at al (2016)’s extension of the CRR12 binomial tree, assuming that \\r\\n{\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n𝑝\\r\\n(𝑗)\\r\\n(∆𝑡): = 𝑔\\r\\n(𝑗) + 𝑣(𝑗)√∆𝑡, 𝑔(𝑗) ∈ (0,1), 𝑣(𝑗) ∈ 𝑅\\r\\n𝑢\\r\\n(𝑗,∆𝑡)\\r\\n: = 1 + 𝛾\\r\\n(𝑗)∆𝑡 + √\\r\\n1−𝑝\\r\\n(𝑗)(∆𝑡)\\r\\n𝑝\\r\\n(𝑗)(∆𝑡)\\r\\n𝜎\\r\\n(𝑗)√∆𝑡, 𝛾(𝑗) ∈ 𝑅, 𝜎(1) > 𝜎(2) > 0\\r\\n𝑑\\r\\n(𝑗,∆𝑡)\\r\\n: = 1 + 𝛿\\r\\n(𝑗)∆𝑡 − √\\r\\n𝑝\\r\\n(𝑗)(∆𝑡)\\r\\n1−𝑝\\r\\n(𝑗)(∆𝑡)\\r\\n𝜎\\r\\n(𝑗)√∆𝑡, 𝛿(𝑗) ∈ 𝑅,𝑗 = 1,2\\r\\n (3)\\r\\nThen the bivariate binomial tree (2) generates a discrete pricing process in Skorokhod space \\r\\n𝐷[0, 𝑇]\\r\\n2\\r\\n, which converges (as ∆𝑡 ↓ 0, 𝑛 ↑ ∞, 𝑛∆𝑡 = 𝑇) to a bivariate geometric Brownian motion\\r\\n𝜔\\r\\n(𝑗)\\r\\n(𝑡) = 𝜔(0) exp ((𝜇\\r\\n(𝑗) −\\r\\n𝜎\\r\\n(𝑗)\\r\\n2\\r\\n2\\r\\n) 𝑡 + 𝜎\\r\\n(𝑗)𝐵(𝑡)) ,𝑡 ≥ 0, (4)\\r\\nwhere 𝜇\\r\\n(𝑗) = 𝑔(𝑗)\\r\\n𝛾\\r\\n(𝑗) + (1 − 𝑔(𝑗)\\r\\n)𝛿\\r\\n(𝑗)\\r\\nand 𝐵(𝑡),𝑡 ≥ 0 is a Brownian motion generating a\\r\\n \\r\\n12 Cox,Ross and Rubinstein (1979).\\nPage | 9\\r\\nstochastic basis (Ω,ℱ, 𝔽 = (ℱ𝑡,𝑡 ≥ 0),ℙ). . Furthermore, the risk-neutral dynamics \\r\\n𝜔\\r\\n(ℚ,𝑗)\\r\\n(𝑡),𝑡 ≥ 0, will be determined by\\r\\n𝜔\\r\\n(ℚ,𝑗)\\r\\n(𝑡) = 𝜔(0) exp ((𝑟 −\\r\\n𝜎\\r\\n(𝑗)\\r\\n2\\r\\n2\\r\\n) 𝑡 + 𝜎\\r\\n(𝑗)𝐵\\r\\nℚ(𝑡)) ,𝑡 ≥ 0, ,𝑗 = 1,2, (5)\\r\\nwhere 𝐵\\r\\nℚ(𝑡),𝑡 ≥ 0 is a Brownian motion generating a stochastic basis (Ω,ℱ, 𝔽 = (ℱ𝑡\\r\\n,𝑡 ≥\\r\\n0), ℚ), 𝑑𝐵\\r\\nℚ(𝑡) = 𝑑𝐵(𝑡) +\\r\\n𝜇\\r\\n(𝑗)−𝑟\\r\\n𝜎\\r\\n(𝑗) 𝑑𝑡 on ℙ. Per Black (1972) model, the riskless rate 𝑟, is given\\r\\nby \\r\\n 𝑟 =\\r\\n𝜇\\r\\n(1)𝜎(2)−𝜇(2)𝜎(1)\\r\\n𝜎\\r\\n(2)−𝜎(1)\\r\\n. (6)\\r\\nThen following the ShM, suppose that at 𝑡 = 0, the aggregate wealth of the 𝑗𝑡ℎ investor is \\r\\n𝑊(𝑗,0),𝑗 = 1,2, 𝑊(1,0) + 𝑊(1,0) = 𝑊(0), and the relative wealth at 𝑡 = 0 , is 𝜔\\r\\n(𝑗)\\r\\n(0) =\\r\\n𝑊(𝑗,0)\\r\\n𝑊(0)\\r\\n, 𝜔\\r\\n(1)\\r\\n(0) + 𝜔\\r\\n(2)\\r\\n(0) = 1. \\r\\nThe risk-neutral approach (5), (6) provides an additional characterization of ShM under \\r\\nthe assumption (2), (3): Suppose that ℶ\\r\\n(𝑗)\\r\\n, 1,2, share the cumulative return of the AC-process in \\r\\nproportions 𝛼 = (𝛼\\r\\n(1)\\r\\n, 𝛼\\r\\n(2)\\r\\n), 𝛼\\r\\n(𝑗) ∈ 𝑅 ∖ {0}, 𝛼(1) + 𝛼(2) = 1, that is, the combined cumulative \\r\\nreturn of the both investors is:\\r\\n 𝛼\\r\\n(1)\\r\\n𝑙𝑛𝜔\\r\\n(1)\\r\\n(𝑡) + 𝛼\\r\\n(2)\\r\\n𝑙𝑛𝜔\\r\\n(2)\\r\\n(𝑡) = 𝑙𝑛𝐺\\r\\n(𝛼)\\r\\n(𝑡),𝑡 ≥ 0. (7) \\r\\nHence, 𝛼 is an acceptable (arbitrage-free) allocation if and only if 𝔊\\r\\n(𝛼)\\r\\nis a perpetual derivative \\r\\nwith price process:\\r\\n 𝐺\\r\\n(𝛼)\\r\\n(𝑡) = 𝑔 (𝜔\\r\\n(1)\\r\\n(𝑡), 𝜔\\r\\n(2)\\r\\n(𝑡)) ,𝑡 ≥ 0, 𝑔(𝑥1, 𝑥2) = 𝑥1\\r\\n𝛼\\r\\n(1)\\r\\n𝑥2\\r\\n𝛼\\r\\n(2)\\r\\n, 𝑥𝑗 > 0,𝑗 = 1,2, (8)\\nPage | 10\\r\\nTo find the acceptable value for 𝛼 , notice first that according (5) and (6), 𝑔(𝑥1, 𝑥2\\r\\n) =\\r\\n𝑥1\\r\\n𝛼\\r\\n(1)\\r\\n𝑥2\\r\\n𝛼\\r\\n(2)\\r\\n, 𝑥𝑗 > 0,𝑗 = 1,2 satisfies the partial differential equation (PDE:\\r\\n \\r\\n𝜕𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥1\\r\\n𝑥1 + 𝑟\\r\\n𝜕𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥2\\r\\n𝑥2 − 𝑟𝑔(𝑥1, 𝑥2\\r\\n) +\\r\\n+\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥1\\r\\n2 𝑥1\\r\\n2𝜎\\r\\n(1)\\r\\n2\\r\\n+\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥2\\r\\n2 𝑥2\\r\\n2𝜎\\r\\n(2)\\r\\n2\\r\\n+\\r\\n𝜕\\r\\n2𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥1𝜕𝑥2\\r\\n𝑥1𝑥2\\r\\n(𝑡)𝜎\\r\\n(1)𝜎(2) = 0. (9)\\r\\nThus, 𝛼 = (𝛼\\r\\n(1)\\r\\n, 𝛼\\r\\n(2)\\r\\n), should satisfy:\\r\\n 𝑟(𝛼\\r\\n(1) + 𝛼(2) − 1) +\\r\\n1\\r\\n2\\r\\n𝛼\\r\\n(1)\\r\\n(𝛼\\r\\n(1) − 1)𝜎(1)\\r\\n2\\r\\n+\\r\\n1\\r\\n2\\r\\n𝛼\\r\\n(2)\\r\\n(𝛼\\r\\n(2) − 1)𝜎(2)\\r\\n2\\r\\n+\\r\\n+𝛼\\r\\n(1)𝛼(2)𝜎(1)𝜎(2) = 0. (10)\\r\\nBecause 𝛼\\r\\n(𝑗) ∈ 𝑅 ∖ {0}, 𝛼(1) + 𝛼(2) = 1 , then 𝜎(1) = 𝜎(2) = 𝜎, with riskless rate 𝑟 in (6) \\r\\nexploding to ±∞ as the derivative 𝔊\\r\\n(𝛼)\\r\\n is an arbitrage security. This shows that ShM leads to \\r\\npricing model with arbitrage opportunities.\\r\\n To illustrate the problem with the ShM again, let us return to the binomial model (1), when \\r\\n𝑢\\r\\n(𝑗,∆𝑡) = 𝑢(∆𝑡) > 1 > 𝑑(𝑗,∆𝑡) = 𝑑(∆𝑡)\\r\\n, ∆𝑡 ↓ 0, 𝑛 ↑ ∞, 𝑛∆𝑡 = 𝑇. We require13 that the first two \\r\\nmoments of the ∆𝑡- increments of processes (1) and (2) coincide, that is 𝑢\\r\\n(∆𝑡) > 1, or 𝑑(∆𝑡) < 1. \\r\\n 𝑢\\r\\n(∆𝑡)𝑝(𝑗)\\r\\n(∆𝑡) + 𝑑\\r\\n(∆𝑡)\\r\\n(1 − 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡)) =\\r\\n= 𝔼 exp ((𝜇\\r\\n(𝑗) −\\r\\n𝜎\\r\\n(𝑗)\\r\\n2\\r\\n2\\r\\n) ∆𝑡 + 𝜎\\r\\n(𝑗)𝐵(∆𝑡)) = 1 + 𝜇(𝑗)∆𝑡, (11)\\r\\nand\\r\\n \\r\\n13\\r\\n All terms of order 𝑜(∆𝑡) are omitted.\\nPage | 11\\r\\n 𝑢\\r\\n(∆𝑡)\\r\\n2\\r\\n𝑝\\r\\n(𝑗)\\r\\n(∆𝑡) + 𝑑\\r\\n(∆𝑡)\\r\\n2\\r\\n(1 − 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡)) =\\r\\n= 𝔼 exp (2 (𝜇\\r\\n(𝑗) −\\r\\n𝜎\\r\\n(𝑗)\\r\\n2\\r\\n2\\r\\n) ∆𝑡 + 2𝜎\\r\\n(𝑗)𝐵(∆𝑡)) = 1 + (2𝜇(𝑗) + 𝜎(𝑗)\\r\\n2\\r\\n) ∆𝑡. (12)\\r\\nWe search for solution of (11) and (12) in the general type: 𝑢\\r\\n(∆𝑡) = 1 + 𝒶∆𝑡 + 𝒷√∆𝑡, 𝑑(∆𝑡) = 1 +\\r\\n𝒸∆𝑡 − 𝒹√∆𝑡, 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡) = 𝑝\\r\\n(𝑗) + 𝒽(𝑗)√∆𝑡 + ℊ(𝑗)∆𝑡. Then, from (8) and (9), it can be shown that \\r\\n𝑝\\r\\n(𝑗) =\\r\\n1\\r\\n2\\r\\n, 𝒷 = 𝒹 = 𝜎\\r\\n(𝑗)\\r\\n, ℎ\\r\\n(𝑗) =\\r\\n𝜇\\r\\n(𝑗)\\r\\n𝑏+𝑑\\r\\n− 𝑎\\r\\n𝑑\\r\\n(𝑏+𝑑)2 + 𝑐\\r\\n𝑑\\r\\n(𝑏+𝑑)2\\r\\n. Furthermore, as ∆𝑡 ↓ 0, 𝑛 ↑ ∞, 𝑛∆𝑡 =\\r\\n𝑇, the dynamics (1) weakly converges to the dynamics of bivariate GBM:\\r\\n 𝜔\\r\\n(𝑗)\\r\\n(𝑡) = 𝜔(0) exp ((𝜇\\r\\n(𝑗) −\\r\\n𝜎\\r\\n2\\r\\n2\\r\\n) 𝑡 + 𝜎\\r\\n2𝐵(𝑡)) ,𝑡 ≥ 0,𝑗 = 1,2. (13)\\r\\n2.2. Binomial tree with transaction costs eliminating the gains from arbitrage trades \\r\\nUnfortunately, the dynamics (13) allows for arbitrages opportunities as soon as 𝜇\\r\\n(1) ≠\\r\\n𝜇\\r\\n(2)\\r\\n. To reconcile this major discrepancy between the behavioral asset pricing and RDAPT we \\r\\nintroduce trading cost to eliminate the gain from potential arbitrage opportunities. We define the \\r\\nfollowing AS-process on a tree with transaction cost14: as ∆𝑡 ↓ 0,\\r\\n \\r\\n14 Having all terms of order 𝑜(∆𝑡) are omitted, and if 𝕔\\r\\n(𝑗) = 1 and 𝜀 = 0, then (14) is the CRR\\x02binomial tree model (see Cox, Ross, Rubinstein (1979))\\nPage | 12\\r\\n𝜔\\r\\n(∗,𝑗)\\r\\n(𝑡\\r\\n(𝑘+1)\\r\\n) =\\r\\n𝜔\\r\\n(∗,𝑗)\\r\\n(𝑡\\r\\n(𝑘)\\r\\n)\\r\\n{\\r\\n \\r\\n \\r\\n \\r\\n {1 + 𝕔\\r\\n(𝑗)𝜎√∆𝑡 +\\r\\n1\\r\\n2\\r\\n(𝕔\\r\\n(𝑗)𝜎)\\r\\n2\\r\\n∆𝑡} , 𝑤. 𝑝. 𝜀\\r\\n(𝑗) 𝑝(𝑗)\\r\\n(∆𝑡)\\r\\n{1 + 𝜎√∆𝑡 +\\r\\n1\\r\\n2\\r\\n𝜎\\r\\n2∆𝑡} , 𝑤. 𝑝. (1 − 𝜀\\r\\n(𝑗)\\r\\n) 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡)\\r\\n{1 − 𝜎√∆𝑡 +\\r\\n1\\r\\n2\\r\\n𝜎\\r\\n2∆𝑡} , 𝑤. 𝑝. (1 − 𝜀\\r\\n(𝑗)\\r\\n)(1 − 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡))\\r\\n{1 − 𝕔\\r\\n(𝑗)𝜎√∆𝑡 +\\r\\n1\\r\\n2\\r\\n(𝕔\\r\\n(𝑗)𝜎)\\r\\n2\\r\\n∆𝑡} , 𝑤. 𝑝. 𝜀\\r\\n(𝑗)\\r\\n(1 − 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡)) ,𝑗 = 1,2,\\r\\n(14) \\r\\nwhere\\r\\n(𝑖) 𝑝\\r\\n(𝑗)\\r\\n(∆𝑡) =\\r\\n1\\r\\n2\\r\\n+\\r\\n𝜇\\r\\n(𝑗)−\\r\\n𝜎\\r\\n2\\r\\n2\\r\\n2𝜎\\r\\n√∆𝑡;\\r\\n(𝑖𝑖) 𝕔\\r\\n(𝑗) > 1, is the transaction rate available to investor ℶ(𝑗)\\r\\n,𝑗 = 1,2 (we assume that ℶ\\r\\n(𝑗)\\r\\n,𝑗 =\\r\\n1,2 have heterogeneous transaction rates: 𝕔\\r\\n(1) ≠ 𝕔(2)\\r\\n);\\r\\n(𝑖𝑖𝑖) 𝜀\\r\\n(𝑗) ∈ (0,1), 𝜀(1) ≠ 𝜀(2)\\r\\n.\\r\\nTo determine the continuous-time dynamics 𝜔\\r\\n(∗,𝑗)\\r\\n(𝑡),𝑡 ≥ 0, derived from (14) as ∆𝑡 ↓ 0,\\r\\n𝜔\\r\\n(∗,𝑗)\\r\\n(𝑡) = 𝜔(0) exp ((𝑚(𝑗) −\\r\\n𝑣\\r\\n(𝑗)\\r\\n2\\r\\n2\\r\\n) 𝑡 + 𝑣\\r\\n(𝑗)𝐵(𝑡)) ,𝑡 ≥ 0, 𝑚(𝑗) ∈ 𝑅, 𝑣(𝑗) > 0,𝑗 = 12, (15)\\r\\nlet us match the first two moments of 𝜔\\r\\n(𝑗)\\r\\n(∆𝑡) and 𝜔\\r\\n(∗,𝑗)\\r\\n(∆𝑡). We readily obtain\\r\\n 𝑚(𝑗) = 𝜇\\r\\n(𝑗) +\\r\\n1\\r\\n2\\r\\n𝜎\\r\\n2\\r\\n𝑐\\r\\n(𝑗)\\r\\n(𝑐\\r\\n(𝑗) − 1)𝜀(𝑗)\\r\\n, 𝑣\\r\\n(𝑗)\\r\\n2\\r\\n= 𝜎\\r\\n2\\r\\n(1 + (𝑐\\r\\n(𝑗) − 1)𝜀(𝑗)\\r\\n),𝑗 = 1,2 , (16)\\r\\nand thus,\\r\\n 𝜔\\r\\n(∗,𝑗)\\r\\n(𝑡) = 𝜔(0) exp ((𝜇\\r\\n(𝑗) −\\r\\n1\\r\\n2\\r\\n𝜎\\r\\n2\\r\\n) 𝑡 + 𝜎√1 + (𝑐\\r\\n(𝑗) − 1)𝜀(𝑗)𝐵(𝑡)) ,𝑡 ≥ 0,𝑗 = 1,2. (17)\\r\\nFurthermore, bivariate binomial tree (14) generates a discrete pricing process in Skorokhod space \\r\\n𝐷[0, 𝑇]\\r\\n2\\r\\n, which converges (as ∆𝑡 ↓ 0, 𝑛 ↑ ∞, 𝑛∆𝑡 = 𝑇) to a bivariate geometric Brownian motion \\r\\n(17).\\nPage | 13\\r\\nNow, following ShM (1), investor ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2, views AC-process 𝜔\\r\\n(𝑗)\\r\\n(𝑡),𝑡 ≥ 0 as \\r\\ndetermined by(13). However, ℶ\\r\\n(𝑗)\\r\\n’s trades are subject to transaction costs and as a result, ℶ\\r\\n(𝑗)\\r\\ntrades the AC-process as 𝜔\\r\\n(∗,𝑗)\\r\\n(𝑡),𝑡 ≥ 0 determined by (17). Next, consider again the perpetual \\r\\nderivative, 𝔊\\r\\n(𝛼)\\r\\nis with price process, 𝐺\\r\\n(𝛼)\\r\\n(𝑡),𝑡 ≥ 0, given by (8). Then the 𝐺\\r\\n(𝛼)\\r\\n(𝑡) =\\r\\n𝑔 (𝜔\\r\\n(1)\\r\\n(𝑡), 𝜔\\r\\n(2)\\r\\n(𝑡)) ,𝑡 ≥ 0 dynamics is given by the Itô formula\\r\\n𝑑𝐺\\r\\n(𝛼)\\r\\n(𝑡) = 𝑑𝑔 (𝜔\\r\\n(1)\\r\\n(𝑡), 𝜔\\r\\n(2)\\r\\n(𝑡)) = \\r\\n=\\r\\n{\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n𝜕𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥1\\r\\n𝜔\\r\\n(1)\\r\\n(𝑡)𝜇\\r\\n(1) +\\r\\n𝜕𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥2\\r\\n𝜔\\r\\n(2)\\r\\n(𝑡)𝜇\\r\\n(2) +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥1\\r\\n2 (𝜔\\r\\n(1)\\r\\n(𝑡))\\r\\n2\\r\\n𝜎\\r\\n2 +\\r\\n1\\r\\n2\\r\\n𝜕𝑔\\r\\n2(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥2\\r\\n2 (𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n2\\r\\n𝜎\\r\\n2 +\\r\\n𝜕\\r\\n2𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥1𝜕𝑥2\\r\\n𝜔\\r\\n(1)\\r\\n(𝑡)𝜔\\r\\n(2)\\r\\n(𝑡)𝜎\\r\\n2\\r\\n}\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n𝑑𝑡 +\\r\\n+ {\\r\\n𝜕𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥1\\r\\n𝜔\\r\\n(1)\\r\\n(𝑡) +\\r\\n𝜕𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥2\\r\\n𝜔\\r\\n(2)\\r\\n(𝑡)} 𝜎𝐵(𝑡). (18)\\r\\nConsider a self-financing strategy 𝐴\\r\\n(𝑗)\\r\\n(𝑡),𝑡 ≥ 0,𝑗 = 1,2,\\r\\n𝑔 (𝜔\\r\\n(1)\\r\\n(𝑡), 𝜔\\r\\n(2)\\r\\n(𝑡)) = 𝐴\\r\\n(1)\\r\\n(𝑡)𝜔\\r\\n(1)\\r\\n(𝑡) + 𝐴\\r\\n(2)\\r\\n(𝑡)𝜔\\r\\n(2)\\r\\n(𝑡). (19)\\r\\nDue to the transaction costs the strategy 𝐴\\r\\n(𝑗)\\r\\n(𝑡),𝑡 ≥ 0,𝑗 = 1,2 detremines the following dynamics \\r\\nof the replicating portfolio:\\r\\n 𝑑𝑔 (𝜔\\r\\n(1)\\r\\n(𝑡), 𝜔\\r\\n(2)\\r\\n(𝑡)) = {𝐴\\r\\n(1)\\r\\n(𝑡)𝜔\\r\\n(1)\\r\\n(𝑡)𝜇\\r\\n(1) + 𝐴(2)\\r\\n(𝑡)𝜔\\r\\n(2)\\r\\n(𝑡)𝜇\\r\\n(2)\\r\\n}𝑑𝑡 +\\r\\n+{𝐴\\r\\n(1)\\r\\n(𝑡)𝜔\\r\\n(1)\\r\\n(𝑡)𝑣\\r\\n(1) + 𝐴(2)\\r\\n(𝑡)𝑣\\r\\n(2)\\r\\n(𝑡)𝜎\\r\\n(2)\\r\\n}𝑑𝐵(𝑡). (20)\\r\\nEquating the terms with 𝑑𝑔 (𝜔\\r\\n(1)\\r\\n(𝑡), 𝜔\\r\\n(2)\\r\\n(𝑡)), leads to:\\nPage | 14\\r\\n{\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n𝐴\\r\\n(1)\\r\\n(𝑡)𝜔\\r\\n(1)\\r\\n(𝑡) =\\r\\n𝜕𝑔(𝜔(1)\\r\\n(𝑡),𝜔(2)(𝑡))\\r\\n𝜕𝑥1\\r\\n𝜔\\r\\n(1)\\r\\n(𝑡)𝜎 +\\r\\n𝜕𝑔(𝜔(1)\\r\\n(𝑡),𝜔(2)(𝑡))\\r\\n𝜕𝑥2\\r\\n𝜔\\r\\n(2)\\r\\n(𝑡)𝜎−𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))𝑣\\r\\n(2)\\r\\n𝑣\\r\\n(1)−𝑣(2)\\r\\n𝐴\\r\\n(2)\\r\\n(𝑡)𝜔\\r\\n(2)\\r\\n(𝑡) =\\r\\n𝑔(𝜔\\r\\n(1)\\r\\n(𝑡),𝜔\\r\\n(2)\\r\\n(𝑡))𝑣\\r\\n(1)−\\r\\n𝜕𝑔(𝜔(1)\\r\\n(𝑡),𝜔(2)(𝑡))\\r\\n𝜕𝑥1\\r\\n𝜔\\r\\n(1)\\r\\n(𝑡)𝜎−\\r\\n𝜕𝑔(𝜔(1)\\r\\n(𝑡),𝜔(2)(𝑡))\\r\\n𝜕𝑥2\\r\\n𝜔\\r\\n(2)\\r\\n(𝑡)𝜎\\r\\n𝑣\\r\\n(1)−𝑣(2)\\r\\n (21)\\r\\nCombining (18) - (21) results in the following PDE for 𝑔(𝑥1, 𝑥2\\r\\n), 𝑥1 > 0, 𝑥2 > 0 :\\r\\n(𝑟\\r\\n(∗) + 𝐶𝑦\\r\\n(1)\\r\\n)\\r\\n𝜕𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥1\\r\\n𝑥1 + (𝑟\\r\\n(∗) + 𝐶𝑦\\r\\n(2)\\r\\n)\\r\\n𝜕𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥2\\r\\n𝑥2 − 𝑟\\r\\n(∗)𝑔(𝑥1, 𝑥2\\r\\n) +\\r\\n+\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥1\\r\\n2 𝑥1\\r\\n2𝜎\\r\\n2 +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥2\\r\\n2 𝑥2\\r\\n2𝜎\\r\\n2 +\\r\\n𝜕\\r\\n2𝑔(𝑥1,𝑥2\\r\\n)\\r\\n𝜕𝑥1𝜕𝑥2\\r\\n𝑥1𝑥2\\r\\n(𝑡)𝜎\\r\\n2 = 0, (22)\\r\\nwhere \\r\\n𝑟\\r\\n(∗) =\\r\\n𝜇\\r\\n(1)𝑣(2)−𝜇(2)𝑣(1)\\r\\n𝑣\\r\\n(2)−𝑣(1)\\r\\n, 𝐶𝑦\\r\\n(𝑗) = 𝑣\\r\\n(𝑗) 𝜇\\r\\n(1)−𝜇(2)\\r\\n𝑣\\r\\n(1)−𝑣(2) − 𝜎\\r\\n𝑚(1)−𝑚(2)\\r\\n𝑣\\r\\n(1)−𝑣(2)\\r\\n,𝑗 = 1,2. (23)\\r\\nThus, arbitrage-free wealth allocation (7) in the presence of transaction costs, is given by 𝛼\\r\\n(1) +\\r\\n𝛼\\r\\n(2) = 1, where\\r\\n 𝛼\\r\\n(1)𝛼(2)𝜎\\r\\n2 − 𝐶𝑦\\r\\n(1)\\r\\n𝛼\\r\\n(1) − 𝐶𝑦\\r\\n(2)\\r\\n𝛼\\r\\n(2) = 0. (24)\\r\\nWe summarize our findings in the following proposition.\\r\\nPROPOSITION 1: Consider a financial market with two investors sharing an aggregate \\r\\nconsumption (AC) amount 𝜔(0) = 1 at 𝑡 = 0. Two investors ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 = 0, share initial \\r\\nwealth 𝜔(0) with allocation weight 𝛼\\r\\n(𝑗) ∈ 𝑅,𝑗 = 1,2 , respectively, 𝛼(1) + 𝛼(2) = 1. Investor ℶ(𝑗)\\r\\nviews the dynamics of the AC-process as (13), but due to transaction cost, ℶ\\r\\n(𝑗)\\r\\ntrades the AC\\x02process under the dynamics (17). Then the no-arbitrage allocation (𝛼\\r\\n(1)\\r\\n, 𝛼\\r\\n(2)\\r\\n) is given by (24). \\r\\nThe no-arbitrage riskless rate 𝑟\\r\\n(∗)\\r\\nand ℶ\\r\\n(𝑗)\\r\\n- transaction yield 𝐶𝑦\\r\\n(𝑖)\\r\\nare given by (23).\\nPage | 15\\r\\nProposition 1 bridges the behavioral asset pricing approach in ShM with the RDAPT in the \\r\\npresence of transaction costs.\\r\\n Shefrin (2005) discusses various behavioral phenomena leading to asymmetric non\\x02Gaussian return and volatility clustering as well as momentum (long-range dependence). Those \\r\\nphenomena can be encompassed in our general setting, extending the dynamics defined (15) and \\r\\n(17) with Barndorff-Nielsen -Ornstein-Uhlenbeck-type wealth process dynamics15. \\r\\n3. Option Pricing with Heterogeneous Views on the Underlying Asset in the Presence of Arb\\x02transaction Costs\\r\\nWe start with the erroneous statement in ShM option pricing formula. On where page 319, \\r\\nHersh Shefrin wrote:\\r\\n(HS-Claim) “Suppose that investors agree on the risk-free process, and agree on the volatility of \\r\\nthe risky asset, but disagree on the drift term for the risky asset. That is let investor 1 believe that \\r\\nthe stock price 𝑆 obeys the process 𝑑𝑆\\r\\n𝑆\\r\\n= 𝜇1𝑑𝑡 + 𝜎𝑑𝑍, where 𝑍 is a Winer process Let investor 2 \\r\\nbelieve that the stock price 𝑆 obeys the process 𝑑𝑆\\r\\n𝑆\\r\\n= 𝜇2𝑑𝑡 + 𝜎𝑑𝑍 . How will options be priced in \\r\\nthis framework? They will be priced according to Black-Scholes. Therefore, heterogeneity will not \\r\\nimpact option prices, and will not give rise to volatility smiles.”\\r\\nIn general, HS-Claim is not true. To see that, suppose investor 1 takes the long position in the \\r\\nEuropean option contract 𝒞, with (1) price process 𝐶(𝑡) = 𝑓(𝑆(𝑡),𝑡),𝑡 ≥ 0; (2) maturity time 𝑇;\\r\\nand (3) payoff function at maturity 𝑓(𝑆(𝑇), 𝑇) = 𝑔(𝑇), where 𝑓(𝑥,𝑡), 𝑥 > 0,𝑡 ≥ 0 is sufficiently \\r\\n \\r\\n15 See Barndorff-Nielsen and Shephard (2001a, b), Barndorff-Nielsen and Stelzer (2007), (2013), \\r\\nMuhle-Karbe, Pfaffel and Stelzer (2012), \\nPage | 16\\r\\nsmooth. Suppose investor 2 takes the short position in 𝒞. We make the following assumption \\r\\n(𝐴𝑆1): Suppose that investors 1 and 2 trade the asset according to their views and they know the \\r\\nviews of each other. If (𝐴𝑆1) is not true what is the point of having different views on 𝑆, if the \\r\\ntrading remains unchanged regarding the views of investors 1 and 2? Secondly, according ShM, \\r\\nthere are only two investors in the market, so to believe that they do not see the history of trades \\r\\nof each other and blindly enter the option contract 𝒞, seems quite unrealistic. Assuming (𝐴𝑆1), \\r\\nthe dynamics of the long position in 𝒞\\r\\n16is given by the Itô formula:\\r\\n𝑑𝐶(𝑡) = 𝑑𝑓(𝑆(𝑡),𝑡) =\\r\\n= (\\r\\n𝜕𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑡 +\\r\\n𝜕𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑥\\r\\n𝜇1𝑆(𝑡) +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑥\\r\\n2 𝜎\\r\\n2𝑆(𝑡)2\\r\\n) 𝑑𝑡 +\\r\\n𝜕𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑥 𝜎𝑆(𝑡)𝑑𝑍(𝑡).\\r\\nInvestor 2, forms a self-financing strategy 𝐶(𝑡) = 𝑓(𝑆(𝑡),𝑡) = 𝑎(𝑡)𝑆(𝑡) + 𝑏(𝑡)𝛽(𝑡),𝑡 ≥ 0,\\r\\nwhere 𝛽(𝑡) = 𝛽(0)𝑒\\r\\n𝑟𝑡\\r\\n,𝑡 ≥ 0, is the riskless asset with a riskless rate 𝑟. Then the dynamics of the \\r\\nreplicating portfolio is given by \\r\\n𝑑𝐶(𝑡) = 𝑑𝑓(𝑆(𝑡),𝑡) = 𝑎(𝑡)𝑑𝑆(𝑡) + 𝑏(𝑡)𝑑𝛽(𝑡) =\\r\\n= (𝑎(𝑡)𝜇2𝑆(𝑡) + 𝑏(𝑡)𝑟𝛽(𝑡))𝑑𝑡 + 𝑎(𝑡)𝜎𝑆(𝑡)𝑑𝑍(𝑡).\\r\\nEquating the expressions for 𝑑𝐶(𝑡) leads to 𝑎(𝑡) =\\r\\n𝜕𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑥 and \\r\\n𝜕𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑡 +\\r\\n𝜕𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑥 𝜇1𝑆(𝑡) +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑓(𝑆(𝑡),𝑡)\\r\\n𝜕𝑥\\r\\n2 𝜎\\r\\n2𝑆(𝑡)2 = 𝑎(𝑡)𝜇2𝑆(𝑡) + 𝑟(𝑓(𝑆(𝑡),𝑡) − 𝑎(𝑡)𝑆(𝑡)).\\r\\nSetting 𝑆(𝑡) = 𝑥, leads to the partial differential equation (PDE):\\r\\n𝜕𝑓(𝑥,𝑡)\\r\\n𝜕𝑡 +\\r\\n𝜕𝑓(𝑥,𝑡)\\r\\n𝜕𝑥 (𝜇1 − 𝜇2 + 𝑟)𝑥 − 𝑟𝑓(𝑥,𝑡) +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑓(𝑥,𝑡)\\r\\n𝜕𝑥\\r\\n2\\r\\n𝜎\\r\\n2𝑥2 = 0,\\r\\n \\r\\n16\\r\\n We now follow the classical derivation of the Black-Scholes formula, see Black and Scholes \\r\\n(1973), and Duffie (2001), Section 5F.\\nPage | 17\\r\\nWhich is the Black-Scholes formula in the homogeneous case 𝜇1 = 𝜇2. In general, the above PDE \\r\\nwill lead to option pricing with arbitrage opportunities. \\r\\n We now start adjusting the HS-Claim to make it consistent with RDAPT. As with Shefrin (2005), \\r\\nChapter 21, suppose there are two traders ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 who view and trade one risky asset (stock) \\r\\n𝒮 17 . ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 trade 𝒮 differently, because they have different views (estimates of the stock \\r\\ndynamics) and potentially having different trading skills. Both assume that 𝒮-price dynamics is \\r\\ngiven by a geometric Brownian motion (GBM) but their opinions on the coefficients of the GBM \\r\\ndiffer: for ℶ\\r\\n(𝑗)\\r\\n, the price dynamics of 𝒮 is given by\\r\\n𝑆\\r\\n(𝑗)\\r\\n(𝑡) = 𝑥\\r\\n(0)\\r\\nexp ((𝜇\\r\\n(𝑗) −\\r\\n𝜎\\r\\n2\\r\\n2\\r\\n) 𝑡 + 𝜎𝐵(𝑡)) ,𝑡 ≥ 0, 𝑥\\r\\n(0) > 0, 𝜇(2) > 𝜇(1) > 0, 𝜎 > 0, (25) \\r\\nwhere 𝐵(𝑡),𝑡 ≥ 0, is a Brownian Motion generating a stochastic basis (Ω,ℱ, 𝔽 = (ℱ𝑡,𝑡 ≥ 0),ℙ). \\r\\nWithout the introduction of transaction costs eliminating in (25) the arbitrage opportunities, as we \\r\\nhave shown already, the market model (25) is useless. Now our first task is to extend Black \\r\\n(1972)‘s model on markets with no riskless asset. The extension consists of removing Black’s \\r\\nassumption that asset volatilities are different. This will require the introduction of transaction \\r\\ncosts on the velocity of hedging portfolio. We shall derive: (25) the general form of those \\r\\n \\r\\n17 Such a difference in trading 𝒮 could be due to (25) ℶ\\r\\n(1)\\r\\n’s and ℶ\\r\\n(2)\\r\\n’s different statistical \\r\\nestimations of the model parameters; (𝑖𝑖) ℶ\\r\\n(1)\\r\\n’s and ℶ\\r\\n(2)\\r\\n’s are choosing different probability \\r\\nweighting functions (see Prelec (1998)) when they temper their views on 𝒮 -return diostribution; \\r\\n(𝑖𝑖𝑖) ℶ\\r\\n(1)\\r\\nand ℶ\\r\\n(2) when trade 𝔚 they exhibit different level of trading performance. \\nPage | 18\\r\\ntransaction costs, and (2) the interest rate determining the discount factor the representative agent\\r\\nshould be using18. \\r\\n3.1. Market with Traders Having Heterogeneous Views on the Underlying Asset in the Presence \\r\\nof Arb- Costs; Determining of the Representative’s Agent Riskless Rate \\r\\nSuppose (25) holds. The (hypothetical) representative agent (designated as ℵ ) sees that \\r\\nmarket with price processes (25) allow for arbitrage opportunities. Realizing that, ℵ′𝑠 tasks are \\r\\ntwo: (𝑇1) Determine the general structure of arb-costs which will eliminate the gains from \\r\\npotential arbitrage opportunities and (𝑇2) determine the riskless rate 𝑟\\r\\n∗\\r\\nin the market (25) with \\r\\ntransaction costs given in (𝑇1).\\r\\nStarting with (𝑇1), ℵ, knowing the price dynamics (25) for both traders ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 , \\r\\ndecides to consider a hypothetical perpetual derivative contract, 𝒢 , with price process 𝐺(𝑡) =\\r\\n𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) ,𝑡 ≥ 0, where 𝑔: (0, ∞)\\r\\n2 → (0, ∞) has continuous second derivatives. As a \\r\\nrepresentative agent, ℵ takes simultaneously the long and the short position in 𝒢. The long-position \\r\\ndynamics is determined by the Itô formula\\r\\n \\r\\n18 The formula (21.9) in Shefrin (2005) page 306 for the risk-free interest rate and formula (32) in \\r\\nBeninga and Mayshar (2000), Section V.a. for the representative agent ‘s discount factors are \\r\\nincorrect from the viewpoint of RDAPT, as they could imply arbitrage opportunities. One might \\r\\nargue that this is not of concern to behavioralists, but we argue that no-arbitrage option pricing \\r\\nshould be “must” to traders. Suppose a trader is using a ShM-behavioral type option pricing in \\r\\npractice. Then, in real markets, there will be at least one” rational” trader who will explore the \\r\\narb-opportunity, ShM-behavioral option pricing is generating.\\nPage | 19\\r\\n𝑑𝐺(𝑡) = 𝑑𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) =\\r\\n=\\r\\n(\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1)\\r\\n𝜇\\r\\n(1)\\r\\n𝑆\\r\\n(1)\\r\\n(𝑡) +\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(2)\\r\\n𝜇\\r\\n(2)\\r\\n𝑆\\r\\n(2)\\r\\n(𝑡) +\\r\\n+\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1)\\r\\n2\\r\\n𝜎\\r\\n2𝑆\\r\\n(1)\\r\\n(𝑡)\\r\\n2 +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(2)\\r\\n2\\r\\n𝜎\\r\\n2𝑆\\r\\n(2)\\r\\n(𝑡)\\r\\n2 +\\r\\n+\\r\\n𝜕\\r\\n2𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1)𝜕𝑥(2)\\r\\n𝜎\\r\\n2𝑆\\r\\n(1)\\r\\n(𝑡)𝑆\\r\\n(2)\\r\\n(𝑡) ) )\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n𝑑𝑡\\r\\n+ (\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1)\\r\\n𝜎𝑆\\r\\n(1)\\r\\n(𝑡) +\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(2)\\r\\n𝜎𝑆\\r\\n(2)\\r\\n(𝑡)) 𝑑𝐵(𝑡).\\r\\nKnowing that the short position in 𝒢 can employ arbitrage self-financing trading strategies, \\r\\nℵ decides to introduce arb-costs on the velocity of the hedge portfolio\\r\\n𝑃 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) : = 𝑎\\r\\n(1)\\r\\n(𝑡)𝑆\\r\\n(1)\\r\\n(𝑡) + 𝑎\\r\\n(2)\\r\\n(𝑡)𝑆\\r\\n(2)\\r\\n(𝑡) ≡ 𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) ,𝑡 ≥ 0,\\r\\nthat is, in the delta- position of 𝑃 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) ,𝑡 ≥ 0. More precisely, ℵ defines the structure \\r\\nof arb-costs in the 𝑃-dynamics as follows:\\r\\n 𝑑𝑃 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) = 𝑑𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) =\\r\\n = (𝜆\\r\\n(1)𝑎(1)\\r\\n(𝑡) − 𝜌\\r\\n(1)\\r\\n𝜕𝑔(𝑆\\r\\n(1)\\r\\n(𝑡),𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1) ) 𝑑𝑆\\r\\n(1)\\r\\n(𝑡) +\\r\\n + (𝜆\\r\\n(2)𝑎(2)\\r\\n(𝑡) − 𝜌\\r\\n(2)\\r\\n𝜕𝑔(𝑆\\r\\n(1)\\r\\n(𝑡),𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1) ) 𝑑𝑆\\r\\n(2)\\r\\n(𝑡), (26)\\nPage | 20\\r\\nwhere the transaction rates 𝜆\\r\\n(𝑗) > 0, 𝜌(𝑗) > 0,𝑗 = 1,2 will be determined by the corresponding \\r\\nBlack-Scholes type PDE guaranteeing a fair-price process 𝐺(𝑡) = 𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) ,𝑡 ≥ 0 for \\r\\n𝒢. Thus, equating the terms with 𝑑𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) leads to \\r\\n𝑎\\r\\n(1)\\r\\n(𝑡)𝑆\\r\\n(1)\\r\\n(𝑡) =\\r\\n1\\r\\n𝜆\\r\\n(1)𝜎(1) − 𝜆(2) 𝜎(2)\\r\\n×\\r\\n×\\r\\n(\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1)\\r\\n𝜎𝑆\\r\\n(1)\\r\\n(𝑡) +\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(2)\\r\\n𝜎𝑆\\r\\n(2)\\r\\n(𝑡) +\\r\\n+𝜌\\r\\n(1)\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1)\\r\\n𝑆\\r\\n(1)\\r\\n(𝑡)𝜎 + 𝜌\\r\\n(2)\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(2)\\r\\n𝑆\\r\\n(2)\\r\\n(𝑡)𝜎 −\\r\\n−𝜆\\r\\n(2) 𝑔 (𝑆(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) 𝜎 )\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n.\\r\\nApplying 𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) = 𝑎\\r\\n(1)\\r\\n(𝑡)𝑆\\r\\n(1)\\r\\n(𝑡) + 𝑎\\r\\n(2)\\r\\n(𝑡)𝑆\\r\\n(2)\\r\\n(𝑡) results in \\r\\n𝜇\\r\\n(2) − 𝜇(1)\\r\\n𝜆\\r\\n(1) − 𝜆(2)\\r\\n𝜆\\r\\n(2)\\r\\n(1 + 𝜌\\r\\n(1)\\r\\n)𝑆\\r\\n(1)\\r\\n(𝑡)\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1) +\\r\\n+\\r\\n𝜇\\r\\n(2) − 𝜆(1)\\r\\n𝜆\\r\\n(1) − 𝜆(2)\\r\\n𝜆\\r\\n(1)\\r\\n(1 + 𝜌\\r\\n(2)\\r\\n)𝑆\\r\\n(2)\\r\\n(𝑡)\\r\\n𝜕𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(2) +\\r\\n−𝑟\\r\\n∗ 𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡)) +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔 (𝑆\\r\\n(1)\\r\\n(𝑡), 𝑆\\r\\n(2)\\r\\n(𝑡))\\r\\n𝜕𝑥\\r\\n(1)\\r\\n2\\r\\n𝜎\\r\\n2𝑆\\r\\n(1)\\r\\n(𝑡)\\r\\n2 = 0,\\r\\nwhere \\r\\n 𝑟\\r\\n∗ =\\r\\n𝜇\\r\\n(2) −𝜇(1)\\r\\n𝜆\\r\\n(1)−𝜆(2)\\r\\n𝜆\\r\\n(1)\\r\\n𝜆\\r\\n(2)\\r\\n. (27) \\r\\nThus setting \\r\\n 𝜌\\r\\n(𝑗) = 1 − 𝜆(𝑗)\\r\\n,𝑗 = 1,2, (28) \\nPage | 21\\r\\nleads to the following Black-Scholes PDE for 𝑔(𝑥\\r\\n(1)\\r\\n, 𝑥\\r\\n(2)\\r\\n), 𝑥\\r\\n(1) > 0, 𝑥(2) > 0,\\r\\n 𝑟\\r\\n∗\\r\\n𝜕𝑔(𝑥\\r\\n(1)\\r\\n,𝑥\\r\\n(2)\\r\\n)\\r\\n𝜕𝑥\\r\\n(1) 𝑥\\r\\n(1) + 𝑟\\r\\n∗\\r\\n𝜕𝑔(𝑥\\r\\n(1)\\r\\n,𝑥\\r\\n(2)\\r\\n)\\r\\n𝜕𝑥\\r\\n(1) 𝑥\\r\\n(2) − 𝑟\\r\\n∗ 𝑔(𝑥\\r\\n(1)\\r\\n, 𝑥\\r\\n(2)\\r\\n) +\\r\\n +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔(𝑥\\r\\n(1)\\r\\n,𝑥\\r\\n(2)\\r\\n)\\r\\n𝜕𝑥\\r\\n(1)\\r\\n2 𝜎\\r\\n2𝑥\\r\\n(1)\\r\\n2\\r\\n+\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝑔(𝑥\\r\\n(1)\\r\\n,𝑥\\r\\n(2)\\r\\n)\\r\\n𝜕𝑥\\r\\n(2)\\r\\n2 𝜎\\r\\n2𝑥\\r\\n(1)\\r\\n2\\r\\n+ \\r\\n+\\r\\n𝜕\\r\\n2𝑔(𝑥\\r\\n(1)\\r\\n,𝑥\\r\\n(2)\\r\\n)\\r\\n𝜕𝑥\\r\\n(1)𝜕𝑥(2)\\r\\n)𝜎\\r\\n2𝑥\\r\\n(1)\\r\\n𝑥\\r\\n(2) = 0. (29) \\r\\nThe representation of the riskless rate (27) can be viewed as a generalization of Black (1972)’s \\r\\nformula for the riskless rate in the case of markets with transaction costs and equal stock\\x02volatilities. \\r\\nNow, the next task ℵ has is to determine the parameters 𝜆\\r\\n(1) > 0 and 𝜆(2) > 0.\\r\\n3.2. Market with Traders Having Heterogeneous Views on the Underlying Asset in the Presence \\r\\nof Arb- transaction Costs; Determining Model Parameters \\r\\nTo determine the parameters 𝜆\\r\\n(𝑗) > 0,𝑗 = 1,2, ℵ considers the market with price processes \\r\\ngiven by (25) and a riskless bond price \\r\\n 𝛽(𝑡) = 𝛽(0)𝑒\\r\\n𝑟\\r\\n∗\\r\\n𝑡\\r\\n,𝑡 ≥ 0, (30)\\r\\nwhere the riskless rate 𝑟\\r\\n∗\\r\\nis given by (27). ℵ decides to enter simultaneously the long and the short \\r\\npositions in a hypothetical perpetual derivative contracts ℋ(𝑗),𝑗 = 1,2, with price processes\\r\\n𝐻\\r\\n(𝑗)\\r\\n(𝑡) = ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡), 𝛽(𝑡)) ,𝑡 ≥ 0, where ℎ\\r\\n(𝑗)\\r\\n(𝑥, 𝑦), 𝑥 > 0, 𝛽(𝑡) has continuous derivatives \\r\\nand 𝜕\\r\\n2ℎ\\r\\n(𝑗)\\r\\n(𝑥,𝑡)\\r\\n𝜕𝑥\\r\\n2\\r\\n,\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑥,𝑦)\\r\\n𝜕𝑦 . By the Itô formula,\\r\\n 𝑑𝐻\\r\\n(𝑗)\\r\\n(𝑡) = 𝑑ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡), 𝛽(𝑡)) =\\nPage | 22\\r\\n= (\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑥 𝑆\\r\\n(𝑗)\\r\\n(𝑡)𝜇\\r\\n(𝑗) +\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑦 𝑟𝛽(𝑡) +\\r\\n𝜕\\r\\n2ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑥\\r\\n2 𝜎\\r\\n2𝑆\\r\\n(𝑗)\\r\\n(𝑡)\\r\\n2) 𝑑𝑡\\r\\n +\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑥 𝑆\\r\\n(𝑗)\\r\\n(𝑡)𝜎𝑑𝐵(𝑡). \\r\\nℵ forms a replicating portfolio, 𝑃\\r\\n(𝑗)\\r\\n(𝑡): = 𝑎\\r\\n(𝑗)\\r\\n(𝑡)𝑆\\r\\n(𝑗)\\r\\n(𝑡) + 𝑏\\r\\n(𝑗)\\r\\n(𝑡)𝛽(𝑡) ≡ ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝑡),𝑡 ≥ 0.\\r\\nNow ℵ defines transactions costs in the hedged portfolio as follows:\\r\\n 𝑑𝑃\\r\\n(𝑗)\\r\\n(𝑡) = 𝑑ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡), 𝛽(𝑡)) = \\r\\n= (𝜆\\r\\n(𝑗)𝑎(𝑗)\\r\\n(𝑡) − (1 − 𝜆\\r\\n(𝑗)\\r\\n)\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑥 ) 𝑑𝑆\\r\\n(𝑗)\\r\\n(𝑡) +\\r\\n+ (𝜆\\r\\n(𝑏𝑜𝑛𝑑)\\r\\n𝑏(𝑡) − (1 − 𝜆\\r\\n(𝑏𝑜𝑛𝑑)\\r\\n)\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑦 ) 𝑑𝛽(𝑡).\\r\\nEquating the terms with 𝑑ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝑡),𝑡 ≥ 0, leads to 𝑎\\r\\n(𝑗)\\r\\n(𝑡) =\\r\\n2+𝜆\\r\\n(𝑗)\\r\\n𝜆\\r\\n(𝑗)\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝑡)\\r\\n𝜕𝑥\\r\\nand \\r\\n(𝜇\\r\\n(𝑗) − (1 + 2𝜆(𝑗)\\r\\n)𝜇\\r\\n(𝑗) + 𝜆(𝑏𝑜𝑛𝑑)\\r\\n𝑟\\r\\n∗\\r\\n2+𝜆\\r\\n(𝑗)\\r\\n𝜆\\r\\n(𝑗)\\r\\n)\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑥 𝑆\\r\\n(𝑗)\\r\\n(𝑡)\\r\\n+ ((1 − 𝜆\\r\\n(𝑏𝑜𝑛𝑑)\\r\\n)𝑟\\r\\n∗\\r\\n)\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑦 𝛽(𝑡) +\\r\\n−𝜆\\r\\n(𝑏𝑜𝑛𝑑)\\r\\n𝑟\\r\\n∗ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡), 𝛽(𝑡)) +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2ℎ\\r\\n(𝑗)\\r\\n(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝛽(𝑡))\\r\\n𝜕𝑥\\r\\n2 𝜎\\r\\n2𝑆\\r\\n(𝑗)\\r\\n(𝑡)\\r\\n2 = 0.\\r\\nThus, 𝜆\\r\\n(𝑏𝑜𝑛𝑑)\\r\\nshould be set to 1. Furthermore, from 𝜇\\r\\n(𝑗) − (1 + 2𝜆(𝑗)\\r\\n)𝜇\\r\\n(𝑗) + 𝜆(𝑏𝑜𝑛𝑑)\\r\\n𝑟\\r\\n∗\\r\\n2+𝜆\\r\\n(𝑗)\\r\\n𝜆\\r\\n(𝑗) =\\r\\n𝑟\\r\\n∗\\r\\n, it follows that 𝜆\\r\\n(𝑗) = √\\r\\n𝑟\\r\\n∗\\r\\n𝜇\\r\\n(𝑗) which together with (27), implies that \\r\\n 𝑟\\r\\n∗ = (√𝜇\\r\\n(2) + √𝜇(1))\\r\\n2\\r\\n, 𝜆\\r\\n(𝑗) =\\r\\n√𝜇\\r\\n(2)+√𝜇(1)\\r\\n√𝜇\\r\\n(𝑗)\\r\\n,𝑗 = 1,2. (31) \\nPage | 23\\r\\nSetting 𝑆\\r\\n(𝑗)\\r\\n(𝑡) = 𝑥, we obtain the Black-Scholes type PDE for ℎ\\r\\n(𝑗)\\r\\n(𝑥, 𝑦), 𝑥 > 0, 𝑦 > 0\\r\\n𝑟\\r\\n∗\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑥,𝑦)\\r\\n𝜕𝑥 𝑥 + 𝑟\\r\\n∗\\r\\n𝜕ℎ\\r\\n(𝑗)\\r\\n(𝑥,𝑦)\\r\\n𝜕𝑦 𝑦 − 𝑟\\r\\n∗ℎ\\r\\n(𝑗)\\r\\n(𝑥, 𝑦) +\\r\\n1\\r\\n2\\r\\n𝜎\\r\\n2\\r\\n𝜕\\r\\n2ℎ\\r\\n(𝑗)\\r\\n(𝑥,𝑦)\\r\\n𝜕𝑥\\r\\n2\\r\\n𝑥\\r\\n2 = 0. (32)\\r\\n Summarizing our findings, we formulate the following proposition.\\r\\nPROPOSITION 2: Suppose ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 trade a single risky asset 𝒮 with different price \\r\\ndynamics given by (25). Then the no-arbitrage riskless 𝑟\\r\\n∗\\r\\nis given by (31) and the bond dynamics \\r\\n𝛽(𝑡),𝑡 ≥ 0, is given by (30). Next, consider a perpetual derivative contract 𝔚, with price process \\r\\n𝒲(𝑡) = 𝑊(𝑆(𝑡),𝑡), where 𝑆(𝑡),𝑡 ≥ 0, is either 𝑆\\r\\n(1)\\r\\n(𝑡),𝑡 ≥ 0 or 𝑆\\r\\n(2)\\r\\n(𝑡),𝑡 ≥ 0, and \\r\\n𝑊(𝑥,𝑡), 𝑥 > 0,𝑡 ≥ 0 has continuous 𝜕𝑊(𝑥,𝑡)\\r\\n𝜕𝑡 and 𝜕\\r\\n2𝑊(𝑥,𝑡)\\r\\n𝜕𝑥\\r\\n2\\r\\n. If ℶ\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 is taking the short \\r\\nposition in 𝔚, ℶ\\r\\n(𝑗)\\r\\n’s replicating hedge-portfolio 𝑃\\r\\n(𝑗)\\r\\n(𝑡): = 𝑎\\r\\n(𝑗)\\r\\n(𝑡)𝑆\\r\\n(𝑗)\\r\\n(𝑡) + 𝑏\\r\\n(𝑗)\\r\\n(𝑡)𝛽(𝑡) ≡\\r\\n𝑊(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝑡),𝑡 ≥ 0, is subject to arb-costs of the following type: \\r\\n 𝑑𝑃\\r\\n(𝑗)\\r\\n(𝑡) ≔ (𝜆\\r\\n(𝑗)𝑎(𝑗)\\r\\n(𝑡) − (1 − 𝜆\\r\\n(𝑗)\\r\\n)\\r\\n𝜕𝑊(𝑆\\r\\n(𝑗)\\r\\n(𝑡),𝑡)\\r\\n𝜕𝑥 ) 𝑑𝑆\\r\\n(𝑗)\\r\\n(𝑡) + 𝑏\\r\\n(𝑗)\\r\\n(𝑡)𝑑𝛽(𝑡), (33)\\r\\nwhere 𝜆\\r\\n(𝑗)\\r\\n,𝑗 = 1,2 are given by (31). Then 𝑊(𝑥,𝑡), 𝑥 > 0,𝑡 ≥ 0, satisfies the Black-Scholes type \\r\\nPDE:\\r\\n \\r\\n𝜕𝑊(𝑥,𝑡)\\r\\n𝜕𝑡 + 𝑟\\r\\n∗\\r\\n𝜕𝑊(𝑥,𝑡)\\r\\n𝜕𝑥 𝑥 − 𝑟\\r\\n∗ 𝑊(𝑥,𝑡) +\\r\\n1\\r\\n2\\r\\n𝜎\\r\\n2\\r\\n𝜕\\r\\n2𝑊(𝑥,𝑡)\\r\\n𝜕𝑥\\r\\n2\\r\\n𝑥\\r\\n2 = 0. (34) \\r\\nThe proof of Proposition 2 follows directly from (31) and (32).\\r\\nFormula (34) adjusts HS-Clam to be consistent with RDAPT, by introducing special form of arb\\x02costs. In the next two sections, we continue studying HS-Claim with different type of arb-costs. \\r\\n4. Option Pricing with Heterogeneous Views on Binomial Lattice\\nPage | 24\\r\\n Suppose the traders ℶ\\r\\n(𝑆)\\r\\nand ℶ\\r\\n(𝑉)\\r\\nobserve each other’s trading history. ℶ\\r\\n(𝑆)\\r\\n (resp. ℶ\\r\\n(𝑉)\\r\\n)\\r\\ntrades a risky asset (stock) 𝔚 on a binomial lattice with price dynamics 𝑆𝑘∆𝑡, 𝑘 ∈ 𝒩(0) =\\r\\n{0, ,1, … , ), 𝑆0 > 0 (resp. 𝑉𝑘∆𝑡, 𝑘 ∈ 𝒩(0), 𝑉0 > 0 ). The joint price process dynamics is given by\\r\\nthe binomial tree19:\\r\\n [\\r\\n𝑆(𝑘+1)∆𝑡\\r\\n𝑉(𝑘+1)∆𝑡\\r\\n] =\\r\\n{\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n[\\r\\n𝑆(𝑘+1)∆𝑡;𝑢𝑝 = 𝑆𝑘∆𝑡(1 + 𝜇∆𝑡 + 𝜎√∆𝑡)\\r\\n𝑉(𝑘+1)∆𝑡;𝑢𝑝 = 𝑉𝑘∆𝑡(1 + 𝑚∆𝑡 + 𝑣√∆𝑡)𝑣√∆𝑡\\r\\n] 𝑤. 𝑝.\\r\\n1\\r\\n2\\r\\n[\\r\\n𝑆(𝑘+1)∆𝑡;𝑑𝑜𝑤𝑛 = 𝑆𝑘∆𝑡(1 + 𝜇∆𝑡 − 𝜎√∆𝑡)\\r\\n𝑉(𝑘+1)∆𝑡;𝑑𝑜𝑤𝑛 = 𝑉𝑘∆𝑡(1 + 𝑚∆𝑡 − 𝑣√∆𝑡)\\r\\n] 𝑤. 𝑝.\\r\\n1\\r\\n2\\r\\n (35) \\r\\n𝑘 ∈ 𝒩(0), ∆𝑡 > 0 , 𝜇 ∈ ℛ, 𝑚 ∈ ℛ, 𝜎 > 0, 𝑣 > 0. For every fixed 𝑇 > 0, the bivariate binomial \\r\\ntree (𝑆𝑘∆𝑡, 𝑉𝑘∆𝑡)𝑘∈0,…,𝑁∆𝑡 generates a bivariate polygon process with trajectories in the Prokhorov\\r\\nspace 𝐶([0, 𝑇]\\r\\n2\\r\\n) which converges weakly to the following bivariate geometric Brownian motion \\r\\n(𝑆𝑡,𝑉𝑡)𝑡∈[0,𝑇]\\r\\n20 ∶ \\r\\n 𝑆𝑡 = 𝑆0𝑒\\r\\n(𝜇−\\r\\n𝜎\\r\\n2\\r\\n2\\r\\n)𝑡+𝜎𝐵(𝑡)\\r\\n, 𝑉𝑡 = 𝑉0𝑒\\r\\n(𝑚−\\r\\n𝑣\\r\\n2\\r\\n2\\r\\n)𝑡+𝑣𝐵(𝑡)\\r\\n,𝑡 ∈ [0, 𝑇], (36)\\r\\nwhere 𝐵(𝑡),𝑡 ≥ 0, is a Brownian Motion generating a stochastic basis (Ω,ℱ, 𝔽 = (ℱ𝑡,𝑡 ≥ 0),ℙ). \\r\\n To derive the state price dynamics of the representative investor ℵ, let us consider a \\r\\nperpetual European derivative contract, 𝒢. 𝒢 has price process 𝐺𝑘∆𝑡 = 𝐺(𝑆𝑘∆𝑡, 𝑉𝑘∆𝑡\\r\\n), 𝑘 ∈ 𝒩(0)\\r\\n.\\r\\n \\r\\n19\\r\\n This binomial tree (11) was introduced in Kim at al (2016) (see also Jarrow and Rudd (2008)) \\r\\nas an extension of the classical CRR-model Cox, Ross and Rubinstein M. (1979). We use this more \\r\\ngeneral binomial pricing tree, because we require the bivariate pricing tree to be driven by one risk \\r\\nfactor, and with that requirement, CRR-model is not appropriate. \\r\\n20\\r\\n The proof is similar to that in Davydov and Rotar (2008), Theorem 2, and thus is omitted. \\nPage | 25\\r\\nWe assume that ℵ is observing historical trading activities of ℶ\\r\\n(𝑆)\\r\\nand ℶ\\r\\n(𝑉)\\r\\n. ℵ (as an \\r\\nrepresentative agent) has taken simultaneously both, the long and the sort position in, 𝒢 . ℵ trades \\r\\n𝑆𝑡(resp. 𝑉𝑡) as ℶ\\r\\n(𝑆)\\r\\n (resp. ℶ\\r\\n(𝑉)\\r\\n) would do. ℵ forms a self-financing strategy (𝑎𝑘∆𝑡, 𝑏𝑘∆𝑡\\r\\n), 𝑘 ∈\\r\\n𝒩 generating a self-financing portfolio 𝑃(𝑡) = 𝑎(𝑡)𝑆(𝑡) + 𝑏(𝑡)𝑉(𝑡). Thus, 𝐺(𝑡\\r\\n(𝑘)\\r\\n) =\\r\\n𝑔 (𝑆(𝑡\\r\\n(𝑘)\\r\\n), 𝑉(𝑡\\r\\n(𝑘)\\r\\n)) = 𝑃(𝑡\\r\\n(𝑘)\\r\\n) = 𝑎(𝑡\\r\\n(𝑘)\\r\\n)𝑆(𝑡\\r\\n(𝑘)\\r\\n) + 𝑏(𝑡\\r\\n(𝑘)\\r\\n)𝑉(𝑡\\r\\n(𝑘)\\r\\n).\\r\\nWhen ℵ trades as ℶ\\r\\n(𝑆)\\r\\n(𝑟𝑒𝑠𝑝. ℶ\\r\\n(𝑉)\\r\\n) ) , at any time interval [𝑡\\r\\n(𝑘+1)\\r\\n,𝑡\\r\\n(𝑘+1)\\r\\n) the trade is \\r\\nsubject to transaction cost: (\\r\\n𝑆(𝑡\\r\\n(𝑘+1)\\r\\n)\\r\\n𝑆(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒮)\\r\\n= 1 + 𝜌(𝒮)𝑙𝑛 𝑆(𝑡\\r\\n(𝑘+1)\\r\\n)\\r\\n𝑆(𝑡\\r\\n(𝑘))\\r\\n, (𝑟𝑒𝑠𝑝. (\\r\\n𝑉(𝑡\\r\\n(𝑘+1)\\r\\n)\\r\\n𝒱(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒱)\\r\\n= 1 +\\r\\n𝜌(𝒱)𝑙𝑛 𝒱(𝑡\\r\\n(𝑘+1)\\r\\n)\\r\\n𝑉(𝑡\\r\\n(𝑘))\\r\\n) where 𝜌(𝒮) = ℭ\\r\\n𝜇\\r\\n𝜎\\r\\n(𝑟𝑒𝑠𝑝. 𝜌(𝒮) = ℭ\\r\\n𝑚\\r\\n𝑣\\r\\n) , and ℭ is an absolute constant, .\\r\\nNext, ℵ chooses (𝑎(𝑡\\r\\n(𝑘)\\r\\n), 𝑏(𝑡\\r\\n(𝑘)\\r\\n)), so that −𝑔 (𝑆(𝑡\\r\\n(𝑘+1)\\r\\n), 𝑉(𝑡\\r\\n(𝑘+1)\\r\\n)) +\\r\\n𝑎(𝑡\\r\\n(𝑘)\\r\\n)𝑆(𝑡\\r\\n(𝑘+1)\\r\\n) (\\r\\n𝑆(𝑡\\r\\n(𝑘+1)\\r\\n)\\r\\n𝑆(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒮)\\r\\n+ 𝑏(𝑡\\r\\n(𝑘)\\r\\n)𝑉(𝑡\\r\\n(𝑘+1)\\r\\n) (\\r\\n𝑉(𝑡\\r\\n(𝑘+1)\\r\\n)\\r\\n𝒱(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒱)\\r\\n= 0. That is,\\r\\n 𝑎(𝑡\\r\\n(𝑘)\\r\\n)𝑆(𝑡\\r\\n(𝑘+1,𝑢𝑝)\\r\\n) (\\r\\n𝑆(𝑡\\r\\n(𝑘+1,𝑢𝑝)\\r\\n)\\r\\n𝑆(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒮)\\r\\n+ 𝑏(𝑡\\r\\n(𝑘)\\r\\n) (\\r\\n𝑉(𝑡\\r\\n(𝑘+1,𝑢𝑝)\\r\\n)\\r\\n𝒱(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒱)\\r\\n=\\r\\n = 𝑔 (𝑆(𝑡\\r\\n(𝑘+1,𝑢𝑝)\\r\\n),𝑉(𝑡\\r\\n(𝑘+1,𝑢𝑝)\\r\\n)) (37) \\r\\nand 𝑎(𝑡\\r\\n(𝑘)\\r\\n)𝑆(𝑡\\r\\n(𝑘+1,𝑑𝑜𝑤𝑛)\\r\\n) (\\r\\n𝑆(𝑡\\r\\n(𝑘+1,𝑑𝑜𝑤𝑛)\\r\\n)\\r\\n𝑆(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒮)\\r\\n+ 𝑏(𝑡\\r\\n(𝑘)\\r\\n)𝑉(𝑡\\r\\n(𝑘+1,𝑑𝑜𝑤𝑛)\\r\\n) (\\r\\n𝑉(𝑡\\r\\n(𝑘+1,𝑑𝑜𝑤𝑛)\\r\\n)\\r\\n𝒱(𝑡\\r\\n(𝑘))\\r\\n)\\r\\n𝜌(𝒱)\\r\\n=\\r\\n𝑔 (𝑆(𝑡\\r\\n(𝑘+1,𝑑𝑜𝑤𝑛)\\r\\n), 𝑉(𝑡\\r\\n(𝑘+1,𝑑𝑜𝑤𝑛)\\r\\n)). Thus, solving the two equations for 𝑎(𝑡\\r\\n(𝑘)\\r\\n) and 𝑏(𝑡\\r\\n(𝑘)\\r\\n), and \\r\\nevaluating 𝐺(𝑡\\r\\n(𝑘)\\r\\n) = 𝑎(𝑡\\r\\n(𝑘)\\r\\n)𝑆(𝑡\\r\\n(𝑘)\\r\\n) + 𝑏(𝑡\\r\\n(𝑘)\\r\\n)𝑉(𝑡\\r\\n(𝑘)\\r\\n), ℵ obtains the binomial option price \\r\\ndynamics:\\r\\n 𝑔 (𝑆(𝑡\\r\\n(𝑘)\\r\\n), 𝑉(𝑡\\r\\n(𝑘)\\r\\n)) = 𝑄\\r\\n(∆𝑡)𝑔(𝑡(𝑘+1,𝑢𝑝)\\r\\n) + (1 − 𝑄\\r\\n(∆𝑡)\\r\\n)𝑔(𝑡\\r\\n(𝑘+1,𝑑𝑜𝑤𝑛)\\r\\n) (38)\\nPage | 26\\r\\nwhere the risk-neutral probabilities (ℵ’s state-price probabilities) are \\r\\n𝑄\\r\\n(∆𝑡) =\\r\\n1\\r\\n2\\r\\n−\\r\\n𝜇(1+ℭ\\r\\n𝜇\\r\\n𝜎\\r\\n)(1+ℭ\\r\\n𝜎\\r\\n2\\r\\n)−𝑚(1+ℭ\\r\\n𝑚\\r\\n𝑣\\r\\n)(1+ℭ\\r\\n𝑣\\r\\n2\\r\\n)\\r\\n2(𝜎−𝑣+ℭ(𝜇−𝑚))\\r\\n√∆𝑡 (39) \\r\\nand 1 − 𝑄\\r\\n(∆𝑡)\\r\\n. Note that even if (1) 𝜎 = 𝑣 (which is an arbitrage pricing model, if ℶ\\r\\n(𝑆)\\r\\nand ℶ\\r\\n(𝑉)\\r\\ntrades without arb-costs), as soon as (2) 𝜇 ≠ 𝑚, risk neutral probabilities exist, and (3) the \\r\\ntransaction costs have offset the arbitrage gains. From Kim at al. (2016), Section 3.2, and Black\\r\\n(1972), 𝑄\\r\\n(∆𝑡)\\r\\nshould have the representation\\r\\n 𝑄\\r\\n(∆𝑡) =\\r\\n1\\r\\n2\\r\\n−\\r\\n1\\r\\n2\\r\\n𝜇\\r\\n(∗)−𝑟(∗)\\r\\n𝜎\\r\\n(∗) √∆𝑡 =\\r\\n1\\r\\n2\\r\\n−\\r\\n1\\r\\n2\\r\\n𝑚(∗)−𝑟\\r\\n(∗)\\r\\n𝑣\\r\\n(∗) √∆𝑡, (40) \\r\\nwhere\\r\\n𝑟\\r\\n(∗)\\r\\n: =\\r\\n𝜇\\r\\n(∗)𝑣(∗)−𝑚(∗)𝜎(∗)\\r\\n𝑣\\r\\n(∗)−𝜎(∗)\\r\\n, 𝜇\\r\\n(∗)\\r\\n: = 𝜇 (1 + ℭ\\r\\n𝜇\\r\\n𝜎\\r\\n) (1 + ℭ\\r\\n𝜎\\r\\n2\\r\\n), \\r\\n𝑚(∗): = 𝑚 (1 + ℭ\\r\\n𝑚\\r\\n𝑣\\r\\n) (1 + ℭ\\r\\n𝑣\\r\\n2\\r\\n), 𝜎\\r\\n(∗) ≔ 𝜎 + ℭ𝜇, 𝑣(∗) = 𝑣 + ℭ𝑚. (41)\\r\\nIn (41), 𝑟\\r\\n(∗)\\r\\nis ℵ’s risk neutral rate, and 𝜇\\r\\n(∗)\\r\\n, 𝑚(∗), 𝜎\\r\\n(∗)\\r\\nand 𝑣\\r\\n(∗)\\r\\nare the adjusted (for arb-cost)\\r\\ndrift and volatility parameters, satisfying (39) and (40). Now, the price processes (𝑆𝑡, 𝑉𝑡)𝑡∈[0,𝑇], as \\r\\nseen by ℵ in the risk -neutral world (Ω,ℱ, 𝔽 = (ℱ𝑡,𝑡 ≥ 0), ℚ), ℚ~ℙ have the price dynamics:\\r\\n 𝑆𝑡 = 𝑆0𝑒\\r\\n(𝑟\\r\\n(∗)−\\r\\n𝜎\\r\\n(∗)\\r\\n2\\r\\n2\\r\\n)𝑡+𝜎\\r\\n(∗)𝐵(∗)\\r\\n(𝑡)\\r\\n, 𝑉𝑡 = 𝑉0𝑒\\r\\n(𝑟\\r\\n(∗)−\\r\\n𝑣\\r\\n(∗)\\r\\n2\\r\\n2\\r\\n)𝑡+𝑣\\r\\n(∗)𝐵(∗)\\r\\n(𝑡)\\r\\n,𝑡 ∈ [0, 𝑇], (42)\\r\\n𝐵\\r\\n(∗)\\r\\n(𝑡),𝑡 ≥ 0, is a Brownian motion on ℚ, and an arithmetic Brownian motion on ℙ with \\r\\n𝐵\\r\\n(∗)\\r\\n(𝑡) = 𝐵(𝑡) + 𝜃\\r\\n(∗)\\r\\n𝑡. The parameter 𝜃\\r\\n(∗) =\\r\\n𝜇\\r\\n(∗)−𝑟(∗)\\r\\n𝜎\\r\\n(∗) =\\r\\n𝑚(∗)−𝑟\\r\\n(∗)\\r\\n𝑣\\r\\n(∗)\\r\\nis the market price of risk in \\r\\nℵ\\r\\n′\\r\\n𝑠 market model with arb-costs.\\nPage | 27\\r\\nNote that in the case of trading with without arb-costs, 𝑄\\r\\n(∆𝑡;𝑛𝑜 𝑎𝑟𝑏−𝑐𝑜𝑠𝑡)\\r\\n≔\\r\\n1\\r\\n2\\r\\n−\\r\\n𝜇−𝑚\\r\\n2(𝜎−𝑣)\\r\\n√∆𝑡. Thus, 𝑄\\r\\n(∆𝑡;𝑛𝑜 𝑎𝑟𝑏−𝑐𝑜𝑠𝑡)\\r\\nis the risk-neural probability in the Black(1972) model: \\r\\n𝑄\\r\\n(∆𝑡;𝑛𝑜 𝑡𝑟𝑎𝑛𝑠 𝑐𝑜𝑠𝑡) ≔\\r\\n1\\r\\n2\\r\\n−\\r\\n1\\r\\n2\\r\\n𝜇−𝑟\\r\\n𝜎\\r\\n√∆𝑡, where 𝜇−𝑟\\r\\n𝜎\\r\\n=\\r\\n𝑚−𝑟\\r\\n𝑣\\r\\n=\\r\\n𝜇−𝑚\\r\\n𝜎−𝑣\\r\\nand 𝑟 =\\r\\n𝜇𝑣−𝑚𝜎\\r\\n𝑣−𝜎\\r\\n. Thus, ℵ\\r\\n′\\r\\n𝑠 model \\r\\ncan be viewed as an extension of the Black (1972) model when special type of transaction costs is\\r\\nintroduced.\\r\\n𝟓. Black-Scholes Formula for Markets with Arb- Transaction Costs in the Presence of \\r\\nTransaction Costs on the Delta-and Gamma-positions\\r\\nIn this section, we extend our binomial tree model with arb- costs in Section 3, to market with \\r\\ncontinuous time price process. ℶ\\r\\n(𝑆)\\r\\nand ℶ\\r\\n(𝑉)\\r\\nview and trade one share of the price process 𝔚 as \\r\\ndifferent diffusions price processes:\\r\\n𝑑𝑆𝑡 = 𝜇𝑡𝑆𝑡𝑑𝑡 + 𝜎𝑡𝑆𝑡𝑑𝐵𝑡, 𝑆0 > 0, 𝑑𝑉𝑡 = 𝑚𝑡𝑆𝑡𝑑𝑡 + 𝑣𝑡𝑆𝑡𝑑𝐵𝑡, 𝑆0 > 0, (43)\\r\\nwhere\\r\\n(𝑆𝑒𝑐1𝑎): 𝐵𝑡\\r\\n,𝑡 ≥ 0, is a standard Brownian motion generating stochastic basis (Ω,ℱ, 𝔽 =\\r\\n(ℱ𝑡\\r\\n,𝑡 ≥ 0),ℙ);\\r\\n(𝑆𝑒𝑐1𝑏): 𝜇𝑡 = 𝜇(𝑡, 𝑆𝑡),𝑡 ≥ 0 and 𝜎𝑡 = 𝜎(𝑡, 𝑆𝑡),𝑡 ≥ 0, are 𝔽- adapted drift and diffusion\\r\\ncoefficients satisfying the regularity conditions guaranteeing existence and uniqueness of the\\r\\nstrong solution of (43)\\r\\n21;\\r\\n \\r\\n21 See for example, Duffie (2001) Chapter 6, Shreve (2004), Chapter 4, Shiryev (1999) Chapter3\\nPage | 28\\r\\n(𝑆𝑒𝑐2): a risk-free asset (bond) 𝔅, with price process 𝛽𝑡\\r\\n,𝑡 ≥ 0, following continuous diffusion:\\r\\n𝑑𝛽𝑡 = 𝑟𝑡𝛽𝑡𝑑𝑡,𝑡 ≥ 0, (44) \\r\\nwhere 𝑟𝑡 = 𝑟(𝑡, 𝑆𝑡\\r\\n) > 0,𝑡 ≥ 0 is 𝔽- adapted, risk-free rate satisfying the regularity condition:\\r\\nsup {𝑟𝑡 +\\r\\n1\\r\\n𝑟𝑡\\r\\n;𝑡 ≥ 0} < ∞,ℙ-a.s.\\r\\nConsider a portfolio, 𝒳, with price dynamics\\r\\n𝑋𝑡 = 𝕏(𝑡, 𝑆𝑡\\r\\n) = 𝑎𝑡𝑆𝑡 + 𝑏𝑡𝛽𝑡\\r\\n,𝑡 ≥ 0, (45)\\r\\nwhere\\r\\n(𝑃1): 𝕏(𝑡, 𝑥),𝑡 ≥ 0, 𝑥 > 0, has continuous derivatives 𝜕𝕏(𝑡,𝑥)\\r\\n𝜕𝑡 and 𝜕\\r\\n2𝕏(𝑡,𝑥)\\r\\n𝜕𝑥\\r\\n2\\r\\n,𝑡 ≥ 0, 𝑥 > 0;\\r\\n(𝑃1): the trading strategy (𝑎𝑡 = 𝑎(𝑡, 𝑆𝑡), 𝑏𝑡 = 𝑏(𝑡, 𝑆𝑡);𝑡 ≥ 0) is 𝔽- adapted, and subject to arb\\x02cost, Θ(𝑡, 𝑆𝑡) = (∆ (𝑡, 𝑆𝑡\\r\\n), Γ (𝑡, 𝑆𝑡),Ψ (𝑡, 𝑆𝑡), 𝒞 (𝑡, 𝑆𝑡)),𝑡 ≥ 0, a 𝔽- adapted quadruplet, where \\r\\n(𝑃1𝑎) ∆ (𝑡, 𝑆𝑡) ∈ (0,1),𝑡 ≥ 0, is the 𝑑𝑒𝑙𝑡𝑎-cost;\\r\\n(𝑃1𝑏) Γ (𝑡, 𝑆𝑡) > 0,𝑡 ≥ 0, is the 𝑔𝑎𝑚𝑚𝑎-cost;\\r\\n(𝑃1𝑐) Ψ (𝑡, 𝑆𝑡) ∈ (0,1),𝑡 ≥ 0, is the cost associated with trading the bond;\\r\\n(𝑃1𝑑) 𝒞 (𝑡, 𝑆𝑡) = 𝒽 (𝑡, 𝑆𝑡) − 𝜅 (𝑡, 𝑆𝑡),𝑡 ≥ 0, is the consumption cost 𝒽 (𝑡, 𝑆𝑡),𝑡 ≥ 0 minus the \\r\\ncost opportunities 𝜅 (𝑡, 𝑆𝑡\\r\\n) =\\r\\n∆ (𝑡,𝑆𝑡\\r\\n)\\r\\n1−∆ (𝑡,𝑆𝑡\\r\\n)\\r\\n(1 − Ψ (𝑡, 𝑆𝑡\\r\\n))𝑟𝑡𝕏(𝑡, 𝑆𝑡) associated with eliminating the\\r\\narbitrage opportunities;\\r\\n(𝑃2): the 𝒳-dynamics is given by\\nPage | 29\\r\\n𝑑𝑋𝑡 = 𝑑𝕏(𝑡, 𝑆𝑡\\r\\n) = (𝑎𝑡(1 − ∆ (𝑡, 𝑆𝑡)) −\\r\\n𝜎𝑡\\r\\n2\\r\\n2\\r\\nΓ (𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕\\r\\n2𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥\\r\\n2\\r\\n𝑆𝑡) 𝑑𝑆𝑡 +\\r\\n+𝑏𝑡(1 − Ψ (𝑡, 𝑆𝑡\\r\\n))𝑑𝛽𝑡 − 𝒞 (𝑡, 𝑆𝑡)𝑑𝑡,𝑡 ≥ 0; (46)\\r\\n(𝑃3): the discount rate ℷ𝑡\\r\\n,𝑡 ≥ 0, associated with 𝒳-dynamics is given by\\r\\nℷ𝑡 = ℷ(𝑡, 𝑆𝑡\\r\\n) = 𝑟𝑡\\r\\n1−Ψ (𝑡,𝑥)\\r\\n1−∆ (𝑡,𝑥)\\r\\n> 0,𝑡 ≥ 0; (47)\\r\\n(𝑃5): the augmented volatility 𝜌(𝑡, 𝑥), associated with 𝒳-dynamics is given by\\r\\n 𝜌(𝑡, 𝑥) = √1 + ℷ(𝑡, 𝑆𝑡\\r\\n)Γ (𝑡, 𝑥) 𝜎(𝑡, 𝑥) > 0,𝑡 ≥ 0, 𝑥 > 0. (48)\\r\\nThe trading strategy (𝑎𝑡, 𝑏𝑡\\r\\n),𝑡 ≥ 0, satisfying (45) - (48), is an arbitrage, if the 𝒳-\\r\\ndynamics generated by (𝑎𝑡, 𝑏𝑡\\r\\n),𝑡 ≥ 0 satisfies: ℙ( 𝑋0 ≤ 0, 𝑋𝑇 ≥ 0) = 1, and ℙ( 𝑋𝑇 > 0) > 0\\r\\nfor some 𝑇 > 0.\\r\\nConsider a new security in the market (𝔖, 𝔅), an European derivative 𝒴, with price process \\r\\n𝑌𝑡 = 𝕐(𝑡, 𝑆𝑡\\r\\n), where 𝕐(𝑡, 𝑥),𝑡 ≥ 0, 𝑥 > 0 has continuous derivatives 𝜕𝕐(𝑡,𝑥)\\r\\n𝜕𝑡 and 𝜕\\r\\n2𝕐(𝑡,𝑥)\\r\\n𝜕𝑥\\r\\n2\\r\\n,𝑡 ≥\\r\\n0, 𝑥 > 0. The payoff of 𝒴 at the expiration date 𝑇 > 0 is 𝑌𝑇 = 𝕐(𝑇, 𝑆𝑇\\r\\n) = 𝑔(𝑆𝑇), for some \\r\\ncontinuous 𝑔(𝑥), 𝑥 ≥ 0.\\r\\nBy the Itô formula:\\r\\n𝑑𝕐(𝑡, 𝑆𝑡\\r\\n) = {\\r\\n𝜕𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑡 +\\r\\n𝜕𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥 𝜇𝑡𝑆𝑡 +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥\\r\\n2\\r\\n𝜎𝑡\\r\\n2𝑆𝑡2} 𝑑𝑡 +\\r\\n𝜕𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥 𝜎𝑡𝑆𝑡𝑑𝐵𝑡\\r\\n.\\r\\nConsider a replicating portfolio 𝑌𝑡 = 𝕐(𝑡, 𝑆𝑡\\r\\n) = 𝑎𝑡𝑆𝑡 + 𝑏𝑡𝛽𝑡\\r\\n,𝑡 ≥ 0, satisfying \\r\\n𝑑𝑌𝑡 = 𝑑𝕐(𝑡, 𝑆𝑡\\r\\n) = (𝑎𝑡(1 − ∆ (𝑡, 𝑆𝑡)) −\\r\\n𝜎𝑡\\r\\n2\\r\\n2\\r\\nΓ (𝑡, 𝑆𝑡\\r\\n)𝑆𝑡\\r\\n𝜕\\r\\n2𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥\\r\\n2\\r\\n) 𝑑𝑆𝑡 +\\nPage | 30\\r\\n+𝑏𝑡(1 − Ψ (𝑡, 𝑆𝑡\\r\\n))𝑑𝛽𝑡 − (\\r\\n∆ (𝑡, 𝑆𝑡\\r\\n)\\r\\n1 − ∆ (𝑡, 𝑆𝑡\\r\\n)\\r\\n(1 − Ψ (𝑡, 𝑆𝑡\\r\\n))𝑟𝑡𝕐(𝑡, 𝑆𝑡) − 𝒽 (𝑡, 𝑆𝑡)) 𝑑𝑡\\r\\nWe have two equations for the 𝒴- dynamics, which together 𝕐(𝑡, 𝑆𝑡\\r\\n) = 𝑎𝑡𝑆𝑡 + 𝑏𝑡𝛽𝑡\\r\\n give \\r\\nexpressions for the trading strategies\\r\\n𝑎𝑡 =\\r\\n1\\r\\n1 − ∆ (𝑡, 𝑆𝑡\\r\\n)\\r\\n(\\r\\n𝜕𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥 +\\r\\n𝜎𝑡\\r\\n2\\r\\n2\\r\\nΓ (𝑡, 𝑆𝑡\\r\\n)𝑆𝑡\\r\\n𝜕\\r\\n2𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥\\r\\n2\\r\\n),\\r\\nand \\r\\n𝑏𝑡 =\\r\\n1\\r\\n𝛽𝑡\\r\\n(𝕐(𝑡, 𝑆𝑡\\r\\n) −\\r\\n1\\r\\n1 − ∆ (𝑡, 𝑆𝑡\\r\\n)\\r\\n(\\r\\n𝜕𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥 𝑆𝑡 +\\r\\n𝜎𝑡\\r\\n2\\r\\n2\\r\\nΓ (𝑡, 𝑆𝑡\\r\\n)𝑆𝑡\\r\\n2\\r\\n𝜕\\r\\n2𝕐(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝑥\\r\\n2\\r\\n)).\\r\\nSubstituting the expressions for 𝑎𝑡 and 𝑏𝑡in the equation the drift of 𝕐(𝑡, 𝑆𝑡\\r\\n),𝑡 ≥ 0, leads to the \\r\\nfollowing proposition:\\r\\nPROPOSITION 3. Consider a market with securities (𝔖, 𝔅, 𝒴) and suppose conditions (43)-(48))\\r\\nhold. Then 𝕐(𝑡, 𝑥),𝑡 ≥ 0, 𝑥 > 0, satisfies the partial differential equation: for all 𝑡 ∈ [0,𝑡), 𝑥 >\\r\\n0,\\r\\n𝜕𝕐(𝑡,𝑥)\\r\\n𝜕𝑡 + ℷ(𝑡, 𝑆𝑡\\r\\n)\\r\\n𝜕𝕐(𝑡,𝑥)\\r\\n𝜕𝑥 𝑥 − ℷ(𝑡, 𝑆𝑡\\r\\n)𝕐(𝑡, 𝑥) +\\r\\n1\\r\\n2\\r\\n𝜕\\r\\n2𝕐(𝑡,𝑥)\\r\\n𝜕𝑥\\r\\n2\\r\\n𝜌(𝑡, 𝑥)\\r\\n2𝑥2 + 𝒽 (𝑡, 𝑥) = 0 (49) \\r\\nwith boundary condition 𝕐(𝑇, 𝑥) = 𝑔(𝑥), 𝑥 > 0.\\r\\nThe solution of (49) is given by the Feynman-Kac formula22:\\r\\n𝕐(𝑡, 𝑥) = 𝔼𝑥,𝑡[∫ 𝜑𝑡.𝑠\\r\\n𝑇\\r\\n𝑡\\r\\n𝒽 (𝑠, 𝑍𝑠\\r\\n)𝑑𝑠 + 𝜑𝑡.𝑇𝑔(𝑍𝑇)], (50)\\r\\n \\r\\n22 See, for example, Duffie (2001), Section E.6.\\nPage | 31\\r\\nwhere \\r\\n(𝐹𝐶1): 𝑍𝑠\\r\\n, 𝑠 ≥ 𝑡, 𝑍𝑡 = 𝑥, is a continuous diffusion given by\\r\\n𝑑𝑍𝑠 = ℷ(𝑠, 𝑍𝑠\\r\\n)𝑍𝑠𝑑𝑠 + 𝜌(𝑠, 𝑍𝑠)𝑍𝑠𝑑𝑊𝑠\\r\\n, 𝑠 ≥ 𝑡, (51)\\r\\nwhere 𝑊𝑡,𝑡 ≥ 0 is a Brownian motion of a stochastic basis (Ω,ℱ, 𝔽 = (ℱ𝑡,𝑡 ≥ 0), ℚ)\\r\\n(𝐹𝐶2):𝜑𝑡.𝑠 = exp{− ∫ ℷ(𝑢, 𝑍𝑢)𝑑𝑢 𝑠\\r\\n𝑡\\r\\n}, 0 ≤ 𝑡 < 𝑠, is the family of discount factors;\\r\\n(𝐹𝐶3) 𝔼𝑥,𝑡\\r\\nstands for the expected value indicating that the process 𝑍𝑠, 𝑠 ≥ 𝑡 starts at 𝑍𝑡 = 𝑥.\\r\\n𝟔. Conclusions \\r\\nIn this paper, we correct statements made in Shefrin (2005) on behavioral option pricing. We \\r\\npointed out that option pricing formulas in Shefrin (2005) and Benninga and Mayshar (2000) are \\r\\nwith flaws from the viewpoint of rational dynamic asset pricing theory. Using those incorrect asset \\r\\npricing formulas could lead to serious losses of the traders, as in the market there will be “rational” \\r\\ntraders who will explore the arbitrage opportunities those behavioral option prices generates. In \\r\\norder to correct Shefrin’s and Benninga and Mayshar’s (2000)’s behavioral approaches to option \\r\\npricing, we introduce several types of arb-costs on the velocity of hedged portfolio, so that the \\r\\ngenerated arbitrage gains be eliminated by those arb-costs. In case the market model without \\r\\narbitrages, the introduction of the transaction costs on the velocity of hedged portfolio increases \\r\\nslightly the volatility of the hedge and indeed worsen the Sharpe ratio of the trader. Since it will \\r\\nbe impossible to introduce transaction costs only when the trades is an arbitrage (as in the \\r\\nbehavioral option pricing), we argue that a universal transaction costs on the velocity of trades \\r\\nshould be introduced in real markets. \\r\\nReferences\\nPage | 32\\r\\nAnsel, J.P., Stricker, C. (1994) Couverture des actifs contingents et prix maximum. Ann. Inst. \\r\\nHenri Poincaré, 30, 303–315;\\r\\nApplebaum D. (2009). Lévy Processes and Stochastic Calculus, Cambridge University Press, \\r\\nCambridge, UK;\\r\\nBarberis N. (2003) A survey of behavioral finance, in Constantinides, G.M., Harris M., and Stulz \\r\\nR.(Edt.), Handbook of the Economics of Finance, Chapter 8, 1051-1121, Elsevier Science B.V., \\r\\nNorth Holland, Amsterdam; \\r\\nBarndorff-Nielsen, O.E., and Shephard, N. (2001a) Modelling by Lévy Processes for Financial\\r\\nEconometrics. in Barndorff-Nielsen, O.,E., Mikosch, T., and Resnick, S. (eds), Lévy Processes\\x02Theory and Applications. Birkhäuser Publishing, 283-318;\\r\\nBarndorff-Nielsen, O.E. and Shephard, N. (2001b) Non-Gaussian Ornstein-Uhlenbeck-based \\r\\nmodels and some of their uses in financial economics, J. R. Stat. Soc. Ser. B Stat. Methodol., 63 \\r\\n(2001), 167–241;\\r\\nBarndorff-Nielsen O.E. and Stelzer R. (2007) Positive-definite matrix processes of finite variation, \\r\\nProbability and Mathematical Statististics , 27, 3–43;\\r\\nBarndorff-Nielsen, O. E. and Stelzer, R. (2013) The multivariate supOU stochastic volatility \\r\\nmodel, Mathematical Finance, 23, 275–296;\\r\\nBenninga S. and Mayshar J. (2000) Heterogeneity and option pricing, Review of Derivatives \\r\\nResearch, 4, 7-27;\\r\\nBondesson L. (1992) Generalized Gamma Convolutions and Related classes of Distributions and \\r\\nDensities, Lecture notes in Statistics 76, Springer, Berlin;\\nPage | 33\\r\\nBlack F. (1972) Capital market equilibrium with restricted borrowing, The Journal of Finance, 45, \\r\\n445-455;\\r\\nBlack F. and Scholes M. (1973) The Pricing of Options and Corporate Liabilities, The Journal of \\r\\nPolitical Economy, 81,637-654;\\r\\nBose A., Dasguota A. and Rubin H. (2002) A contemporary review and bibliography of infinitely \\r\\ndivisible distributions and processes, Sankhyā: The Indian Journal of Statistics, 64, 763-819;\\r\\nBrav, A., Heaton J.B. and Rosenberg A. (2004) The rational–behavioral debate in financial\\r\\nEconomics, Journal of Economic Methodology 11, 393–409; \\r\\nBuraschi A. and Jiltsov A. (2006) Model uncertainty and option markets with heterogeneous \\r\\nbeliefs, The Journal of Finance ,61, 2841-2897;\\r\\nChabakauri G. (2013) Dynamic equilibrium with two stocks, heterogeneous investors, and \\r\\nportfolio constraints, The Review of Financial Studies, 26, 3104-3141;\\r\\nCox, J., Ross, S., Rubinstein, M., 1979. Options pricing: a simplified approach. Journal of \\r\\nFinancial Economics, 7, 229–263; \\r\\nCurtis G. and Statman M. (2004) Modern portfolio theory and behavioral finance, The Journal of \\r\\nWealth Management, 7, 16-22;\\r\\nDalang R.C., Morton A. Willinger W. (1990) Equivalent martingale measures and no-arbitrage in \\r\\nstochastic securities market model. Stochastics and Stochastic Reports, 29, 185-201;\\r\\nDavydov Y and Rotar V. (2008), On a non-classical invariance principle, Statistics & Probability \\r\\nLetters, 78, 2031-2038;\\nPage | 34\\r\\nDelbaen, F., Schachermayer W. (1994). A general version of the fundamental theorem \\r\\nof asset pricing, Mathematische Annalen, 300, 463–520;\\r\\nDelbaen, F., Schachermayer W. (1997). A non-arbitrage and the fundamental theorem of asset \\r\\npricing: Summary of main results, Proceedings of “Symposia in Applied Mathematics“of the \\r\\nAMS, 1-10;\\r\\nDelbaen, F., Schachermayer W. (1998). The Fundamental Theorem of Asset Pricing for \\r\\nUnbounded Stochastic Processes, Mathematische Annalen, 312, 215–260;\\r\\nDelbaen, F., Schachermayer W.(2006). The Mathematics of Arbitrage, Springer, Berlin;\\r\\nLocke P. (2010) Derivative markets, in Baker H.K. and Nofsinger J.R. (edt.) Behavioral Finance: \\r\\nInvestors, Corporations and Markets, Wiley Hoboken,613-627;\\r\\nDuffie D. (2001) Dynamic Asset Pricing Theory, 3rd Edition, Princeton University Press, \\r\\nPrinceton Oxford;\\r\\nFrittelli M. (1997) Semimartingales and asset pricing under constraints, in Dempster M.A.H. and \\r\\nPliska S.R.(ets), Mathematics of Derivative Securities, Cambridge University Press, Cambridge \\r\\nUK, 255-270;\\r\\nHarrison J.M. and Kreps D.M. (1979) Martingales and arbitrage in multiperiod securities markets, \\r\\nJournal of Economic Theory, 20, 381-408;\\r\\nHarrison J.M. and Pliska S.R. (1981) Martingales and stochastic integrals in the theory of \\r\\ncontinuous trading. Stochastic Processes and their Applications, 11, 215-260;\\r\\nHarrison J.M. and Pliska S.R. (1983) A stochastic calculus model of continuous trading: complete \\r\\nmarkets. Stochastic Processes and their Applications, 15, 313-316;\\nPage | 35\\r\\nHe, X-Z, and Shi L (2016) A binomial model of asset and option pricing with heterogeneous \\r\\nbeliefs, JMSE, 1, 94-113;\\r\\nJarrow R. and Rudd A. (1983) Option Pricing, Dow Jones- Irwin Publishing, Homewood, Il.\\r\\nKardaras C. (2010) Finitely additive probabilities and the fundamental theorem of asset pricing, in \\r\\nChiarella, S., and Novikov A. (eds.), Contemporary Quantitative Finance, Springer, Berlin; \\r\\nKim, Y.S., Stoyanov, S., Rachev, S. and Fabozzi F. (2016) Multi-purpose binomial model: Fitting \\r\\nall moments to the underlying geometric Brownian motion, Economics Letters, 145, 225-229;\\r\\nKyprianou A. (2006) Introductory Lectures on Fluctuations of Lévy Processes, Springer Berlin;\\r\\nMatsumura K. and Kawamoto M. (2013) Derivative trade optimizing model utilizing GP based on \\r\\nbehavioral finance theory, Electronics and Communications in Japan, 96, 15-28;\\r\\nMerton R.C. (1973) Theory of Rational Option Pricing, The Bell Journal of Economics and \\r\\nManagement, 4, 141-183;\\r\\nMishura Y. (2008) Stochastic Calculus for Fractional Brownian Motion and Related Processes. \\r\\nSpringer, Berlin;\\r\\nMuhle-Karbe J. and Nutz M. (2016) A risk-neutral equilibrium leading to uncertain volatility \\r\\npricing, Technical Report, University of Michigan, Department of Mathematics\\r\\nhttps://arxiv.org/abs/1612.09152;\\r\\nParisi F., Smith, V. L. (2005) The Law and Economics of Irrational Behavior, Stanford University \\r\\nPress;\\nPage | 36\\r\\nPena A., Alemanni, B., and Zanotti, G. (2011) On the Role of Behavioral Finance in the Pricing \\r\\nof Financial Derivatives: The Case of the S&P 500, CAREFIN Research Paper No. 03/2010, \\r\\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=1798607;\\r\\nPrelec D. (1998) The probability weighting function, Econometrica, 66, 497-527;\\r\\nSato K-I. (1999), Lévy Processes and Infinitely Divisible Distributions, Cambridge University \\r\\nPress, Cambridge, UK;\\r\\nRachev S.T., Kim Y-S., Bianchi M.L. Fabozzi F.J. (2011) Financial Models with Lévy Processes \\r\\nand Volatility Clustering, Wiley, Hoboken;\\r\\nRicciardi V. (2008) Risk: Traditional Finance Versus Behavioral Finance, in Frank J. Fabozzi, ed.\\r\\nHandbook of Finance: Volume 3: Valuation, Financial Modeling, and Quantitative Tools, 11-38, \\r\\nJohn Wiley & Sons;\\r\\nShefrin H. (2005) A Behavioral Approach to Asset Pricing, Elsevier/Academic Press, Burlington \\r\\nMA;\\r\\nShiller R.J. (2003) From efficient market theory to behavioral finance, Journal of Economic \\r\\nPerspectives, 17, 83-104;\\r\\nStatman M. (1995) Behavioral Finance versus Standard Finance; CFA Institute Publications\\r\\nwww.cfapubs.org/doi/pdf/10.2469/cp.v1995.n7.4;\\r\\nSteutel F.W. and van Harn K. (2004) Infinitely Divisibility of Probability Distributions on the Real \\r\\nLine, Marcel Dekker New York;\\nPage | 37\\r\\nStrong W. (2014) Fundamental theorems of asset pricing for piecewise semimartingales of \\r\\nstochastic dimension, Finance and Stochastics, 18, 487–514;\\r\\nThaler R.H. (Edt) (2005) Advances in Behavioral Finance, Volume II, Princeton University Press, \\r\\nPrinceton and Oxford;\\r\\nVerma R. (2012) Behavioral finance and pricing of derivatives: implications for Dodd-Frank act, \\r\\nReview of Futures Markets, 20, 21-67;\\r\\nZeckhauser R. (1986) Comments: Behavioral versus rational economics: What you see is what \\r\\nyou conquer, Journal of Business, 59,435-449.'},\n",
       " {'name': '2411.04554v1.pdf',\n",
       "  'content': 'Peri-midFormer: Periodic Pyramid Transformer for\\r\\nTime Series Analysis\\r\\nQiang Wu Gechang Yao Zhixi Feng† Shuyuan Yang\\r\\nKey Laboratory of Intelligent Perception and Image Understanding of\\r\\nMinistry of Education, School of Artificial Intelligence, Xidian University, China\\r\\n{wu_qiang, yao_gechang}@stu.xidian.edu.cn, {zxfeng, syyang}@xidian.edu.cn\\r\\nAbstract\\r\\nTime series analysis finds wide applications in fields such as weather forecasting,\\r\\nanomaly detection, and behavior recognition. Previous methods attempted to\\r\\nmodel temporal variations directly using 1D time series. However, this has been\\r\\nquite challenging due to the discrete nature of data points in time series and the\\r\\ncomplexity of periodic variation. In terms of periodicity, taking weather and traffic\\r\\ndata as an example, there are multi-periodic variations such as yearly, monthly,\\r\\nweekly, and daily, etc. In order to break through the limitations of the previous\\r\\nmethods, we decouple the implied complex periodic variations into inclusion and\\r\\noverlap relationships among different level periodic components based on the\\r\\nobservation of the multi-periodicity therein and its inclusion relationships. This\\r\\nexplicitly represents the naturally occurring pyramid-like properties in time series,\\r\\nwhere the top level is the original time series and lower levels consist of periodic\\r\\ncomponents with gradually shorter periods, which we call the periodic pyramid. To\\r\\nfurther extract complex temporal variations, we introduce self-attention mechanism\\r\\ninto the periodic pyramid, capturing complex periodic relationships by computing\\r\\nattention between periodic components based on their inclusion, overlap, and\\r\\nadjacency relationships. Our proposed Peri-midFormer demonstrates outstanding\\r\\nperformance in five mainstream time series analysis tasks, including short- and\\r\\nlong-term forecasting, imputation, classification, and anomaly detection. The code\\r\\nis available at https://github.com/WuQiangXDU/Peri-midFormer.\\r\\n1 Introduction\\r\\nTime series analysis stands as a foundational challenge pivotal across diverse real-world scenarios [1],\\r\\nsuch as weather forecasting [2], imputation of missing data within offshore wind speed time series [3],\\r\\nanomaly detection for industrial maintenance [4], and classification [5]. Due to its substantial practical\\r\\nutility, time series analysis has garnered considerable interest, leading to the development of a large\\r\\nnumber of deep learning-based methods for it.\\r\\nDifferent from other forms of sequential data, like language or video, time series data is continuously\\r\\nrecorded, capturing scalar values at each time point. Furthermore, real-world time series variations\\r\\noften entail complex temporal patterns, where multiple fluctuations (e.g., ascents, descents, fluctu\\x02ations, etc.) intermingle and intertwine, particularly salient is the presence of various overlapping\\r\\nperiodic components in it, rendering the modeling of temporal variations exceptionally challenging.\\r\\nDeep learning models, known for their powerful non-linear capabilities, capture intricate temporal\\r\\nvariations in real-world time series. Recurrent neural networks (RNNs) leverage sequential data,\\r\\nallowing past information to influence future predictions [6; 7]. However, they face challenges with\\r\\nlong-term dependencies and computational inefficiency due to their sequential nature. Temporal\\r\\nconvolutional neural networks (TCNs) [8; 9] extract variation information but struggle with capturing\\r\\nlong-term dependencies. Transformers with attention mechanisms have gained popularity for sequen\\x0238th Conference on Neural Information Processing Systems (NeurIPS 2024).\\r\\narXiv:2411.04554v1 [cs.LG] 7 Nov 2024\\nPeriod 3\\r\\nPeriod 2\\r\\nPeriod 1\\r\\nOriginal time series\\r\\nMulti-periodicity Inclusion and overlap Periodic Pyramid\\r\\nFigure 1: Multi-periodicity, inclusion of periodic components, and Periodic Pyramid.\\r\\ntial modeling [10; 11], capturing pairwise temporal dependencies among time points. Yet, discerning\\r\\nreliable dependencies directly from scattered time points remains challenging [12]. Timesnet [13]\\r\\ninnovatively transforms 1D time series into 2D tensors, unifying intra- and inter-period variations in\\r\\n2D space. However, it overlooks inclusion relationships between periods of different scales and is\\r\\nconstrained by limited feature extraction capability of CNNs, hindering its ability to explore complex\\r\\nrelationships within time series.\\r\\nWe analyze time series by examining the inclusion and overlap (hereinafter collectively referred to as\\r\\ninclusion) relationships between various periodic components to address complex temporal variations.\\r\\nReal-world time series often show multiple periodicities, like yearly and daily weather variations\\r\\nor weekly and daily traffic fluctuations. And these periods exhibit clear inclusion relationships, for\\r\\ninstance, yearly weather variations encompass multiple daily weather variations. Besides variations\\r\\nbetween different period levels, it also occur within periods of the same level. For example, daily\\r\\nweather variations differ based on conditions like sunny or cloudy. Due to these inclusion and\\r\\nadjacency relationships, different periods show similarities, with short and long periods being\\r\\nconsistent in overlapping portions, and periods of the same level being similar. Additionally, a long\\r\\nperiod can be decomposed into multiple short ones, forming a hierarchical pyramid structure. In our\\r\\nstudy, time series without explicit periodicity are treated as having infinitely long periods.\\r\\nBased on the above analysis, we decompose the time series into multiple periodic components,\\r\\nforming a pyramid structure where longer components encompass shorter ones, termed the Periodic\\r\\nPyramid as shown in Figure 1, which illustrates the intricate periodic inclusion relationships within\\r\\nthe time series. Each level consists of components with the same period, exhibiting the same phase,\\r\\nwhile different levels contain components with inclusion relationships. This transformation converts\\r\\nthe original 1D time series into a 2D representation, explicitly showing the implicit multi-period\\r\\nrelationships. Within the Periodic Pyramid, a shorter period may belong to two longer periods\\r\\nsimultaneously, reflecting the complexity of the time series. There is a clear similarity between\\r\\ncomponents within the same level and those in adjacent levels where inclusion relationships exist.\\r\\nThus inspired by Pyraformer [10], we propose the Periodic Pyramid Transformer (Peri-midFormer),\\r\\nwhich computes self-attention among periodic components to capture complex temporal variations in\\r\\ntime series. Furthermore, we consider each branch in the Periodic Pyramid as a Periodic Feature Flow,\\r\\nand aggregating features from multiple flows to provide rich periodic information for downstream\\r\\ntasks. In experiments, Peri-midFormer achieves state-of-the-art performance in various analytic tasks,\\r\\nincluding forecasting, imputation, anomaly detection, and classification.\\r\\n1. Based on the inclusion relationships of multiple periods in time series, this paper proposes a\\r\\ntop-down constructed Periodic Pyramid structure, which expands 1D time series variations\\r\\ninto 2D, explicitly representing the implicit multi-period relationships within the time series.\\r\\n2. We propose Peri-midFormer, which uses the Periodic Pyramid Attention Mechanism to\\r\\nautomatically capture dependencies between different and same-level periodic components,\\r\\nextracting diverse temporal variations in time series. Additionally, to further harness the\\r\\npotential of Peri-midFormer, we introduce Periodic Feature Flows to provide rich periodic\\r\\ninformation for downstream tasks.\\r\\n3. We conduct extensive experiments on five mainstream time series analysis tasks, and Peri\\x02midFormer achieves state-of-the-art across all of them, demonstrating its superior capability\\r\\nin time series analysis.\\r\\nThe remainder of this paper is structured as follows. Section 2 briefly summarizes the related\\r\\nwork. Section 3 details the proposed model structure. Section 4 extensively evaluates our method’s\\r\\nperformance across five main time series analysis tasks. Section 5 presents ablations analysis, Section\\r\\n6 presents complexity analysis, and Section 7 discusses our results and future directions.\\r\\n2\\n2 Related Work\\r\\nTemporal variation modeling, a crucial aspect of time series analysis, has been extensively investi\\x02gated. In recent years, numerous deep models have emerged for this purpose, including MLP [14; 15],\\r\\nTCN [8], and RNN [6; 7]-based architectures. Furthermore, Transformers have shown remarkable\\r\\nperformance in time series forecasting [16; 12; 17; 18]. They utilize attention mechanisms to uncover\\r\\ntemporal dependencies among time points. For instance, Wu et al. [12] introduce Autoformer with an\\r\\nAuto-Correlation mechanism, adept at capturing series-wise temporal dependencies derived from\\r\\nlearned periods. Moreover, to address complex temporal patterns, they adopted a deep decomposition\\r\\narchitecture to extract seasonal and trend parts from input series. Subsequently, FEDformer [17]\\r\\nenhances seasonal-trend decomposition through a mixture-of-expert design and introduces sparse\\r\\nattention within the frequency domain. Pyraformer [10] constructs a down-top pyramid structure\\r\\nthrough multiple convolution operations on time series to address the issue of long information prop\\x02agation paths in Transformers, significantly reducing both time and space complexity. PatchTST [19]\\r\\npartitions individual data points into patches and uses them as tokens for the Transformer, thereby\\r\\nenhancing its understanding of local information in time series. Additionally, PatchTST innovatively\\r\\nprocesses each channel separately, making it particularly suitable for forecasting tasks.\\r\\nAdditionally, there are some recent innovative works. Timesnet [13] unravels intricate temporal\\r\\npatterns by exploring the multi-periodicity of time series and captures temporal 2D-variations using\\r\\ncomputer vision CNN backbones. GPT4TS [20] ingeniously utilizes the large language model GPT2\\r\\nas a pretrained model, fine-tuning some of its structures with time series, achieving state-of-the-art\\r\\nresults. FITS [21] proposes a time series analysis model based on frequency domain operations,\\r\\nrequiring very low parameter count and memory consumption. And recent works have considered\\r\\nmulti-scale information in time series. PDF [22] captures both short-term and long-term variations\\r\\nby transforming 1D time series into 2D tensors using a multi-periodic decoupling block. It achieves\\r\\nsuperior forecasting performance by modeling these decoupled variations and integrating them for\\r\\naccurate predictions. SCNN [23] decomposes multivariate time series into long-term, seasonal, short\\x02term, co-evolving, and residual components, enhancing interpretability, adaptability to distribution\\r\\nshifts, and scalability by modeling each component separately. TimeMixer [24] uses a novel multiscale\\r\\nmixing approach, decomposing time series into fine and coarse scales to capture both detailed and\\r\\nmacroscopic variations. It employs Past-Decomposable-Mixing to extract historical information\\r\\nand Future-Multipredictor-Mixing to leverage multiscale forecasting capabilities, achieving great\\r\\nperformance in forecasting task.\\r\\n3 Methodology\\r\\n3.1 Model Structure\\r\\nThe overall flowchart of the proposed approach is shown in Figure 2, it begins with time embedding\\r\\nof the original time series at the top. Then, we use the FFT to decompose it into multiple periodic\\r\\ncomponents of varying lengths across different levels, with lines indicating the inclusion relationships\\r\\nbetween them. Moving down, padding and projection are then applied to ensure uniform dimensions,\\r\\nforming the Periodic Pyramid. Each component is treated as an independent token and receives\\r\\npositional embedding. Next, the Periodic Pyramid is fed into Peri-midFormer, which consists of\\r\\nmultiple layers for computing Periodic Pyramid Attention. Finally, depending on the task, two\\r\\nstrategies are employed: for classification, components are directly concatenated and projected\\r\\ninto the category space; for other reconstruction tasks (since forecasting, imputation, and anomaly\\r\\ndetection all necessitate the model to reconstruct the channel dimensions or input lengths, we\\r\\ncollectively refer to such tasks as reconstruction tasks), features from different pyramid branches\\r\\nare integrated through Periodic Feature Flows Aggregation to generate the final output. Please note\\r\\nthat we referred to [11] for de-normalization and [12] for time series decomposition to maximize the\\r\\neffectiveness of our method, but we omitted these details from the figure to maintain simplicity. See\\r\\nAppendix A for a complete flowchart. Further details are provided below.\\r\\n3.2 Periodic Pyramid Construction\\r\\nMultiple periods in the time series exhibit clear inclusion relationships, however, the 1D structure\\r\\nlimits the representation of variations between them. Hence, it’s crucial to separate periodic com\\x023\\nFFT for periods\\r\\n0\\r\\n3\\r\\n4 5 7 8\\r\\n1 2\\r\\n6\\r\\nPeri-midFormer\\r\\nTemporal embedding\\r\\nDownstream \\r\\ntasks\\r\\nPadding & Projection\\r\\nPositional embedding\\r\\nTime domain\\r\\nFrequency domain\\r\\nOriginal Length\\r\\nFrequency 1\\r\\nFrequency 2\\r\\nFrequency 3\\r\\n3 5 9\\r\\nTop k \\r\\nperiods\\r\\n0\\r\\n1 2 3\\r\\n4 5 6 7 8\\r\\nPPAM\\r\\nAdd\\r\\nFeed Forward\\r\\nAdd & Norm\\r\\nPeriodic Feature \\r\\nFlows Aggregation\\r\\nFlatten & \\r\\nProjection\\r\\nClassification \\r\\ntask\\r\\nReconstruction \\r\\ntasks\\r\\nNⅹ\\r\\nStrategy 1 for classification task\\r\\nStrategy 2 for reconstruction task\\r\\nPeriodic pyramid\\r\\n1\\r\\nf =1\\r\\n2\\r\\nf = 3\\r\\n3\\r\\nf = 5\\r\\nFigure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.\\r\\nponents with inclusion relationships to explicitly represent implicit periodic relationships. Firstly,\\r\\nas Peri-midFormer is designed to focus on periodic components, we first normalize the original\\r\\ntime series X ∈ R\\r\\nL×C that with length L and C channels, then decompose it to obtain the seasonal\\r\\npart Xs ∈ R\\r\\nL×C , thus removing the interference of the trend part. For a detailed description of\\r\\nnormalization and decomposition, please refer to the Appendix A. Then we partition Xs into periodic\\r\\ncomponents, following the approach used in Timesnet [13]. It’s important to note that, inspired by\\r\\nPatchTST [19], we retain the channel dimension C, as it is advantageous for Peri-midFormer in\\r\\ncapturing periodic features within each channel (note that we adopt a channel independent strategy\\r\\nand the Figure 2 shows the processing of only one of the channels). The periodic components are\\r\\nextracted in the frequency domain, accomplished specifically through FFT:\\r\\nA = Avg (Amp (F F T (Xs))), {f1, · · · , fk} = arg Topk\\r\\nf∗∈{1,··· ,⌈\\r\\nL\\r\\n2 ⌉}\\r\\n(A), pi =\\r\\nl\\r\\nL\\r\\nfi\\r\\nm\\r\\n, i ∈ {1, · · · , k}, (1)\\r\\nwhere F F T(·) and Amp(·) denote Fourier Transform and amplitude calculation, respectively. A ∈\\r\\nR\\r\\nL represents the amplitude of each frequency, averaged across C channels using Avg(·). Note that\\r\\nthe j-th value Aj denotes the intensity of the j-th frequency periodic basis function, associated with\\r\\nperiod length l\\r\\nL\\r\\nj\\r\\nm\\r\\n. To handle frequency domain sparsity and minimize noise from irrelevant high\\r\\nfrequencies [17], the top-k amplitude values {Af1, · · · , Afk} corresponding to the most significant\\r\\nfrequencies {f1, · · · , fk} are selected, where k is a hyper-parameter, beginning from 2 to ensure the\\r\\nfundamental pyramid structure. Additionally, to ensure the top level of the pyramid corresponds to\\r\\nthe original time series, we define f1 = 1, with other frequencies arranged in ascending order. These\\r\\nselected frequencies correspond to k period lengths {p1, · · · , pk}, arranged in descending order.\\r\\nDue to the frequency domain’s conjugacy, only frequencies within \\x081, · · · ,\\r\\n\\x06\\r\\nL\\r\\n2\\r\\n\\x07\\t are considered.\\r\\nBased on the selected frequencies {f1, · · · , fk} and their associated period lengths {p1, · · · , pk}, we\\r\\npartition the original time series into periodic components for each pyramid level, denoted as Cℓ:\\r\\nCℓ = {C1\\r\\nℓ\\r\\n, C2\\r\\nℓ\\r\\n, · · · , Cn\\r\\nℓ }, ℓ ∈ {1, · · · , k}, n ∈ {1, · · · , fk}, (2)\\r\\nwhere Cn\\r\\nℓ\\r\\ndenotes the n-th periodic component in the ℓ-th pyramid level. Here, ℓ is the pyramid level\\r\\nindex, starting from the top and increasing, with a maximum value of k, indicating the number of\\r\\nlevels determined by the selected periods. Similarly, n represents the component index within a level,\\r\\nincreasing from left to right, with a maximum value of fk, indicating the number of components per\\r\\nlevel is determined by the frequency corresponding to that period in the original series. The Periodic\\r\\nPyramid can thus be represented as:\\r\\nP = Stack (Cℓ), ℓ ∈ {1, · · · , k}, (3)\\r\\n4\\n0\\r\\n1 2 3\\r\\n4 5 6 7 8\\r\\nIndex = 0\\r\\n3 4\\r\\n2 3 Index Index ( ) ( ) 0 C C \\uf03e\\r\\n1 1\\r\\n2 3 Index Index ( ) ( ) 0 C C \\uf03e\\r\\n0\\r\\n3\\r\\n4 5 7 8\\r\\n1 2\\r\\n6\\r\\nIndex L = −1\\r\\nFigure 3: Inclusion relationships of periodic components (left) and Periodic Pyramid Attention Mechanism\\r\\n(right).\\r\\nwhere Stack(·) denotes a stacking operation. The constructed Periodic Pyramid, depicted in the\\r\\nupper right of Figure 2, displays evident inclusion relationship among different levels, shown by\\r\\nconnections between levels. Let R denote the relationship between pairs of periodic components\\r\\nfrom the upper and lower levels, determined by the presence or absence of overlap as follows:\\r\\nR\\r\\nnℓ−1,nℓ\\r\\nℓ−1,ℓ =\\r\\n\\x1a\\r\\n1, Index(C\\r\\nnℓ−1\\r\\nℓ−1\\r\\n)\\r\\nT\\r\\nIndex(C\\r\\nnℓ\\r\\nℓ\\r\\n) > 0\\r\\n0, else , ℓ ∈ {2, · · · , k}, nℓ−1, nℓ ∈ {1, · · · , fk}, (4)\\r\\nwhen R = 1, it signifies an inclusion relationship, while R = 0 indicates no overlap. This is\\r\\nillustrated in the left half of Figure 3. nℓ denotes the index of the n-th component in the ℓ-th level.\\r\\nIndex(·) denotes the positional index of each data point within the periodic component at that level.\\r\\nIndices for points in the first level are contained in {0, . . . , L − 1}. For subsequent levels, most\\r\\nindices match those of the first level. However, due to varying component lengths, there may be slight\\r\\ndifferences in indices for the last portion. Nonetheless, this doesn’t impact relationship determination\\r\\nbetween components across levels. In practice, the relationship between the components is realized\\r\\nby masking the corresponding elements in the attention matrix.\\r\\nThanks to the inclusion relationships between periodic components across different levels in the\\r\\nPeriodic Pyramid, complex periodic relationships inherent in 1D time series are explicitly represented.\\r\\nNext, due to the varying lengths of the components, it’s necessary to map Cℓto the same scale for\\r\\nsubsequent Periodic Pyramid Attention Mechanism, with the equation provided as follows:\\r\\nP\\r\\n′ = P rojection (P adding (Cn\\r\\nℓ\\r\\n)), ℓ ∈ {1, · · · , k}, n ∈ {1, · · · , fk}, (5)\\r\\nwhere P adding(·) denotes zero-padding the periodic components across the time dimension to match\\r\\nthe length of the original data, while P rojection(·) represents a single linear mapping layer.\\r\\n3.3 Periodic Pyramid Transformer (Peri-midFormer)\\r\\nOnce we have the Periodic Pyramid, it can be inputted into the Peri-midFormer, as depicted in\\r\\nFigure 2. The Peri-midFormer introduces a specialized attention mechanism tailored for the Periodic\\r\\nPyramid, called Periodic Pyramid Attention Mechanism (PPAM), shown in the right half of Figure\\r\\n3. Here, original connections are replaced with bidirectional arrows, and also added within the\\r\\nsame level. These bidirectional arrows signify attention between periodic components. In PPAM,\\r\\ninter-level attention focuses on period dependencies across levels, while intra-level attention focuses\\r\\non dependencies within the same level. Note that attention occurs among all components within the\\r\\nsame level, not just between adjacent ones. However, for clarity, not all attention connections within\\r\\nthe same level are depicted.\\r\\nIn Periodic Pyramid, a periodic component Cn\\r\\nℓ\\r\\ngenerally has three types of interconnected relation\\x02ships (denoted as I) with other components: the parent node in the level above (denoted as P), all\\r\\nnodes within the same level including itself (denoted as A), and the child nodes in its next level\\r\\n(denoted as C). Therefore, the relationships of Cn\\r\\nℓ\\r\\ncan be expressed by the following equation:\\r\\n\\uf8f1\\r\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\r\\nI\\r\\n(n)\\r\\nℓ = P\\r\\n(n)\\r\\nℓ\\r\\nS\\r\\nA\\r\\n(n)\\r\\nℓ\\r\\nS\\r\\nC\\r\\n(n)\\r\\nℓ\\r\\nP\\r\\n(n)\\r\\nℓ =\\r\\nn\\r\\nC\\r\\nj\\r\\nℓ−1\\r\\n: j = {nℓ−1 : R\\r\\nnℓ−1,nℓ\\r\\nℓ−1,ℓ = 1}\\r\\no\\r\\n, if ℓ ≥ 2 else ∅\\r\\nA\\r\\n(n)\\r\\nℓ =\\r\\nn\\r\\nC\\r\\nj\\r\\nℓ\\r\\n: 1 ≤ j ≤ fk\\r\\no\\r\\nC\\r\\n(n)\\r\\nℓ =\\r\\nn\\r\\nC\\r\\nj\\r\\nℓ+1 : j = {nℓ+1 : R\\r\\nnℓ,nℓ+1\\r\\nℓ,ℓ+1 = 1}\\r\\no\\r\\n, if ℓ ≤ k − 1 else ∅\\r\\n. (6)\\r\\n5\\n0\\r\\n3\\r\\n4 5 7 8\\r\\n1 2\\r\\n6\\r\\n1 4\\r\\n1 5\\r\\n2 5\\r\\n2 6\\r\\n2 7\\r\\n3 7\\r\\n3 8\\r\\nProjection\\r\\n0\\r\\n0\\r\\n0\\r\\n0\\r\\n0\\r\\n0\\r\\n0\\r\\nMean pooling\\r\\nFigure 4: Periodic Feature Flows Aggregation.\\r\\nThe equation shows that a component at the topmost level lacks a parent node, while one at the\\r\\nbottommost level lacks a child node. Based on the interconnected relationships I, the attention of the\\r\\ncomponent Cn\\r\\nℓ\\r\\ncan be expressed as:\\r\\nai =\\r\\nX\\r\\nm∈I\\r\\n(n)\\r\\nℓ\\r\\nexp qik\\r\\n⊤\\r\\nm/\\r\\n√\\r\\ndK\\r\\n\\x01\\r\\nvm P\\r\\nm∈I\\r\\n(n)\\r\\nℓ\\r\\nexp \\r\\nqik⊤\\r\\nm/\\r\\n√\\r\\ndK\\r\\n\\x01 , (7)\\r\\nwhere q, k, and v denote query, key, and value vectors, respectively, as in the classical self-attention\\r\\nmechanism. m used for selecting components that have interconnected relationships with Cn\\r\\nℓ\\r\\n. k\\r\\n⊤\\r\\nm\\r\\nrepresents the transpose of row m in K. dK refers to the dimension of key vectors, ensuring stable\\r\\nattention scores through scaling.\\r\\nWe apply this attention mechanism to each component across all levels of the Periodic Pyramid,\\r\\nenabling the automatic detection of dependencies among all components in the Periodic Pyramid and\\r\\ncapturing the intricate temporal variations in the time series. For a detailed theoretical proof of the\\r\\nPPAM see the Appendix F.\\r\\n3.4 Periodic Feature Flows Aggregation\\r\\nHere we explain the Periodic Feature Flows Aggregation used for reconstruction tasks. The output\\r\\nof the Peri-midFormer retains the original pyramid structure. To leverage the diverse periodic\\r\\ncomponents across different levels, we treat a single branch from the top to the bottom of the pyramid\\r\\nas a periodic feature flow, highlighted by the red line in Figure 4. Since a periodic feature flow passes\\r\\nthrough periodic components at different levels, it contains periodic features of different scales from\\r\\nthe time series. Additionally, due to variations among periodic components within each level, each\\r\\nfeature flow carries distinct information. Therefore, we aggregate multiple feature flows through\\r\\nPeriodic Feature Flow Aggregation. This involves linearly mapping each feature flow to match\\r\\nthe length of the target time series and then averaging it across multiple feature flows to obtain the\\r\\naggregated result Ys, as expressed in the following equation:\\r\\nYs = MeanP olling \\x10P rojection \\x10nCˆ n1\\r\\n1\\r\\n, Cˆ n2\\r\\n2\\r\\n, · · · , Cˆ nk\\r\\nℓ\\r\\no\\x11\\x11 , ℓ ∈ {2, · · · , k}, nk ∈ {1, · · · , fk}, (8)\\r\\nwhere Cˆ nk\\r\\nℓ\\r\\nrepresents a specific periodic component in the Peri-midFormer’s output, and\\r\\n{Cˆ n1\\r\\n1\\r\\n, Cˆ n2\\r\\n2\\r\\n, · · · , Cˆ nk\\r\\nℓ\\r\\n} forms a feature flow, as shown in Figure 4. P rojection(·) maps each feature\\r\\nflow to match the target output length. Meanpooling(·) averages the feature flows. Ys indicates\\r\\nthat this is the output from the seasonal part. Since we retained the channel dimension of the original\\r\\ntime series, the result obtained after aggregating the periodic feature flows here becomes the shape of\\r\\nthe final output. Finally, adding the trend part and de-normalization to obtain the ultimate output.\\r\\n4 Experiments\\r\\nWe extensively test Peri-midFormer on five mainstream analysis tasks: short- and long-term fore\\x02casting, imputation, classification, and anomaly detection. We adopted the same benchmarks as\\r\\nTimesnet [13], see Appendix C for details. Due to space limits, we provide only a summary of the\\r\\nresults here, more details about the datasets, experiment implementation, model configuration, and\\r\\nfull results can be found in Appendix.\\r\\n6\\nBaselines The baselines include CNN-based models: TimesNet [13]; MLP-based models:\\r\\nLightTS [15], DLinear [14] and FITS [21]; Transformer-based models: GPT4TS [20], Time\\x02LLM [25], iTransformer [26], TSLANet [27] , Reformer [28], Pyraformer [10], Informer [16],\\r\\nAutoformer [12], FEDformer [17], Non-stationary Transformer [11], ETSformer [29], PatchTST [19].\\r\\nBesides, N-HiTS [30] and N-BEATS [31] are used for short-term forecasting. Anomaly Trans\\x02former [32] is used for anomaly detection. Rocket [33], LSTNet [34], TCN [8] and Flowformer [35]\\r\\nare used for classification.\\r\\n4.1 Main Results\\r\\nLong-term Forecasting (MSE)\\r\\nClassification\\r\\n(Accuracy)\\r\\nAnomaly Detection\\r\\n(F1-Score)\\r\\nImputation(MSE)\\r\\nShort-term \\r\\nForecasting\\r\\n(OWA)\\r\\n0.3\\r\\n77\\r\\n87\\r\\n77\\r\\n0.045\\r\\n0.055\\r\\n0.85\\r\\n0.095 65\\r\\n0.4\\r\\nFigure 5: Model performance comparison.\\r\\nFigure 5 displays the comprehen\\x02sive comparison results between Peri\\x02midFormer and other methods, it con\\x02sistently excels across all five tasks.\\r\\n4.2 Long-term Forecasting\\r\\nSetups Referring to [13], we adopt\\r\\neight real-world benchmark datasets\\r\\nfor long-term forecasting, including\\r\\nWeather [36], Traffic [37], Electric\\x02ity [38], Exchange [34], and four\\r\\nETT [16] datasets (ETTh1, ETTh2,\\r\\nETTm1, ETTm2). Forecast lengths\\r\\nare set to 96, 192, 336, and 720. For\\r\\nthe fairness of the comparison, we set\\r\\nthe look-back window for all the meth\\x02ods to 512 (64 on Exchange), the re\\x02sults for other look-back windows can\\r\\nbe found in the Appendix H.3\\r\\nTable 1: Long-term forecasting task. The results are averaged from four different series length\\r\\n{96, 192, 336, 720}. See Table 13 and 14 for full results. Red: best, Blue: second best.\\r\\nModels Peri-mid GPT4TS TSLANet Time-LLM FITS DLinear PatchTST TimesNet Pyraformer\\r\\nFormer [20] [27] [25] [21] [14] [19] [13] [10]\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nWeather 0.233 0.271 0.237 0.270 0.276 0.291 0.225 0.257 0.244 0.281 0.241 0.294 0.226 0.266 0.249 0.286 0.281 0.349\\r\\nETTh1 0.409 0.430 0.427 0.426 0.422 0.440 0.408 0.423 0.407 0.422 0.418 0.438 0.430 0.444 0.492 0.490 0.913 0.748\\r\\nETTh2 0.317 0.377 0.354 0.394 0.328 0.385 0.334 0.383 0.333 0.382 0.504 0.482 0.388 0.414 0.408 0.440 3.740 1.554\\r\\nETTm1 0.354 0.385 0.352 0.383 0.348 0.383 0.329 0.372 0.358 0.378 0.356 0.379 0.363 0.391 0.398 0.418 0.724 0.609\\r\\nETTm2 0.258 0.320 0.266 0.326 0.263 0.325 0.251 0.313 0.254 0.313 0.275 0.342 0.273 0.329 0.287 0.343 1.356 0.797\\r\\nElectricity 0.152 0.249 0.167 0.263 0.159 0.224 0.158 0.252 0.168 0.263 0.167 0.268 0.162 0.257 0.217 0.314 0.299 0.391\\r\\nTraffic 0.392 0.270 0.414 0.294 0.397 0.272 0.388 0.264 0.420 0.287 0.433 0.305 0.392 0.270 0.622 0.332 0.705 0.401\\r\\nExchange 0.346 0.393 0.373 0.410 0.365 0.410 0.371 0.409 0.393 0.429 0.495 0.493 0.418 0.433 0.701 0.593 1.157 0.844\\r\\nAverage 0.308 0.337 0.324 0.346 0.320 0.341 0.308 0.334 0.322 0.344 0.361 0.375 0.331 0.350 0.422 0.402 1.147 0.711\\r\\nResults From Table 1, it is evident that Peri-midFormer performs exceptionally well, even completely\\r\\noutperforms GPT4TS and closely approaching the state-of-the-art method Time-LLM. While Time\\x02LLM demonstrates remarkable capabilities in long-term forecasting, our Peri-midFormer shows\\r\\nclear advantages on the ETTh2, Electricity, and Exchange datasets. Although Time-LLM achieves\\r\\nthe best results, it relies on a very large model, leading to significant computational overhead\\r\\nthat is unavoidable. The same issue exists for GPT4TS. In contrast, our Peri-midFormer achieves\\r\\nperformance close to that of Time-LLM without requiring excessive computational resources, making\\r\\nit more suitable for practical applications. Further analysis of model complexity is provided in Section\\r\\n6. In addition, our Peri-midFormer exhibits better performance with longer look-back window, as\\r\\nfurther detailed in the Appendix E.6.\\r\\n7\\nTable 2: Short-term forecasting task on M4. The prediction lengths are in {6, 48} and results are\\r\\nweighted averaged from several datasets under different sample intervals. (∗ means former, Station\\r\\nmeans the Non-stationary Transformer.) See Table 12 for full results. Red: best, Blue: second best.\\r\\nModels Peri-mid Time-LLM GPT4TS TimesNet PatchTST N-HiTS N-BEATS ETS∗ LightTS DLinear FED∗ Station Auto∗ Pyra∗ In∗ Re∗\\r\\nFormer [25] [20] [13] [19] [40] [31] [29] [41] [14] [17] [11] [12] [10] [16] [28]\\r\\nSMAPE 11.833 11.983 11.991 11.829 12.059 11.927 11.851 14.718 13.525 13.639 12.840 12.780 12.909 16.987 14.086 18.200\\r\\nMASE 1.584 1.595 1.600 1.585 1.623 1.613 1.599 2.408 2.111 2.095 1.701 1.756 1.771 3.265 2.718 4.223\\r\\nOWA 0.850 0.859 0.861 0.851 0.869 0.861 0.855 1.172 1.051 1.051 0.918 0.930 0.939 1.480 1.230 1.775\\r\\n4.3 Short-term Forecasting\\r\\nSetups For short-term analysis, we adopt the M4 [39], which contains the yearly, quarterly and\\r\\nmonthly collected univariate marketing data. We measure forecast performance using the symmetric\\r\\nmean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted\\r\\naverage (OWA), which are calculated as detailed in the Appendix D.1.\\r\\nResults Table 2 shows that Peri-midFormer outperforms Time-LLM, GPT4TS, TimesNet, and N\\x02BEATS, highlighting its exceptional performance in short-term forecasting. In the M4 dataset, some\\r\\ndata lacks clear periodicity, such as the Yearly data, which mainly exhibits a strong trend. A similar\\r\\nsituation is observed in the Exchange dataset for long-term forecasting task. Peri-midFormer performs\\r\\nwell on these datasets due to its time series decomposition strategy. For a detailed analysis, please\\r\\nrefer to the Appendix E.7.\\r\\n4.4 Time Series Classification\\r\\nFigure 6: Model comparison in classification. The\\r\\nresults are averaged from 10 subsets of UEA.\\r\\nSetups We assessed Peri-midFormer’s capac\\x02ity for high-level representation learning via\\r\\nclassification task. Mimicking settings akin\\r\\nto TimesNet [13], we tested it on 10 multi\\x02variate UEA classification datasets from [42],\\r\\ncovering tasks like gesture recognition, ac\\x02tion recognition, audio recognition, medical\\r\\ndiagnoses, and other real-world applications.\\r\\nResults As shown in Figure 6, Peri\\x02midFormer achieves an average accuracy of\\r\\n76.6%, surpassing all baselines including\\r\\nTSLANet (76.0%), GPT4TS (74.0%), Times\\x02Net (73.6%), and all other Transformer-based\\r\\nmethods. This suggests that Peri-midFormer\\r\\nhas excellent time series representation capa\\x02bilities. See Appendix H.1 for full results.\\r\\n4.5 Imputation\\r\\nSetups To validate Peri-midFormer’s imputation capabilities, we conduct experiment on six real\\x02world datasets, including four ETT datasets [16] (ETTh1, ETTh2, ETTm1, ETTm2), Electricity [38],\\r\\nand Weather [36]. We evaluate different random mask ratios (12.5%, 25%, 37.5%, 50%) for varying\\r\\nlevels of missing data. Notably, due to the large number of missing values, the time series do not\\r\\nreflect their original periodicity. Therefore, before imputation, we simply interpolate the original\\r\\nmissing data through a linear interpolation strategy in order to use Peri-midFormer efficiently, which\\r\\nwe call pre-interpolation. For a description of pre-interpolation and its impact on other methods,\\r\\nplease refer to the Appendix E.5.\\r\\nResults Table 3 demonstrates Peri-midFormer’s outstanding performance on specific datasets (Elec\\x02tricity and Weather), surpassing other methods significantly and securing the highest average scores.\\r\\nHowever, its performance on other datasets was ordinary, possibly due to the lack of obvious periodic\\r\\ncharacteristics in them.\\r\\n8\\nTable 3: Imputation task. We randomly mask {12.5%, 25%, 37.5%, 50%} time points of length-96\\r\\ntime series. The results are averaged from 4 different mask ratios. (∗ means former, Station means\\r\\nthe Non-stationary Transformer.) See Table 15 for full results. Red: best, Blue: second best.\\r\\nModels Peri-mid GPT4TS TimesNet PatchTST ETS∗ LightTS DLinear FED∗ Station Auto∗ Pyra∗\\r\\nFormer [20] [13] [19] [29] [15] [14] [17] [11] [12] [10]\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nETTm1 0.034 0.116 0.028 0.105 0.027 0.107 0.047 0.140 0.120 0.253 0.104 0.218 0.093 0.206 0.062 0.177 0.036 0.126 0.051 0.150 0.717 0.570\\r\\nETTm2 0.025 0.087 0.021 0.084 0.022 0.088 0.029 0.102 0.208 0.327 0.046 0.151 0.096 0.208 0.101 0.215 0.026 0.099 0.029 0.105 0.465 0.508\\r\\nETTh1 0.083 0.191 0.069 0.173 0.078 0.187 0.115 0.224 0.202 0.329 0.284 0.373 0.201 0.306 0.117 0.246 0.094 0.201 0.103 0.214 0.842 0.682\\r\\nETTh2 0.054 0.146 0.048 0.141 0.049 0.146 0.065 0.163 0.367 0.436 0.119 0.250 0.142 0.259 0.163 0.279 0.053 0.152 0.055 0.156 1.079 0.792\\r\\nElectricity 0.060 0.160 0.090 0.207 0.092 0.210 0.072 0.183 0.214 0.339 0.131 0.262 0.132 0.260 0.130 0.259 0.100 0.218 0.101 0.225 0.297 0.382\\r\\nWeather 0.028 0.039 0.031 0.056 0.030 0.054 0.060 0.144 0.076 0.171 0.055 0.117 0.052 0.110 0.099 0.203 0.032 0.059 0.031 0.057 0.152 0.235\\r\\nAverage 0.047 0.123 0.048 0.128 0.050 0.132 0.053 0.159 0.164 0.309 0.123 0.229 0.119 0.225 0.112 0.230 0.057 0.143 0.062 0.151 0.592 0.528\\r\\n4.6 Time Series Anomaly Detection\\r\\nSetups For anomaly detection, we assess models on five standard datasets: SMD [43], MSL [44],\\r\\nSMAP [44], SWaT [45] and PSM [46]. To ensure fairness, we exclusively use classical reconstruction\\r\\nerror for all baseline models, aligning with the approach in TimesNet [13]. Specifically, normal\\r\\ndata is used for training, and a simple reconstruction loss is employed to help the model learn the\\r\\ndistribution of normal data. In the testing phase, parts of the reconstructed output that exceed a certain\\r\\nthreshold are considered anomalies. We use a point adjustment technique combined with a manually\\r\\nset threshold for this purpose.\\r\\nTable 4: Anomaly detection task. We calculate the F1-score (as %) for each dataset. (∗ means\\r\\nformer, Station means the Non-stationary Transformer.) A higher value of F1-score indicates a better\\r\\nperformance. See Table 16 for full results. Red: best, Blue: second best.\\r\\nModels Peri-mid GPT4TS TimesNet PatchTST ETS∗ FED∗ LightTS DLinear Station Auto∗ Pyra∗ Ano∗ In∗ Re∗ LogTrans∗ Trans∗\\r\\nFormer [20] [13] [19] [29] [17] [15] [14] [11] [12] [10] [32] [16] [28] [47] [48]\\r\\nSMD 85.95 86.89 84.61 84.62 83.13 85.08 82.53 77.1 84.62 85.11 83.04 85.49 81.65 75.32 76.21 79.56\\r\\nMSL 81.83 82.45 81.84 78.7 85.03 78.57 78.95 84.88 77.5 79.05 84.86 83.31 84.06 84.40 79.57 78.68\\r\\nSMAP 68.62 72.88 69.39 68.82 69.50 70.76 69.21 69.26 71.09 71.12 71.09 71.18 69.92 70.40 69.97 69.70\\r\\nSWaT 93.40 94.23 93.02 85.72 84.91 93.19 93.33 87.52 79.88 92.74 91.78 83.10 81.43 82.80 80.52 80.37\\r\\nPSM 97.19 97.13 97.34 96.08 91.76 97.23 97.15 93.55 97.29 93.29 82.08 79.40 77.10 73.61 76.74 76.07\\r\\nAvg F1 85.40 86.72 85.24 82.79 82.87 84.97 84.23 82.46 82.08 84.26 82.57 80.50 78.83 77.31 76.60 76.88\\r\\nResults The results in Table 4 illustrate that Peri-midFormer’s anomaly detection capability is second\\r\\nonly to GPT4TS, but a large gap does exist. This is due to the binary event data nature in the\\r\\nanomaly detection dataset [21], which makes it difficult for Peri-midFormer to capture useful periodic\\r\\ncharacteristics, leading to general performance.\\r\\n5 Ablations\\r\\nTable 5: Ablation experiments on long-term forecasting task to verify the effect of each component.\\r\\nRed: best, Blue: second best.\\r\\nVariant\\r\\nDatasets ETTh1 ETTh2 Electricity Weather Traffic\\r\\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\r\\nPyraformer 0.913 0.748 0.826 0.703 0.299 0.391 0.281 0.349 0.705 0.401\\r\\nw/o period components 0.734 0.781 0.407 0.442 0.212 0.345 0.261 0.333 0.499 0.391\\r\\nw/o PPAM 0.581 0.599 0.344 0.393 0.164 0.259 0.254 0.291 0.415 0.364\\r\\nw/o Feature Flows Aggregation 0.433 0.447 0.341 0.391 0.155 0.250 0.244 0.280 0.397 0.277\\r\\nPeri-midFormer 0.409 0.430 0.317 0.377 0.152 0.249 0.233 0.271 0.391 0.269\\r\\nSetups We performed ablation experiments on key modules of Peri-midFormer for the long-term\\r\\nforecasting task, with results presented in Table 5. The table outlines the progression of module\\r\\nadditions, from top to bottom. \"Pyraformer\" refers to the Pyraformer [10], building the pyramid down\\x02top with convolutions and employing a simple two-fold relationship for attention distribution. \"w/o\\r\\nperiodic components\" constructs a pyramid top-down by dividing the time series into patches without\\r\\n9\\nClassification on the UEA Heartbeat\\r\\n0 1 2 3 4 5\\r\\nNumber of Parameters (Millions)\\r\\n70\\r\\n75\\r\\n80\\r\\n85\\r\\n90\\r\\n95\\r\\nAccuracy (%)\\r\\nPeri-midFormer\\r\\nGPT4TS\\r\\nTimesnet\\r\\nPatchTST\\r\\nPyraformer\\r\\nInformer\\r\\nCrossformer Autoformer\\r\\nFEDformer\\r\\nTSLANet\\r\\n1G FLOPs 10G FLOPs\\r\\nLong-term forecasting of length 720 on ETTh2\\r\\n0 10 20 30 40 50 60\\r\\nNumber of Parameters (Millions)\\r\\n0.30\\r\\n0.35\\r\\n0.40\\r\\n0.45\\r\\n0.50\\r\\n0.55\\r\\n0.60\\r\\nMSE\\r\\nPeri-midFormer\\r\\nTime-LLM \\r\\n(12750G FLOPs)\\r\\nGPT4TS\\r\\nTimesnet\\r\\nPatchTST\\r\\nDLinear\\r\\nAutoformer\\r\\nTSLANet\\r\\n10G FLOPs 100G FLOPs\\r\\nFigure 7: Number of training parameters and FLOPs for Peri-midFormer versus baseline in terms of\\r\\nclassification accuracy for the UEA Heartbeat dataset (left) and long-term forecasting MSE for the\\r\\nETTh2 dataset (right). In the left graph, the closer to the top left, the better, while in the right graph,\\r\\nthe closer to the bottom left, the better. Note that in the long-term forecasting, we did not fully depict\\r\\nthe corresponding sizes due to the oversized FLOPs of Time-LLM, but instead illustrated it with text.\\r\\nconsidering periodic components. \"w/o PPAM\" divides the time series into periodic components but\\r\\nwithout Period Pyramid Attention Mechanism, using periodic full attention instead, wherein attention\\r\\nis computed among all periodic components. \"w/o Feature Flows Aggregation\" employs PPAM but\\r\\nwithout Periodic Feature Flows Aggregation. \"Peri-midFormer\" indicates our final approach.\\r\\nResults Table 5 illustrates the incremental performance enhancement achieved with the integration\\r\\nof each additional module, validating the effectiveness of Peri-midFormer. Notably, good results\\r\\nare achieved even without PPAM. This can be attributed to the model’s ability to extract periodic\\r\\ncharacteristics inherent in the original time series data by delineating the periodic components.\\r\\nHowever, without highlighting inclusion relationships through PPAM, periodic full attention’s ability\\r\\nto capture temporal changes is limited, emphasizing the significance of PPAM.\\r\\n6 Complexity Analysis\\r\\nWe conducted experiments on the model complexity of Peri-midFormer using the Heartbeat dataset\\r\\nfor the classification task and the ETTh2 dataset for the long-term forecasting task. We considered\\r\\nthe number of training parameters, FLOPs, and accuracy (or MSE). The results are depicted in\\r\\nFigure 7. In the classification task, Peri-midFormer not only achieves a significant advantage in\\r\\naccuracy but also requires relatively fewer training parameters and FLOPs, much less than many\\r\\nmethods such as TimesNet, GPT4TS, Crossformer, and PatchTST. In the long-term forecasting task,\\r\\nPeri-midFormer achieves the lowest MSE without requiring the enormous FLOPs that Time-LLM\\r\\ndoes. This shows that although Time-LLM has strong long-term forecasting capabilities on most\\r\\ndatasets, its computational demands are unacceptable. See Appendix E.4 for more analysis.\\r\\nFurther Model Analysis is provided in the Appendix E.\\r\\n7 Conclusions\\r\\nIn this paper, we introduced a method for general time series analysis called Peri-midFormer. It\\r\\nleverages the multi-periodicity of time series and the inclusion relationships between different\\r\\nperiods. By segmenting the original time series into different levels of periodic components, Peri\\x02midFormer constructs a Periodic Pyramid along with its corresponding attention mechanism. Through\\r\\nextensive experiments covering forecasting, classification, imputation, and anomaly detection tasks,\\r\\nwe validated the capabilities of Peri-midFormer in time series analysis, achieving outstanding results\\r\\nacross all tasks. However, Peri-midFormer exhibits limitations, particularly in scenarios where the\\r\\nperiodic characteristics are less apparent. We aim to address this limitation in future research to\\r\\nbroaden its applicability.\\r\\n10\\nAcknowledgement\\r\\nThis work was supported by the National Natural Science Foundation of China under Grant Nos.\\r\\n62276205, U22B2018, and Graduate Student Innovation Fund under Grant Nos. YJSJ24012.\\r\\nReferences\\r\\n[1] Qingsong Wen, Linxiao Yang, Tian Zhou, and Liang Sun. Robust time series analysis and\\r\\napplications: An industrial perspective. In Proceedings of the 28th ACM SIGKDD Conference\\r\\non Knowledge Discovery and Data Mining, pages 4836–4837, 2022.\\r\\n[2] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate\\r\\nmedium-range global weather forecasting with 3d neural networks. Nature, 619(7970):533–538,\\r\\n2023.\\r\\n[3] Yaoran Chen, Candong Cai, Leilei Cao, Dan Zhang, Limin Kuang, Yan Peng, Huayan Pu,\\r\\nChuhan Wu, Dai Zhou, and Yong Cao. Windfix: Harnessing the power of self-supervised\\r\\nlearning for versatile imputation of offshore wind speed time series. Energy, 287:128995, 2024.\\r\\n[4] Haotian Si, Changhua Pei, Hang Cui, Jingwen Yang, Yongqian Sun, Shenglin Zhang, Jingjing\\r\\nLi, Haiming Zhang, Jing Han, Dan Pei, et al. Timeseriesbench: An industrial-grade benchmark\\r\\nfor time series anomaly detection models. arXiv preprint arXiv:2402.10802, 2024.\\r\\n[5] Zhen Liu, Wenbin Pei, Disen Lan, and Qianli Ma. Diffusion language-shapelets for semi\\x02supervised time-series classification. In Proceedings of the AAAI Conference on Artificial\\r\\nIntelligence, volume 38, pages 14079–14087, 2024.\\r\\n[6] Hongming Li, Shujian Yu, and Jose Principe. Causal recurrent variational autoencoder for\\r\\nmedical time series generation. In Proceedings of the AAAI Conference on Artificial Intelligence,\\r\\nvolume 37, pages 8562–8570, 2023.\\r\\n[7] Minrong Lu and Xuerong Xu. Trnn: An efficient time-series recurrent neural network for stock\\r\\nprice prediction. Information Sciences, 657:119951, 2024.\\r\\n[8] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable represen\\x02tation learning for multivariate time series. Advances in Neural Information Processing Systems,\\r\\n32, 2019.\\r\\n[9] Yangdong He and Jiabao Zhao. Temporal convolutional networks for anomaly detection in time\\r\\nseries. J. Phys. Conf. Ser, 2019.\\r\\n[10] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.\\r\\nPyraformer: Low-complexity pyramidal attention for long-range time series modeling and\\r\\nforecasting. In International Conference on Learning Representations, 2021.\\r\\n[11] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers:\\r\\nExploring the stationarity in time series forecasting. In Advances in Neural Information\\r\\nProcessing Systems, 2022.\\r\\n[12] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition\\r\\ntransformers with auto-correlation for long-term series forecasting. In Advances in Neural\\r\\nInformation Processing Systems, pages 101–112, 2021.\\r\\n[13] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:\\r\\nTemporal 2d-variation modeling for general time series analysis. In The Eleventh International\\r\\nConference on Learning Representations, 2023.\\r\\n[14] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\\r\\nforecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages\\r\\n11121–11128, 2023.\\r\\n[15] Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian\\r\\nLi. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp\\r\\nstructures. arXiv preprint arXiv:2207.01186, 2022.\\r\\n11\\n[16] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\\r\\nZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In\\r\\n35th AAAI Conference on Artificial Intelligence, pages 11106–11115, 2021.\\r\\n[17] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer:\\r\\nFrequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th\\r\\nInternational Conference on Machine Learning, 2022.\\r\\n[18] Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, and Weiyao Lin. Basisformer: Attention-based\\r\\ntime series forecasting with learnable and interpretable basis. Advances in Neural Information\\r\\nProcessing Systems, 36, 2024.\\r\\n[19] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth\\r\\n64 words: Long-term forecasting with transformers. In The Eleventh International Conference\\r\\non Learning Representations, 2022.\\r\\n[20] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series\\r\\nanalysis by pretrained lm. Advances in Neural Information Processing Systems, 36, 2024.\\r\\n[21] Zhijian Xu, Ailing Zeng, and Qiang Xu. Fits: Modeling time series with 10k parameters. In\\r\\nThe Twelfth International Conference on Learning Representations, 2023.\\r\\n[22] Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong Jiang, and Shu-Tao Xia.\\r\\nPeriodicity decoupling framework for long-term series forecasting. In The Twelfth International\\r\\nConference on Learning Representations, 2024.\\r\\n[23] Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor W Tsang.\\r\\nDisentangling structured components: Towards adaptive, interpretable and scalable time series\\r\\nforecasting. IEEE Transactions on Knowledge and Data Engineering, 2024.\\r\\n[24] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang,\\r\\nand JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In\\r\\nThe Twelfth International Conference on Learning Representations, 2024.\\r\\n[25] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu\\r\\nChen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series\\r\\nforecasting by reprogramming large language models. In The Twelfth International Conference\\r\\non Learning Representations, 2024.\\r\\n[26] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng\\r\\nLong. itransformer: Inverted transformers are effective for time series forecasting. In The\\r\\nTwelfth International Conference on Learning Representations, 2023.\\r\\n[27] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, and Xiaoli Li. Tslanet: Re\\x02thinking transformers for time series representation learning. arXiv preprint arXiv:2404.08472,\\r\\n2024.\\r\\n[28] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In\\r\\nInternational Conference on Learning Representations, 2020.\\r\\n[29] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Expo\\x02nential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381,\\r\\n2022.\\r\\n[30] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler\\r\\nCanseco, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series\\r\\nforecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages\\r\\n6989–6997, 2023.\\r\\n[31] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural\\r\\nbasis expansion analysis for interpretable time series forecasting. In International Conference\\r\\non Learning Representations, 2019.\\r\\n12\\n[32] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series\\r\\nanomaly detection with association discrepancy. In International Conference on Learning\\r\\nRepresentations, 2021.\\r\\n[33] Angus Dempster, François Petitjean, and Geoffrey I Webb. ROCKET: Exceptionally fast\\r\\nand accurate time series classification using random convolutional kernels. Data Mining and\\r\\nKnowledge Discovery, 34(5):1454–1495, 2020.\\r\\n[34] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term\\r\\ntemporal patterns with deep neural networks. In The 41st international ACM SIGIR conference\\r\\non research & development in information retrieval, pages 95–104, 2018.\\r\\n[35] Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Lineariz\\x02ing transformers with conservation flows. In International Conference on Machine Learning,\\r\\npages 24226–24242. PMLR, 2022.\\r\\n[36] Wetterstation. Weather. https://www.bgc-jena.mpg.de/wetter/.\\r\\n[37] PeMS. Traffic. http://pems.dot.ca.gov/.\\r\\n[38] UCI. Electricity. https://archive.ics.uci.edu/ml/datasets/\\r\\nElectricityLoadDiagrams20112014.\\r\\n[39] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition:\\r\\nResults, findings, conclusion and way forward. International Journal of Forecasting, 34(4):802–\\r\\n808, 2018.\\r\\n[40] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and\\r\\nArtur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. arXiv\\r\\npreprint arXiv:2201.12886, 2022.\\r\\n[41] T. Zhang, Yizhuo Zhang, Wei Cao, J. Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more:\\r\\nFast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv\\r\\npreprint arXiv:2207.01186, 2022.\\r\\n[42] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom,\\r\\nPaul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018.\\r\\narXiv preprint arXiv:1811.00075, 2018.\\r\\n[43] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection\\r\\nfor multivariate time series through stochastic recurrent neural network. In Proceedings of the\\r\\n25th ACM SIGKDD international conference on knowledge discovery & data mining, pages\\r\\n2828–2837, 2019.\\r\\n[44] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Söder\\x02ström. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding.\\r\\nProceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &\\r\\nData Mining, pages 387–395, 2018.\\r\\n[45] Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research and\\r\\ntraining on ics security. In 2016 international workshop on cyber-physical systems for smart\\r\\nwater networks (CySWater), pages 31–36. IEEE, 2016.\\r\\n[46] Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronous\\r\\nmultivariate time series anomaly detection and localization. In Proceedings of the 27th ACM\\r\\nSIGKDD conference on knowledge discovery & data mining, pages 2485–2494, 2021.\\r\\n[47] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng\\r\\nYan. Enhancing the locality and breaking the memory bottleneck of transformer on time series\\r\\nforecasting. In NeurIPS, 2019.\\r\\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\r\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information\\r\\nProcessing Systems, 30, 2017.\\r\\n[49] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,\\r\\n2015.\\r\\n13\\nA Method Details\\r\\nFFT for periods\\r\\nTop k periods\\r\\n0\\r\\n3\\r\\n4 5 7 8\\r\\n1 2\\r\\n6\\r\\n0\\r\\n1 2 3\\r\\n4 5 6 7 8\\r\\nPeriodic pyramid\\r\\nPeri-midFormer\\r\\nTemporal \\r\\nembedding\\r\\nDownstream \\r\\ntasks\\r\\nPadding & Projection\\r\\nProjection\\r\\nNormalization\\r\\nDe-normalization\\r\\nDecomposition\\r\\nSeasonal part Trend part\\r\\nStrategy 1\\r\\nStrategy 2\\r\\nStrategy 1 Input Strategy 2\\r\\nPositional embedding\\r\\n1\\r\\nf =1\\r\\n2\\r\\nf = 3\\r\\n3\\r\\nf = 5\\r\\nˆ\\r\\nclasses Y\\uf0ce\\r\\nˆ\\r\\nT C\\uf0b4 Y\\uf0ce\\r\\nT C\\r\\nt\\r\\n\\uf0b4 L C H \\uf0ce t\\r\\n\\uf0b4 X \\uf0ce\\r\\nL C\\r\\ns\\r\\n\\uf0b4 X \\uf0ce\\r\\nL C\\uf0b4 X\\uf0ce \\uf06d \\uf073, X X\\r\\nT C\\r\\ns\\r\\n\\uf0b4 Y \\uf0ce\\r\\nFigure 8: Full flowchart.\\r\\nHere, we provide further elaboration on the details of Peri-midFormer. Its full flowchart is depicted in\\r\\nFigure 8, depicts two strategies for input. The strategy indicated by the red line is for classification task,\\r\\nwhere de-normalization and time series decomposition are not used. This is because, in classification\\r\\ntask, there is no need to reconstruct the original data; therefore, the trend part does not need to be\\r\\nextracted and added back. Additionally, it is important to note that the trend part is a significant\\r\\ndiscriminative feature for classification data, so it cannot be separated from the original data before\\r\\nfeature extraction. In the strategy indicated by the blue line, employed for reconstruction tasks, Peri\\x02midFormer needs to focus on the periodicity in the time series. Therefore, we utilize de-normalization\\r\\nand time series decomposition to eliminate other influencing factors. To achieve this, we first refer to\\r\\n[11] to address instability factors. We normalize the input X = [x1, x2, ..., xL] ∈ R\\r\\nL×C to obtain\\r\\nXnorm = [x\\r\\n′\\r\\n1\\r\\n, x′\\r\\n2\\r\\n, ..., x′\\r\\nL\\r\\n] ∈ R\\r\\nL×C :\\r\\nµx =\\r\\n1\\r\\nL\\r\\nX\\r\\nL\\r\\ni=1\\r\\nxi, σ2\\r\\nx =\\r\\n1\\r\\nL\\r\\nX\\r\\nL\\r\\ni=1\\r\\n(xi − µx)\\r\\n2\\r\\n, x′\\r\\ni =\\r\\n1\\r\\nσx\\r\\n⊙ (xi − µx), (9)\\r\\nwhere µx, σx ∈ R\\r\\nC×1\\r\\nare the mean and variance, respectively, 1\\r\\nσx\\r\\nmeans the element-wise division\\r\\nand ⊙ is the element-wise product. Normalization reduces the disparity in distribution among\\r\\nindividual input time series, thereby stabilizing the model input distribution.\\r\\nThen, to remove the trend part from the time series and only preserve the seasonal part for Peri\\x02midFormer, we refer to [12] for time series decomposition, as shown in the following equation:\\r\\nXt = AvgP ool(P adding(Xnorm)),\\r\\nXs = Xnorm − Xt,\\r\\n(10)\\r\\nwhere Xs, Xt ∈ R\\r\\nL×C denote the seasonal and the trend part respectively. We adopt the AvgP ool(·)\\r\\nfor moving average with the padding operation to keep the series length unchanged.\\r\\nThe seasonal part Xs obtained after decomposition can be directly input into Peri-midFormer. After\\r\\nthe output from Peri-midFormer, we add the trend part back, then de-normalize it to obtain the final\\r\\noutput Yˆ = [ ˆy1, yˆ2, ..., yˆT ]:\\r\\nYs = H(Xs), Y = Ys + P rojection(Xt), yˆi = σx ⊙ yi + µx, (11)\\r\\nwhere H represents the Peri-midFormer model, Ys represents the output of Peri-midFormer,\\r\\nP rojection(·) represents mapping the trend part to the target output length, and yˆi = σx ⊙ yi + µx\\r\\ndenotes de-normalization.\\r\\nB Visualization\\r\\nTo provide a clearer demonstration of Peri-midFormer’s representational capabilities, Figure 9, 10\\r\\nand 11 visualize some of the results for imputation, long-term forecasting and short-term forecasting,\\r\\nrespectively. It illustrates that Peri-midFormer outperforms other methods in capturing periodic\\r\\nvariations in time series.\\r\\n14\\n(1) Imputation on Weather dataset under 50% mask ratio\\r\\n0 20 40 60 80\\r\\n0.00\\r\\n0.01\\r\\n0.02\\r\\n0.03\\r\\n0.04\\r\\n0.05\\r\\n0.06\\r\\n0.07\\r\\n0.08\\r\\nPrediction\\r\\nGroundTruth\\r\\nPeri-midFormer (ours)\\r\\n0 20 40 60 80\\r\\n0.00\\r\\n0.01\\r\\n0.02\\r\\n0.03\\r\\n0.04\\r\\n0.05\\r\\n0.06\\r\\n0.07\\r\\n0.08\\r\\nPrediction\\r\\nGroundTruth\\r\\nGPT4TS\\r\\n0 20 40 60 80\\r\\n0.00\\r\\n0.01\\r\\n0.02\\r\\n0.03\\r\\n0.04\\r\\n0.05\\r\\n0.06\\r\\n0.07\\r\\n0.08\\r\\nPrediction\\r\\nGroundTruth\\r\\nPatchTST\\r\\n0 20 40 60 80\\r\\n0.00\\r\\n0.01\\r\\n0.02\\r\\n0.03\\r\\n0.04\\r\\n0.05\\r\\n0.06\\r\\n0.07\\r\\n0.08\\r\\nPrediction\\r\\nGroundTruth\\r\\nTimesNet\\r\\n0 20 40 60 80\\r\\n0.04\\r\\n0.02\\r\\n0.00\\r\\n0.02\\r\\n0.04\\r\\n0.06\\r\\n0.08\\r\\n0.10 Prediction\\r\\nGroundTruth\\r\\nFEDformer\\r\\n0 20 40 60 80\\r\\n0.00\\r\\n0.02\\r\\n0.04\\r\\n0.06\\r\\n0.08\\r\\nPrediction\\r\\nGroundTruth\\r\\nDlinear\\r\\n(2) Imputation on Electricity dataset under 50% mask ratio\\r\\n0 20 40 60 80\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\n2.5\\r\\n3.0\\r\\n3.5 Prediction\\r\\nGroundTruth\\r\\nPeri-midFormer (ours)\\r\\n0 20 40 60 80\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\n2.5\\r\\n3.0\\r\\n3.5 Prediction\\r\\nGroundTruth\\r\\nGPT4TS\\r\\n0 20 40 60 80\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\n2.5\\r\\n3.0\\r\\n3.5 Prediction\\r\\nGroundTruth\\r\\nPatchTST\\r\\n0 20 40 60 80\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nPrediction\\r\\nGroundTruth\\r\\nTimesNet\\r\\n0 20 40 60 80\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\nPrediction\\r\\nGroundTruth\\r\\nFEDformer\\r\\n0 20 40 60 80\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\n2.5\\r\\n3.0\\r\\n3.5 Prediction\\r\\nGroundTruth\\r\\nDlinear\\r\\nFigure 9: Visualization of imputation.\\r\\n15\\n(1) Long-term forecasting with 96 prediction length on ETTh2\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0 GroundTruth\\r\\nPrediction\\r\\nPeri-midFormer (ours)\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0 GroundTruth\\r\\nPrediction\\r\\nGPT4TS\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0 GroundTruth\\r\\nPrediction\\r\\nTSLANet\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0 GroundTruth\\r\\nPrediction\\r\\nPatchTST\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0 GroundTruth\\r\\nPrediction\\r\\nTimesNet\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.6\\r\\n1.4\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0 GroundTruth\\r\\nPrediction\\r\\nDlinear\\r\\n(2) Long-term forecasting with 96 prediction length on Electricity\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\nGroundTruth\\r\\nPrediction\\r\\nPeri-midFormer (ours)\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\nGroundTruth\\r\\nPrediction\\r\\nGPT4TS\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\nGroundTruth\\r\\nPrediction\\r\\nTSLANet\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\nGroundTruth\\r\\nPrediction\\r\\nPatchTST\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\nGroundTruth\\r\\nPrediction\\r\\nTimesNet\\r\\n0 25 50 75 100 125 150 175 200\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\nGroundTruth\\r\\nPrediction\\r\\nDlinear\\r\\nFigure 10: Visualization of long-term forecasting.\\r\\n16\\n(1) Short-term forecasting on M4 Weekly\\r\\n0 5 10 15 20 25 30 35\\r\\n2400\\r\\n2450\\r\\n2500\\r\\n2550\\r\\n2600\\r\\n2650\\r\\nGroundTruth\\r\\nPrediction\\r\\nPeri-midFormer (ours)\\r\\n0 5 10 15 20 25 30 35\\r\\n2400\\r\\n2450\\r\\n2500\\r\\n2550\\r\\n2600\\r\\n2650\\r\\nGroundTruth\\r\\nPrediction\\r\\nGPT4TS\\r\\n0 5 10 15 20 25 30 35\\r\\n2400\\r\\n2450\\r\\n2500\\r\\n2550\\r\\n2600\\r\\n2650\\r\\nGroundTruth\\r\\nPrediction\\r\\nPatchTST\\r\\n0 5 10 15 20 25 30 35\\r\\n2400\\r\\n2450\\r\\n2500\\r\\n2550\\r\\n2600\\r\\n2650\\r\\nGroundTruth\\r\\nPrediction\\r\\nTimesNet\\r\\n0 5 10 15 20 25 30 35\\r\\n2400\\r\\n2450\\r\\n2500\\r\\n2550\\r\\n2600\\r\\n2650\\r\\nGroundTruth\\r\\nPrediction\\r\\nFDEformer\\r\\n0 5 10 15 20 25 30 35\\r\\n2400\\r\\n2450\\r\\n2500\\r\\n2550\\r\\n2600\\r\\n2650\\r\\nGroundTruth\\r\\nPrediction\\r\\nDlinear\\r\\n(2) Short-term forecasting on M4 Monthly\\r\\n0 10 20 30 40 50\\r\\n4000\\r\\n6000\\r\\n8000\\r\\n10000\\r\\n12000\\r\\n14000\\r\\nGroundTruth\\r\\nPrediction\\r\\nPeri-midFormer (ours)\\r\\n0 10 20 30 40 50\\r\\n4000\\r\\n6000\\r\\n8000\\r\\n10000\\r\\n12000\\r\\n14000\\r\\nGroundTruth\\r\\nPrediction\\r\\nGPT4TS\\r\\n0 10 20 30 40 50\\r\\n4000\\r\\n6000\\r\\n8000\\r\\n10000\\r\\n12000\\r\\n14000\\r\\nGroundTruth\\r\\nPrediction\\r\\nPatchTST\\r\\n0 10 20 30 40 50\\r\\n4000\\r\\n6000\\r\\n8000\\r\\n10000\\r\\n12000\\r\\n14000\\r\\nGroundTruth\\r\\nPrediction\\r\\nTimesNet\\r\\n0 10 20 30 40 50\\r\\n4000\\r\\n6000\\r\\n8000\\r\\n10000\\r\\n12000\\r\\n14000\\r\\nGroundTruth\\r\\nPrediction\\r\\nFEDformer\\r\\n0 10 20 30 40 50\\r\\n4000\\r\\n6000\\r\\n8000\\r\\n10000\\r\\n12000\\r\\n14000\\r\\nGroundTruth\\r\\nPrediction\\r\\nDlinear\\r\\nFigure 11: Visualization of short-term forecasting.\\r\\n17\\nC Dataset Details\\r\\nA detailed description of the dataset is given in Table 6.\\r\\nTable 6: Dataset descriptions. The dataset size is organized in (Train, Validation, Test).\\r\\nTasks Dataset Dim Series Length Dataset Size Information (Frequency)\\r\\nETTm1, ETTm2 7 {96, 192, 336, 720} (34465, 11521, 11521) Electricity (15 mins)\\r\\nETTh1, ETTh2 7 {96, 192, 336, 720} (8545, 2881, 2881) Electricity (15 mins)\\r\\nForecasting Electricity 321 {96, 192, 336, 720} (18317, 2633, 5261) Electricity (Hourly)\\r\\n(Long-term) Traffic 862 {96, 192, 336, 720} (12185, 1757, 3509) Transportation (Hourly)\\r\\nWeather 21 {96, 192, 336, 720} (36792, 5271, 10540) Weather (10 mins)\\r\\nExchange 8 {96, 192, 336, 720} (5120, 665, 1422) Exchange rate (Daily)\\r\\nM4-Yearly 1 6 (23000, 0, 23000) Demographic\\r\\nM4-Quarterly 1 8 (24000, 0, 24000) Finance\\r\\nForecasting M4-Monthly 1 18 (48000, 0, 48000) Industry\\r\\n(short-term) M4-Weakly 1 13 (359, 0, 359) Macro\\r\\nM4-Daily 1 14 (4227, 0, 4227) Micro\\r\\nM4-Hourly 1 48 (414, 0, 414) Other\\r\\nImputation\\r\\nETTm1, ETTm2 7 96 (34465, 11521, 11521) Electricity (15 mins)\\r\\nETTh1, ETTh2 7 96 (8545, 2881, 2881) Electricity (15 mins)\\r\\nElectricity 321 96 (18317, 2633, 5261) Electricity (15 mins)\\r\\nWeather 21 96 (36792, 5271, 10540) Weather (10 mins)\\r\\nEthanolConcentration 3 1751 (261, 0, 263) Alcohol Industry\\r\\nFaceDetection 144 62 (5890, 0, 3524) Face (250Hz)\\r\\nHandwriting 3 152 (150, 0, 850) Handwriting\\r\\nHeartbeat 61 405 (204, 0, 205) Heart Beat\\r\\nClassification JapaneseVowels 12 29 (270, 0, 370) Voice\\r\\n(UEA) PEMS-SF 963 144 (267, 0, 173) Transportation (Daily)\\r\\nSelfRegulationSCP1 6 896 (268, 0, 293) Health (256Hz)\\r\\nSelfRegulationSCP2 7 1152 (200, 0, 180) Health (256Hz)\\r\\nSpokenArabicDigits 13 93 (6599, 0, 2199) Voice (11025Hz)\\r\\nUWaveGestureLibrary 3 315 (120, 0, 320) Gesture\\r\\nSMD 38 100 (566724, 141681, 708420) Server Machine\\r\\nAnomaly MSL 55 100 (44653, 11664, 73729) Spacecraft\\r\\nDetection SMAP 25 100 (108146, 27037, 427617) Spacecraft\\r\\nSWaT 51 100 (396000, 99000, 449919) Infrastructure\\r\\nPSM 25 100 (105984, 26497, 87841) Server Machine\\r\\nSince our datasets setup is the same as Timesnet [13], an excerpt from it describes the datasets.\\r\\nD Experimental Details\\r\\nAll the deep learning networks are implemented in PyTorch and trained on NVIDIA 4090 24GB\\r\\nGPU. We repeated each experiment three times to eliminate randomness. The detailed experiment\\r\\nconfiguration is shown in Table 7.\\r\\nD.1 Metrics\\r\\nWe utilize various metrics to evaluate different tasks. For long-term forecasting and imputations,\\r\\nwe employ the mean square error (MSE) and mean absolute error (MAE). In anomaly detection,\\r\\nwe utilize the F1-score, which combines precision and recall. For short-term forecasting, we utilize\\r\\nthe symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and\\r\\noverall weighted average (OWA), with OWA being a unique metric used in the M4 competition.\\r\\n18\\nTable 7: Experiment configuration of Peri-midFormer. All the experiments use the ADAM [49]\\r\\noptimizer with the default hyperparameter configuration for (β1, β2) as (0.9, 0.999).\\r\\nTasks / Configurations Model Hyper-parameter Training Process\\r\\nk Layers dmodel LR∗ Loss Batch Size Epochs\\r\\nLong-term Forecasting 2-5 1-3 128-768 10−4- 5 × 10−4 MSE 2-32 15\\r\\nShort-term Forecasting 2-3 1-2 256 10−4SMAPE 2 15\\r\\nImputation 2-5 1-2 64-768 10−4- 5 × 10−4 MSE 2-4 15\\r\\nClassification 2-8 1-4 16-128 10−4- 5 × 10−3 Cross Entropy 2-64 20\\r\\nAnomaly Detection 2-5 1-4 8-32 10−4- 5 × 10−3 MSE 2-32 3\\r\\n∗ LR means the initial learning rate.\\r\\nThese metrics are computed as follows:\\r\\nSMAPE =\\r\\n200\\r\\nT\\r\\nX\\r\\nT\\r\\ni=1\\r\\n|Xi − Yb i|\\r\\n|Xi| + |Yb i|\\r\\n,\\r\\nMAPE =\\r\\n100\\r\\nT\\r\\nX\\r\\nT\\r\\ni=1\\r\\n|Xi − Yb i|\\r\\n|Xi|\\r\\n,\\r\\nMASE =\\r\\n1\\r\\nT\\r\\nX\\r\\nT\\r\\ni=1\\r\\n|Xi − Yb i|\\r\\n1\\r\\nT −q\\r\\nPT\\r\\nj=q+1 |Xj − Xj−q|\\r\\n,\\r\\nOWA =\\r\\n1\\r\\n2\\r\\n\\x14\\r\\nSMAPE\\r\\nSMAPENaïve2\\r\\n+\\r\\nMASE\\r\\nMASENaïve2 \\x15\\r\\n,\\r\\n(12)\\r\\nwhere q is the periodicity of the data. X, Yb ∈ R\\r\\nT ×C are the ground truth and prediction result of the\\r\\nfuture with T time pints and C dimensions. Xi means the i-th future time point.\\r\\nE Model Analysis\\r\\nE.1 Hyper Parameter Analysis and Model Limitations\\r\\nIn Equation (1), we introduced a hyperparameter k to select the most important frequency, which\\r\\nalso determines the number of levels in the Periodic Pyramid. We conducted sensitivity analysis on\\r\\nit, as shown in Figure 12. It’s evident that our proposed Peri-midFormer exhibits relatively stable\\r\\nperformance across different choices of k for all four tasks. However, there are still some fluctuation\\r\\nin results among different k values depending on the task and dataset, which are determined by the\\r\\nperiodic characteristics in the dataset. To illustrate this, we visualize individual data for long-term\\r\\nforecasting and classification tasks, as shown in Figure 13. It can be observed that in the long-term\\r\\nforecasting task, the Etth1 dataset exhibits clear periodicity. Therefore, with larger k, Peri-midFormer\\r\\ncan capture more periodic information, resulting in better performance. In contrast, the Etth2\\r\\ndataset has less obvious periodicity, so it achieves better results with smaller k, as larger k introduce\\r\\nunnecessary noise, affecting model performance. In the classification task, the EthanolConcentration\\r\\ndataset is difficult to classify, whereas a larger k allows for the construction of Periodic Pyramid with\\r\\nmore levels, thus extracting more representative features and achieving higher accuracy. In addition,\\r\\nthe SelfRegulationSCP1 dataset contains a lot of high-frequency noise, so larger k would focus on\\r\\nirrelevant information, leading to decreased accuracy. Therefore, we adjusted k values differently for\\r\\ndifferent tasks and datasets, as shown in the range in Table 7.\\r\\nThe above analysis reveals that the limitation of Peri-midFormer lies in its inability to fully leverage\\r\\nits advantages on datasets with poor periodicity characteristics. We plan to address this issue in our\\r\\nfuture work.\\r\\n19\\nImputation of 50% masked\\r\\n2 3 4 5 6\\r\\nk value\\r\\n0.030\\r\\n0.035\\r\\n0.040\\r\\n0.045\\r\\n0.050\\r\\n0.055\\r\\n0.060\\r\\n0.065\\r\\nMSE\\r\\nETTh2\\r\\nETTm1\\r\\nWeather\\r\\nLong-term Forecasting of 96 prediction length\\r\\n2 3 4 5 6\\r\\nk value\\r\\n0.15\\r\\n0.20\\r\\n0.25\\r\\n0.30\\r\\n0.35\\r\\n0.40\\r\\nMSE\\r\\nETTh1\\r\\nETTh2\\r\\nETTm1\\r\\nETTm2\\r\\nWeather\\r\\nAnomaly Detection\\r\\n2 3 4 5 6\\r\\nk value\\r\\n67.5\\r\\n70.0\\r\\n72.5\\r\\n75.0\\r\\n77.5\\r\\n80.0\\r\\n82.5\\r\\n85.0\\r\\nF1-Score\\r\\nMSL\\r\\nSMD\\r\\nSMAP\\r\\nClassification\\r\\n2 3 4 5 6 7 8\\r\\nk value\\r\\n30\\r\\n40\\r\\n50\\r\\n60\\r\\n70\\r\\n80\\r\\n90\\r\\n100\\r\\nAccuracy(%)\\r\\nEthanolConcentration\\r\\nSelfRegulationSCP1\\r\\nSpokenArabicDigits\\r\\nUWaveGestureLibrary\\r\\nFigure 12: Sensitivity analysis of hyper-parameters k in each task.\\r\\n(1) Long-term Forecasting task\\r\\n0 100 200 300 400 500\\r\\n3\\r\\n2\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\nEtth1 dataset\\r\\n0 100 200 300 400 500\\r\\n4\\r\\n2\\r\\n0\\r\\n2\\r\\n4\\r\\n6\\r\\nEtth2 dataset\\r\\n(2) Classification task\\r\\n0 250 500 750 1000 1250 1500 1750\\r\\n10.0\\r\\n7.5\\r\\n5.0\\r\\n2.5\\r\\n0.0\\r\\n2.5\\r\\n5.0\\r\\n7.5\\r\\nEthanolConcentration dataset\\r\\n0 200 400 600 800\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\nSelfRegulationSCP1 dataset\\r\\nFigure 13: Visualization of the Etth1 and Etth2 datasets for the Long-term forecasting task (1), and\\r\\nthe EthanolConcentration and SelfRegulationSCP1 datasets for the classification task (2).\\r\\nE.2 Periodic Pyramid Attention Mechanism Analysis\\r\\nTo illustrate the PPAM more clearly, we visualize the original time series and attention scores within\\r\\nthe periodic pyramid for the SelfRegulationSCP2 dataset in the classification task, as shown in Figure\\r\\n14. It is evident that the attention scores among components are distributed based on inclusion and\\r\\nadjacency relationships, meaning that components in different levels with inclusion relationships\\r\\nor those in the same level have higher attention scores. This demonstrates the rationality of the\\r\\nPeriodic Pyramid structure. Additionally, when the hyperparameter k in Equation (1) is set to 2, the\\r\\nPPAM degenerates into periodic full attention, wherein attention is computed among all periodic\\r\\n20\\n0\\r\\n1 2\\r\\n3 4 5 6\\r\\nOriginal time series Periodic pyramid Attention score\\r\\n6\\r\\n05\\r\\n4\\r\\n3\\r\\n2\\r\\n1\\r\\n0\\r\\n0 1 2 3 4 5 6\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0\\r\\n-0.2\\r\\n-0.4\\r\\n-0.6\\r\\n-0.8\\r\\nFigure 14: Visualization of the original time series and attention scores within the periodic pyramid\\r\\nfor the SelfRegulationSCP2 dataset in the classification task. The left side shows the original data, the\\r\\nmiddle displays the corresponding pyramid structure, and the right side depicts the attention scores\\r\\nwithin the pyramid. In this example, k = 3, f1 = 1, f2 = 2, f3 = 4.\\r\\n0 100 200 300 400 500\\r\\n2.0\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n0 2 4 6 8 10 12 14 16 18 20\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n7\\r\\n8\\r\\n9\\r\\n10\\r\\n11\\r\\n12\\r\\n13\\r\\n14\\r\\n15\\r\\n16\\r\\n17\\r\\n18\\r\\n19\\r\\n20\\r\\n21\\r\\n60\\r\\n40\\r\\n20\\r\\n0\\r\\n20\\r\\n40\\r\\n60\\r\\nFigure 15: Original time series of Electricity dataset (left) and Periodic Ptramid Attention score\\r\\n(right).\\r\\ncomponents. To illustrate this, we visualize the Electricity dataset in the long-term forecasting\\r\\ntask and its corresponding attention distribution, as shown in Figure 15. The figure shows that the\\r\\nPeriodic Attention Mechanism can capture the dependencies among the periodic components and\\r\\nidentify which components belong to a longer component (note that k = 2 corresponds to 21 periodic\\r\\ncomponents with larger amplitudes). This is attributed to the separation of the periodic components,\\r\\nwhich explicitly expresses the hidden periodic inclusion relationships in the time series. The above\\r\\nanalysis demonstrates the effectiveness of the PPAM.\\r\\nE.3 Periodic Feature Flows Analysis\\r\\nTo intuitively understand the Periodic Feature Flows, we visualize it as shown in Figure 16. On\\r\\nthe left side is the pyramid representation of the Periodic Feature Flows, with the horizontal axis\\r\\nrepresenting the number of feature flows and the vertical axis representing the length of each feature\\r\\nflow. The feature flows are divided into multiple levels, each containing multiple periodic components.\\r\\nThe red box encloses one feature flow, with its position from top to bottom corresponding to the\\r\\npyramid from top to bottom. It can be observed that some adjacent feature flows are the same at the\\r\\ncorresponding positions, this is because they pass through the same periodic component. On the right\\r\\nside, the waveform of each feature flow is displayed in different colors, with the position from left to\\r\\nright corresponding to the top to the bottom of the pyramid. It can be observed that each feature flow\\r\\ndiffers, which is why it is necessary to aggregate different feature flows. The aim is to fully utilize\\r\\nthe information from each periodic component to better reconstruct the target sample.\\r\\nE.4 Training/Inferencing Cost\\r\\nTo further validate the computational complexity and scalability of the proposed method, we con\\x02ducted detailed experiments on Electricity and ETTh1 datasets for the long-term forecasting task.\\r\\nThese experiments focused on the complexity and actual time of training and inference, as well\\r\\nas memory usage, with results presented in Tables 8 and 9. It can be seen that our proposed Peri\\x0221\\n0 20 40 60 80 100 120\\r\\n2\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\nFigure 16: Visualization of the pyramid form of Periodic Feature Flows (left) and the waveform of\\r\\nPeriodic Feature Flows (right).\\r\\nTable 8: Complexity and scalability experiments in the long-term forecasting of length 720 on\\r\\nElectricity\\r\\nMethods Train Test GPU Train CPU Test CPU Train time Test time MSE FLOPs FLOPs memory memory memory (s) (per sample)\\r\\nPeri-midFormer 244G 61G 4816M 2753M 2101M 5807s 0.0263s 0.181\\r\\nTime-LLM 12.69T - 61454M 12503M - - - 0.192\\r\\nGPT4TS 716G 3G 11236M 10572M 10471M 22366s 0.0280s 0.294\\r\\nPatchTST 549G 137G 11500M 8894M 2717M 7029s 0.0219s 0.294\\r\\nTimesNet 29.6T 934G 18112M 21021M 4225M 9769s 0.0501s 0.294\\r\\nDLinear 8G 237M 1892M 8045M 1427M 331s 0.0032s 0.204\\r\\nAutoformer 238G 8G 13157M 8488M 2738M 1477s 0.1390s 0.236\\r\\nTable 9: Complexity and scalability experiments in the long-term forecasting of length 720 on ETTh1\\r\\nMethods Train Test GPU Train CPU Test CPU Train time Test time MSE FLOPs FLOPs memory memory memory (s) (per sample)\\r\\nPeri-midFormer 23.51G 4.28G 2443M 3190M 75M 1623s 0.0036s 0.445\\r\\nTime-LLM 25.36T 25.36T 94994M 5795M 2893M 30506s 0.0384s 0.442\\r\\nGPT4TS 716G 3G 11657M 3250M 170M 2920s 0.0043s 0.477\\r\\nPatchTST 51.24G 1.58G 3683M 2895M 30M 550s 0.0013s 0.473\\r\\nTimesNet 116.32G 3.67G 2932M 3005M 20M 1640s 0.0076s 0.551\\r\\nDLinear 165.05M 5.17M 1449M 1310M 28M 190s 0.0006s 0.472\\r\\nAutoformer 202.34G 6.34G 12981M 3216M 16M 3400s 0.0123s 0.501\\r\\nmidFormer demonstrates a significant advantage in computational complexity on the Electricity\\r\\ndataset and achieves the lowest MSE. Similarly, on the ETTh1 dataset, Peri-midFormer’s computa\\x02tional cost and inference time are not disadvantages. Instead, it achieves a lower MSE, second only to\\r\\nTime-LLM, while having much lower computational cost and inference time compared to it. These\\r\\nresults highlight the advantages of our method in terms of computational complexity and scalability.\\r\\nE.5 Pre-interpolation\\r\\nSince Peri-midFormer is designed to focus on the periodic components of time series, directly\\r\\nhandling data with missing values in imputation tasks may prevent it from correctly capturing the\\r\\nperiodic characteristics of the original data without missing values, thus affecting its imputation\\r\\neffectiveness. Therefore, to adapt Peri-midFormer for imputation tasks, we first apply a linear\\r\\ninterpolation strategy (Equation (13)) to the data with missing values to partially restore the periodic\\r\\ncharacteristics of the original data before inputting it into Peri-midFormer for further imputation. The\\r\\nvisualization of original data, data with 50% missing values, and pre-interpolated data of Electricity\\r\\ndataset are illustrated in Figure 17. It is evident that missing values significantly disrupt the periodic\\r\\n22\\n(1) Data 1\\r\\n0 20 40 60 80\\r\\n1.25\\r\\n1.00\\r\\n0.75\\r\\n0.50\\r\\n0.25\\r\\n0.00\\r\\n0.25\\r\\nOriginal data\\r\\n0 20 40 60 80\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\nData with 50% missing values\\r\\n0 20 40 60 80\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n0.2\\r\\n0.0\\r\\nPre-interpolated data\\r\\n(2) Data 2\\r\\n0 20 40 60 80\\r\\n1.00\\r\\n0.75\\r\\n0.50\\r\\n0.25\\r\\n0.00\\r\\n0.25\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\nOriginal data\\r\\n0 20 40 60 80\\r\\n1.00\\r\\n0.75\\r\\n0.50\\r\\n0.25\\r\\n0.00\\r\\n0.25\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\nData with 50% missing values\\r\\n0 20 40 60 80\\r\\n1.00\\r\\n0.75\\r\\n0.50\\r\\n0.25\\r\\n0.00\\r\\n0.25\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\nPre-interpolated data\\r\\n(3) Data 3\\r\\n0 20 40 60 80\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\nOriginal data\\r\\n0 20 40 60 80\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\nData with 50% missing values\\r\\n0 20 40 60 80\\r\\n1.5\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\nPre-interpolated data\\r\\nFigure 17: Visualization of original data, data with 50% missing values, and pre-interpolated data of\\r\\nElectricity dataset.\\r\\ncharacteristics of the original data, while pre-interpolation partially restores them. It is worth noting\\r\\nthat this is not a speculative action but an effort to further explore the potential of deep learning\\r\\nmodels in imputation tasks. Our goal is for deep models to capture subtle variations in time series\\r\\nto achieve imputation in the details rather than wasting effort on goals achievable through simple\\r\\nlinear interpolation. Additionally, since we can retain the indices of missing values in practical\\r\\napplications, there is no concern about the reliability of evaluating the imputation results. To validate\\r\\nthe effectiveness of the pre-interpolation strategy, we applied it to other methods, as shown in Table\\r\\n10. We conducted experiments on four mask ratios {0.125, 0.25, 0.375, 0.5}, where the first row uses\\r\\nonly the pre-interpolation strategy. From the results, it can be seen that pre-interpolation significantly\\r\\nimproves the performance of each method across all four mask ratios, demonstrating its importance\\r\\nfor deep learning-based methods in interpolation tasks.\\r\\nThe simple linear interpolation strategy we adopted can be represented by the equation:\\r\\nxinter =\\r\\n\\uf8f1\\r\\n\\uf8f2\\r\\n\\uf8f3\\r\\nxbef ore+xaf ter\\r\\n2\\r\\n, if (xbefore ̸= None) and (xaf ter ̸= None)\\r\\nxaf ter, if (xbefore = None) and (xaf ter ̸= None)\\r\\nxbefore, if (xbefore ̸= None) and (xaf ter = None)\\r\\n, (13)\\r\\nwhere xinter represents the value used to replace the 0 value, xbefore represents the nearest non-zero\\r\\nvalue before the 0 value, and xaf ter represents the nearest non-zero value after the 0 value.\\r\\n23\\nTable 10: Ablation Experiments of pre-interpolation in inputation task on ECL dataset.\\r\\nMethods Metrics\\r\\nw/o pre-interpolation Pre-interpolation\\r\\n0.125 0.25 0.375 0.5 0.125 0.25 0.375 0.5\\r\\nOnly pre-interpolation MSE - - - - 0.086 0.110 0.149 0.206\\r\\nMAE - - - - 0.188 0.213 0.251 0.301\\r\\nPyri-midFormer MSE 0.073 0.092 0.107 0.122 0.044 0.052 0.063 0.079\\r\\nMAE 0.187 0.214 0.231 0.248 0.135 0.149 0.166 0.189\\r\\nTimesnet MSE 0.085 0.089 0.094 0.100 0.081 0.083 0.086 0.091\\r\\nMAE 0.202 0.206 0.213 0.221 0.196 0.198 0.201 0.207\\r\\nPyraformer MSE 0.297 0.294 0.296 0.299 0.165 0.165 0.171 0.173\\r\\nMAE 0.383 0.380 0.381 0.383 0.290 0.291 0.293 0.295\\r\\nDlinear MSE 0.092 0.118 0.144 0.175 0.050 0.062 0.789 0.105\\r\\nMAE 0.214 0.247 0.276 0.305 0.144 0.164 0.189 0.225\\r\\nPatchTST MSE 0.055 0.065 0.076 0.091 0.050 0.059 0.070 0.087\\r\\nMAE 0.160 0.175 0.189 0.208 0.148 0.164 0.181 0.202\\r\\nETSformer MSE 0.196 0.207 0.219 0.235 0.102 0.104 0.109 0.117\\r\\nMAE 0.321 0.332 0.344 0.357 0.230 0.233 0.238 0.247\\r\\n100 200 300 400 500 600 700 800 900 1000\\r\\nlook-back window length\\r\\n0.14\\r\\n0.16\\r\\n0.18\\r\\n0.20\\r\\n0.22\\r\\n0.24\\r\\nMSE\\r\\n96\\r\\n192\\r\\n336\\r\\n720\\r\\n1000\\r\\nFigure 18: Performance of different length look-back window on the long-term forecasting task in\\r\\nElectricity dataset. Prediction lengths are {96, 192, 336, 720, 1000}.\\r\\nE.6 Look-back window length\\r\\nHere, we investigate the performance of Peri-midFormer under different look-back window lengths\\r\\nin long-term forecasting task on the Electricity dataset. As shown in Figure 18, we experimented not\\r\\nonly with prediction lengths of {96, 192, 336, 720}, but also with longer lengths such as 1000. It can\\r\\nbe observed that there is a clear improvement in performance across all five prediction lengths as the\\r\\nlook-back window length increases. This improvement is particularly notable for longer prediction\\r\\nlengths such as 336, 720, and 1000. It indicates that Peri-midFormer can effectively utilize more\\r\\nhistorical information, as longer look-back windows contain more stable periodic characteristics,\\r\\nwhich is precisely what Peri-midFormer requires.\\r\\nE.7 Time Series Decomposition Analysis\\r\\nPeri-midFormer performs well on the Exchange dataset for long-term forecasting task and the Yearly\\r\\ndataset for short-term forecasting. Although these two datasets lack obvious periodicity, they exhibit\\r\\nstrong trends, as shown in Figure (19)). Peri-midFormer uses a time series decomposition strategy,\\r\\nwhere the trend part is first separated from the original data before dividing the periodic components.\\r\\nAfter the output of Peri-midFormer, the predicted trend part is added back, as illustrated in Figure\\r\\n(8). The separated trend part is easier to predict, especially when the trend is strong. This is why\\r\\nPeri-midFormer achieves strong performance on the Exchange and Yearly datasets.\\r\\n24\\nExchange\\r\\n0 10 20 30 40 50 60\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n0 10 20 30 40 50 60\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n0 10 20 30 40 50 60\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\nYearly\\r\\n0 2 4 6 8 10\\r\\n8000\\r\\n9000\\r\\n10000\\r\\n11000\\r\\n12000\\r\\n13000\\r\\n14000\\r\\n15000\\r\\n0 2 4 6 8 10\\r\\n2500\\r\\n3000\\r\\n3500\\r\\n4000\\r\\n4500\\r\\n0 2 4 6 8 10\\r\\n4000\\r\\n4250\\r\\n4500\\r\\n4750\\r\\n5000\\r\\n5250\\r\\n5500\\r\\nFigure 19: Visualization of Exchange and Yearly dataset.\\r\\nF Proof\\r\\nTo demonstrate the essence of attention computation among multi-level periodic components, we\\r\\nneed to analyze how the interactions between periodic components at different levels affect the final\\r\\nfeature extraction. In time series analysis, different periodic components correspond to different time\\r\\nscales. This means that through decomposition, we can capture components of various frequencies\\r\\nwithin the time series. The essence of the periodic pyramid is to capture these different frequency\\r\\ncomponents through its hierarchical structure.\\r\\nUsing single-channel data as an example, and given that we adopt an independent channel strategy,\\r\\nthis can be easily extended to all channels. Assume the time series x(t) can be decomposed into\\r\\nmultiple periodic components xn(t) :\\r\\nx(t) = X\\r\\nN\\r\\nn=1\\r\\nxn(t). (14)\\r\\nTaking two different periodic components as examples:\\r\\nxi(t) = Ai sin \\x12\\r\\n2πt\\r\\nTi\\r\\n+ ϕi\\r\\n\\x13\\r\\n, xj (t) = Aj cos \\x12\\r\\n2πt\\r\\nTj\\r\\n+ ϕj\\r\\n\\x13\\r\\n, (15)\\r\\nwhere A is amplitude, T is period, and ϕ is phase. Due to the overlap and inclusion relationships\\r\\nbetween different periodic components, we employ an attention mechanism in the periodic pyramid\\r\\nto capture the similarities between different periodic components, focusing on important periodic\\r\\nfeatures. When applying the attention mechanism, we have:\\r\\nQi = WQxi(t), Kj = WKxj (t), Vj = WV xj (t), (16)\\r\\nwhere WQ, WK and WV are learnable weight matrices. From Equations (15) and (16):\\r\\nQi = WQAi sin \\x12\\r\\n2πt\\r\\nTi\\r\\n+ ϕi\\r\\n\\x13\\r\\n, Kj = WKAj cos \\x12\\r\\n2πt\\r\\nTj\\r\\n+ ϕj\\r\\n\\x13\\r\\n. (17)\\r\\nFurther, the dot-product attention can be expressed as:\\r\\nQiKT\\r\\nj = AiAj\\r\\n\\x12\\r\\nWQ sin \\x12\\r\\n2πt\\r\\nTi\\r\\n+ ϕi\\r\\n\\x13\\x13 \\x12WK cos \\x12\\r\\n2πt\\r\\nTj\\r\\n+ ϕj\\r\\n\\x13\\x13T\\r\\n. (18)\\r\\nUsing the trigonometric identity sin(a) cos(b) = 1\\r\\n2\\r\\n[sin(a + b) + sin(a − b)], the dot-product QiKT\\r\\nj\\r\\ncan be further expressed as:\\r\\nQiKT\\r\\nj =\\r\\n1\\r\\n2\\r\\nAiAj\\r\\n\\x1a\\r\\nWQ\\r\\n\\x14\\r\\nsin \\x12\\r\\n2πt\\r\\nTi\\r\\n+ ϕi +\\r\\n2πt\\r\\nTj\\r\\n+ ϕj\\r\\n\\x13\\r\\n+ sin \\x12\\r\\n2πt\\r\\nTi\\r\\n+ ϕi −\\r\\n2πt\\r\\nTj\\r\\n− ϕj\\r\\n\\x13\\x15\\x1b (WK)\\r\\nT\\r\\n.\\r\\n(19)\\r\\n25\\nBased on this, considering the periodicity and symmetry of sin(a + b) and sin(a − b), when the\\r\\nperiods of two time series components are close / same (intra-layer attention in the pyramid, see the\\r\\nright side of Figure 3 in the original paper) or have overlapping / inclusive parts (inter-layer attention\\r\\nin the pyramid, see the right side of Figure 3 in the original paper), the values of these two sine\\r\\nfunctions will be highly correlated, resulting in a large QiKT\\r\\nj\\r\\nvalue. This indicates that the periodic\\r\\npyramid model can effectively capture similar periodic patterns across different time scales.\\r\\nNext, incorporating this into the calculation of the attention score:\\r\\nsij =\\r\\nexp \\x12\\r\\nQiKT\\r\\n√ j\\r\\ndk\\r\\n\\x13\\r\\nP\\r\\nj\\r\\n′\\r\\nexp \\x12\\r\\nQiKT\\r\\nj\\r\\n′ √\\r\\ndk\\r\\n\\x13. (20)\\r\\nIt can be seen that the attention scores between highly correlated periodic components will be higher,\\r\\nwhich we have already validated in Figures 13 and 14 of the original paper.\\r\\nFurther, the attention vector ai of xi(t) can be obtained as:\\r\\nai =\\r\\nX\\r\\nj\\r\\nsijVj , (21)\\r\\nwhere Vj = WV xj (t) = WV Aj cos \\x10\\r\\n2πt\\r\\nTj\\r\\n+ ϕj\\r\\n\\x11\\r\\n.\\r\\nFrom the above derivation, it can be seen that the attention mechanism can measure the similarity\\r\\nbetween different periodic components. This similarity reflects the alignment between different\\r\\nperiodic components in the time series, allowing the model to capture important periodic patterns. By\\r\\ncapturing these periodic patterns, the periodic pyramid can extract key features of the time series,\\r\\nresulting in a comprehensive and accurate time series representation. This representation not only\\r\\nincludes information across different time scales but also enhances the representation of important\\r\\nperiodic patterns.\\r\\nG Broader Impacts\\r\\nOur research has implications for time-series-based analyses such as weather forecasting and anomaly\\r\\ndetection for industrial maintenance, as discussed in the Introduction 1. Our work carries no negative\\r\\nsocial impact.\\r\\nH Full Results\\r\\nH.1 Full Results of Classification (Table 11)\\r\\nH.2 Full Results of Short-term Forecasting (Table 12)\\r\\nH.3 Full Results of Long-term Forecasting (Table 13 and 14)\\r\\nH.4 Full Results of Imputation (Table 15)\\r\\nH.5 Full Results of Anomaly Detection (Table 16)\\r\\n26\\nTable 11: Full results for the classification task. (∗ means former, T-LLM means Time-LLM,\\r\\nGPT means GPT4TS, T-Net means TimesNet, Patch means PatchTST, Light means LightTS,\\r\\nStation means the Non-stationary Transformer.) We report the classification accuracy (%) as\\r\\nthe result. The standard deviation is within 1%. We reproduced the results of PatchTST\\r\\nby https://github.com/thuml/Time-Series-Library, reproduced TSLANet by https://\\r\\ngithub.com/emadeldeen24/TSLANet, and copied the others from GPT4TS [20]. Red: best, Blue:\\r\\nsecond best.\\r\\nDatasets / Models\\r\\nRocket Patch TCN Trans∗ Re∗ Pyra∗ Auto∗ Station FED∗ ETS∗ Flow∗ DLinear Light T-Net GPT TLSANet Peri-mid\\r\\n[33] [19] [8] [48] [28] [10] [12] [11] [17] [29] [35] [14] [15] [13] [20] [27] Former\\r\\nEthanolConcentration 45.2 29.6 28.9 32.7 31.9 30.8 31.6 32.7 31.2 28.1 33.8 32.6 29.7 35.7 34.2 30.4 50.6\\r\\nFaceDetection 64.7 67.8 52.8 67.3 68.6 65.7 68.4 68.0 66.0 66.3 67.6 68.0 67.5 68.6 69.2 66.7 68.7\\r\\nHandwriting 58.8 23.2 53.3 32.0 27.4 29.4 36.7 31.6 28.0 32.5 33.8 27.0 26.1 32.1 32.7 57.88 31.5\\r\\nHeartbeat 75.6 75.7 75.6 76.1 77.1 75.6 74.6 73.7 73.7 71.2 77.6 75.1 75.1 78.0 77.2 77.5 95.1\\r\\nJapaneseVowels 96.2 94.0 98.9 98.7 97.8 98.4 96.2 99.2 98.4 95.9 98.9 96.2 96.2 98.4 98.6 99.2 97.3\\r\\nPEMS-SF 75.1 80.9 68.8 82.1 82.7 83.2 82.7 87.3 80.9 86.0 83.8 75.1 88.4 89.6 87.9 83.8 88.2\\r\\nSelfRegulationSCP1 90.8 82.2 84.6 92.2 90.4 88.1 84.0 89.4 88.7 89.6 92.5 87.3 89.8 91.8 93.2 91.8 92.1\\r\\nSelfRegulationSCP2 53.3 53.6 55.6 53.9 56.7 53.3 50.6 57.2 54.4 55.0 56.1 50.5 51.1 57.2 59.4 61.6 58.2\\r\\nSpokenArabicDigits 71.2 98.0 95.6 98.4 97.0 99.6 100.0 100.0 100.0 100.0 98.8 81.4 100.0 99.0 99.2 99.9 98.2\\r\\nUWaveGestureLibrary 94.4 81.7 88.4 85.6 85.6 83.4 85.9 87.5 85.3 85.0 86.6 82.1 80.3 85.3 88.1 91.2 85.6\\r\\nAverage Accuracy 72.5 68.7 70.3 71.9 71.5 70.8 71.1 72.7 70.7 71.0 73.0 67.5 70.4 73.6 74.0 76.0 76.5\\r\\nTable 12: Full results for the short-term forecasting task in the M4 dataset. (∗ means former, T-LLM\\r\\nmeans Time-LLM, GPT means GPT4TS, T-Net means TimesNet, Patch means PatchTST, HiTS\\r\\nmeans N-HiTS, BEATS means N-BEATS, Light means LightTS, Station means the Non-stationary\\r\\nTransformer.) The standard deviation is within 0.5%. We copied the results of Time-LLM from\\r\\nTime-LLM [25], Pyraformer from TimesNet [13], and the remaining results from GPT4TS [20]. Red:\\r\\nbest, Blue: second best.\\r\\nModels\\r\\nPeri-mid T-LLM GPT T-Net Patch HiTS BEATS ETS∗ Light DLinear FED∗ Station Auto∗ Pyra∗ In∗ Re∗\\r\\nFormer [25] [20] [13] [19] [30] [31] [29] [15] [14] [17] [11] [12] [10] [16] [28]\\r\\nYearlySMAPE13.35813.41913.53113.38713.47713.41813.43618.00914.24716.96513.72813.71713.97415.53014.72716.169\\r\\nMASE 3.021 3.005 3.015 2.996 3.019 3.045 3.043 4.487 3.109 4.283 3.048 3.078 3.134 3.711 3.418 3.800\\r\\nOWA 0.789 0.789 0.793 0.786 0.792 0.793 0.794 1.115 0.827 1.058 0.803 0.807 0.822 0.942 0.881 0.973\\r\\nQuarterlySMAPE10.05810.11010.17710.10010.38010.20210.12413.37611.36412.14510.79210.95811.33815.44911.36013.313\\r\\nMASE 1.170 1.178 1.194 1.182 1.233 1.194 1.169 1.906 1.328 1.520 1.283 1.325 1.365 2.350 1.401 1.775\\r\\nOWA 0.883 0.889 0.898 0.890 0.921 0.899 0.886 1.302 1.000 1.106 0.958 0.981 1.012 1.558 1.027 1.252\\r\\nMonthlySMAPE12.71712.98012.89412.67012.95912.79112.67714.58814.01413.51414.26013.91713.95817.64214.06220.128\\r\\nMASE 0.933 0.963 0.956 0.933 0.970 0.969 0.937 1.368 1.053 1.037 1.102 1.097 1.103 1.913 1.141 2.614\\r\\nOWA 0.879 0.903 0.897 0.878 0.905 0.899 0.880 1.149 0.981 0.956 1.012 0.998 1.002 1.511 1.024 1.927\\r\\nOthersSMAPE 4.845 4.795 4.940 4.891 4.952 5.061 4.925 7.267 15.880 6.709 4.954 6.302 5.485 24.78624.46032.491\\r\\nMASE 3.217 3.178 3.228 3.302 3.347 3.216 3.391 5.240 11.434 4.953 3.264 4.064 3.865 18.58120.96033.355\\r\\nOWA 1.017 1.006 1.029 1.035 1.049 1.040 1.053 1.591 3.474 1.487 1.036 1.304 1.187 5.538 5.879 8.679\\r\\nWeighted\\r\\nAverageSMAPE11.83311.98311.99111.82912.05911.92711.85114.71813.52513.63912.84012.78012.90916.98714.08618.200\\r\\nMASE 1.584 1.595 1.600 1.585 1.623 1.613 1.599 2.408 2.111 2.095 1.701 1.756 1.771 3.265 2.718 4.223\\r\\nOWA 0.850 0.859 0.861 0.851 0.869 0.861 0.855 1.172 1.051 1.051 0.918 0.930 0.939 1.480 1.230 1.775\\r\\n27\\nTable 13: Full results of 512 look-back window length in long-term forecasting task (since FITS defaults to a look-back window length\\r\\nof 720, its results at 720 length are attached at the end). The standard deviation is within 0.5%. We copied the results of GPT4TS\\r\\nfrom GPT4TS [20], Time-LLM from TSLANet [27], reproduced TSLANet by https://github.com/emadeldeen24/TSLANet,\\r\\nand reproduced the others by https://github.com/thuml/Time-Series-Library. Red: best, Blue: second best.\\r\\nLook-back 512 720\\r\\nMethods Peri-mid GPT4TS TSLANet Time-LLM FITS Dlinear PatchTST TimesNet Pyraformer FITS\\r\\nFormer [20] [27] [25] [21] [14] [19] [13] [10] [21]\\r\\nMetrics MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.153 0.200 0.162 0.212 0.196 0.232 0.147 0.201 0.172 0.226 0.171 0.231 0.147 0.199 0.159 0.214 0.215 0.298 0.170 0.225\\r\\n192 0.203 0.251 0.204 0.248 0.241 0.269 0.189 0.235 0.216 0.262 0.213 0.270 0.196 0.246 0.220 0.269 0.252 0.335 0.213 0.260\\r\\n336 0.255 0.294 0.254 0.286 0.297 0.308 0.262 0.279 0.261 0.295 0.259 0.311 0.243 0.283 0.276 0.307 0.282 0.352 0.258 0.295\\r\\n720 0.321 0.338 0.326 0.337 0.371 0.356 0.304 0.316 0.326 0.342 0.322 0.362 0.317 0.334 0.342 0.354 0.373 0.412 0.321 0.340\\r\\nAvg 0.233 0.271 0.237 0.270 0.276 0.291 0.225 0.257 0.244 0.281 0.241 0.294 0.226 0.266 0.249 0.286 0.281 0.349 0.241 0.280\\r\\nETTh1\\r\\n96 0.367 0.396 0.376 0.397 0.374 0.402 0.362 0.392 0.372 0.396 0.367 0.397 0.374 0.402 0.442 0.457 0.702 0.628 0.379 0.402\\r\\n192 0.404 0.420 0.416 0.418 0.416 0.431 0.398 0.418 0.405 0.415 0.402 0.420 0.415 0.433 0.492 0.491 0.927 0.743 0.414 0.423\\r\\n336 0.421 0.432 0.442 0.433 0.404 0.431 0.430 0.427 0.424 0.429 0.431 0.442 0.458 0.458 0.481 0.486 0.973 0.785 0.434 0.440\\r\\n720 0.445 0.470 0.477 0.456 0.495 0.495 0.442 0.457 0.425 0.448 0.472 0.494 0.473 0.484 0.551 0.525 1.048 0.834 0.431 0.454\\r\\nAvg 0.409 0.430 0.427 0.426 0.422 0.440 0.408 0.423 0.407 0.422 0.418 0.438 0.430 0.444 0.492 0.490 0.913 0.748 0.415 0.430\\r\\nETTh2\\r\\n96 0.268 0.337 0.285 0.342 0.270 0.339 0.268 0.328 0.271 0.337 0.303 0.368 0.298 0.349 0.382 0.420 1.497 0.934 0.271 0.337\\r\\n192 0.321 0.376 0.354 0.389 0.317 0.374 0.329 0.375 0.330 0.374 0.393 0.425 0.371 0.399 0.407 0.434 4.498 1.699 0.331 0.375\\r\\n336 0.331 0.394 0.373 0.407 0.320 0.382 0.368 0.409 0.353 0.395 0.510 0.497 0.418 0.433 0.392 0.438 4.385 1.743 0.354 0.396\\r\\n720 0.348 0.402 0.406 0.441 0.405 0.443 0.372 0.420 0.378 0.421 0.808 0.637 0.465 0.474 0.451 0.469 4.579 1.838 0.377 0.423\\r\\nAvg 0.317 0.377 0.354 0.394 0.328 0.385 0.334 0.383 0.333 0.382 0.504 0.482 0.388 0.414 0.408 0.440 3.740 1.554 0.333 0.383\\r\\nETTm1\\r\\n96 0.291 0.349 0.292 0.346 0.297 0.354 0.272 0.334 0.306 0.349 0.304 0.347 0.302 0.353 0.331 0.375 0.567 0.502 0.309 0.352\\r\\n192 0.334 0.374 0.332 0.372 0.329 0.373 0.310 0.358 0.338 0.367 0.335 0.366 0.335 0.373 0.394 0.409 0.626 0.546 0.338 0.368\\r\\n336 0.367 0.398 0.366 0.394 0.353 0.390 0.352 0.384 0.368 0.384 0.366 0.385 0.372 0.399 0.414 0.431 0.752 0.641 0.366 0.385\\r\\n720 0.425 0.418 0.417 0.421 0.411 0.416 0.383 0.411 0.421 0.413 0.420 0.417 0.442 0.437 0.454 0.457 0.952 0.745 0.415 0.412\\r\\nAvg 0.354 0.385 0.352 0.383 0.348 0.383 0.329 0.372 0.358 0.378 0.356 0.379 0.363 0.391 0.398 0.418 0.724 0.609 0.357 0.379\\r\\nETTm2\\r\\n96 0.164 0.254 0.173 0.262 0.169 0.263 0.161 0.253 0.165 0.254 0.169 0.265 0.169 0.260 0.191 0.277 0.369 0.464 0.163 0.253\\r\\n192 0.228 0.301 0.229 0.301 0.231 0.306 0.219 0.293 0.219 0.291 0.226 0.306 0.243 0.308 0.251 0.321 0.571 0.573 0.222 0.298\\r\\n336 0.279 0.335 0.286 0.341 0.286 0.339 0.271 0.329 0.272 0.326 0.298 0.362 0.297 0.353 0.319 0.364 1.193 0.813 0.268 0.326\\r\\n720 0.362 0.388 0.378 0.401 0.364 0.391 0.353 0.379 0.359 0.381 0.408 0.434 0.381 0.395 0.388 0.408 3.289 1.339 0.349 0.378\\r\\nAvg 0.258 0.319 0.266 0.326 0.263 0.325 0.251 0.313 0.254 0.313 0.275 0.342 0.273 0.329 0.287 0.343 1.356 0.797 0.251 0.314\\r\\nElectricity\\r\\n96 0.126 0.223 0.139 0.238 0.131 0.226 0.131 0.224 0.140 0.238 0.141 0.241 0.131 0.228 0.181 0.286 0.281 0.375 0.137 0.235\\r\\n192 0.144 0.240 0.153 0.251 0.145 0.240 0.152 0.241 0.153 0.250 0.154 0.254 0.149 0.246 0.191 0.294 0.295 0.389 0.151 0.248\\r\\n336 0.157 0.255 0.169 0.266 0.162 0.139 0.160 0.248 0.169 0.266 0.169 0.271 0.166 0.262 0.203 0.304 0.312 0.405 0.167 0.264\\r\\n720 0.181 0.279 0.206 0.297 0.199 0.290 0.192 0.298 0.208 0.298 0.204 0.304 0.203 0.293 0.294 0.371 0.309 0.396 0.206 0.296\\r\\nAvg 0.152 0.249 0.167 0.263 0.159 0.224 0.158 0.252 0.168 0.263 0.167 0.268 0.162 0.257 0.217 0.314 0.299 0.391 0.165 0.261\\r\\nTraffic\\r\\n96 0.361 0.255 0.388 0.282 0.371 0.258 0.362 0.248 0.398 0.277 0.411 0.294 0.364 0.256 0.601 0.320 0.708 0.406 0.387 0.272\\r\\n192 0.380 0.263 0.407 0.290 0.388 0.268 0.374 0.247 0.409 0.280 0.421 0.298 0.382 0.263 0.609 0.328 0.699 0.401 0.398 0.274\\r\\n336 0.393 0.269 0.412 0.294 0.397 0.272 0.385 0.271 0.418 0.285 0.431 0.304 0.392 0.270 0.619 0.330 0.699 0.398 0.411 0.281\\r\\n720 0.434 0.293 0.450 0.312 0.431 0.288 0.430 0.288 0.456 0.306 0.467 0.324 0.430 0.290 0.657 0.348 0.714 0.398 0.449 0.301\\r\\nAvg 0.392 0.270 0.414 0.294 0.397 0.272 0.388 0.264 0.420 0.287 0.433 0.305 0.392 0.270 0.622 0.332 0.705 0.401 0.411 0.282\\r\\nExchange\\r\\n96 0.081 0.197 0.089 0.207 0.082 0.201 0.088 0.208 0.100 0.225 0.110 0.252 0.101 0.224 0.206 0.342 0.654 0.659 0.095 0.221\\r\\n192 0.171 0.292 0.185 0.305 0.171 0.293 0.182 0.303 0.201 0.326 0.238 0.373 0.215 0.335 0.391 0.472 0.741 0.691 0.200 0.323\\r\\n336 0.311 0.402 0.335 0.421 0.319 0.407 0.334 0.419 0.350 0.437 0.466 0.525 0.296 0.399 0.606 0.595 1.042 0.835 0.369 0.444\\r\\n720 0.822 0.682 0.884 0.705 0.888 0.739 0.879 0.704 0.920 0.728 1.166 0.821 1.058 0.775 1.601 0.961 2.190 1.189 0.994 0.750\\r\\nAvg 0.346 0.393 0.373 0.410 0.365 0.410 0.371 0.409 0.393 0.429 0.495 0.493 0.418 0.433 0.701 0.593 1.157 0.844 0.415 0.435\\r\\nAverage 0.308 0.337 0.324 0.346 0.320 0.341 0.308 0.334 0.322 0.344 0.361 0.375 0.331 0.350 0.422 0.402 1.147 0.711 0.323 0.345\\r\\n28\\nTable 14: Full results of 96 look-back window length in long-term forecasting task. (∗ means former, Station means the Non-stationary\\r\\nTransformer.) The standard deviation is within 0.5%. We copied the results of iTransformer from TSLANet [27], Pyraformer from\\r\\nTimesNet [13], reproduced FITS by https://github.com/VEWOXIC/FITS, and copied the others from GPT4TS [20]. Red: best,\\r\\nBlue: second best.\\r\\nLook-back 96\\r\\nMethods Peri-mid iTrans∗ FITS Dlinear PatchTST TimesNet Pyra∗ FED∗ Auto∗ Station ETS∗ LightTS In∗ Re∗\\r\\nFormer [26] [21] [14] [19] [13] [10] [17] [12] [11] [29] [15] [16] [28]\\r\\nMetrics MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.155 0.200 0.174 0.214 0.197 0.237 0.176 0.237 0.149 0.198 0.172 0.220 0.622 0.556 0.217 0.296 0.266 0.336 0.173 0.223 0.197 0.281 0.182 0.242 0.300 0.384 0.689 0.596\\r\\n192 0.203 0.244 0.221 0.254 0.241 0.272 0.220 0.282 0.194 0.241 0.219 0.261 0.739 0.624 0.276 0.336 0.307 0.367 0.245 0.285 0.237 0.312 0.227 0.287 0.598 0.544 0.752 0.638\\r\\n336 0.262 0.289 0.278 0.296 0.293 0.308 0.265 0.319 0.245 0.282 0.280 0.306 1.004 0.753 0.339 0.380 0.359 0.395 0.321 0.338 0.298 0.353 0.282 0.334 0.578 0.523 0.639 0.596\\r\\n720 0.345 0.344 0.358 0.349 0.365 0.354 0.333 0.362 0.314 0.334 0.365 0.359 1.420 0.934 0.403 0.428 0.419 0.428 0.414 0.410 0.352 0.288 0.352 0.386 1.059 0.741 1.130 0.792\\r\\nAvg 0.241 0.269 0.258 0.278 0.274 0.293 0.248 0.300 0.225 0.264 0.259 0.287 0.946 0.717 0.309 0.360 0.338 0.382 0.288 0.314 0.271 0.334 0.261 0.312 0.634 0.548 0.803 0.656\\r\\nETTh1\\r\\n96 0.373 0.396 0.386 0.405 0.385 0.393 0.375 0.399 0.370 0.399 0.384 0.402 0.664 0.612 0.376 0.419 0.449 0.459 0.513 0.491 0.494 0.479 0.424 0.432 0.865 0.713 0.837 0.728\\r\\n192 0.425 0.429 0.441 0.436 0.435 0.422 0.405 0.416 0.413 0.421 0.436 0.429 0.790 0.681 0.420 0.448 0.500 0.482 0.534 0.504 0.538 0.504 0.475 0.462 1.008 0.792 0.923 0.766\\r\\n336 0.464 0.445 0.487 0.458 0.475 0.444 0.439 0.443 0.422 0.436 0.491 0.469 0.891 0.738 0.459 0.465 0.521 0.496 0.588 0.535 0.574 0.521 0.518 0.488 1.107 0.809 1.097 0.835\\r\\n720 0.479 0.467 0.503 0.491 0.463 0.459 0.472 0.490 0.447 0.466 0.521 0.500 0.963 0.782 0.506 0.507 0.514 0.512 0.643 0.616 0.562 0.535 0.547 0.533 1.181 0.865 1.257 0.889\\r\\nAvg 0.435 0.434 0.454 0.448 0.440 0.429 0.422 0.437 0.413 0.430 0.458 0.450 0.827 0.703 0.440 0.460 0.496 0.487 0.570 0.537 0.542 0.510 0.491 0.479 1.040 0.795 1.029 0.805\\r\\nETTh2\\r\\n96 0.287 0.337 0.297 0.349 0.292 0.339 0.289 0.353 0.274 0.336 0.340 0.374 0.645 0.597 0.358 0.397 0.346 0.388 0.476 0.458 0.340 0.391 0.397 0.437 3.755 1.525 2.626 1.317\\r\\n192 0.368 0.387 0.380 0.400 0.377 0.391 0.383 0.418 0.339 0.379 0.402 0.414 0.788 0.683 0.429 0.439 0.456 0.452 0.512 0.493 0.430 0.439 0.520 0.504 5.602 1.931 11.120 2.979\\r\\n336 0.414 0.424 0.428 0.432 0.416 0.425 0.448 0.465 0.329 0.380 0.452 0.452 0.907 0.747 0.496 0.487 0.482 0.486 0.552 0.551 0.485 0.479 0.626 5.559 4.721 1.833 3.233 2.769\\r\\n720 0.397 0.425 0.427 0.445 0.417 0.436 0.605 0.551 0.379 0.422 0.462 0.468 0.963 0.783 0.463 0.474 0.515 0.511 0.562 0.560 0.500 0.497 0.863 0.672 3.647 1.625 3.874 1.697\\r\\nAvg 0.367 0.393 0.383 0.407 0.376 0.398 0.431 0.446 0.330 0.379 0.415 0.427 0.826 0.703 0.437 0.449 0.450 0.459 0.526 0.516 0.439 0.452 0.602 0.543 4.431 7.000 1.729 6.736\\r\\nETTm1\\r\\n96 0.330 0.368 0.334 0.368 0.353 0.375 0.299 0.343 0.290 0.342 0.338 0.375 0.543 0.510 0.379 0.419 0.505 0.475 0.386 0.398 0.375 0.398 0.370 0.400 0.677 0.571 0.533 0.528\\r\\n192 0.371 0.388 0.377 0.391 0.392 0.393 0.335 0.365 0.332 0.369 0.374 0.387 0.556 0.537 0.426 0.441 0.553 0.496 0.459 0.444 0.408 0.410 0.400 0.407 0.795 0.669 0.658 0.592\\r\\n336 0.402 0.409 0.426 0.420 0.425 0.414 0.369 0.386 0.366 0.392 0.410 0.411 0.754 0.655 0.445 0.459 0.621 0.537 0.495 0.464 0.435 0.428 0.438 0.438 1.212 0.871 0.898 0.721\\r\\n720 0.466 0.445 0.491 0.459 0.486 0.448 0.425 0.421 0.416 0.420 0.478 0.450 0.908 0.724 0.543 0.490 0.671 0.561 0.585 0.516 0.499 0.462 0.527 0.502 1.166 0.823 1.102 0.841\\r\\nAvg 0.392 0.402 0.407 0.410 0.414 0.408 0.357 0.378 0.351 0.380 0.400 0.406 0.691 0.607 0.448 0.452 0.588 0.517 0.481 0.456 0.429 0.425 0.435 0.437 0.961 0.734 0.799 0.671\\r\\nETTm2\\r\\n96 0.171 0.254 0.180 0.264 0.183 0.266 0.167 0.269 0.165 0.255 0.187 0.267 0.435 0.507 0.203 0.287 0.255 0.339 0.192 0.274 0.189 0.280 0.209 0.308 0.365 0.453 0.658 0.619\\r\\n192 0.240 0.299 0.250 0.309 0.247 0.305 0.224 0.303 0.220 0.292 0.249 0.309 0.730 0.673 0.269 0.328 0.281 0.340 0.280 0.339 10.253 0.319 0.311 0.382 0.533 0.563 1.078 0.827\\r\\n336 0.306 0.341 0.311 0.348 0.307 0.342 0.281 0.342 0.274 0.329 0.321 0.351 1.201 0.845 0.325 0.366 0.339 0.372 0.334 0.361 0.314 0.357 0.442 0.466 1.363 0.887 1.549 0.972\\r\\n720 0.404 0.397 0.412 0.407 0.407 0.397 0.397 0.421 0.362 0.385 0.408 0.403 3.625 1.451 0.421 0.415 0.433 0.432 0.417 0.413 0.414 0.413 0.675 0.587 3.379 1.338 2.631 1.242\\r\\nAvg 0.280 0.323 0.288 0.332 0.286 0.328 0.267 0.333 0.255 0.315 0.291 0.333 1.498 0.869 0.305 0.349 0.327 0.371 0.306 0.347 0.293 0.342 0.409 0.436 1.410 0.810 1.479 0.915\\r\\nElectricity\\r\\n96 0.142 0.236 0.148 0.240 0.197 0.274 0.140 0.237 0.129 0.222 0.168 0.272 0.386 0.449 0.193 0.308 0.201 0.317 0.169 0.273 0.187 0.304 0.207 0.307 0.274 0.368 0.312 0.402\\r\\n192 0.159 0.251 0.162 0.253 0.197 0.276 0.153 0.249 0.157 0.240 0.184 0.289 0.378 0.443 0.201 0.315 0.222 0.334 0.182 0.286 0.199 0.315 0.213 0.316 0.296 0.386 0.348 0.430\\r\\n336 0.176 0.270 0.178 0.269 0.212 0.293 0.160 0.267 0.163 0.259 0.198 0.300 0.376 0.443 0.214 0.329 0.230 0.333 0.200 0.304 0.212 0.329 0.230 0.333 0.300 0.394 0.350 0.433\\r\\n720 0.204 0.307 0.225 0.317 0.253 0.325 0.203 0.301 0.197 0.290 0.220 0.320 0.376 0.445 0.246 0.355 0.254 0.361 0.222 0.321 0.233 0.345 0.265 0.360 0.373 0.439 0.340 0.42\\r\\nAvg 0.170 0.266 0.178 0.270 0.215 0.292 0.166 0.263 0.161 0.252 0.192 0.295 0.379 0.445 0.214 0.327 0.227 0.334 0.193 0.296 0.208 0.323 0.229 0.329 0.311 0.397 0.338 0.422\\r\\nTraffic\\r\\n96 0.453 0.300 0.395 0.268 0.642 0.388 0.410 0.282 0.360 0.249 0.593 0.321 0.867 0.468 0.587 0.366 0.613 0.388 0.612 0.338 0.607 0.392 0.615 0.391 0.719 0.391 0.732 0.423\\r\\n192 0.459 0.301 0.417 0.276 0.597 0.362 0.423 0.287 0.379 0.256 0.617 0.336 0.869 0.467 0.604 0.373 0.616 0.382 0.613 0.340 0.621 0.399 0.601 0.382 0.696 0.379 0.733 0.420\\r\\n336 0.477 0.310 0.433 0.283 0.603 0.365 0.436 0.296 0.392 0.264 0.629 0.336 0.881 0.469 0.621 0.383 0.622 0.337 0.618 0.328 0.622 0.396 0.613 0.386 0.777 0.420 0.742 0.420\\r\\n720 0.510 0.329 0.467 0.302 0.641 0.384 0.466 0.315 0.432 0.286 0.640 0.350 0.896 0.473 0.626 0.382 0.660 0.408 0.653 0.355 0.632 0.396 0.658 0.407 0.864 0.472 0.755 0.423\\r\\nAvg 0.475 0.310 0.428 0.282 0.621 0.375 0.433 0.295 0.390 0.263 0.620 0.336 0.878 0.469 0.610 0.376 0.628 0.379 0.624 0.340 0.621 0.396 0.623 0.392 0.764 0.416 0.741 0.422\\r\\nExchange\\r\\n96 0.082 0.198 0.086 0.206 0.087 0.208 0.088 0.218 0.088 0.205 0.107 0.234 1.748 1.105 0.148 0.278 0.197 0.323 0.111 0.237 0.085 0.204 0.116 0.262 0.847 0.752 1.065 0.829\\r\\n192 0.172 0.295 0.177 0.299 0.178 0.302 0.176 0.315 0.175 0.298 0.226 0.344 1.874 1.151 0.271 0.380 0.300 0.369 0.219 0.335 0.182 0.303 0.215 0.359 1.204 0.895 1.188 0.906\\r\\n336 0.319 0.407 0.331 0.417 0.325 0.413 0.313 0.427 0.302 0.398 0.367 0.448 1.943 1.172 0.460 0.500 0.509 0.524 0.421 0.476 0.348 0.428 0.377 0.466 1.672 1.036 1.357 0.976\\r\\n720 0.846 0.692 0.847 0.691 0.843 0.694 0.839 0.695 0.871 0.703 0.964 0.746 2.085 1.206 1.195 0.841 1.447 0.941 1.092 0.769 1.025 0.774 0.831 0.699 2.478 1.310 1.510 1.016\\r\\nAvg 0.355 0.398 0.360 0.403 0.358 0.404 0.354 0.414 0.359 0.401 0.416 0.443 1.913 1.159 0.519 0.500 0.613 0.539 0.461 0.454 0.410 0.427 0.385 0.447 1.550 0.998 1.280 0.932\\r\\nAverage 0.339 0.349 0.345 0.353 0.373 0.366 0.335 0.358 0.311 0.335 0.381 0.372 0.995 0.709 0.410 0.409 0.458 0.433 0.431 0.408 0.402 0.401 0.429 0.422 0.953 0.803 1.650 0.877\\r\\n29\\nTable 15: Full results for the imputation task. We randomly mask 12.5%, 25%, 37.5% and 50% time points to compare the model\\r\\nperformance under different missing degrees. (∗ means former, Station means the Non-stationary Transformer.) The standard deviation\\r\\nis within 0.5%. We reproduced the results of Pyraformer by https://github.com/thuml/Time-Series-Library, and copied\\r\\nthe others from GPT4TS [20]. Red: best, Blue: second best.\\r\\nModels Peri-mid GPT4TS Timesnet PatchTST ETS∗ LightTS DLinear FED∗ Station Auto∗ Pyra∗ In∗ Re∗\\r\\nFormer (ours) [20] [13] [19] [29] [15] [14] [17] [11] [12] [10] [16] [28]\\r\\nMask Ratio MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTm1\\r\\n12.5% 0.030 0.108 0.017 0.085 0.019 0.092 0.041 0.130 0.067 0.188 0.075 0.180 0.058 0.162 0.035 0.135 0.026 0.107 0.034 0.124 0.670 0.541 0.047 0.155 0.032 0.126\\r\\n25% 0.031 0.112 0.022 0.096 0.023 0.101 0.044 0.135 0.096 0.229 0.093 0.206 0.080 0.193 0.052 0.166 0.032 0.119 0.046 0.144 0.689 0.553 0.063 0.180 0.042 0.146\\r\\n37.5% 0.034 0.117 0.029 0.111 0.029 0.111 0.049 0.143 0.133 0.271 0.113 0.231 0.103 0.219 0.069 0.191 0.039 0.131 0.057 0.161 0.737 0.581 0.079 0.200 0.063 0.182\\r\\n50% 0.041 0.125 0.040 0.128 0.036 0.124 0.055 0.151 0.186 0.323 0.134 0.255 0.132 0.248 0.089 0.218 0.047 0.145 0.067 0.174 0.770 0.605 0.093 0.218 0.082 0.208\\r\\nAvg 0.034 0.116 0.028 0.105 0.027 0.107 0.047 0.140 0.120 0.253 0.104 0.218 0.093 0.206 0.062 0.177 0.036 0.126 0.051 0.150 0.717 0.570 0.071 0.188 0.055 0.166\\r\\nETTm2\\r\\n12.5% 0.022 0.081 0.017 0.076 0.018 0.080 0.026 0.094 0.108 0.239 0.034 0.127 0.062 0.166 0.056 0.159 0.021 0.088 0.023 0.092 0.394 0.470 0.133 0.270 0.108 0.228\\r\\n25% 0.024 0.084 0.020 0.080 0.020 0.085 0.028 0.099 0.164 0.294 0.042 0.143 0.085 0.196 0.080 0.195 0.024 0.096 0.026 0.101 0.421 0.482 0.135 0.272 0.136 0.262\\r\\n37.5% 0.026 0.089 0.022 0.087 0.023 0.091 0.030 0.104 0.237 0.356 0.051 0.159 0.106 0.222 0.110 0.231 0.027 0.103 0.030 0.108 0.478 0.521 0.155 0.293 0.175 0.300\\r\\n50% 0.029 0.095 0.025 0.095 0.026 0.098 0.034 0.110 0.323 0.421 0.059 0.174 0.131 0.247 0.156 0.276 0.030 0.108 0.035 0.119 0.568 0.560 0.200 0.333 0.211 0.329\\r\\nAvg 0.025 0.087 0.021 0.084 0.022 0.088 0.029 0.102 0.208 0.327 0.046 0.151 0.096 0.208 0.101 0.215 0.026 0.099 0.029 0.105 0.465 0.508 0.156 0.292 0.157 0.280\\r\\nETTh1\\r\\n12.5% 0.060 0.164 0.043 0.140 0.057 0.159 0.093 0.201 0.126 0.263 0.240 0.345 0.151 0.267 0.070 0.190 0.060 0.165 0.074 0.182 0.857 0.609 0.114 0.234 0.074 0.194\\r\\n25% 0.069 0.181 0.054 0.156 0.069 0.178 0.107 0.217 0.169 0.304 0.265 0.364 0.180 0.292 0.106 0.236 0.080 0.189 0.090 0.203 0.829 0.672 0.140 0.262 0.102 0.227\\r\\n37.5% 0.092 0.200 0.072 0.180 0.084 0.196 0.120 0.230 0.220 0.347 0.296 0.382 0.215 0.318 0.124 0.258 0.102 0.212 0.109 0.222 0.830 0.675 0.174 0.293 0.135 0.261\\r\\n50% 0.112 0.220 0.107 0.216 0.102 0.215 0.141 0.248 0.293 0.402 0.334 0.404 0.257 0.347 0.165 0.299 0.133 0.240 0.137 0.248 0.854 0.691 0.215 0.325 0.179 0.298\\r\\nAvg 0.083 0.191 0.069 0.173 0.078 0.187 0.115 0.224 0.202 0.329 0.284 0.373 0.201 0.306 0.117 0.246 0.094 0.201 0.103 0.214 0.842 0.682 0.161 0.279 0.122 0.245\\r\\nETTh2\\r\\n12.5% 0.049 0.140 0.039 0.125 0.040 0.130 0.057 0.152 0.187 0.319 0.101 0.231 0.100 0.216 0.095 0.212 0.042 0.133 0.044 0.138 0.976 0.754 0.305 0.431 0.163 0.289\\r\\n25% 0.051 0.141 0.044 0.135 0.046 0.141 0.061 0.158 0.279 0.390 0.115 0.246 0.127 0.247 0.137 0.258 0.049 0.147 0.050 0.149 1.037 0.774 0.322 0.444 0.206 0.331\\r\\n37.5% 0.055 0.147 0.051 0.147 0.052 0.151 0.067 0.166 0.400 0.465 0.126 0.257 0.158 0.276 0.187 0.304 0.056 0.158 0.060 0.163 1.107 0.800 0.353 0.462 0.252 0.370\\r\\n50% 0.062 0.156 0.059 0.158 0.060 0.162 0.073 0.174 0.602 0.572 0.136 0.268 0.183 0.299 0.232 0.341 0.065 0.170 0.068 0.173 1.193 0.838 0.369 0.472 0.316 0.419\\r\\nAvg 0.054 0.146 0.048 0.141 0.049 0.146 0.065 0.163 0.367 0.436 0.119 0.250 0.142 0.259 0.163 0.279 0.053 0.152 0.055 0.156 1.079 0.792 0.337 0.452 0.234 0.352\\r\\nElectricity\\r\\n12.5% 0.044 0.135 0.080 0.194 0.085 0.202 0.055 0.160 0.196 0.321 0.102 0.229 0.092 0.214 0.107 0.237 0.093 0.210 0.089 0.210 0.297 0.383 0.218 0.326 0.190 0.308\\r\\n25% 0.052 0.149 0.087 0.203 0.089 0.206 0.065 0.175 0.207 0.332 0.121 0.252 0.118 0.247 0.120 0.251 0.097 0.214 0.096 0.220 0.294 0.380 0.219 0.326 0.197 0.312\\r\\n37.5% 0.063 0.166 0.094 0.211 0.094 0.213 0.076 0.189 0.219 0.344 0.141 0.273 0.144 0.276 0.136 0.266 0.102 0.220 0.104 0.229 0.296 0.381 0.222 0.328 0.203 0.315\\r\\n50% 0.079 0.189 0.101 0.220 0.100 0.221 0.091 0.208 0.235 0.357 0.160 0.293 0.175 0.305 0.158 0.284 0.108 0.228 0.113 0.239 0.299 0.383 0.228 0.331 0.210 0.319\\r\\nAvg 0.060 0.160 0.090 0.207 0.092 0.210 0.072 0.183 0.214 0.339 0.131 0.262 0.132 0.260 0.130 0.259 0.100 0.218 0.101 0.225 0.297 0.382 0.222 0.328 0.200 0.313\\r\\nWeather\\r\\n12.5% 0.025 0.037 0.026 0.049 0.025 0.045 0.029 0.049 0.057 0.141 0.047 0.101 0.039 0.084 0.041 0.107 0.027 0.051 0.026 0.047 0.140 0.220 0.037 0.093 0.031 0.076\\r\\n25% 0.026 0.037 0.025 0.052 0.029 0.052 0.031 0.053 0.065 0.155 0.052 0.111 0.048 0.103 0.064 0.163 0.029 0.056 0.030 0.054 0.147 0.229 0.042 0.100 0.035 0.082\\r\\n37.5% 0.028 0.040 0.033 0.060 0.031 0.057 0.035 0.058 0.081 0.180 0.058 0.121 0.057 0.117 0.107 0.229 0.033 0.062 0.032 0.060 0.156 0.240 0.049 0.111 0.040 0.091\\r\\n50% 0.031 0.042 0.037 0.065 0.034 0.062 0.083 0.063 0.102 0.207 0.065 0.133 0.066 0.134 0.183 0.312 0.037 0.068 0.037 0.067 0.164 0.249 0.053 0.114 0.046 0.099\\r\\nAvg 0.028 0.039 0.031 0.056 0.030 0.054 0.060 0.144 0.076 0.171 0.055 0.117 0.052 0.110 0.099 0.203 0.032 0.059 0.031 0.057 0.152 0.235 0.045 0.104 0.038 0.087\\r\\nAverage 0.047 0.123 0.048 0.128 0.050 0.132 0.053 0.159 0.164 0.309 0.123 0.229 0.119 0.225 0.112 0.23 0.057 0.143 0.062 0.151 0.592 0.528 0.165 0.274 0.134 0.241\\r\\n30\\nTable 16: Full results for the anomaly detection task. The P, R and F1 represent the precision, recall and\\r\\nF1-score (%) respectively. F1-score is the harmonic mean of precision and recall. A higher value of P, R and\\r\\nF1 indicates a better performance. (Station means the Non-stationary Transformer.) The standard deviation is\\r\\nwithin 1%. We copied the results from GPT4TS [20]. Red: best, Blue: second best.\\r\\nDatasets SMD MSL SMAP SWaT PSM Avg F1\\r\\nMetrics P R F1 P R F1 P R F1 P R F1 P R F1 (%)\\r\\nTransformer [48] 83.58 76.13 79.56 71.57 87.37 78.68 89.37 57.12 69.70 68.84 96.53 80.37 62.75 96.56 76.07 76.88\\r\\nLogTrans [47] 83.46 70.13 76.21 73.05 87.37 79.57 89.15 57.59 69.97 68.67 97.32 80.52 63.06 98.00 76.74 76.60\\r\\nReformer [28] 82.58 69.24 75.32 85.51 83.31 84.40 90.91 57.44 70.40 72.50 96.53 82.80 59.93 95.38 73.61 77.31\\r\\nInformer [16] 86.60 77.23 81.65 81.77 86.48 84.06 90.11 57.13 69.92 70.29 96.75 81.43 64.27 96.33 77.10 78.83\\r\\nAnomaly [32] 88.91 82.23 85.49 79.61 87.37 83.31 91.85 58.11 71.18 72.51 97.32 83.10 68.35 94.72 79.40 80.50\\r\\nPyraformer [10] 85.61 80.61 83.04 83.81 85.93 84.86 92.54 57.71 71.09 87.92 96.00 91.78 71.67 96.02 82.08 82.57\\r\\nAutoformer [12] 88.06 82.35 85.11 77.27 80.92 79.05 90.40 58.62 71.12 89.85 95.81 92.74 99.08 88.15 93.29 84.26\\r\\nStation [11] 88.33 81.21 84.62 68.55 89.14 77.50 89.37 59.02 71.09 68.03 96.75 79.88 97.82 96.76 97.29 82.08\\r\\nDLinear [14] 83.62 71.52 77.10 84.34 85.42 84.88 92.32 55.41 69.26 80.91 95.30 87.52 98.28 89.26 93.55 82.46\\r\\nLightTS [41] 87.10 78.42 82.53 82.40 75.78 78.95 92.58 55.27 69.21 91.98 94.72 93.33 98.37 95.97 97.15 84.23\\r\\nETSformer [29] 87.44 79.23 83.13 85.13 84.93 85.03 92.25 55.75 69.50 90.02 80.36 84.91 99.31 85.28 91.76 82.87\\r\\nFEDformer [17] 87.95 82.39 85.08 77.14 80.07 78.57 90.47 58.10 70.76 90.17 96.42 93.19 97.31 97.16 97.23 84.97\\r\\nPatchTST [19] 87.26 82.14 84.62 88.34 70.96 78.70 90.64 55.46 68.82 91.1 80.94 85.72 98.84 93.47 96.08 82.79\\r\\nTimesNet [13] 87.91 81.54 84.61 89.54 75.36 81.84 90.14 56.4 69.39 90.75 95.4 93.02 98.51 96.20 97.34 85.24\\r\\nGPT4TS [20] 88.89 84.98 86.89 82.00 82.91 82.45 90.60 60.95 72.88 92.20 96.34 94.23 98.62 95.68 97.13 86.72\\r\\nPeri-midFormer (ours) 87.30 84.65 85.95 89.66 75.31 81.83 90.40 56.10 68.62 91.91 94.95 93.40 98.4 96.01 97.19 85.40\\r\\n31'},\n",
       " {'name': '2310.03775v1.pdf',\n",
       "  'content': 'UNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II - IDENTIFICATION AND OPTIMAL CONTROL 1 `\\r\\nHidden Markov Models for Stock Market Prediction\\r\\nLuigi Catello, Ludovica Ruggiero, Lucia Schiavone, and Mario Valentino\\r\\nAbstract— The stock market presents a challenging environ\\x02ment for accurately predicting future stock prices due to its\\r\\nintricate and ever-changing nature. However, the utilization of\\r\\nadvanced methodologies can significantly enhance the precision\\r\\nof stock price predictions. One such method is Hidden Markov\\r\\nModels (HMMs). HMMs are statistical models that can be used\\r\\nto model the behavior of a partially observable system, making\\r\\nthem suitable for modeling stock prices based on historical data.\\r\\nAccurate stock price predictions can help traders make better\\r\\ninvestment decisions, leading to increased profits.\\r\\nIn this article, we trained and tested a Hidden Markov Model\\r\\nfor the purpose of predicting a stock closing price based on\\r\\nits opening price and the preceding day’s prices. The model’s\\r\\nperformance has been evaluated using two indicators: Mean\\r\\nAverage Prediction Error (MAPE), which specifies the average\\r\\naccuracy of our model, and Directional Prediction Accuracy\\r\\n(DPA), a newly introduced indicator that accounts for the number\\r\\nof fractional change predictions that are correct in sign.\\r\\nIndex Terms—Hidden Markov Models, Stock market, forecast\\x02ing.\\r\\nI. INTRODUCTION\\r\\nP\\r\\nREDICTION of the stock market, with its inherent\\r\\nvolatility and potential for substantial financial gains,\\r\\nhas long captivated the attention of institutional investors,\\r\\nhedge funds, and proprietary trading firms. These sophisticated\\r\\nmarket participants are driven by the desire to make accurate\\r\\npredictions about future price movements and trends in order\\r\\nto gain a competitive edge and maximize their investment\\r\\nreturns.\\r\\nInstitutional investors, such as pension funds and insurance\\r\\ncompanies, manage large pools of capital on behalf of their\\r\\nclients or beneficiaries. The primary objective of these in\\x02vestors is to generate consistent returns over the long term\\r\\nto fulfill their financial obligations. Accurate predictions in\\r\\nthe stock market allow them to identify opportunities and\\r\\nmitigate risks, enhancing portfolio performance and meeting\\r\\ntheir fiduciary responsibilities [1].\\r\\nHedge funds, on the other hand, are private investment\\r\\npartnerships that pool capital from accredited investors. Hedge\\r\\nfund managers seek to generate significant absolute returns,\\r\\noften irrespective of market conditions, by employing diverse\\r\\ninvestment strategies. Accurate predictions empower hedge\\r\\nfunds to identify mispriced securities, exploit market ineffi\\x02ciencies, and construct profitable trading strategies, ultimately\\r\\nattracting investors and earning substantial profits [2].\\r\\nThese sophisticated market participants employ a wide\\r\\nrange of quantitative and qualitative methods to make pre\\x02dictions in the stock market.\\r\\nAlgorithmic strategies rely on computer algorithms that\\r\\nsystematically analyze vast amounts of market data, identify\\r\\npatterns, and generate trade signals. These algorithms are\\r\\nprogrammed to execute trades automatically, often leveraging\\r\\ncomplex mathematical models and statistical techniques. It is\\r\\nestimated that 50 percent of stock trading volume in the U.S.\\r\\nis currently being driven by algorithmic trading [3].\\r\\nWhile algorithmic trading offers significant advantages for\\r\\ninstitutional investors, retail investors should exercise caution\\r\\nwhen relying solely on algorithmic strategies. The success of\\r\\nalgorithmic trading is often attributed to extensive research,\\r\\nrobust infrastructure, and substantial resources that may not\\r\\nbe readily available to individual retail investors. Moreover,\\r\\nmarket conditions can change rapidly, rendering algorithms\\r\\nineffective or even exacerbating losses. Retail investors, lack\\x02ing the same level of resources and risk management capabil\\x02ities, face greater risks when solely depending on algorithmic\\r\\ntrading.\\r\\nIn recent years, many strategies have been developed for\\r\\nalgorithmic trading. Since Hidden Markov Models (HMMs)\\r\\nhave emerged as powerful tools for the prediction of time\\r\\nseries data, we expect them to give promising results in the\\r\\ncontext of the stock market prediction.\\r\\nThis paper is organized as follows: in Section II, we briefly\\r\\nreview various stock market prediction approaches; Section III\\r\\noutlines our theoretical approach with relevant definitions and\\r\\nmathematical equations; in Section IV describes our technical\\r\\nimplementation and solutions to encountered challenges; in\\r\\nSection V we present our results, compare them with other\\r\\nmodels, and introduce a novel evaluation metric; finally, in\\r\\nSection VI we summarize key findings and suggest directions\\r\\nfor further developments.\\r\\nII. PREVIOUS WORKS\\r\\nVarious approaches have been explored in the quest for\\r\\nreliable prediction models as accurate forecasting of stock\\r\\nprices and identification of trends are crucial for investors and\\r\\nfinancial institutions seeking informed decisions.\\r\\nBefore the emergence of artificial intelligence, probabilistic\\r\\ntechniques formed the foundation of stock market forecasting.\\r\\nThese theories encompassed random walk models [4], [5],\\r\\ncorrelation-based methods [6], [7], scaling properties [8], [9],\\r\\nstock market volatility and investor sentiment [10], [11],\\r\\nprobability distributions of stock price returns [12], [13], and\\r\\nother relevant approaches.\\r\\nWith the advancement of machine learning and the increas\\x02ing computational power of computers, several techniques\\r\\nutilizing big data and artificial intelligence algorithms gained\\r\\npopularity. Support Vector Machines (SVM) were quickly\\r\\nrecognized as promising for time series prediction [14], and\\r\\nwere first employed for financial forecasting in 2001 [15].\\r\\nHybrid models combining ARIMA (Autoregressive Integrated\\r\\nMoving Average) and SVM were introduced by Pai et al. in\\r\\n2005 [16]. Neural networks, including specific types coupled\\r\\narXiv:2310.03775v1 [eess.SY] 5 Oct 2023\\nUNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II - IDENTIFICATION AND OPTIMAL CONTROL 2 `\\r\\nwith Genetic Algorithms, have been utilized to detect temporal\\r\\npatterns [17], [18]. Bollen et al. scraped data from Twitter to\\r\\nforecast the stock market based on user mood in 2011 [19].\\r\\nIn 2005, Hassan et al. employed Hidden Markov Models\\r\\nfor time series prediction [20], building upon the concepts\\r\\nintroduced in Rabiner’s influential tutorial published in 1989\\r\\n[21]. Their work demonstrated the effectiveness of HMMs,\\r\\nleading to their widespread adoption in subsequent studies\\r\\n[22], [23], [24].\\r\\nIn this project, we aimed to replicate and extend the\\r\\nfindings reported by Gupta et al. in their 2012 study [25]. We\\r\\nused Mathworks Statistics and Machine Learning Toolbox™.\\r\\nMoreover, we decided to make our project open-sourced [26],\\r\\nhoping to foster collaboration and encourage the research\\r\\ncommunity to build upon our work, replicate our experiments,\\r\\nand explore further enhancements to advance the field.\\r\\nIII. APPROACH\\r\\nA. Markov Chains\\r\\nA Markov chain is a stochastic model that represents a\\r\\nsequence of events or states. In our analysis, we will specifi\\x02cally focus on first-order Markov chains, which adhere to the\\r\\nMarkov property. Let S = {S1, S2, . . . , Sn} be the set of all\\r\\npossible states, and X = {Xk|Xk ∈ S, k = 1, . . . , T} be the\\r\\nstates time series. The Markov property states that for any\\r\\nk ≥ 0 and states X0, . . . , Xk:\\r\\nP(Xk+1 = Sj |Xk = Si, Xk−1, . . . , X0 (1)\\r\\n= P(Xk+1 = Sj|Xk = Si)\\r\\nIn other words, the probability of transitioning to a certain\\r\\nstate Sj at time step k + 1 only depends on the current state\\r\\nSi at time step k and not on any previous states. This property\\r\\nallows us to compute the probability distribution of the Markov\\r\\nchain at any future time step based exclusively on its current\\r\\nstate.\\r\\nFormally, a first order Markov chain is defined by the set of\\r\\nstates S and a transition probability matrix A = [aij ], where\\r\\naij represents the probability of transitioning from state Sito\\r\\nstate Sj in one step.\\r\\nFig. 1. Example of a Markov Chain with three states.\\r\\nThe elements of the transition matrix must satisfy the\\r\\nstandard stochastic constraints:\\r\\n• 0 ≤ aij ≤ 1 ∀i, j.\\r\\n•\\r\\nPn\\r\\nj=1 aij = 1 ∀i.\\r\\nLet xk ∈ R\\r\\nn be the vector containing the probabilities of\\r\\nbeing in each state at time k, the system evolves according to:\\r\\nx\\r\\nT\\r\\nk+1 = x\\r\\nT\\r\\nk A (2)\\r\\nWe refer to the initial distribution of probabilities as\\r\\nπ = {π1, π2 . . . πn}.\\r\\nB. Hidden Markov Models\\r\\nThe model presented above implicitly assumes that each\\r\\nstate corresponds to an observable (physical) event. Markov\\r\\nchains have proven to be valuable tools for modeling sequen\\x02tial data in various domains. However, many real-world scenar\\x02ios involve underlying states that influence observations but are\\r\\nnot directly observable. This limitation led to the development\\r\\nof Hidden Markov Models, which extend the basic Markov\\r\\nchain model by introducing hidden or unobservable states that\\r\\naffect the observed data.\\r\\nThe hidden state process of a HMM is a Markov chain,\\r\\nwhere each state generates an observation having a certain\\r\\nprobability distribution that only depends on the state itself.\\r\\nLet O = {Ok|Ok ∈ O, k = 1, . . . , T} be the observed\\r\\nsequence, where O is the set of possible observations. The\\r\\nhidden states are assumed to evolve according to Equation 2.\\r\\nThe probability of symbol o ∈ O being emitted by state\\r\\nSiis described by the emission probability function bi: o 7→\\r\\nbi(o) ∈ [0, 1], ∀i.\\r\\nBoth in the current and following sections, we refer to the\\r\\nemission probability matrix B = {bij = bi(oj )}.\\r\\nFig. 2. Hidden Markov Chain Example with two states and two possible\\r\\noutputs.\\r\\nC. Applications of HMMs: Common Problems and Solutions\\r\\nThe most common problems analyzed using HMMs include\\r\\nthe evaluation problem, decoding problem, and parameter\\r\\nestimation problem. These problems and their corresponding\\r\\nsolutions have been extensively studied and documented in\\r\\nthe literature [21]. In the following paragraphs, we provide an\\r\\noverview of these problems and their solutions.\\r\\n1) Evaluation Problem: The evaluation problem in HMMs\\r\\ninvolves computing the probability of a known observation\\r\\nsequence, given the model. Specifically, given an HMM with\\r\\nthe parameters π, A, and B, and an observed sequence O, we\\r\\nwant to compute P(O|π, A, B).\\nUNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II - IDENTIFICATION AND OPTIMAL CONTROL 3 `\\r\\nThe procedure used to solve this problem involves comput\\x02ing the forward variable, denoted as αk(i), which represents\\r\\nthe probability of being in state Si at time k and observing\\r\\nthe sequence up to time k. It is computed recursively for each\\r\\ntime step k and state Si.\\r\\nAt the initialization step, the forward variable for the ini\\x02tial time step k = 1 is calculated as: α1(i) = πi\\r\\n· bi(O1), ∀i,\\r\\nwhere πiis the initial probability of being in state Si and\\r\\nbi(O1) is the emission probability for the first observation O1.\\r\\nFor each subsequent time step k > 1, the forward variable\\r\\nαk(i) is computed as the sum of the probabilities of all\\r\\npossible paths that could have generated the observations up to\\r\\ntime k and reached state Si. This is expressed by the following\\r\\nformula:\\r\\nαk(i) =\\r\\n\\uf8eb\\r\\n\\uf8ed\\r\\nXn\\r\\nj=1\\r\\nαk−1(j) · aji\\r\\n\\uf8f6\\r\\n\\uf8f8 · bi(Ok), ∀i (3)\\r\\nBy recursively calculating the forward variables for each\\r\\ntime step, the algorithm evaluates the probability of observing\\r\\nthe entire sequence, given the HMM parameters.\\r\\nFinally, to obtain the overall probability of the observed\\r\\nsequence, the forward procedure computes the sum of the\\r\\nforward variables at the final time step T:\\r\\nP(O|π, A, B) = Xn\\r\\ni=1\\r\\nαT (i) (4)\\r\\n2) Decoding Problem: The decoding problem involves de\\x02termining the most likely sequence of hidden states given an\\r\\nobservation sequence and the model. Given an HMM with\\r\\nthe parameters π, A, and B, and an observed sequence O,\\r\\nwe want to find the hidden state sequence X that maximizes\\r\\nP(X|O, π, A, B).\\r\\nThe solution to the decoding problem is commonly ad\\x02dressed using the Viterbi algorithm.\\r\\nThe algorithm works by iteratively calculating the most\\r\\nlikely path to each state at each time step, considering both\\r\\nthe current observation and the previous states probabilities.\\r\\nAt each time step, it computes the probability of being in each\\r\\nstate and tracks the most probable path leading to that state.\\r\\n3) Parameter Estimation Problem: The parameter estima\\x02tion problem in HMMs involves adjusting the model param\\x02eters to maximize the probability of an observed sequence.\\r\\nGiven an observation sequence O, we want to estimate the\\r\\noptimal values for the parameters π, A, and B that maximize\\r\\nP(O|π, A, B).\\r\\nThe solution to the this problem is typically addressed using\\r\\nthe forward-backward algorithm, also known as the Baum\\x02Welch algorithm.\\r\\nThe Baum-Welch algorithm is a specific implementation\\r\\nof the Expectation–Maximization (EM) algorithm, tailored\\r\\nfor HMMs. It iteratively performs three main steps: forward,\\r\\nbackward, and update step. In the forward step, the algorithm\\r\\ncomputes the probability of being in a specific state at each\\r\\ntime step, given the observed sequence up to that point. This\\r\\nis the evaluation problem explained before.\\r\\nIn the backward step, the algorithm computes the probability\\r\\nof observing the remaining part of the sequence from a given\\r\\nstate at each time step. It complements the forward procedure\\r\\nby computing the backward variable, denoted as βk(i), which\\r\\nrepresents the probability of being in state Si at time k and\\r\\nobserving the remaining sequence from time k + 1 to the end.\\r\\nThe backward variable is computed recursively for each time\\r\\nstep k and state Si. At the initialization step, the backward\\r\\nvariable for the final time step is set as: βT (i) = 1, ∀i.\\r\\nFor each time step k < T, the backward variable βk(i) is\\r\\ncomputed as the sum of the probabilities of all possible paths\\r\\nthat can be generated from state Si at time k to the end of the\\r\\nsequence. This is expressed by the following formula:\\r\\nβk(i) = Xn\\r\\nj=1\\r\\nβk+1(j) · aij · bj (Ok+1) (5)\\r\\nAfter computing the forward and backward variables, they\\r\\nare used to refine the parameters of the HMM based on the\\r\\nobserved sequence. In fact, let ξk(i, j) be the probability of\\r\\nbeing in state Si at time k and state Sj at time k + 1, given\\r\\nthe model and the observation sequence. It can be computed\\r\\nas:\\r\\nξk(i, j) = P(Xk = i, Xk+1 = j, O|π, A, B)\\r\\nP(O|π, A, B)\\r\\n(6)\\r\\n=\\r\\nαk(i) · aij · bj (Ok+1) · βk+1(j)\\r\\nPn\\r\\nr=1\\r\\nPn\\r\\nh=1 αk(r) · arh · bh(Ok+1) · βk+1(h)\\r\\nLet’s also define γk(i) as the probability of being in state Si\\r\\nat time k, given the observation sequence and the model:\\r\\nγk(i) = P(Xk = i|O, π, A, B)\\r\\n=\\r\\nαk(i) · βk(i)\\r\\nPn\\r\\nj=1 αk(j) · βk(j)\\r\\n(7)\\r\\nWe can relate those quantities:\\r\\nγk(i) = Xn\\r\\nj=1\\r\\nξk(i, j) (8)\\r\\nMoreover, PT −1\\r\\nk=1 γk(i) represents the expected number of\\r\\ntransitions from state Si, and PT −1\\r\\nk=1 ξk(i, j) represents the\\r\\nexpected number of transitions from state Sito state Sj .\\r\\nThe update step involves estimating the new values for the\\r\\ninitial state probabilities, transition probabilities, and emission\\r\\nprobabilities. For the initial state probabilities, the updated\\r\\nvalues are calculated as the normalized forward-backward\\r\\nprobabilities at the initial time step:\\r\\nπi = γ1(i) (9)\\r\\nThe transition probabilities are updated based on the ex\\x02pected number of transitions between states, normalized over\\r\\nthe expected number of transitions from each state\\r\\naij =\\r\\nPT −1\\r\\nk=1 ξk(i, j)\\r\\nPT −1\\r\\nk=1 γk(i)\\r\\n(10)\\r\\nThe estimate for the emission probability bj (o) — the prob\\x02ability of observing symbol o from state Sj — is calculated by\\r\\nevaluating the expected number of times in which o is emitted\\nUNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II - IDENTIFICATION AND OPTIMAL CONTROL 4 `\\r\\nfrom state j. This value is then normalized by the expected\\r\\nnumber of transitions from state Sj :\\r\\nbj (o) =\\r\\nPT\\r\\nk=1 \\x10\\r\\nγk(j) · δ(Ok, o)\\r\\n\\x11\\r\\nPT\\r\\nk=1 γk(j)\\r\\n(11)\\r\\nwhere δ(Ok, o) is the Kronecker delta function that evaluates\\r\\nto 1 when Ok is equal to the observed symbol o, and 0\\r\\notherwise.\\r\\nThis process iteratively refines the model parameters until\\r\\nconvergence, where they stabilize, meaning that the likelihood\\r\\nof the sequence is maximized.\\r\\nChoosing the initial conditions wisely is crucial as it can\\r\\nsignificantly impact the estimation of the HMM parameters.\\r\\nBy selecting appropriate initial conditions, the Baum-Welch\\r\\nalgorithm is more likely to converge to some parameters that\\r\\nprovide accurate modeling of the underlying dynamics.\\r\\nD. Gaussian Mixture Models\\r\\nGaussian Mixture Models (GMMs) are powerful probabilis\\x02tic models used to represent complex probability distributions.\\r\\nA GMM is a weighted sum of multiple Gaussian distributions,\\r\\nwhere each component represents a subpopulation within the\\r\\ndata.\\r\\nWhen a single observation is composed by multiple data\\r\\n(multivariate data), GMMs are employed as multivariate mix\\x02ture models. Each Gaussian component in a GMM represents a\\r\\nmultivariate distribution with its own mean vector and covari\\x02ance matrix. The probability density function of a multivariate\\r\\nGMM is given by:\\r\\np(y) = X\\r\\nK\\r\\ni=1\\r\\nci N (y|µi, Σi) (12)\\r\\nwhere K is the number of Gaussian components, ci rep\\x02resents the weight associated with the i-th component, and\\r\\nN (y|µi, Σi) denotes the multivariate Gaussian distribution\\r\\nwith mean vector µi and covariance matrix Σi.\\r\\nThe weights ci satisfy the constraint PK\\r\\ni=1 ci = 1, ensuring\\r\\nthat the probabilities sum up to one. These weights control\\r\\nthe contribution of each Gaussian component to the overall\\r\\ndistribution.\\r\\nGMMs are particularly useful when modeling complex mul\\x02tivariate data that exhibit multiple modes or clusters because\\r\\nthey can capture the inherent structure and variability present\\r\\nin the data.\\r\\nIn the context of Hidden Markov Models, GMMs are\\r\\nemployed to initialize the emission probabilities of the hidden\\r\\nstates. The GMM is fitted on the training dataset: the training\\r\\nalgorithm estimates the parameters of each Gaussian compo\\x02nent, i.e. the mean vectors and covariance matrices. These\\r\\nparameters are used to initialize the emission probabilities of\\r\\nthe corresponding hidden states.\\r\\nIV. TECHNICAL IMPLEMENTATION\\r\\nIn our study, three specific functions provided by the\\r\\nStatistics and Machine Learning Toolbox™ in MATLAB were\\r\\nutilised for the training and testing of the HMM: hmmtrain,\\r\\nhmmdecode and fitgmdist.\\r\\nThe model was trained and tested on the historical daily\\r\\nprices of Apple, IBM and Dell stocks that are publicly avail\\x02able on the web. Each observation in our dataset comprises\\r\\nthree distinct values representing the daily fractional change,\\r\\nfractional high, and fractional low prices.\\r\\nOk =\\r\\n\\x12\\r\\nClose − Open\\r\\nOpen ,\\r\\nHigh − Open\\r\\nOpen ,\\r\\nOpen − Low\\r\\nOpen \\x13\\r\\nOk := fracChange, fracHigh, fracLow\\x01(13)\\r\\nThe array Ok is a three-dimensional array consisting of\\r\\nreal values. Since the probability of guessing any real value\\r\\nis mathematically zero, it was necessary to discretize the\\r\\nobservations. The number of points used for the discretization\\r\\nis specified in Table I.\\r\\nTABLE I\\r\\nNUMBER OF DISCRETE POINTS FOR EACH DIMENSION\\r\\nVariable Number of points\\r\\nfracChange nfC = 50\\r\\nfracHigh nfH = 10\\r\\nfracLow nfL = 10\\r\\nWe used dynamic edges for the discretization: at each train,\\r\\nthe maximum and minimum values of fracChange, fracHigh,\\r\\nand fracLow are computed. We then generated three linearly\\r\\nspaced vectors for the edges, using as the lowest values the\\r\\nmin, and as the highest values the max values. Finally, we\\r\\nassigned to each discrete value its corresponding index in the\\r\\nedges array.\\r\\nFurthermore, compatibility with our monovariate Hidden\\r\\nMarkov Model required mapping the three-dimensional space\\r\\nto a one-dimensional space. The mapping process was accom\\x02plished by enumerating the points within the discrete three\\x02dimensional set, as described in Equation 14. The inverse\\r\\nmapping was achieved using Equations 15:\\r\\nn = (z − 1) · (xmax · ymax) + (y − 1) · xmax + x (14)\\r\\nz =\\r\\n\\x16\\r\\nn − 1\\r\\nxmax · ymax \\x17\\r\\n+ 1\\r\\ny =\\r\\n\\x16\\r\\nn − 1 − (z − 1) · xmax · ymax\\r\\nxmax \\x17\\r\\n+ 1 (15)\\r\\nx = (n − 1) mod (xmax) + 1\\r\\nA. Training\\r\\nDuring the initial trains we assumed four underlying hidden\\r\\nstates, where each state generates outputs represented by a\\r\\nGMM with four components. We then re-trained our model\\r\\nvarying the values. The specific parameters used for each\\r\\ntrain can be found on our GitHub repository [26]. The pa\\x02rameters of these GMMs were estimated using the MATLAB\\r\\nfunction fitgmdist, which optimizes the model using the\\r\\nExpectation-Maximization (EM) algorithm. The initial param\\x02eters of the GMM are obtained through k-means clustering.\\nUNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II - IDENTIFICATION AND OPTIMAL CONTROL 5 `\\r\\nThe resulting probability density function served as the initial\\r\\nestimate for the emission matrix. Moreover, we assigned a\\r\\nuniform distribution of probabilities as the initial estimate for\\r\\nthe transition matrix.\\r\\nThe training data for the HMM is constructed by using a\\r\\nrolling window approach. In this approach, each observation\\r\\nsequence spans a fixed duration of 10 days. We refer to this\\r\\nduration as latency. The window is shifted incrementally along\\r\\nthe training period: the first sequence captures observations\\r\\nfrom the initial time point, while each following sequence\\r\\nincorporates new observations by sliding the window by one\\r\\nday. The dataset is then provided as input to the MATLAB\\r\\nfunction hmmtrain. This function utilizes the Baum-Welch\\r\\nalgorithm to estimate the transition and emissions matrices.\\r\\nThese estimations are initialized with the initial guesses dis\\x02cussed previously.\\r\\nB. Prediction\\r\\nFollowing the training phase, we proceeded to test our\\r\\nmodel by predicting the stock daily close prices for different\\r\\ntime frames. For each day d within the target period, the\\r\\nprediction process involved the following steps:\\r\\n1) We considered the last latency − 1 = 9 observations\\r\\navailable. These observations represent the preceding 9\\r\\ndays.\\r\\n2) Next, we appended each possible output for the current\\r\\nday d, creating a 10-day sequence. This sequence now\\r\\nencompasses the 9 historical observations and one po\\x02tential observation for the next day. There are nfc·nfH·nfL\\r\\npossibilities for the current day.\\r\\n3) We computed the probability for each sequence to be\\r\\ngenerated from our trained model. Finally, we selected\\r\\nthe observation with the highest emission probability as\\r\\nthe observation for the next day.\\r\\nIn certain cases, there may arise situations where the prob\\x02ability of emitting the historical observations, along with any\\r\\nhypothetical observation, becomes 0 or very close to 0. This\\r\\ncan occur due to numerical errors or limitations inherent the\\r\\nmodel. However, we have found that incorporating a dynamic\\r\\nwindow can enhance the performance of our model.\\r\\nTo address the issue, we have modified the prediction\\r\\nalgorithm as follows: if the highest probability obtained is 0,\\r\\nwe repeat the prediction algorithm while gradually reducing\\r\\nthe latency by one day. By reducing the latency, we aim to find\\r\\na viable solution where the emitted probabilities are non-zero.\\r\\nThis process of reducing latency is repeated iteratively until a\\r\\nsolution is found, while ensuring that the historical sequence\\r\\nremains sufficiently long. In our case, we have set a minimum\\r\\nrequirement of four days for the historical sequence.\\r\\nV. RESULTS\\r\\nWe used multiple metrics to evaluate the performance of\\r\\nour models. Mean Absolute Percentage Error (MAPE) is\\r\\ncalculated between the actual and predicted stock closing\\r\\nprices.\\r\\nMAPE =\\r\\n1\\r\\nnp\\r\\nXnp\\r\\ni=1\\r\\n|pi − ci|\\r\\n|ci|\\r\\n· 100% (16)\\r\\nOn day i, piis the predicted stock closing price, ciis the actual\\r\\nstock closing price, and np is the total number of predictions.\\r\\nTable II shows our results on two different stocks, compared\\r\\nto the results from other papers [25], [24].\\r\\nTABLE II\\r\\nCOMPARISON OF MEAN ABSOLUTE PERCENTAGE ERROR\\r\\nStock\\r\\nName\\r\\nOur\\r\\nHMM\\r\\nHMM\\x02MAP [25]HMM + Fuzzy\\r\\nModel [24]\\r\\nARIMA /\\r\\nANN\\r\\nApple\\r\\nInc.\\r\\n1.50 1.51 1.77 1.80\\r\\nIBM\\r\\nCorp.\\r\\n0.68 0.61 0.78 0.97\\r\\nPredicting the movement of stock prices for a single day is\\r\\na challenging task in itself. Achieving accuracy in predicting\\r\\nmultiple consecutive days becomes even more difficult and\\r\\noften approaches the realm of impossibility. Recognizing\\r\\nthe potential utility of predicting whether the stock value\\r\\nwill increase or decrease during the day, we introduced a\\r\\nnovel evaluation metric called Directional Prediction Accuracy\\r\\n(DPA). DPA measures the percentage of correct directional\\r\\npredictions, providing valuable information about the accuracy\\r\\nof our model.\\r\\nDPA =\\r\\n1\\r\\nnp\\r\\nXnp\\r\\ni=1\\r\\nδ\\r\\n\\r\\nsgn(pi − si),sgn(ci − si)\\r\\n\\x01\\r\\n· 100% (17)\\r\\nIn Equation 17, np is the total number of predictions, δ is\\r\\nthe Kronecker delta function, piis the predicted stock closing\\r\\nprice, cithe actual closing price, and siis the stock opening\\r\\nprice. Table III demonstrates that MAPE and DPA are not\\r\\nequivalent measures as they are in general not correlated.\\r\\nTABLE III\\r\\nDIRECTIONAL PREDICTION ACCURACY\\r\\nStock Name MAPE DPA Training Testing\\r\\nApple Inc. 1.50% 52.11% 2003-02-10\\r\\n2004-09-10\\r\\n2004-09-13\\r\\n2005-01-21\\r\\n1 1.73% 63.27% 2003-02-10\\r\\n2004-09-10\\r\\n2004-10-13\\r\\n2005-01-21\\r\\n1.05% 53.06% 2021-01-04\\r\\n2022-01-03\\r\\n2023-01-03\\r\\n2023-06-30\\r\\n1.01% 40.57% 2017-01-03\\r\\n2019-01-03\\r\\n2023-01-03\\r\\n2023-06-30\\r\\nIBM Corp. 0.77% 54.55% 2003-02-10\\r\\n2004-09-10\\r\\n2004-10-13\\r\\n2005-01-21\\r\\n0.82% 57.58% 2003-02-10\\r\\n2004-09-10\\r\\n2004-10-13\\r\\n2005-01-21\\r\\n0.68% 60.23% 2003-02-10\\r\\n2004-09-10\\r\\n2004-09-13\\r\\n2005-01-21\\r\\n2 0.88% 52.73% 2021-01-04\\r\\n2023-01-03\\r\\n2023-01-04\\r\\n2023-07-11\\r\\nDell Inc. 1.45% 60.32% 2021-01-04\\r\\n2022-01-03\\r\\n2023-01-03\\r\\n2023-07-11\\r\\nFigure 4 remarks the difference between the two indicators:\\r\\nthe candlestick chart shows the actual prices for AAPL during\\r\\n1Figure 3 shows predicted and actual close values for this train. Green lines\\r\\nindicate predictions that guessed right the sign of the fractional change with\\r\\nrespect to the opening price. Therefore, 63.27% of the predictions are correct\\r\\nin sign. The high MAPE reflects the accuracy of the predictions that often\\r\\ndeviate from the actual close value, although being correct in sign.\\r\\n2The results of this train are shown in Figure 5: since the MAPE is low,\\r\\npredicted close values are, on average, more accurate.\\nUNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II - IDENTIFICATION AND OPTIMAL CONTROL 6 `\\r\\nFig. 3. Actual vs Forecasted close prices for AAPL.\\r\\nFig. 4. Candlestick chart along with predicted close values for AAPL\\r\\nthe testing period 2004-10-13 to 2005-01-21. The dots show\\r\\nthe predicted close values for the corresponding day. When the\\r\\ndots are green, they contribute to increasing the DPA. They\\r\\nonly improve the MAPE when they get closer to the actual\\r\\nclose value.\\r\\nVI. CONCLUSIONS AND FURTHER DEVELOPMENTS\\r\\nIn this study, we have undertaken a comprehensive im\\x02plementation of the Stock Prediction Hidden Markov Model\\r\\noriginally proposed by Gupta et al. [25]. Building upon their\\r\\nfoundational work, we aimed to assess the model’s perfor\\x02mance on diverse datasets and explore its effectiveness in\\r\\npredicting stock market movements. By carefully replicating\\r\\nand extending their approach, we have conducted a rigorous\\r\\nevaluation, comparing the results with benchmark models to\\r\\ngain valuable insights into the model capabilities and limita\\x02tions.\\r\\nOur model was trained on a time period of one to two\\r\\nyears and used to make predictions on a different time span,\\r\\ndemonstrating its flexibility and re-usability. The complete\\r\\nsource code, along with pre-trained models and a summary\\r\\nof their characteristics, is available on GitHub [26].\\r\\nOur implementation underwent rigorous testing on various\\r\\nstocks, and we compared the results with those obtained from\\r\\nHMM-MAP, HMM-Fuzzy, ARIMA, and ANN models. We\\r\\nconducted thorough exploration of different hyperparameters,\\r\\nmeasuring their impact in search of optimal values. The\\r\\noutcomes demonstrated a significantly lower Mean Absolute\\r\\nPercentage Error compared to HMM-Fuzzy, ARIMA, and\\r\\nANN models when trained on the same years as those in\\r\\n[25] and [24]. Furthermore, to gain additional insights into\\r\\nthe efficiency of our model, we developed a novel evaluation\\r\\nmetric named Directional Prediction Accuracy (DPA). The\\r\\nDPA allowed us to assess the accuracy of our predictions in\\r\\ncapturing stock price movements, providing valuable informa\\x02tion for model performance assessment.\\r\\nFor further improvement, we propose fine-tuning the Emis\\x02sion and Transmission matrices on the latest time window each\\r\\nday before making predictions. This approach involves training\\r\\nthe main model on historical data, then updating the matrices\\r\\nwith data from a more recent period just before making\\r\\neach prediction. For example, we could train the main model\\r\\nfrom 2021-01-01 to 2023-01-01 and fine-tune the matrices\\r\\nusing data from 2022-06-01 to 2023-06-01 before making\\r\\na prediction for 2023-06-02. This process could potentially\\r\\ncapture more recent market trends and improve the accuracy\\r\\nof predictions.\\r\\nIn conclusion, our implementation of the Stock Prediction\\r\\nHMM exhibited strong predictive capabilities, outperforming\\r\\nother benchmark models. The proposed fine-tuning approach\\r\\nholds promise for enhancing future predictions and warrants\\nUNIVERSITA DEGLI STUDI DI NAPOLI FEDERICO II - IDENTIFICATION AND OPTIMAL CONTROL 7 `\\r\\nFig. 5. Actual vs Forecasted close prices for IBM\\r\\nfurther investigation to capitalize on recent market dynamics.\\r\\nOur work contributes to the growing field of stock market\\r\\nprediction and provides valuable insights for traders and\\r\\nfinancial analysts.\\r\\nJuly 2023\\r\\nREFERENCES\\r\\n[1] “Institutional investors definition — nasdaq.” [Online]. Available:\\r\\nhttps://www.nasdaq.com/glossary/i/institutional-investors\\r\\n[2] “Investor bulletin: Hedge funds.” [Online]. Avail\\x02able: https://www.investor.gov/introduction-investing/investing-basics/\\r\\ninvestment-products/private-investment-funds/hedge-funds\\r\\n[3] “Algo or algorithmic trading definition — nasdaq.” [Online]. Available:\\r\\nhttps://www.nasdaq.com/glossary/a/algo-trading\\r\\n[4] E. F. Fama, “Random walks in stock market prices,” Financial Analysts\\r\\nJournal, vol. 51, no. 1, pp. 75–80, 1995, publisher: Routledge.\\r\\n[5] A. W. Lo and A. C. MacKinlay, “Stock market prices do not follow\\r\\nrandom walks: Evidence from a simple specification test,” The Review\\r\\nof Financial Studies, vol. 1, no. 1, pp. 41–66, 2015-04.\\r\\n[6] F. Longin and B. Solnik, “Is the correlation in international equity returns\\r\\nconstant: 1960–1990?” Journal of International Money and Finance,\\r\\nvol. 14, no. 1, pp. 3–26, 1995.\\r\\n[7] Z. Ding, C. W. J. Granger, and R. F. Engle, “A long memory property of\\r\\nstock market returns and a new model,” Journal of Empirical Finance,\\r\\nvol. 1, no. 1, pp. 83–106, 1993.\\r\\n[8] T. D. Matteo, T. Aste, and M. M. Dacorogna, “Scaling behaviors in\\r\\ndifferently developed markets,” Physica A: Statistical Mechanics and\\r\\nits Applications, vol. 324, no. 1, pp. 183–188, 2003.\\r\\n[9] R. N. Mantegna and H. E. Stanley, “Scaling behaviour in the dynamics\\r\\nof an economic index,” Nature, vol. 376, no. 6535, pp. 46–49, 1995.\\r\\n[10] K. R. French, G. W. Schwert, and R. F. Stambaugh, “Expected stock\\r\\nreturns and volatility,” Journal of Financial Economics, vol. 19, no. 1,\\r\\npp. 3–29, 1987.\\r\\n[11] W. Y. Lee, C. X. Jiang, and D. C. Indro, “Stock market volatility,\\r\\nexcess returns, and the role of investor sentiment,” Journal of Banking\\r\\n& Finance, vol. 26, no. 12, pp. 2277–2299, 2002.\\r\\n[12] X. Gabaix, P. Gopikrishnan, V. Plerou, and H. E. Stanley, “A theory\\r\\nof power-law distributions in financial market fluctuations,” Nature, vol.\\r\\n423, no. 6937, pp. 267–270, 2003, publisher: Nature Publishing Group\\r\\nUK London.\\r\\n[13] R. N. Mantegna, Z. Palagyi, and H. E. Stanley, “Applications of ´\\r\\nstatistical mechanics to finance,” Physica A: Statistical Mechanics and\\r\\nits Applications, vol. 274, no. 1, pp. 216–221, 1999.\\r\\n[14] S. Mukherjee, E. Osuna, and F. Girosi, “Nonlinear prediction of chaotic\\r\\ntime series using support vector machines,” in Neural Networks for\\r\\nSignal Processing VII. Proceedings of the 1997 IEEE Signal Processing\\r\\nSociety Workshop, 1997, pp. 511–520.\\r\\n[15] F. E. Tay and L. Cao, “Application of support vector machines in\\r\\nfinancial time series forecasting,” Omega, vol. 29, no. 4, pp. 309–317,\\r\\n2001.\\r\\n[16] P.-F. Pai and C.-S. Lin, “A hybrid ARIMA and support vector machines\\r\\nmodel in stock price forecasting,” Omega, vol. 33, no. 6, pp. 497–505,\\r\\n2005.\\r\\n[17] H. jung Kim and K. shik Shin, “A hybrid approach based on neural\\r\\nnetworks and genetic algorithms for detecting temporal patterns in stock\\r\\nmarkets,” Applied Soft Computing, vol. 7, no. 2, pp. 569–576, 2007.\\r\\n[18] K. jae Kim and I. Han, “Genetic algorithms approach to feature\\r\\ndiscretization in artificial neural networks for the prediction of stock\\r\\nprice index,” Expert Systems with Applications, vol. 19, no. 2, pp. 125–\\r\\n132, 2000.\\r\\n[19] J. Bollen, H. Mao, and X. Zeng, “Twitter mood predicts the stock\\r\\nmarket,” Journal of Computational Science, vol. 2, no. 1, pp. 1–8, 2011.\\r\\n[20] M. R. Hassan and B. Nath, “Stock market forecasting using hidden\\r\\nmarkov model: a new approach,” in 5th International Conference on\\r\\nIntelligent Systems Design and Applications (ISDA’05), 2005, pp. 192–\\r\\n196.\\r\\n[21] L. Rabiner, “A tutorial on hidden markov models and selected applica\\x02tions in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2,\\r\\npp. 257–286, 1989.\\r\\n[22] M. Zhang, X. Jiang, Z. Fang, Y. Zeng, and K. Xu, “High-order hidden\\r\\nmarkov model for trend prediction in financial time series,” Physica A:\\r\\nStatistical Mechanics and its Applications, vol. 517, pp. 1–12, 2019.\\r\\n[23] Y. Zhou and J. Zheng, “Research on stock market forecasting model\\r\\nbased on data mining and markov chain,” in 2022 IEEE 2nd Interna\\x02tional Conference on Mobile Networks and Wireless Communications\\r\\n(ICMNWC), 2022, pp. 1–5.\\r\\n[24] M. R. Hassan, “A combination of hidden markov model and fuzzy\\r\\nmodel for stock market forecasting,” Neurocomputing, vol. 72, no. 16,\\r\\npp. 3439–3446, 2009.\\r\\n[25] A. Gupta and B. Dhingra, “Stock market prediction using hidden markov\\r\\nmodels,” in 2012 Students Conference on Engineering and Systems,\\r\\n2012, pp. 1–4.\\r\\n[26] (2023) valentinomario/HMM-stock-market-prediction. [Online]. Avail\\x02able: https://github.com/valentinomario/HMM-Stock-Market-Prediction'},\n",
       " {'name': '1810.09936v2.pdf',\n",
       "  'content': 'Enhancing Stock Movement Prediction with Adversarial Training\\r\\nFuli Feng1, Huimin Chen2, Xiangnan He3∗, Ji Ding4, Maosong Sun2, Tat-Seng Chua1\\r\\n1National University of Singapore, 2Tsinghua Unversity, 3University of Science and Technology of\\r\\nChina, 4University of Illinois at Urbana-Champaign, jiding2@illinois.edu, sms@tsinghua.edu.cn,\\r\\n{fulifeng93,huimchen1994,xiangnanhe,chuats@gmail.com}@gmail.com\\r\\nAbstract\\r\\nThis paper contributes a new machine learning so\\x02lution for stock movement prediction, which aims\\r\\nto predict whether the price of a stock will be up or\\r\\ndown in the near future. The key novelty is that we\\r\\npropose to employ adversarial training to improve\\r\\nthe generalization of a neural network prediction\\r\\nmodel. The rationality of adversarial training here\\r\\nis that the input features to stock prediction are typ\\x02ically based on stock price, which is essentially a\\r\\nstochastic variable and continuously changed with\\r\\ntime by nature. As such, normal training with static\\r\\nprice-based features (e.g., the close price) can eas\\x02ily overfit the data, being insufficient to obtain reli\\x02able models. To address this problem, we propose\\r\\nto add perturbations to simulate the stochasticity of\\r\\nprice variable, and train the model to work well un\\x02der small yet intentional perturbations. Extensive\\r\\nexperiments on two real-world stock data show that\\r\\nour method outperforms the state-of-the-art solu\\x02tion [Xu and Cohen, 2018] with 3.11% relative im\\x02provements on average w.r.t. accuracy, validating\\r\\nthe usefulness of adversarial training for stock pre\\x02diction task.\\r\\n1 Introduction\\r\\nStock market is one of the largest financial markets, hav\\x02ing reached a total value of 80 trillion dollars1\\r\\n. Predicting\\r\\nthe future status of a stock has always been of great inter\\x02est to many players in a stock market. While the exact price\\r\\nof a stock is known to be unpredictable [Walczak, 2001;\\r\\nNguyen et al., 2015], research efforts have been focused\\r\\non predicting the stock price movement — e.g., whether\\r\\nthe price will go up/down, or the price change will ex\\x02ceed a threshold — which is more achievable than stock\\r\\nprice prediction [Adebiyi et al., 2014; Feng et al., 2018;\\r\\nXu and Cohen, 2018].\\r\\nStock movement prediction can be addressed as a classi\\x02fication task. After defining the label space and features to\\r\\n∗Xiangnan He is the corresponding author.\\r\\n1https://data.worldbank.org/indicator/CM.\\r\\nMKT.TRAD.CD?view=chart.\\r\\n(a) Training (b) Validation\\r\\nFigure 1: Training process of Attentive LSTM with L2 regu\\x02larization coefficient of 0, 0.01, and 0.1.\\r\\ndescribe a stock at a time, we can apply standard supervised\\r\\nlearning methods such as support vector machines [Huang et\\r\\nal., 2005] and neural networks [Xu and Cohen, 2018] to build\\r\\nthe predictive model. Although technically feasible, we argue\\r\\nthat such methods could suffer from weak generalization due\\r\\nto the highly stochastic property of stock market. Figure 1\\r\\nprovides an empirical evidence on the weak generalization,\\r\\nwhere we split the data into training and validation by time,\\r\\nand train an Attentive LSTM model [Qin et al., 2017] on the\\r\\nhistorical prices of stocks to predict their movements. From\\r\\nFigure 1(a), we can see the training loss gradually decreases\\r\\nwith more training epochs, which is as expected. However,\\r\\nthe validation loss shown in Figure 1(b) does not exhibit a\\r\\ndecreasing trend; instead, it only fluctuates around the initial\\x02ization state without a clear pattern. In other words, the bene\\x02fits of the model learned on training examples do not translate\\r\\nto improvements on predicting unknown validation examples.\\r\\nWe have thoroughly explored the L2 regularization (results of\\r\\ndifferent lines), a common technique to improve model gen\\x02eralization, however, the situation has not improved.\\r\\nWe postulate the reason is that standard classification meth\\x02ods are assumed to learn from static inputs, such as pixel\\r\\nvalues in images and term frequencies in documents. When\\r\\ndealing with stochastic variable such as stock price, the static\\r\\ninput assumption does not hold and such methods fail to gen\\x02eralize well. Specifically, existing methods for stock pre\\x02diction typically feed into price-based features, such as the\\r\\nprice at a particular time-step or average price on multi\\x02ple time-steps [Edwards et al., 2007; Nelson et al., 2017].\\r\\nSince a stock’s price continuously changes with time (during\\r\\nmarket hours), price-based features are essentially stochastic\\r\\nvariables, being fundamentally different from the traditional\\r\\narXiv:1810.09936v2 [q-fin.TR] 1 Jun 2019\\nstatic inputs. To be more specific, the features of a training in\\x02stance can be seen as a “sample” drawn from the distribution\\r\\nof input variables at a particular time-step. Without properly\\r\\nhandling the stochasticity of input variables, the method can\\r\\neasily overfit the training data and suffer from weak general\\x02ization ability.\\r\\nIn this work, we propose to employ adversarial training to\\r\\naccount for the stochastic property of stock market to learn\\r\\nstock movement prediction model. Our primary considera\\x02tion is that given a training example at a particular time-step\\r\\nwith fixed input features, the trained model is expected to\\r\\ngenerate the same prediction on other samples drawn from\\r\\nthe inherent distribution of input variables. To implement this\\r\\nidea, we can generate additional samples (simulation of the\\r\\nstochasticity) by adding small perturbations on input features,\\r\\nand train the model to perform well on both clean examples\\r\\nand perturbed examples. It is the adversarial training method\\r\\nthat has been commonly used in computer vision tasks [Ku\\x02rakin et al., 2017]. However, the problem is that the features\\r\\nto stock prediction models are usually sequential (see Figure\\r\\n2), such that adding perturbations on the features of all time\\r\\nunits can be very time-consuming; moreover, it may cause\\r\\nunintentional interactions among the perturbations of differ\\x02ent units which are uncontrollable. To resolve the concern,\\r\\nwe instead add perturbations on the high-level prediction fea\\x02tures of the model, e.g., the last layer which is directly pro\\x02jected to the final prediction. Since most deep learning meth\\x02ods learn abstract representation in the higher layers, their\\r\\nsizes are usually much smaller than the input size. As such,\\r\\nadding perturbations to high-level features is more efficient,\\r\\nand meanwhile it can also retain the stochasticity.\\r\\nWe implement our adversarial training proposal on an At\\x02tentive LSTM model, which is a highly expressive model for\\r\\nsequential data. We add perturbations to the prediction fea\\x02tures of the last layer, and dynamically optimize the pertur\\x02bations to make them change the model’s output as much as\\r\\npossible. We then train the model to make it perform well\\r\\non both clean features and perturbed features. As such, the\\r\\nadversarial training process can be understood as enforcing a\\r\\ndynamic regularizer, which stabilizes the model training and\\r\\nmakes the model perform well under stochasticity.\\r\\nThe main contributions of this paper are summarized as:\\r\\n• We investigate the generalization difficulty in stock move\\x02ment prediction and highlight the necessity of dealing with\\r\\nthe stochastic property of input features.\\r\\n• We propose an adversarial training solution to address the\\r\\nstochastic challenge, and implement it on a deep learning\\r\\nmodel for stock movement prediction.\\r\\n• We conduct extensive experiments on two public bench\\x02marks, validating improvements over several state-of-the\\x02art methods and showing that adversarial learning makes\\r\\nthe classifier more robust and more generalizable.\\r\\n2 Problem Formulation\\r\\nWe use bold capital letters (e.g., X) and bold lower letters\\r\\n(e.g., x) to denote matrices and vectors, respectively. In ad\\x02dition, normal lower case letters (e.g., x) and Greek letters\\r\\nFigure 2: Illustration of the Attentive LSTM.\\r\\n(e.g., λ) are used to represent scalars and hyper-parameters,\\r\\nrespectively. All vectors are in column form, if not otherwise\\r\\nspecified. The symbols tanh and σ stand for the hyperbolic\\r\\ntangent function and sigmoid function, respectively.\\r\\nThe formulation of stock movement prediction task is to\\r\\nlearn a prediction function yˆ\\r\\ns = f(Xs\\r\\n; Θ) which maps\\r\\na stock (s) from its sequential features (Xs) to the label\\r\\nspace. In other words, the function f with parameters Θ\\r\\naims to predict the movement of stock s at the next time-step\\r\\nfrom the sequential features Xsin the latest T time-steps.\\r\\nXs = [x\\r\\ns\\r\\n1\\r\\n, · · · , x\\r\\ns\\r\\nT\\r\\n] ∈ R\\r\\nD×T\\r\\nis a matrix which represents\\r\\nthe sequential input features (e.g., open and close prices, as\\r\\ndetailed in Table 1) in the lag of past T time-steps, where D\\r\\nis the dimension of features.\\r\\nAssuming that we have S stocks, we learn the pre\\x02diction function by fitting their ground truth labels y =\\r\\n[y\\r\\n1\\r\\n, · · · , yS] ∈ R\\r\\nS, where ys\\r\\n(1/-1) is the ground truth la\\x02bel of stock s in the next time-step. We then formally define\\r\\nthe problem as:\\r\\nInput: A set of training examples {(Xs, ys)}.\\r\\nOutput: A prediction function f(Xs; Θ), predicting the\\r\\nmovement of stock s in the following time-step.\\r\\nIn the practical scenario, we could typically access a long\\r\\nhistory of each stock, and construct many training examples\\r\\nfor each stock by moving the lag along the history. Never\\x02theless, we use a simplified formulation without loss of gen\\x02erality by only considering one specific lag (i.e., one training\\r\\nexample for each stock) for briefness of presenting the pro\\x02posed method.\\r\\n3 Adversarial Attentive LSTM (Adv-ALSTM)\\r\\n3.1 Attentive LSTM\\r\\nThe Attentive LSTM (ALSTM) mainly contains four compo\\x02nents: feature mapping layer, LSTM layer, temporal atten\\x02tion, and prediction layer, as shown in Figure 2.\\r\\nFeature mapping layer. Previous work shows that a\\r\\ndeeper input gate would benefit the modeling of temporal\\r\\nstructures of LSTM [Graves et al., 2013; Wu et al., 2018].\\r\\nInspired by their success, we employ a fully connected layer\\r\\nto project the input features into a latent representation. At\\r\\neach time-step, it performs as ms\\r\\nt = tanh(Wmx\\r\\ns\\r\\nt + bm),\\r\\nwhich projects the input features to a latent space with dimen\\x02sionality of E. Wm ∈ R\\r\\nE×D and bm ∈ RE are parameters\\r\\nto be learned.\\nLSTM layer. Owing to its ability to capture long-term\\r\\ndependency, LSTM has been widely used to process sequen\\x02tial data [Qin et al., 2017; Chen et al., 2018a]. The general\\r\\nidea of LSTM is to recurrently project the input sequence\\r\\ninto a sequence of hidden representations. At each time-step,\\r\\nthe LSTM learns the hidden representation (h\\r\\ns\\r\\nt\\r\\n) by jointly\\r\\nconsidering the input (ms\\r\\nt\\r\\n) and previous hidden representa\\x02tion (h\\r\\ns\\r\\nt−1\\r\\n) to capture sequential dependency. We formulate\\r\\nit as h\\r\\ns\\r\\nt = LSTM(mst\\r\\n, h\\r\\ns\\r\\nt−1\\r\\n) of which the detailed for\\x02mulation can be referred to [Hochreiter and Schmidhuber,\\r\\n1997]. To capture the sequential dependencies and tempo\\x02ral patterns in the historical stock features, an LSTM layer\\r\\nis applied to map [ms\\r\\n1\\r\\n, · · · ,ms\\r\\nT\\r\\n] into hidden representations\\r\\n[h\\r\\ns\\r\\n1\\r\\n, · · · , h\\r\\ns\\r\\nT\\r\\n] ∈ R\\r\\nU×T with the dimension of U.\\r\\nTemporal Attention Layer. The attention mechanism has\\r\\nbeen widely used in LSTM-based solutions for sequential\\r\\nlearning problems[Cho et al., 2014; Chen et al., 2018a]. The\\r\\nidea of attention is to compress the hidden representations at\\r\\ndifferent time-steps into an overall representation with adap\\x02tive weights. The attention mechanism aims to model the fact\\r\\nthat data at different time-steps could contribute differently\\r\\nto the representation of the whole sequence. For stock repre\\x02sentation, status at different time-steps might also contribute\\r\\ndifferently. For instance, days with maximum and minimum\\r\\nprices in the lag might have higher contributions to the over\\x02all representation. As such, we use an attention mechanism\\r\\nto aggregate the hidden representations as,\\r\\na\\r\\ns =\\r\\nX\\r\\nT\\r\\nt=1\\r\\nα\\r\\ns\\r\\nth\\r\\ns\\r\\nt\\r\\n, αs\\r\\nt =\\r\\nexpαe\\r\\ns\\r\\nt\\r\\nPT\\r\\nt=1 expαe\\r\\ns\\r\\nt\\r\\n,\\r\\nαe\\r\\ns\\r\\nt = u\\r\\nT\\r\\na\\r\\ntanh(Wah\\r\\ns\\r\\nt + ba),\\r\\n(1)\\r\\nwhere Wa ∈ R\\r\\nE\\r\\n0×U , ba and ua ∈ R\\r\\nE\\r\\n0\\r\\nare parameters to be\\r\\nlearned; and a\\r\\ns\\r\\nis the aggregated representation that encodes\\r\\nthe overall patterns in the sequence.\\r\\nPrediction Layer. Instead of directly making prediction\\r\\nfrom a\\r\\ns\\r\\n, we first concatenate a\\r\\ns with the last hidden state\\r\\nh\\r\\ns\\r\\nT\\r\\ninto the final latent representation of stock s,\\r\\ne\\r\\ns = [asT\\r\\n, h\\r\\ns\\r\\nT\\r\\nT\\r\\n]\\r\\nT\\r\\n, (2)\\r\\nwhere e\\r\\ns ∈ R2U . The intuition behind is to further emphasize\\r\\nthe most recent time-step, which is believed to be informative\\r\\nfor the following movement [Fama and French, 2012]. With\\r\\ne\\r\\ns\\r\\n, we use a fully connected layer as the predictive function\\r\\nto estimate the classification confidence yˆ\\r\\ns = wT\\r\\np e\\r\\ns + bp.\\r\\nNote that the final prediction is sign(ˆy\\r\\ns\\r\\n).\\r\\n3.2 Adversarial Training\\r\\nAs with most classification solutions, the normal way of\\r\\ntraining the ALSTM is to minimize an objective function Γ:\\r\\nXS\\r\\ns=1\\r\\nl(y\\r\\ns\\r\\n, yˆ\\r\\ns\\r\\n) + α\\r\\n2\\r\\nkΘk\\r\\n2\\r\\nF , l(y\\r\\ns\\r\\n, yˆ\\r\\ns\\r\\n) = max(0, 1 − y\\r\\ns\\r\\nyˆ\\r\\ns\\r\\n). (3)\\r\\nThe first term is hinge loss [Rosasco et al., 2004], which is\\r\\nwidely used for optimizing classification models (more rea\\x02sons of choosing it is further explained in the end of the sec\\x02tion). The second term is a regularizer on the trainable pa\\x02rameters to prevent overfitting.\\r\\nFigure 3: Illustration of the Adversarial Attentive LSTM.\\r\\nDespite the wide usage of normal training, we argue that\\r\\nit is inappropriate for learning stock prediction models. This\\r\\nis because normal training assumes that the inputs are static,\\r\\nignoring the stochastic property of these features (a training\\r\\nexample is a sample drawn from the stochastic distribution of\\r\\ninput variables). Note that the features are calculated from\\r\\nstock price, which continuously changes with time and is af\\x02fected by stochastic trading behaviours at a particular time\\x02step [Musgrave, 1997]. As such, normal training might lead\\r\\nto model that overfits the data and lacks generalization ability\\r\\n(as shown in Figure 1). Note that is a model performs well\\r\\nunder stochasticity would make same predictions for samples\\r\\ndrawn from the inherent distribution. Considering that stock\\r\\nprice is continuous, our intuition is to intentionally simulate\\r\\nsamples by adding small perturbations on static input fea\\x02tures. By enforcing the predictions on the simulated samples\\r\\nto be same, the model could capture stochasticity.\\r\\nAdversarial training [Goodfellow et al., 2015; Kurakin et\\r\\nal., 2017] implements the aforementioned intuition. It trains a\\r\\nmodel with both clean examples (i.e., examples in the training\\r\\nset) and adversarial examples (AEs) [Szegedy et al., 2013].\\r\\nThe AEs are malicious inputs generated by adding inten\\x02tional perturbations to features of clean examples. The per\\x02turbation, named as adversarial perturbation (AP) is the di\\x02rection that leads to the largest change of model prediction.\\r\\nDespite its success in image classification [Kurakin et al.,\\r\\n2017], it is infeasible to be directly applied to stock predic\\x02tion. This is because calculating perturbations relies on cal\\x02culation of the gradients regarding the input, which would\\r\\nbe time-consuming (caused by the back-propagation through\\r\\ntime-step of the LSTM layer). Besides, considering the fact\\r\\nthat the gradients of the input are dependent across different\\r\\ntime-steps, there might be unintentional interactions among\\r\\nthe perturbations on different time-steps, which are uncon\\x02trollable. To address these problems, we propose to generate\\r\\nAEs from latent representation e\\r\\ns\\r\\n, as shown in Figure 3.\\r\\nBefore introducing the calculation of AEs, we first elabo\\x02rate the objective function of Adv-ALSTM:\\r\\nΓadv =\\r\\nX\\r\\nS\\r\\ns=1\\r\\nl(y\\r\\ns\\r\\n, yˆ\\r\\ns\\r\\n) + β\\r\\nX\\r\\nS\\r\\ns=1\\r\\nl(y\\r\\ns\\r\\n, yˆ\\r\\ns\\r\\nadv) + α\\r\\n2\\r\\nkΘk\\r\\n2\\r\\nF . (4)\\r\\nThe second term is an adversarial loss where yˆ\\r\\ns\\r\\nadv is the clas\\x02sification confidence of the AE of stock s. β is a hyper\\x02parameter to balance the losses of clean and adversarial ex\\x02amples. By minimizing the objective function, the model is\\nencouraged to correctly classify both clean and adversarial\\r\\nexamples. Note that a model correctly classifying an AE can\\r\\nmake right predictions for examples with arbitrary perturba\\x02tions at the same scale. This is because AP is the direction\\r\\nleading to the largest change of model prediction. There\\x02fore, adversarial learning could enable ALSTM to capture the\\r\\nstochastic property of stock inputs.\\r\\nAt each iteration, the latent representation of an AE (e\\r\\ns\\r\\nadv)\\r\\nis generated by the following formulation,\\r\\ne\\r\\ns\\r\\nadv = e\\r\\ns + rs\\r\\nadv, r\\r\\ns\\r\\nadv = arg max\\r\\nrs,krsk≤\\x0f\\r\\nl(y\\r\\ns\\r\\n, yˆ\\r\\ns\\r\\nadv), (5)\\r\\nwhere e\\r\\ns\\r\\n(introduced in Equation 2) is the final latent rep\\x02resentation of stock s. r\\r\\ns\\r\\nadv is the associated AP. \\x0f is a\\r\\nhyper-parameter to explicitly control the scale of perturba\\x02tion. Since it is intractable to directly calculate r\\r\\ns\\r\\nadv, we em\\x02ploy the fast gradient approximation method [Goodfellow et\\r\\nal., 2015], r\\r\\ns\\r\\nadv = \\x0f\\r\\ng\\r\\ns\\r\\nkgsk\\r\\n, g\\r\\ns =\\r\\n∂l(y\\r\\ns\\r\\n,yˆ\\r\\ns\\r\\n)\\r\\n∂es . Specifically, the\\r\\ncalculated perturbation is the gradient of loss function regard\\x02ing the latent representation e\\r\\ns under a L2-norm constraint.\\r\\nNote that the gradient denotes the direction where the loss\\r\\nfunction increase the most at the given point e\\r\\ns\\r\\n, i.e., , it would\\r\\nlead to the largest change on the model prediction.\\r\\nFigure 4 illustrates the generation of adversarial examples.\\r\\nIn a training iteration, given a clean example having loss\\r\\nlarger than 0 (i.e., y\\r\\nsyˆs < 1), an AE is generated. The model\\r\\nis then updated to jointly minimize the losses for clean and\\r\\nadversarial examples, which would enforce the margin be\\x02tween clean examples and the decision boundary2\\r\\n. As such,\\r\\nit would benefit the model to predict examples with pertur\\x02bations into the same class as the clean one. That is, the\\r\\nmodel could correctly predict samples drawn from the inher\\x02ent stochastic distribution of inputs, capturing the stachastic\\x02ity. While traditional models like support vector machines\\r\\nalso push the decision boundary far from clean examples, the\\r\\nadversarial training adaptively adjusts the strength of enforc\\x02ing margins during the training process since the AP (r\\r\\ns\\r\\nadv)\\r\\nvaries across iterations. Note that we select the hinge loss to\\r\\nencourage the training process to focus more on the examples\\r\\nclose to the decision boundary.\\r\\n4 Experiments\\r\\n4.1 Experimental Settings\\r\\nDatasets. We evaluate the proposed method on two bench\\x02marks on stock movement prediction, ACL18 [Xu and Co\\x02hen, 2018] and KDD17 [Zhang et al., 2017].\\r\\nACL18 contains historical data from Jan-01-2014 to Jan\\x0201-2016 of 88 high-trade-volume-stocks in NASDAQ and\\r\\nNYSE markets. Following [Xu and Cohen, 2018], we first\\r\\nalign the trading days in the history, i.e., removing weekends\\r\\nand public holidays that lack historical prices. We then move\\r\\na lag with length of T along the aligned trading days to con\\x02struct candidate examples (i.e., one example for a stock on\\r\\n2 Minimizing the hinge loss of the AE is adjusting wp to enlarge\\r\\ny\\r\\ns\\r\\nyˆ\\r\\ns\\r\\nadv = y\\r\\ns\\r\\n(wT\\r\\np e\\r\\ns + b) + yswT\\r\\np r\\r\\ns\\r\\nadv, which would increase the\\r\\nfirst term y\\r\\ns\\r\\n(wT\\r\\np e\\r\\ns +b) = ys\\r\\nyˆ\\r\\ns\\r\\n. The results in Figure 5 (in Section\\r\\n4) empirically demonstrate the effect of enforcing margins.\\r\\n-1\\r\\n0\\r\\n1\\r\\n2\\r\\n3 Clean (+/-) Example\\r\\nAdversarial (+/-) Example\\r\\n!\\r\\n\"\\r\\n#!\\r\\n\" (Label * classification confidence)\\r\\nFigure 4: Intuitive illustration of adversarial examples.\\r\\nTable 1: Features to describe the daily trend of a stock.\\r\\nFeatures Calculation\\r\\nc open, c high, c low e.g., c open = opent/closet − 1\\r\\nn close, n adj close e.g., n close = (closet/closet−1 − 1\\r\\n5-day, 10-day, 15-day,\\r\\n20-day, 25-day, 30-day e.g., 5-day =\\r\\nP4\\r\\ni=0 adj closet−i/5\\r\\nadj closet\\r\\n− 1\\r\\nevery trading day). We label the candidate examples accord\\x02ing to the movement percent of stock close prices3\\r\\n. Examples\\r\\nwith movement percent ≥ 0.55% and ≤ −0.5% are identified\\r\\nas positive and negative examples, respectively. We tempo\\x02rally split the identified examples into training (Jan-01-2014\\r\\nto Aug-01-2015), validation (Aug-01-2015 to Oct-01-2015),\\r\\nand testing (Oct-01-2015 to Jan-01-2016).\\r\\nKDD17 includes longer history ranging from Jan-01-2007\\r\\nto Jan-01-2016 of 50 stocks in U.S. markets. As the dataset\\r\\nis originally collected for predicting stock prices rather than\\r\\nmovements, we follow the same approach as ACL18 to iden\\x02tify positive and negative examples. We then temporally split\\r\\nthe examples into training (Jan-01-2007 to Jan-01-2015), val\\x02idation (Jan-01-2015 to Jan-01-2016) and testing (Jan-01-\\r\\n2016 to Jan-01-2017).\\r\\nFeatures. Instead of using the raw EOD data, we define\\r\\n11 temporal features (x\\r\\ns\\r\\nt\\r\\n) to describe the trend of a stock s at\\r\\ntrading day t. Table 1 elaborates the features associated with\\r\\ncalculation. Our aim of defining these features are to: 1) nor\\x02malize the prices of different stocks; 2) and explicitly capture\\r\\nthe interaction of different prices (e.g., open and close).\\r\\nBaselines. We compare the following methods:\\r\\n• MOM Momentum (MOM) is a technical indicator that\\r\\npredicts negative or positive for each example with the\\r\\ntrend in the last 10 days.\\r\\n• MR Mean reversion (MR) predicts the movement of each\\r\\nexample as the opposite direction of latest price towards\\r\\nthe 30-day moving average.\\r\\n• LSTM is a neural network with an LSTM layer and a pre\\x02diction layer [Nelson et al., 2017]. We tune three hyper\\x02parameters, number of hidden units (U), lag size (T), and\\r\\nweight of regularization term (λ).\\r\\n• ALSTM is the Attentive LSTM [Qin et al., 2017], which\\r\\nis optimized with normal training. Similar as LSTM, we\\r\\nalso tune U, T, and λ.\\r\\n3Given a candidate example of stock s in the lag of [T0 − T +\\r\\n1, T0], the movement percent is calculated as p\\r\\ns\\r\\nT 0+1/psT 0 −1, where\\r\\np\\r\\ns\\r\\nT 0 is the adjusted close price of stock s on day T\\r\\n0\\r\\n.\\n• StockNet uses a Variational Autoencoder (VAE) to encode\\r\\nthe stock input so as to capture the stochasticity, and a tem\\x02poral attention to model the importance of different time\\x02steps [Xu and Cohen, 2018]. Here we take our tempo\\x02ral features in Table 1 as inputs and tune its hidden size,\\r\\ndropout ratio, and auxiliary rate (α).\\r\\nEvaluation Metrics. We evaluate the prediction perfor\\x02mance with two metrics, Accuracy (Acc) and Matthews Cor\\x02relation Coefficient (MCC) [Xu and Cohen, 2018] of which\\r\\nthe ranges are in [0, 100] and [−1, 1]. Note that better perfor\\x02mance is evidenced by higher value of the metrics.\\r\\nParameter Settings. We implement the Adv-ALSTM\\r\\nwith Tensorflow and optimize it using the mini-batch\\r\\nAdam[Diederik and Jimmy, 2015] with a batch size of 1,024\\r\\nand an initial learning rate of 0.01. We search the opti\\x02mal hyper-parameters of Adv-ALSTM on the validation set.\\r\\nFor U, T, and λ, Adv-ALSTM inherits the optimal settings\\r\\nfrom ALSTM, which are selected via grid-search within the\\r\\nranges of [4, 8, 16, 32], [2, 3, 4, 5, 10, 15], and [0.001,\\r\\n0.01, 0.1, 1], respectively. We further tune β and \\x0f within\\r\\n[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1] and [0.001, 0.005, 0.01,\\r\\n0.05, 0.1], respectively. We report the mean testing perfor\\x02mance when Adv-ALSTM performs best on the validation\\r\\nset over five different runs. Code could be accessed through\\r\\nhttps://github.com/hennande/Adv-ALSTM.\\r\\n4.2 Experimental Results\\r\\nPerformance Comparison. Tables 2 shows the prediction\\r\\nperformance of compared methods on the two datasets re\\x02garding Acc and MCC, respectively. From the table, we have\\r\\nthe following observations:\\r\\n• Adv-ALSTM achieves the best results in all the cases.\\r\\nCompared to the baselines, Adv-ALSTM exhibits an im\\x02provement of 4.02% and 42.19% (2.14% and 56.12%) on\\r\\nthe ACL18 (KDD17) dataset regarding Acc and MCC, re\\x02spectively. This justifies the effectiveness of adversarial\\r\\ntraining, which might be due to enhancing the model gen\\x02eralization via adaptively simulating perturbations during\\r\\nthe training.\\r\\n• Specifically, compared to StockNet, which captures\\r\\nstochasticity of stock inputs with VAE, Adv-ALSTM\\r\\nachieves significant improvements. We postulate the rea\\x02son is that StockNet cannot explicitly model the scale and\\r\\ndirection of stochastic perturbation since it relies on Monte\\r\\nCarlo sampling during the training process.\\r\\n• Among the baselines, ALSTM outperforms LSTM by\\r\\n1.93% and 48.69% on average w.r.t. Acc and MCC, which\\r\\nvalidates the impact of attention [Qin et al., 2017]. Be\\x02sides, MOM and MR performs worse than all the machine\\r\\nlearning-based methods as expected, which justifies that\\r\\nhistorical patterns help in stock prediction task.\\r\\nStochastic Perturbation VS. Adversarial Perturbation.\\r\\nWe further investigate the effectiveness of adversarial train\\x02ing via comparing adversarial perturbations and random ones.\\r\\nRand-ALSTM is a variance of Adv-ALSTM, which gener\\x02ates additional examples by adding random perturbations to\\r\\nthe input of clean examples. Table 3 shows the performance\\r\\nTable 2: Performance comparison on the two datasets.\\r\\nMethod ACL18 KDD17\\r\\nAcc MCC Acc MCC\\r\\nMOM 47.01±—– -0.0640±—– 49.75±—– -0.0129±—–\\r\\nMR 46.21±—– -0.0782±—– 48.46±—– -0.0366±—–\\r\\nLSTM 53.18±5e-1 0.0674±5e-3 51.62±4e-1 0.0183±6e-3\\r\\nALSTM 54.90±7e-1 0.1043±7e-3 51.94±7e-1 0.0261±1e-2\\r\\nStockNet 54.96±—– 0.0165±—– 51.93±4e-1 0.0335±5e-3\\r\\nAdv-ALSTM 57.20±—– 0.1483±—– 53.05±—– 0.0523±—–\\r\\nRI 4.02% 42.19% 2.14% 56.12%\\r\\nRI denotes the relative improvement of Adv-ALSTM compared to the best baseline.\\r\\nThe performance of StockNet is directly copied from [Xu and Cohen, 2018].\\r\\nTable 3: Performance of Rand-ALSTM on the two datasets.\\r\\nDatasets Acc MCC\\r\\nACL18 55.08±2e0 0.1103±4e-2\\r\\nKDD17 52.43±5e-1 0.0405±8e-3\\r\\nof Rand-ALSTM on the two datasets. By cross comparing\\r\\nit with Table 2, we observe that: 1) Compared to Rand\\x02ALSTM, Adv-ALSTM achieves significant improvements.\\r\\nFor instance, its performance w.r.t. Acc on ACL18 is 3.95%\\r\\nbetter than that of Rand-ALSTM. It demonstrates that adver\\x02sarial perturbations are helpful for stock prediction, similar to\\r\\nthat reported in the original image classification tasks [Good\\x02fellow et al., 2015]. 2) Rand-ALSTM outperforms ALSTM,\\r\\nwhich is purely trained with clean examples, with an average\\r\\nimprovement of 0.64% w.r.t. Acc on the two datasets. This\\r\\nhighlights the necessity of dealing with stochastic property of\\r\\nstock features.\\r\\nImpacts of Adversarial Training. We now investigate the\\r\\nimpacts of adversarial training to answer: 1) Whether the ad\\x02versarial training enforces the margin between clean exam\\x02ples and the decision boundary. 2) Whether the adversarial\\r\\ntraining enhances the robustness of the model against adver\\x02sarial examples. Note that we only show the results on the\\r\\nACL18 dataset as the results on KDD17 admit the same ob\\x02servations.\\r\\nEnforcing margin. Recall that the only difference between\\r\\nAdv-ALSTM and ALSTM is learning parameters with ad\\x02versarial training and standard training. As such, we answer\\r\\nthe first question by comparing the classification confidence\\r\\nof clean examples (larger value denotes larger margin to the\\r\\ndecision boundary) assigned by Adv-ALSTM and ALSTM.\\r\\nFigure 5 illustrates the distributions of the classification con\\x02fidences assigned by ALSTM and Adv-ALSTM. As can be\\r\\nseen, the confidences of Adv-ALSTM distribute in a range\\r\\n([-0.6, 0.6] roughly), which is about 1.5 times larger than that\\r\\nof ALSTM ([-0.2, 0.3]). It indicates that adversarial training\\r\\npushes the decision boundary far from clean examples, which\\r\\nis believed to help enhance the robustness and generalization\\r\\nability of the model.\\r\\nRobustness against adversarial examples. We then inves\\x02tigate the second question via comparing the performance\\r\\nof ALSTM and Adv-ALSTM on the clean and associated\\r\\nadversarial examples. Figures 6(a) and 6(b) illustrate the\\r\\nrelative performance decrease (RPD) of ALSTM and Adv\\x02ALSTM on adversarial examples regarding the one on clean\\r\\nexamples, respectively. Note that larger absolute value of\\r\\nRPD indicates that the model is more vulnerable to adver\\x02sarial perturbations. As can be seen, the average RPD of\\n(a) Validation of ACL18 (b) Testing of ACL18\\r\\nFigure 5: Distributions of classification confidences assigned\\r\\nby ALSTM and Adv-ALSTM for clean examples.\\r\\n(a) Acc (b) MCC\\r\\nFigure 6: Robustness against adversarial example of AL\\x02STM and Adv-ALSTM. Each plotted number is the RPD of\\r\\na model on adversarial examples compared to clean ones.\\r\\nALSTM is 4.31 (6.34) times larger as compared to Adv\\x02ALSTM regarding Acc (MCC). This justifies the potential\\r\\nof enhancing model robustness with adversarial training.\\r\\n5 Related Work\\r\\n5.1 Stock Movement Prediction\\r\\nRecent works on stock movement prediction, mainly fall un\\x02der two categories, technical analysis and fundamental anal\\x02ysis (FA). The technical analysis (TA) takes historical prices\\r\\nof a stock as features to forecast its movement. Most of re\\x02cent methods in TA mine stock movements with deep models\\r\\n[Lin et al., 2017; Nelson et al., 2017; Chong et al., 2017].\\r\\nAmong them, recurrent neural networks like LSTM have be\\x02come key components to capture the temporal patterns of\\r\\nstock prices [Nelson et al., 2017; Lin et al., 2017]. Besides,\\r\\nother advanced neural models, such as convolution neural\\r\\nnetworks (CNN) [Lin et al., 2017] and deep Boltzmann ma\\x02chine [Chong et al., 2017], are also evidenced to be beneficial\\r\\nfor capturing the non-linearity of stock prices.\\r\\nIn addition to price features, FA also examines related eco\\x02nomic, financial, and other qualitative and quantitative fac\\x02tors [Hu et al., 2018; Zhang et al., 2018; Li et al., 2018;\\r\\nXu and Cohen, 2018]. For instance, Xu and Cohen [2018] in\\x02corporate signals from social media, which reflects opinions\\r\\nfrom general users, to enhance stock movement prediction.\\r\\nSpecifically, they employ a VAE to learn a stock representa\\x02tion by jointly encoding the historical prices and tweets men\\x02tioning it. Moreover, Zhang et al. [2018] further consider\\r\\nnews events related to a stock or the associated company via\\r\\na coupled matrix and tensor factorization framework.\\r\\nBoth TA and FA studies show that price features play cru\\x02cial roles in stock movement prediction. However, most of\\r\\nthe existing works assume stock price as stationary, which\\r\\nthus lack the ability to deal with its stochastic property.\\r\\nStockNet [Xu and Cohen, 2018] is the only exception which\\r\\ntackles this problem via VAE. VAE encodes the inputs into a\\r\\nlatent distribution and enforces samples from the latent dis\\x02tribution to be decoded with the same prediction. Generally,\\r\\nthe philosophy behind is similar as the simulation of stochas\\x02tic perturbations since one sample from the latent distribution\\r\\ncan be seen as adding stochastic perturbation to the latent rep\\x02resentation. As compared to our method, our perturbation is\\r\\nintentionally generated which indicates leads to hardest ex\\x02amples for the model to obtain the target prediction. In ad\\x02dition, the proposed method can be easily adapted to other\\r\\nsolutions of stock movement predictions.\\r\\n5.2 Adversarial Learning\\r\\nAdversarial learning has been intensively studied by train\\x02ing a classification model to defense adversarial examples,\\r\\nwhich are intentionally generated to perturb the model. Ex\\x02isting works of adversarial learning mainly concentrate on\\r\\ncomputer vision tasks like image classification [Goodfel\\x02low et al., 2015; iyato et al., 2017; Kurakin et al., 2017;\\r\\nYang et al., 2018; Chen et al., 2018b]. Owing to the prop\\x02erty that image features are typically continued real val\\x02ues, adversarial examples are directly generated in the fea\\x02ture space. Recently, several works extend the adversar\\x02ial learning to tasks with discrete inputs such as text clas\\x02sification (a sequence of words) [iyato et al., 2017], rec\\x02ommendation (user and item IDs) [He et al., 2018], and\\r\\ngraph node classification (graph topology) [Dai et al., 2018;\\r\\nFeng et al., 2019]. Rather than in the feature space, these\\r\\nworks generate adversarial examples from embedding of in\\x02puts such as word, user (item), and node embeddings. Al\\x02though this work is inspired by these adversarial learning re\\x02search efforts, it targets a distinct task—stock movement pre\\x02diction, of which the data are time series with stochastic prop\\x02erty. To the best of our knowledge, this work is the first one\\r\\nto explore the potential of adversarial training in time-series\\r\\nanalytics.\\r\\n6 Conclusion\\r\\nIn this paper, we showed that neural network solutions for\\r\\nstock movement prediction could suffer from weak gener\\x02alization ability since they lack the ability to deal with the\\r\\nstochasticity of stock features. To solve this problem, we pro\\x02posed an Adversarial Attentive LSTM solution, which lever\\x02ages adversarial training to simulate the stochasticity during\\r\\nmodel training. We conducted extensive experiments on two\\r\\nbenchmark datasets and validated the effectiveness of the pro\\x02posed solution, signifying the importance of accounting for\\r\\nthe stochasticity of stock prices in stock movement predic\\x02tion. Morever, the results showed that adversarial training\\r\\nenhances the robustness and generalization of the prediction\\r\\nmodel.\\r\\nIn future, we plan to explore the following directions: 1)\\r\\nwe are interested in testing Adv-ALSTM in movement pre\\x02diction of more assets such as commodities. 2) We plan to\\r\\napply adversarial training to stock movement solutions with\\r\\ndifferent structures such as the CNNs [Lin et al., 2017]. 3)\\nWe will explore the effect of adversarial training over funda\\x02mental analysis methods of stock movement prediction.\\r\\nReferences\\r\\n[Adebiyi et al., 2014] A. Adebiyi, A. Adewumi, and C. Ayo.\\r\\nComparison of arima and artificial neural networks models\\r\\nfor stock price prediction. JAM, 2014.\\r\\n[Chen et al., 2018a] J. Chen, C. Ngo, F. Feng, and T. Chua.\\r\\nDeep understanding of cooking procedure for cross-modal\\r\\nrecipe retrieval. In MM, pages 1020–1028. ACM, 2018.\\r\\n[Chen et al., 2018b] L. Chen, H. Zhang, J. Xiao, W. Liu, and\\r\\nS. Chang. Zero-shot visual recognition using semantics\\x02preserving adversarial embedding networks. In CVPR,\\r\\npages 1043–1052, 2018.\\r\\n[Cho et al., 2014] K. Cho, M. Van, C. Gulcehre, et al. Learn\\x02ing phrase representations using rnn encoder–decoder for\\r\\nstatistical machine translation. In EMNLP, pages 1724–\\r\\n1734, 2014.\\r\\n[Chong et al., 2017] E. Chong, C. Han, and F. Park. Deep\\r\\nlearning networks for stock market analysis and predic\\x02tion: Methodology, data representations, and case studies.\\r\\nESA, 83:187–205, 2017.\\r\\n[Dai et al., 2018] H. Dai, H. Li, T. Tian, X. Huang, L. Wang,\\r\\nJ. Zhu, and L. Song. Adversarial attack on graph structured\\r\\ndata. In ICML, pages 1115–1124, 2018.\\r\\n[Diederik and Jimmy, 2015] K. Diederik and B. Jimmy.\\r\\nAdam: A method for stochastic optimization. ICLR, 2015.\\r\\n[Edwards et al., 2007] R. Edwards, J. Magee, and C. Bas\\x02setti. Technical analysis of stock trends. CRC press, 2007.\\r\\n[Fama and French, 2012] E. F. Fama and K. R. French. Size,\\r\\nvalue, and momentum in international stock returns. JFE,\\r\\n105(3):457–472, 2012.\\r\\n[Feng et al., 2018] F. Feng, X. He, X. Wang, C. Luo, Y. Liu,\\r\\nand T. Chua. Temporal relational ranking for stock predic\\x02tion. TOIS, 2018.\\r\\n[Feng et al., 2019] F. Feng, X. He, J. Tang, and T. Chua.\\r\\nGraph adversarial training: Dynamically regularizing\\r\\nbased on graph structure. arXiv, 2019.\\r\\n[Goodfellow et al., 2015] I. Goodfellow, J. Shlens, and\\r\\nC. Szegedy. Explaining and harnessing adversarial exam\\x02ples. ICLR, 2015.\\r\\n[Graves et al., 2013] A. Graves, A. Mohamed, and G. Hin\\x02ton. Speech recognition with deep recurrent neural net\\x02works. In ICASSP, pages 6645–6649. IEEE, 2013.\\r\\n[He et al., 2018] X. He, Z. He, X. Du, and T. Chua. Adver\\x02sarial personalized ranking for recommendation. In SIGIR,\\r\\npages 355–364, 2018.\\r\\n[Hochreiter and Schmidhuber, 1997] S. Hochreiter and\\r\\nJ. Schmidhuber. Long short-term memory. Neural\\r\\ncomputation, 9(8):1735–1780, 1997.\\r\\n[Hu et al., 2018] Z. Hu, W. Liu, J. Bian, X. Liu, and T. Liu.\\r\\nListening to chaotic whispers: A deep learning framework\\r\\nfor news-oriented stock trend prediction. In WSDM, pages\\r\\n261–269, 2018.\\r\\n[Huang et al., 2005] W. Huang, Y. Nakamori, and S. Wang.\\r\\nForecasting stock market movement direction with support\\r\\nvector machine. COR, 32(10):2513–2522, 2005.\\r\\n[iyato et al., 2017] MT. iyato, A. M. Dai, and I. Goodfel\\x02low. Adversarial training methods for semi-supervised text\\r\\nclassification. ICLR, 2017.\\r\\n[Kurakin et al., 2017] A. Kurakin, I. Goodfellow, and\\r\\nS. Bengio. Adversarial machine learning at scale. ICLR,\\r\\n2017.\\r\\n[Li et al., 2018] Q. Li, Y. Chen, J. Wang, Y. Chen, and\\r\\nH. Chen. Web media and stock markets: A survey and\\r\\nfuture directions from a big data perspective. TKDE,\\r\\n30(2):381–399, 2018.\\r\\n[Lin et al., 2017] T. Lin, T. Guo, and K. Aberer. Hybrid neu\\x02ral networks for learning the trend in time series. In IJCAI,\\r\\npages 2273–2279, 2017.\\r\\n[Musgrave, 1997] G. Musgrave. A random walk down wall\\r\\nstreet. Business Economics, 32(2):74–76, 1997.\\r\\n[Nelson et al., 2017] D. Nelson, A. Pereira, and R. Oliveira.\\r\\nStock market’s price movement prediction with lstm neu\\x02ral networks. In IJCNN, pages 1419–1426. IEEE, 2017.\\r\\n[Nguyen et al., 2015] T. H. Nguyen, K. Shirai, and J. Velcin.\\r\\nSentiment analysis on social media for stock movement\\r\\nprediction. ESA, 42(24):9603–9611, 2015.\\r\\n[Qin et al., 2017] Y. Qin, D. Song, H. Cheng, W. Cheng,\\r\\nG. Jiang, and G. Cottrell. A dual-stage attention-based\\r\\nrecurrent neural network for time series prediction. In IJ\\x02CAI, pages 2627–2633, 2017.\\r\\n[Rosasco et al., 2004] L. Rosasco, E. D. Vito, A. Capon\\x02netto, M. Piana, and A. Verri. Are loss functions all the\\r\\nsame? Neural Computation, 16(5):1063–1076, 2004.\\r\\n[Szegedy et al., 2013] C. Szegedy, W. Zaremba, I. Sutskever,\\r\\nJ. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intrigu\\x02ing properties of neural networks. arXiv, 2013.\\r\\n[Walczak, 2001] S. Walczak. An empirical analysis of data\\r\\nrequirements for financial forecasting with neural net\\x02works. JMIS, 17(4):203–222, 2001.\\r\\n[Wu et al., 2018] L. Wu, C. Quan, C. Li, and D. Ji. Parl: Let\\r\\nstrangers speak out what you like. In CIKM, pages 677–\\r\\n686. ACM, 2018.\\r\\n[Xu and Cohen, 2018] Y. Xu and S. Cohen. Stock movement\\r\\nprediction from tweets and historical prices. In ACL, vol\\x02ume 1, pages 1970–1979, 2018.\\r\\n[Yang et al., 2018] X. Yang, H. Zhang, and J. Cai. Shuffle\\x02then-assemble: Learning object-agnostic visual relation\\x02ship features. In ECCV, pages 36–52, 2018.\\r\\n[Zhang et al., 2017] L. Zhang, C. Aggarwal, and G. Qi.\\r\\nStock price prediction via discovering multi-frequency\\r\\ntrading patterns. In SIGKDD, pages 2141–2149. ACM,\\r\\n2017.\\r\\n[Zhang et al., 2018] X. Zhang, Y. Zhang, S. Wang, Y. Yao,\\r\\nB. Fang, and P. Yu. Improving stock market prediction\\r\\nvia heterogeneous information fusion. KBS, 143:236–247,\\r\\n2018.'},\n",
       " {'name': '2311.09536v2.pdf',\n",
       "  'content': 'Correlation networks: Interdisciplinary approaches beyond thresholding\\r\\nNaoki Masuda1,2, Zachary M. Boyd3, Diego Garlaschelli4,5, and Peter J. Mucha6\\r\\n1Department of Mathematics, State University of New York at Buffalo\\r\\n2\\r\\nInstitute for Artificial Intelligence and Data Science, State University of New York at Buffalo\\r\\n3Department of Mathematics, Brigham Young University\\r\\n4Lorentz Institute for Theoretical Physics, Leiden University, The Netherlands\\r\\n5\\r\\nIMT School of Advanced Studies, Lucca, Italy\\r\\n6Department of Mathematics, Dartmouth College\\r\\nAbstract\\r\\nMany empirical networks originate from correlational data, arising in domains as diverse as psychology, neu\\x02roscience, genomics, microbiology, finance, and climate science. Specialized algorithms and theory have been\\r\\ndeveloped in different application domains for working with such networks, as well as in statistics, network sci\\x02ence, and computer science, often with limited communication between practitioners in different fields. This\\r\\nleaves significant room for cross-pollination across disciplines. A central challenge is that it is not always clear\\r\\nhow to best transform correlation matrix data into networks for the application at hand, and probably the most\\r\\nwidespread method, i.e., thresholding on the correlation value to create either unweighted or weighted networks,\\r\\nsuffers from multiple problems. In this article, we review various methods of constructing and analyzing corre\\x02lation networks, ranging from thresholding and its improvements to weighted networks, regularization, dynamic\\r\\ncorrelation networks, threshold-free approaches, comparison with null models, and more. Finally, we propose and\\r\\ndiscuss recommended practices and a variety of key open questions currently confronting this field.\\r\\n1 Introduction\\r\\nCorrelation matrices capture pairwise similarity of multiple, often temporally evolving signals, and are used to de\\x02scribe system interactions in various diverse disciplines of science and society, from financial economics to psychology,\\r\\nbioinformatics, neuroscience, and climate science, to name a few. Correlation analysis is often a first step in trying\\r\\nto understand complex systems data [1]. Existing methods for analyzing correlation matrix data are abundant. Very\\r\\nwell established methods include principal component analysis (PCA) [2] and factor analysis (FA) [3, 4], which can\\r\\nyield a small number of interpretable components from correlation matrices, such as a global market trend when\\r\\napplied to stock market data, or spatio-temporal patterns of air pressure when applied to atmospheric data. Another\\r\\nmajor method for analyzing correlation matrix data is the Markowitz’s portfolio theory in mathematical finance,\\r\\nwhich aims to minimize the variance of financial returns while keeping the expected return above a given thresh\\x02old [5, 6]. The model takes as input a correlation matrix among different assets that an investor can invest in. In a\\r\\nrelated vein, random matrix theory (RMT) [7–9] has been a key theoretical tool for estimating and analyzing eco\\x02nomic and other correlation matrix data for a couple of decades [6]. Various new methods for analyzing correlation\\r\\nmatrix data have also been proposed. Examples include detrended cross-correlation analysis [10–12], correlation de\\x02pendency, defined as the difference between the partial correlation coefficient and the Pearson correlation coefficient\\r\\ngiven three nodes [13, 14], determination of optimal paths between distant locations in correlation matrix data [15],\\r\\nearly warning signals for anticipating abrupt changes in multidimensional dynamical systems including the case of\\r\\nnetworked systems [16–18], and energy landscape analysis for multivariate time series data particularly employed in\\r\\nneuroscience [19, 20].\\r\\nThe last two decades have also seen successful applications of tools from network science and graph theory to\\r\\ncorrelational data. A correlation matrix can be mapped onto a network, which we refer to here as a correlation\\r\\nnetwork, where nodes represent elements and edges are informed by the strength of the correlation between pairs of\\r\\nelements. Correlation network analysis generally intends to extract useful information from data, such as the patterns\\r\\nof interactions among nodes or a ranking of nodes. We show a typical workflow of correlation network analysis in\\r\\nFig. 1. With multivariate data with multiple (and not too few) samples as input, the analysis entails calculation of\\r\\ncorrelation matrices, construction of correlation networks from the correlation matrices, and downstream network\\r\\nanalysis on the resulting correlation networks. The network analysis often includes discussion of the implication of\\r\\n1\\r\\narXiv:2311.09536v2 [physics.soc-ph] 3 Oct 2024\\nquestion Q1 Q2 Q3 Q4\\r\\nNode 1 5 3 2 2\\r\\nNode 2 3 5 1 2\\r\\nNode 3 2 3 4 1 Node 4 3 4 1 \\r\\n⋮ ⋮\\r\\nNode 10 1 3 5 4\\r\\nmultivariate data such as correlation matrix\\r\\nnetwork analysis correlation network\\r\\nor\\r\\nFigure 1: Typical workflow of correlation network analysis. First, we are given multivariate data with L samples. A\\r\\nsample may correspond to a time point in the case of time series data or a respondent in the case of psychological\\r\\nquestionnaires. Second, we compute the correlation matrix. In the present article, we focus on the case of Pearson\\r\\ncorrelation matrix although the subsequent steps of analysis equally apply to the case of other types of correlation\\r\\nor similarity matrices. Third, we construct a correlation network from the correlation matrix by, for example,\\r\\nthresholding on the pairwise correlation value. Fourth, we carry out network analysis on the generated correlation\\r\\nnetwork. Network analysis typically includes calculation of some quantities or network features, such as communities\\r\\n(also called modules) and node centrality values, for the network and assess their implications, such as the difference\\r\\nbetween a disease group of samples and healthy control group of samples. See e.g. [21, 24, 26] for similar diagrams.\\r\\nthe network analysis results in application domains in question. An ideal correlation network analysis appropriately\\r\\nadapts concepts and methods developed in network science to the case of correlation networks, generating knowl\\x02edge that standard methods for correlation matrices (such as PCA) do not produce. Although correlation does not\\r\\nnecessarily reflect a physical connection or direct interaction between two nodes, correlation matrices are convention\\x02ally used as a relatively inexpensive substitute of such direct connections, whose data are often less available than\\r\\ncorrelation matrix data. Correlation networks are also useful for visualization [21]. Correlation network analysis has\\r\\nbeen used in various research disciplines, typically not much behind wherever correlation matrix analysis is used, as\\r\\nwe will review in section 2. In our survey here, we focus on correlation networks, with an emphasis on identifying\\r\\ndifferent methods used to transform correlation matrices into correlation networks. See [21–25] for shorter reviews.\\r\\nThe validity of correlation network analysis remains an outstanding question, especially because the decisions\\r\\nabout how to best construct network representations from correlation matrices is far from straightforward. One\\r\\nof the simplest methods is to threshold on the correlation value measured for each pair of nodes (see section 3.2).\\r\\nHowever, while such a simple thresholding is widely used, it introduces various problems. These problems have led\\r\\nto proposals of alternative methods for generating correlation networks, which we will cover in sections 3.4–3.7.\\r\\nBefore proceeding, we raise some important clarifications. First, correlation networks as we consider here are\\r\\ndifferent from network architectures that exploit correlation in data [27–29]1. These “networks” are in the sense of\\r\\n1For example, the Progressive Spatio-Temporal Correlation Network (PSCC-net) is an algorithm to detect and localize manipulations\\r\\nin the input image data by taking advantage of spatial correlation structure in images [27]. The superpixel group-correlation network\\r\\n2\\nneural network architecture in artificial intelligence and machine learning, whereas here we consider “networks” to\\r\\ndenote graphs in network science.\\r\\nSecond, we focus on correlation networks based on the Pearson correlation coefficient or its close variants such as\\r\\nthe partial correlation coefficient. In fact, there are numerous other definitions for quantifying the similarity between\\r\\ndata obtained from node pairs [23, 24, 30–34]. Examples include similarity networks whose edges are determined\\r\\nusing the rank correlation coefficient [35,36], the mutual information [37–39], and partial mutual information [40,41].\\r\\nCo-occurrence of two nodes over samples, such as two authors co-authoring a paper (a paper is a sample in this\\r\\nexample), also gives us an unnormalized variant of correlation. See sections 2.5 and 2.8 for examples of co-occurrence\\r\\nnetworks. However, a majority of concepts and techniques explained in our main technical sections 3 and 4, such\\r\\nas the detrimental effect of thresholding and dichotomizing the edge weight, use of weighted networks, graphical\\r\\nlasso, and importance of null models, also hold true when one constructs correlation networks using these or other\\r\\nalternative methods.\\r\\nThird, we do not discuss causal inference in the present paper. A plethora of methods are available for inferring\\r\\ncausality between nodes and associated directed networks. For example, a Bayesian network is a directed acyclic\\r\\ngraph that fully represents the joint probability distribution of the N variables. The edge of the directed acyclic\\r\\ngraph represents directed and pairwise conditional dependency of one random variable (corresponding to a node)\\r\\non another variable. The absence of the edge represents conditional independence between the two nodes. For\\r\\nBayesian networks, see, e.g., [42,43]. For other techniques, see, e.g., [21,34,44]. While these methods reveal potential\\r\\ncausal links, even from cross-sectional data, we do not consider them further here in our discussion of correlation\\r\\nmatrices and related general similarity matrices, whose inherently symmetric natures mean that these matrices or\\r\\nnetworks do not in principle inform us of causality or directionality between nodes (at least not in a straightforward\\r\\nmanner [45–47]). In a related vein, we do not discuss time-lagged correlation of multivariate time series in this paper,\\r\\nsince these are also asymmetric in general, although many of the same considerations we raise here also apply to\\r\\nlagged correlations.\\r\\n2 Data types leading to correlation networks\\r\\nCorrelation network analysis is common in many research fields. In this section, we survey typical correlation\\r\\nnetworks and their analysis in representative research fields.\\r\\n2.1 Psychological networks\\r\\nThere are various multivariate psychological data, from which one can construct networks [21, 25]. For example, in\\r\\npersonality research, researchers construct personality networks in which each node can be a personal trait or goal\\r\\nsuch as being organized, being lazy, and wanting to stay safe. Edges between a pair of nodes typically represent a type\\r\\nof conditional association between the two nodes. Samples are frequently participants in the research responding to\\r\\nvarious questionnaires on a numeric scale (e.g., 5-point scale ranging from 1: strongly disagree to 5: strongly agree)\\r\\ncorresponding to nodes. From a cross-sectional data set, one can calculate (conditional) correlation between pairs of\\r\\nnodes. Researchers are also increasingly combining surveys with alternative data collection modalities, for example,\\r\\nsensor data for daily movement or neural markers of stress [48,49]. It is reasonable to use correlation between signals\\r\\nfrom different modalities (e.g., smartphones and brain scanners) to construct a correlation network [49].\\r\\nAnother major type of psychological network is symptom networks employed in mental health research. Symp\\x02toms of a psychological, including psychopathological, condition, such as major depression and schizophrenia, are\\r\\ninterrelated. Furthermore, causality between symptoms such as fatigue, headaches, concentration problems, and in\\x02somnia, and a psychopathological disorder, is often unclear. It has been suggested that a disorder does not originate\\r\\nfrom a single root cause, which motivates the study of symptom networks [21, 50–52]. Nodes in a symptom network\\r\\nare symptoms, and one can use association between pairs of symptoms calculated from the participants’ responses\\r\\nto define edges. Analysis of symptom networks may help us to predict how an individual develops psychopathology\\r\\nin the future, understand comorbidity as strong connection between symptoms of two mental disorders, and propose\\r\\ncentral nodes as possible targets of intervention [52]. Health-promoting behaviors can also be treated as nodes in\\r\\nthese networks to suggest key behavioral intervention points [53].\\r\\nPanel data, i.e., longitudinal measurements of variables from samples, are increasingly common for network\\r\\napproaches [21]. In this case, one obtains correlation networks at multiple time points. Then, one can construct\\r\\ntime-varying correlation networks (see section 3.9) or within-person correlation networks [54] that reflect temporal\\r\\n(SGCN) [28] and the deep correlation network (DCNet) [29] are encoder-decoder and deep-learning network architectures, respectively,\\r\\nfor salient object detection in images.\\r\\n3\\nsymptom patterns and ideally expose individual differences and possible causal pathways in mental health patterns\\r\\nrelated to disorders [55, 56]. However, the validity of psychological network approach should be further studied.\\r\\nResearch has shown that symptom networks have poor reproducibility across samples, likely due to measurement\\r\\nerror in assessing symptoms among other reasons [51, 57].\\r\\n2.2 Brain networks\\r\\nVarious notions of brain connectivity have been essential to better understanding different neural functions. Studies of\\r\\nsuch brain networks constitute a major part of a research field that is often referred to as network neuroscience [58,59].\\r\\n(See also the related material about network representations in [60].) Multivariate time series of neuronal signals\\r\\nrecorded from the brain are a major source of data used in network neuroscience research. Such data may be recorded\\r\\nin a resting state or when participants are performing some task. Functional networks or functional connectivity\\r\\nrefer to correlation-based networks constructed from multivariate neuronal time series data, obtained through, e.g.,\\r\\nneuroimaging or electroencephalography, where the term “functional” in this setting effectively means correlational.\\r\\nA typical node in the brain networks is either a voxel (i.e., cube of side length of, e.g., 1 mm) or a spherical region\\r\\nof interest (ROI), which is a sphere in the brain. In the case of multivariate time series data, there are various other\\r\\nmethods to infer directed brain networks, which is referred to as effective connectivity, but we do not cover directed\\r\\nnetworks in this article. Brain networks based on anatomical connectivity between brain regions are referred to\\r\\nas structural networks. Functional connectivity, or a correlation-based edge between two nodes in the brain does\\r\\nnot imply the presence of an edge between the same pair of nodes in the structural network. Indeed, one does\\r\\nnot expect a one-to-one correspondence between functional and structural brain networks because several brain\\r\\nstates and functions continuously arise from the same anatomical structure [61]. Still, the possibility of studying\\r\\nstructural networks in combination with functional networks on the same set of nodes is a distinguishing feature of\\r\\nbrain networks, which can be used for an additional comparison when validating the outcome of correlation-based\\r\\nnetworks [62]. See [24, 32, 58, 62–65] for reviews and comparisons of techniques for the estimation and validation\\r\\nof brain networks from the (partial) correlations. Examples of the use of these methods for (functional) network\\r\\nanalysis are discussed later in this review.\\r\\nThe most typical functional neuronal networks come from neuroimaging data, in particular functional magnetic\\r\\nresonance imaging (fMRI) data, which are measured using blood-oxygeneration-level-dependent (BOLD) imaging\\r\\ntechniques [66]. Functional connectivity between voxels or between spherical ROIs, or other types of nodes, is\\r\\ncalculated by a correlation between fMRI time signals at the two nodes after one has bandpassed the fMRI time\\r\\nseries at each node to remove artifacts, with a frequency band of, e.g., 0.01-0.1 Hz. Functional MRI improves on\\r\\nelectroencephalogram (EEG) and magnetoencephalography (MEG) in spatial resolution at the expense of temporal\\r\\nresolution, but functional EEG and MEG networks are not uncommon. We also note that EEG and MEG signals\\r\\nare oscillatory, so one has to calculate the functional connectivity between each pair of nodes using methods that\\r\\nare aware of the oscillatory nature of the signal, such as using phase lag index or amplitude envelope correlation [67]\\r\\nrather than conventional correlation coefficients or mutual information.\\r\\nStructural covariance networks are another type of correlation brain network where the edges are defined as\\r\\nthe correlation/covariance of the corrected gray matter volume or cortical thickness between brain regions i and j,\\r\\nwhere the samples are participants [68, 69]. Morphometric similarity networks are a variant of structural covariance\\r\\nnetworks. In morphometric similarity networks, one uses various morphometric variables, not just a single one such\\r\\nas cortical thickness, for each node (i.e., ROI) [70]. One calculates the correlation between two nodes by regarding\\r\\neach morphometric variable as a sample. Therefore, differently from structural covariance networks based on cortical\\r\\nthickness, one can calculate a correlation network for each individual.\\r\\nIn neuroreceptor similarity networks, an edge between two nodes, or ROIs, is the correlation in terms of receptor\\r\\ndensity [71]. Specifically, one first calculates a vector of neurotransmitter density for each ROI, with each entry of\\r\\nthe vector corresponding to one type of receptor. Then, one computes the correlation between each pair of ROIs,\\r\\ncalled receptor similarity.\\r\\n2.3 Gene co-expression networks\\r\\nGenes do not work in isolation. Gene co-expression networks have been useful for figuring out webs of interaction\\r\\namong genes using network analysis methods [34, 72–79]. They are a type of data in a subfield of network science\\r\\noften referred to as network biology or network medicine. Gene co-expression networks are correlation networks in\\r\\nthe generalized sense considered here, including the case of other measures of similarity. A typical measurement is\\r\\nthe amount of gene expression for different genes and samples, where a sample most commonly corresponds to a\\r\\nhuman or animal individual. If one measures the expression of various genes for the same set of samples, we can\\r\\n4\\ncalculate the co-expression between each pair of genes by calculating the sample correlation, yielding a correlation\\r\\nmatrix. Depending on the questions being asked in the study, it may be important to calculate the underlying\\r\\ncorrelations with different factors to account for the effects of heterogeneous gene frequencies [80, 81]. It is common\\r\\nto transform a correlation matrix into a network and then apply various network analysis methods, for example\\r\\ncommunity detection with the aim of estimating the group of genes that are associated with the same phenotype2\\r\\nsuch as a disease. In this manner, correlation network analysis has been a useful tool for gene screening, which can\\r\\nlead to identification of biomarkers and therapeutic targets. In addition to community detection, identifying hub\\r\\ngenes in co-expression networks helps finding key genes, for example, for cancer [82].\\r\\nDifferent ways of defining co-expression matrices and networks from gene expression data include tissue-to-tissue\\r\\nco-expression (TTC) networks [83] (also see [84, 85]). A TTC network proposed in [83] is a bipartite network, and\\r\\nits node is a gene-tissue pair. An edge between two nodes, denoted by (˜gi,t˜i) and (˜gj ,t˜j ), where ˜gi and ˜gj are genes,\\r\\nand t˜i and t˜j are tissues, represents the sample correlation as in conventional co-expression networks. However, by\\r\\ndefinition, the correlation is calculated only between node pairs belonging to different tissues, i.e., only for t˜i ̸= t˜j .\\r\\nTherefore, TTC networks characterize co-expression of genes across different tissues.\\r\\nCo-expression of genes i and j implies that i and j are both expressed at a high level in some samples (usually\\r\\nindividuals) and both expressed at a low level in other individuals. Co-expressed genes tend to be involved in common\\r\\nbiological functions. There are in fact multiple biophysical and non-biophysical reasons for gene co-expression [76].\\r\\nFor example, a transcription factor, a protein that binds to DNA, may regulate different genes i and j that are\\r\\nphysically close on a chromosome. If this is the case, differential levels of regulation by the transcription factor across\\r\\nindividuals can create co-expression of i and j. Another mechanism of co-expression is that the expression of genes\\r\\ni and j, which may be located far from each other on the chromosome or on different chromosomes, may depend on\\r\\nthe temperature. Then, i and j would be co-expressed if different individuals are sampled from living environments\\r\\nwith different temperatures. Variation in ages of the individuals can similarly create co-expression among age-related\\r\\ngenes. Alternatively, co-expression may originate from non-biological sources, such as technical or laboratory ones,\\r\\nwhose exact origins are often unknown.\\r\\nOne is often interested in looking for differential co-expression, which refers to the different levels of gene co\\x02expression between two phenotypically different sets of samples, such as a disease set versus a control set, or in two\\r\\ntypes of tissues [76, 78]. Differential co-expression often reveals information that one cannot obtain by examining\\r\\ndifferential expression (as opposed to co-expression), i.e., different levels of gene expression between the two sets of\\r\\nsamples [86].\\r\\n2.4 Metabolite networks\\r\\nMetabolites are small molecules (e.g., amino acids, lipids, vitamins) that are intermediates or end products of\\r\\nmetabolic reactions. One can also construct correlation networks from metabolomics data, or data of metabolites\\r\\nand their reactions [87, 88].To inform the edge, one measures pairwise correlation between the amounts of two\\r\\nmetabolites given the samples. Like gene co-expression, correlation between metabolites can occur for multiple\\r\\nreasons, including knock-out of a gene coding an enzyme that is involved in a chemical reaction consuming or\\r\\nproducing two metabolites, different temperatures or other environmental conditions under which different samples\\r\\nare obtained, or intrinsic variability owing to cellular metabolism [87]. Note that mass conservation within a moiety\\x02conserved cycle produces negative correlation between at least one pair of metabolites involved in the reaction [89].\\r\\nThat said, in some cases one may consider correlation or other similarity between only a subset of metabolites that\\r\\nare not necessarily associated to one another by direct chemical processes but instead draw from a set of alternative\\r\\nbiochemical processes (see, e.g., [90]).\\r\\n2.5 Microbiome networks\\r\\nMicrobes interact with other microbe species as well as with their environments. Understanding of microbial com\\x02position and interaction in the human gut is expected to inform multiple diseases. Similarly, understanding soil\\r\\nmicrobial communities may contribute to enhancing plant productivity. Network analysis is adept at revealing, e.g.,\\r\\necological community assembly and keystone taxa, and has been increasingly contributing to these fields.\\r\\nIn microbiome network analysis, one collects samples from, e.g., soil, at various time points or locations. Each\\r\\nsample from an environment (e.g., soil, gut, animal corpus, or water) contains various microorganisms with different\\r\\nquantities. Co-occurrence network analysis is increasingly common in this field, aided by an increasing amount\\r\\nand accuracy of data [33, 91, 92]. In a microbiome co-occurrence network, nodes are microorganisms (e.g., bacteria,\\r\\n2A phenotype is a set of observable traits of an organism and is usually contrasted with the underlying genotype that causes (or\\r\\ninfluences) the phenotype.\\r\\n5\\narchaea, or viruses), specified at the taxa level, for example, and an edge is defined to exist if two nodes co-occur across\\r\\nthe samples. Therefore, microbiome co-occurrence networks are essentially microbiome correlation networks, and the\\r\\nusual correlation measures, such as Pearson correlation, can be used to determine edge data, but more sophisticated\\r\\nmethods to define edges are more often used. (See [33, 91] for various co-occurrence network construction methods.)\\r\\nPositively weighted edges result because of, e.g., cooperation between two taxa, sharing of niche requirements, or co\\x02colonization. Negatively edges result because of, e.g., competition for space or resources, prey-predator relationships,\\r\\nor niche partitioning. A historically famous example of negative co-occurrence in ecological community assembly\\r\\nstudy is the checkerboard-like presence-absence patterns of two bird species inhabiting an island, discussed by Jared\\r\\nDiamond [93]. (Also see [33] for a historical account.) Regardless, one should keep in mind that correlation, or\\r\\nco-occurrence, does not immediately imply physical interaction between two taxa.\\r\\n2.6 Disease networks\\r\\nA node in a disease network is a disease phenotype. Correlation between two diseases defines an edge, and there are\\r\\nvarious definitions of edges as we introduce in this section. Each definition of edge creates a different type of disease\\r\\nnetwork.\\r\\nComorbidity, also called multimorbidity [94], is the simultaneous occurrence of multiple diseases within an indi\\x02vidual. One cause of comorbidity is that the same gene or disease-associated protein can trigger multiple diseases.\\r\\nOther causes, such as environmental factors or behaviors, such as smoking, can also result in comorbidity. A collec\\x02tion of potentially comorbid diseases can be modeled as the nodes of a network, and the edges, which are based on\\r\\ncomorbidity or other similarity index between diseases [88], are correlational in nature.\\r\\nThe authors of [95] constructed phenotypic disease networks (PDNs) in which nodes are disease phenotypes.\\r\\nThe edges are sample correlation coefficients or a variant, and the samples are patients in a hospital claim record\\r\\n(i.e., Medicare claims in the US). Note that here one uses correlation for binary variables because each sample (i.e.,\\r\\npatient) is either affected or not affected by any disease i. The authors found that, for example, patients tend to\\r\\ndevelop illness along the edges of the PDN [95].\\r\\nSimilarly, prior work constructed a human disease network when two diseases share at least one associated gene,\\r\\nwhich is similar in principle to the phenotypic disease network despite that the edge of the human disease network\\r\\nis not a conventional correlation coefficient [96] (also see [97]). Similarly, an edge in a metabolic disease network is\\r\\ndefined to exist when two diseases are either associated with the same metabolic reaction or their metabolic reactions\\r\\nare adjacent to each other in the sense that they share a compound that is not too common [98]. (H2O and ATP, for\\r\\nexample, are excluded because they are too common.) Alternatively, in a human symptom disease network [99], the\\r\\nedge between a pair of diseases is a correlation measure in which each sample is a symptom. In other words, roughly\\r\\nspeaking, the edge weight is large when two diseases share many symptoms.\\r\\n2.7 Financial correlation networks\\r\\nStocks of different companies are interrelated, and the prices of some of them tend to change similarly over time. A\\r\\ncommon transformation of such financial time series before constructing correlation matrices and networks is into\\r\\nthe time series of logarithmic return, i.e., the successive differences of the logarithm of the price, given by\\r\\nxi(t) = ln zi(t + 1)\\r\\nzi(t)\\r\\n, (1)\\r\\nwhere zi(t) is the price of the ith financial asset at time t, such as the closure price of the ith stock on day t, and\\r\\nt ∈ {0, . . . , T − 1}. An advantage of this method is that xi(t) is not susceptible to changes in the scale of zi(t) over\\r\\ntime [100]. Then, one constructs the correlation matrix for N time series {xi(1), . . . , xi(T −1)}, where i ∈ {1, . . . , N}.\\r\\nFinancial correlation matrices have been analyzed for decades. For example, Markowitz’s portfolio theory provides\\r\\nan optimal investment strategy as vector w = (w1, . . . , wN )\\r\\n⊤, where wi represents the fraction of investment in the\\r\\nith financial asset, and ⊤ represents the transposition [5, 6]. The theory formulates the minimizer of the variance of\\r\\nthe return, w⊤Cw, where C is the covariance matrix, as the solution of a quadratic optimization problem with the\\r\\nconstraint that the expected return, w⊤g, where g = (g1, . . . , gN )\\r\\n⊤, and gi\\r\\nis the expected return for the ith asset,\\r\\nis larger than a prescribed threshold.\\r\\nFinancial correlation matrices have also been extensively studied in econophysics research since the 1990s, with\\r\\nsuccessful uses of RMT [6, 100–106] and network methods such as maximum spanning trees [107, 108], community\\r\\ndetection [104–106, 109, 110], and more advanced methods (see [22, 23] for reviews). One usually employs RMT in\\r\\nthis context to verify that most eigenvalues of the empirical financial correlation matrices lie in the bulk part of the\\r\\ndistribution of eigenvalues for random matrices. Such results imply that most eigenvalues of the empirical correlation\\r\\n6\\nmatrices can be regarded as noise, and one is primarily interested in other dominant eigenvalues of the empirical\\r\\ncorrelation matrices whose values are not explained by random matrix theory [101–103, 106]. The largest eigenvalue\\r\\nis usually not explained by RMT and is often called the market mode because it represents the movement of the\\r\\nmarket as a whole; moreover, other deviating eigenvalues are also present, encoding for the presence of groups of\\r\\nstocks that move coherently, as we discuss in section 4.3.\\r\\nOther types of financial data are possible. For example, correlation networks were constructed from pairwise\\r\\ncorrelation between the daily time series of the investor’s behavior (i.e., the net volume of Nokia stock traded or its\\r\\nnormalized version) for two investors [111, 112]. One can also renormalize the covariance matrix using other indices,\\r\\nsuch as momentum [113].3\\r\\n2.8 Bibliometric networks\\r\\nApart from microbiome studies, bibliometric and scientometric studies are another research field in which co\\x02occurrence networks are often used [115,116]. For example, in an academic co-authorship network, a node represents\\r\\nan author, and an edge represents co-occurrence (i.e., collaboration) of two authors in at least one paper. One can\\r\\nweigh the edge according to the number of coauthored papers or its normalized variant [117]. While keeping authors\\r\\nas nodes, one can also create other types of co-occurrence networks, such as co-citation networks in which an edge\\r\\nconnects two authors whose papers are cited together by a later paper, and keyword-based co-occurrence networks\\r\\nin which an edge connects two authors sharing keywords associated with their papers. Nodes of co-occurrence bib\\x02liometric networks can also be journals, institutions, research areas, and so forth. These co-occurrence networks\\r\\nare mathematically close to correlation networks and have been useful for understanding research communities and\\r\\nspecialities, communication among researchers, interdisciplinarities, and the structure and evolution of science, for\\r\\nexample.\\r\\nVarious other web-based information has also been analyzed as co-occurrence networks. For example, tags\\r\\nannotating posts in social bookmarking systems can be used as nodes of co-occurrence networks [118]. Two tags\\r\\nare defined to form an edge if both tags appear on the same post at least once. One can also use the number of\\r\\nthe posts in which the two tags co-appear as the edge weight. Another example is co-purchase networks in online\\r\\nmarketplaces, in which a node represents an item, and an edge represents that customers frequently purchase the\\r\\ntwo items together [119].\\r\\n2.9 Climate networks\\r\\nClimate can be analyzed as a network of interconnected dynamical systems [120–123]. In most analyses, the nodes\\r\\nof the network are equal-angle latitude-longitude grid points on the globe. However, such angular partitions lead to\\r\\ngrid cells with geometric areas that vary with latitude, which in particular might lead to spurious correlations in the\\r\\nmeasured quantities, especially near the poles; such biases might be addressed either by a node splitting scheme that\\r\\naims to obtain consistent weights for the network parameters [124], or by choosing instead to work on a grid with\\r\\n(possibly only approximately) equal grid cell areas [125]. Each node has, for example, a time series measurement of\\r\\nthe pressure level, which represents wind circulation of the atmosphere. The edge between a pair of nodes is based\\r\\non the correlation between the two time series. An early study showed that all nodes in equatorial regions have\\r\\nlarge degree (i.e., the number of edges that a node has) regardless of the longitude, whereas only a small fraction of\\r\\nnodes in the mid-latitude regions had large degrees [120]. Climate networks have been further used for understanding\\r\\nmechanisms of climate dynamics and predicting extreme events. For example, early warning signals were constructed\\r\\nfrom the degree of the nodes and clustering coefficient for climate networks of the Atlantic temperature field [126].\\r\\nThe proposed early warning signals were effective at anticipating the collapse of Atlantic Meridional Overturning\\r\\nCirculation. See section 2.1 of [123] for more examples.\\r\\n3 Methods for creating networks from correlation matrices\\r\\nTo apply network analysis to correlation matrix data, we need to generate a network from correlation data (usually\\r\\nin the form of a correlation matrix). We call such a network a correlation network. Whether correlation network\\r\\nanalysis works or is justified depends on this process. Although there are various methods for constructing correlation\\r\\nnetworks from data, they have pros and cons. Furthermore, there are various unjustified practices around correlation\\r\\n3Momentum in finance generally refers to the rate of change in price. If the prices of two assets are correlated, the momentum of one\\r\\nasset can be informative of the future price of the correlated asset [114]. In [113], momentum and price correlation are mixed in various\\r\\nways to construct correlation-type networks that reflect collective price dynamics, and, for example, network centrality is predictive of\\r\\nlarge upcoming swings in asset prices.\\r\\n7\\nnetwork generation, which may yield serious limitations on the power of correlation network analysis. In this section,\\r\\nwe review several major methods.\\r\\n3.1 Estimation of covariance and correlation matrices\\r\\nHow to estimate covariance matrices from observed data when the matrix size is not small is a long-standing question\\r\\nin statistics and surrounding research fields. In particular, the sample covariance matrix, a most natural candidate, is\\r\\nknown to be an unreliable estimate of the covariance matrix. See [6,127–129] for surveys on estimation of covariance\\r\\nmatrices. Although the primary focus of this paper is estimation of correlation networks, not covariance or correlation\\r\\nmatrices, it is of course important to realize that correlation networks created from unreliably estimated correlation\\r\\nmatrices are themselves unreliable. Therefore, we briefly survey a few techniques of covariance and correlation\\r\\nmatrix estimation in this section, including providing the notations and preliminaries used in the remainder of this\\r\\npaper. This exposition is important in particular because correlation network analysis in non-statistical research\\r\\nfields such as network science and also various applications often ignores statistical perspectives examined in the\\r\\nprevious studies.\\r\\nWe denote by (xiℓ) an N × L data matrix, where N is the number of nodes to observe the signal from, and L is\\r\\nthe number of samples, which is typically the length of the time series or the number of participants in an experiment\\r\\nor questionnaire. The sample covariance matrix, C\\r\\nsam = (Csam\\r\\nij ), is given by\\r\\nC\\r\\nsam\\r\\nij =\\r\\n1\\r\\nL − 1\\r\\nX\\r\\nL\\r\\nℓ=1\\r\\n(xiℓ − xi)(xjℓ − xj ), (2)\\r\\nwhere\\r\\nxi =\\r\\nX\\r\\nL\\r\\nℓ=1\\r\\nxiℓ\\r\\nL\\r\\n(3)\\r\\nis the sample mean of the signal from the ith node.4 Because Eq. (2) is a sum of L outer products, the rank of C\\r\\nsam\\r\\nis at most L. One can understand this fact more easily by rewriting Eq. (2) as\\r\\nC\\r\\nsam =\\r\\n1\\r\\nL − 1\\r\\nX\\r\\nL\\r\\nℓ=1\\r\\nx˜ℓx˜\\r\\n⊤\\r\\nℓ\\r\\n, (4)\\r\\nwhere x˜ℓ = (x1ℓ − x1, . . . , xNℓ − xN )\\r\\n⊤ Therefore, Csam is singular if L < N, while the converse does not hold true\\r\\nin general.\\r\\nAlthough covariance matrices are mathematically convenient, they are not normalized. In particular, if we\\r\\nmultiply the data from the ith node by c (> 0), then C\\r\\nsam\\r\\nij (= C\\r\\nsam\\r\\nji ), where j ̸= i, changes by a factor of c, and C\\r\\nsam\\r\\nii\\r\\nchanges by a factor of c\\r\\n2\\r\\n, whereas all the other entries of C\\r\\nsam remain the same. In practice, the data from different\\r\\nnodes may have different baseline fluctuation levels. For example, the price of the ith stock may fluctuate much more\\r\\nthan that of the jth stock because the former has a larger average or the industry to which the ith company belongs\\r\\nmay be subject to higher temporal variability. The correlation matrix, denoted by ρ, normalizes the covariance\\r\\nmatrix such that ρ is not subject to the effect of different overall amounts of fluctuations across different nodes. The\\r\\nsample Pearson correlation matrix, denoted by ρ\\r\\nsam = (ρsam\\r\\nij ), is defined by\\r\\nρ\\r\\nsam\\r\\nij =\\r\\nPL\\r\\nℓ=1(xiℓ − xi)(xjℓ − xj )\\r\\nqPL\\r\\nℓ=1(xiℓ − xi)\\r\\n2\\r\\nqPL\\r\\nℓ=1(xjℓ − xj )\\r\\n2\\r\\n=\\r\\nC\\r\\nsam\\r\\np\\r\\nij\\r\\nCsam\\r\\nii Csamjj\\r\\n. (5)\\r\\nNote that ρ\\r\\nsam\\r\\nii = 1 ∀i ∈ {1, . . . , N}. Also note that every sample correlation matrix is a sample covariance matrix\\r\\nof some data but not vice versa. A correlation matrix is characterized by positive semidefiniteness, symmetry, range\\r\\nof the entries only being [−1, 1], and the diagonal being equal to 1 [130]. The set of full-rank correlation matrices\\r\\nfor a fixed N is called the elliptope, which has its own geometric structure [131, 132]. For standardized samples\\r\\nyiℓ = (xiℓ − xi)/\\r\\np\\r\\nCsam\\r\\nii , the Euclidean distance between vectors (yi1, . . . , yiL) and (yj1, . . . , yjL) is given by\\r\\nd\\r\\n2\\r\\nij =\\r\\nX\\r\\nL\\r\\nℓ=1\\r\\n(yiℓ − yjℓ)\\r\\n2 = 2 − 2ρ\\r\\nsam\\r\\nij . (6)\\r\\n4Note the L − 1 in the denominator of Eq. (2) is necessary to obtain an unbiased estimator.\\r\\n8\\nTherefore, given the sample correlation matrix, dij =\\r\\nq\\r\\n2(1 − ρ\\r\\nsam\\r\\nij ) defines a Euclidean distance [100, 107].\\r\\nIn the following, we refer to covariance matrices in some cases and correlation matrices in others, often inter\\x02changeably. This is because the correlation matrix, which is a normalized quantity, should be used in most data\\r\\nanalyses, while the covariance matrix allows better mathematical analysis in most cases. This convention is not\\r\\nproblematic because, if we standardize the given data first and then calculate the covariance matrix for the stan\\x02dardized data (yiℓ), then the obtained covariance matrix is also a correlation matrix. Therefore, the mathematical\\r\\nresults for covariance matrices also hold true for correlation matrices as long as we feed the pre-standardized data\\r\\nto the analysis pipeline.\\r\\nWith the above consideration in mind, we now stress that it is important to distinguish the sample covariance\\r\\nmatrix, which is calculated from empirical data, from the theoretical or ‘true’ (also called population) covariance\\r\\nmatrix. One may use the true covariance matrix to model the observed data mathematically in terms of a random\\r\\nprocess described by a (stationary) probability distribution. Let Xi denote a random variable for i ∈ 1, . . . , N. The\\r\\ntrue covariance matrix C = (Cij ) is given by\\r\\nCij = E[(Xi − µi)(Xj − µj )], (7)\\r\\nwhere E represents the expectation, and µi = E[Xi] is the ensemble mean of Xi. Equation (7) implies that a\\r\\ncovariance matrix is a symmetric matrix. It is also a positive semidefinite matrix. Conversely, a symmetric positive\\r\\nsemidefinite matrix C is always a covariance matrix for the following reason. Any positive semidefinite matrix M\\r\\nallows a positive semidefinite matrix, denoted by M\\r\\n1\\r\\n2 , as square root. We set\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ed\\r\\nX1\\r\\n.\\r\\n.\\r\\n.\\r\\nXN\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f8 = C\\r\\n1\\r\\n2\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ed\\r\\nU1\\r\\n.\\r\\n.\\r\\n.\\r\\nUN\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f8 , (8)\\r\\nwhere U1, . . ., UN are independent random variables, with each having mean 0 and variance 1. Then, it is straight\\x02forward to see that (X1, . . . , XN )\\r\\n⊤ has mean 0 and covariance matrix C.\\r\\nHaving distinguished population and sample covariance matrices, we now look for a relationship between the two.\\r\\nIf we regard Xi as a random variable, and the observed data as a possible realization of that variable, then we must\\r\\nalso regard the sample covariance matrix C\\r\\nsam as a random variable, and the empirical sample correlation matrix as\\r\\na single realization of that variable. Then, the relevant question is the characterization of the probability distribution\\r\\ngoverning the sample covariance matrix, given the true covariance matrix C which acts a set of fixed parameters (to\\r\\nbe estimated from the data) for the distribution. Clearly, the number L of observations, regarded as the number of\\r\\nindependent draws for each random variable Xi, is another parameter (not to be estimated). For any finite L, the\\r\\nsample covariance matrix obeys the so-called Wishart distribution with L degrees of freedom, denoted by WN (L, C),\\r\\nunder the assumption that the L samples are i.i.d. and obey the multivariate normal distribution whose covariance\\r\\nmatrix is C [3, 6, 133, 134]. We obtain E[C\\r\\nsam] = C. In other words, the sample covariance matrix is an unbiased\\r\\nestimator of the true covariance matrix, called the Pearson estimator in statistics. The variance of C\\r\\nsam\\r\\nij is equal to\\r\\n(C\\r\\n2\\r\\nij + CiiCjj )/L. In fact, C\\r\\nsam is a problematic substitute of C, and the use of Csam in applications in place of C\\r\\ntends to fail; see [6] for an example in portfolio optimization. An intuitive reason why C\\r\\nsam is problematic is that, if\\r\\nL is not much larger than N, which is often the case in practice, one would need to estimate many parameters from\\r\\na relatively few observations. Specifically, the covariance and correlation matrices have N(N + 1)/2 and N(N − 1)/2\\r\\nunknowns to infer, respectively, whereas there are L samples of vector (x1ℓ, . . . , xNℓ) available [135]. If N/L is not\\r\\nvanishingly small (called the large dimension limit or the Kolmogorov regime [6]), then the estimation would fail. As\\r\\nan extreme example, if L < N, then C\\r\\nsam is singular, but the true C may be nonsingular. Even if L ≥ N, matrix\\r\\nC\\r\\nsam may be ill-conditioned if L is not sufficiently greater than N, whereas C may be well-conditioned.\\r\\nTherefore, covariance selection to reduce the number of parameters to be estimated is a recommended practice\\r\\nwhen L is not large relative to N [135]. One also says that we impose some structure on the estimator of the\\r\\ncovariance matrix, with the mere use of the sample covariance matrix as an estimate of the true covariance matrix\\r\\ncorresponding to no assumed structure.\\r\\nA major method of covariance selection is to impose sparsity on the covariance matrix or the so-called precision\\r\\nmatrix (also called the concentration matrix), which is the inverse of the covariance matrix (with entries representing\\r\\nun-normalized partial correlation coefficients; see section 3.6). Note that a sparse precision matrix does not imply that\\r\\nthe corresponding covariance matrix (i.e., its inverse) is sparse. Graphical lasso (see section 3.7) is a popular method\\r\\nto estimate a sparse precision matrix. Another major method to estimate a sparse correlation matrix is to threshold\\r\\non the value of the correlation to discard node pairs with correlation values close to 0 (see section 3.2). Another\\r\\ncommon method of covariance selection, apart from estimating a sparse covariance matrix, is covariance shrinkage\\r\\n9\\n(see [136] for a review). With covariance shrinkage, the estimated covariance matrix is a linear weighted sum of\\r\\nthe sample covariance matrix, C\\r\\nsam, and a much simpler matrix, called the shrinkage target, such as the identity\\r\\nmatrix [137–139] or the so-called single-index model (which is a one-factor model in factor analysis terminology and\\r\\nis an approximation of C\\r\\nsam by a rank-one matrix plus residuals) [137]. Note that the shrinkage target is a biased\\r\\nestimator of C. These and other covariance selection methods balance between the estimation biases and variances.\\r\\nAn advanced estimation method for the entire correlation matrix, based on RMT, is the so-called optimal rota\\x02tionally invariant estimator (RIE) [6, 65]. Roughly speaking, the RIE is the closest (in some spectral sense) matrix,\\r\\namong all those having the same eigenvectors as the sample correlation matrix, to the ‘true’ correlation matrix. It\\r\\nuses a certain self-averaging property to infer the spectrum of the true matrix from that of the sample matrix, and\\r\\nthen to compute the spectral distance from the true matrix to be minimized [6]. A specific cross-validated version\\r\\nof the RIE has been recently shown to outperform several other estimators [65]. Since the RIE requires some notion\\r\\nof RMT to be properly defined, we discuss it in section 4.3.\\r\\n3.2 Dichotomization\\r\\nIn this and the following subsections, we present several methods to generate undirected networks from correlation\\r\\nmatrices.\\r\\nA simple method to generate a network from the given correlation matrix is thresholding, which in its simplest\\r\\nform entails setting a threshold θ, and placing an unweighted edge (i, j) if and only if the Pearson correlation ρij ≥ θ;\\r\\notherwise, we do not include an edge (i, j). It is also often the case that one thresholds on |ρij |. There are mainly two\\r\\nchoices after the thresholding. First, we may discard the weight of the surviving edges to force it to 1, creating an\\r\\nunweighted network. Second, we may keep the weight of the surviving edge to create a weighted network. See Fig. 2\\r\\nfor these two cases. The literature often use the term thresholding in one of the two meanings without clarification.\\r\\nIn the remainder of this paper, we call the first case dichotomizing (which can be also called binarizing), which is,\\r\\nprecisely speaking, a shorthand for “thresholding followed by dichotomizing”. We discuss dichotomized networks in\\r\\nthis section and threshold networks without dichotomization (yielding weighted networks) in sections 3.4 and 3.7.\\r\\nDichotomizing has commonly been used across research areas. However, researchers have repeatedly pointed out\\r\\nthat dichotomizing is not recommended for multiple reasons.\\r\\nFirst, no consensus exists regarding the method for choosing the threshold value [24,140–144] despite that results\\r\\nof correlation network analysis are often sensitive to the threshold value [141–147]. In a related vein, if a single\\r\\nthreshold value is applied to the correlation matrix obtained from different participants in an experiment, which\\r\\nis typical in neuroimaging data analysis and referred to as an absolute threshold [142, 148], the edge density can\\r\\nvary greatly across participants. Since edge density is heavily correlated with many network measures, this can be\\r\\nseen as introducing a confound into subsequent analyses and casts doubt on consequent conclusions, e.g., that sick\\r\\nparticipants tend to have less small-world brain networks than healthy controls. (In this example, a network with\\r\\na large edge density would in general yield a small average path length and large clustering coefficient, leading to\\r\\nthe small-world property, so that density differences alone could have driven the observed effect.) An alternative\\r\\nmethod for setting the threshold is the so-called proportional thresholding, with which one keeps a fixed fraction\\r\\nof the strongest (i.e., most correlated) edges to create a network, separately for each participant [142, 148]; also\\r\\nsee [149–151] for an early study. In this manner, the thresholded networks for different participants have the same\\r\\ndensity of edges. However, while the proportional thresholding may sound reasonable, it has its own problems [148].\\r\\nFirst, because different participants have different magnitudes of overall correlation coefficient values, the proportional\\r\\nthreshold implies that one includes relatively weakly correlated node pairs as edges for participants with an overall\\r\\nlow correlation coefficients. This procedure increases the probability of including relatively spurious node pairs, which\\r\\ncan be regarded as type I errors (i.e., false positives), increasing noise in the resulting network. (Also see [152, 153]\\r\\nfor discussion on this matter.) Second, the overall correlation strength is often predictive of, for example, a disease\\r\\nin question. The proportional threshold enforces the same edge density for the different participants’ networks.\\r\\nTherefore, it gives up the possibility of using the edge density, which is a simplest network index, to account for the\\r\\ngroup difference. If one uses the absolute threshold, the edge density is different among participants, and one can\\r\\nuse it to characterize participants. The edge density in the proportional thresholding is also an arbitrary parameter.\\r\\nSecond, apart from false positives due to keeping small-correlation node pairs as edges, correlation networks at\\r\\nleast in its original form suffer from false positives because pairwise correlation does not differentiate between direct\\r\\neffects (i.e., nodes i and j are correlated because they directly interact) and indirect effects (i.e., nodes i and j are\\r\\ncorrelated because i and k interact and j and k interact). In other words, correlations are transitive. The correlation\\r\\ncoefficient is lower-bounded by [154]\\r\\nρij ≥ ρikρjk −\\r\\nq\\r\\n(1 − ρik)\\r\\n2(1 − ρjk)2. (9)\\r\\n10\\n0.2\\r\\n-0.1\\r\\n0.35\\r\\n0.15\\r\\n0.1\\r\\n0.2\\r\\n1 2\\r\\n3 4\\r\\n1 2\\r\\n3 4\\r\\n0\\r\\nBB@\\r\\n1 0.35 0.1 0.1\\r\\n0.35 1 0.2 0.15\\r\\n0.1 0.210.2\\r\\n0.1 0.15 0.2 1\\r\\n1\\r\\nCCA\\r\\n=\\r\\ncorrelation matrix\\r\\nthresholded network\\r\\n1 2\\r\\n3 4\\r\\nthresholded and \\r\\ndichotomized network\\r\\n0.2\\r\\n0.35\\r\\n0.15 0.2\\r\\n1\\r\\n1\\r\\n1 1\\r\\nweighted network analysis\\r\\nunweighted network analysis\\r\\ndichotomizing\\r\\nthresholding (with threshold = 0.12)\\r\\nFigure 2: Thresholding a correlation matrix. We set the threshold at θ = 0.12. If we only threshold the correlation\\r\\nmatrix, we obtain a weighted network. If we further dichotomize the thresholded matrix, we obtain an unweighted\\r\\nnetwork. A different θ yields a different network in general.\\r\\n11\\nEquation (9) implies that if ρik and ρjk are large, i.e., sufficiently close to 1, then ρij is positive. Furthermore, this\\r\\nlower bound of ρij is usually not tight, suggesting that ρij tends to be more positive than what Eq. (9) suggests\\r\\nwhen ρik, ρjk > 0 [155, 156]. This false positive problem is the main motivation behind the definition of the partial\\r\\ncorrelation networks and related methods with which to remove such a third-party effect, i.e., influence of node k in\\r\\nEq. (9). (See section 3.6.) Instead, one may want to suppress false positives by carefully choosing a threshold value.\\r\\nLet us consider the absolute thresholding. For example, if i and j do not directly interact, i and k do, j and k also\\r\\ndo, yielding ρij = 0.4, ρik = 0.7 and ρjk = 0.6, then setting θ = 0.5 enables us to remove the indirect effect by k.\\r\\nHowever, it may be the case that i\\r\\n′ and j′ do not directly interact, i′ and k′ do, j′ and k′ also do, yielding ρi\\r\\n′j′ = 0.2,\\r\\nρi\\r\\n′k′ = 0.3, and ρj′k′ = 0.4. Then, thresholding with θ = 0.5 dismisses direct as well as indirect interactions (that\\r\\nis, it introduces false negatives). A related artifact introduced by the combination of thresholding and indirect\\r\\neffects is that thresholding tends to inflate the abundance of triangles, as measured by the clustering coefficient\\r\\nfor dichotomized (and therefore unweighted) networks, and other short cycles [87, 156]; even correlation networks\\r\\ngenerated by dichotomizing randomly and independently generated data {xiℓ} have high clustering coefficients [156].\\r\\nThis phenomenon resembles the fact that spatial networks tend to have high clustering just because the network is\\r\\nspatially embedded [157, 158].\\r\\nThird, whereas thresholding has been suggested to be able to mitigate uncertainty on weak links (including the\\r\\ncase of the proportional thresholding to some extent) and enhance interpretability of the graph-theoretical results\\r\\n(e.g., [24]), thresholding in fact discards the information contained in the values of the correlation coefficient. For\\r\\nexample, in Fig. 2, thresholding turns a correlation of −0.1 and 0.1 into the absence of an edge. Furthermore, if\\r\\nwe dichotomize the edges that have survived thresholding, a correlation of 0.2 and 0.35 are both turned into the\\r\\npresence of an edge.\\r\\nThere are various methods to try to mitigate some of these problems. In the remainder of this section, we cover\\r\\nmethods related to dichotomizing.\\r\\nOne family of solutions is to integrate network analysis results obtained with different threshold values [24] (but\\r\\nsee [159] for a critical discussion). For example, one can calculate a network index, such as the node’s degree, denoted\\r\\nby α, as a function of the threshold value, θ, and fit a functional form to the obtained function α(θ) to characterize\\r\\nthe node [146]. Similarly, one can calculate α for a range of θ values and take an average of α [26, 160]. In the case\\r\\nof group-to-group comparison, an option along this line of idea is the functional data analysis (FDA), with which\\r\\none looks at α as a function of θ across a range of θ values and statistically test the difference between the obtained\\r\\nfunction for different groups by a nonparametric permutation test [161, 162]. In these methods, how to choose the\\r\\nrange of θ is a nontrivial question.\\r\\nA different strategy is to determine the threshold value according to an optimization criterion. For example, a\\r\\nmethod was proposed [143] for determining the threshold value as a solution of the optimization of the trade-off\\r\\nbetween the efficiency of the network [163] and the density of edges. Another method to set θ is to use the highest\\r\\npossible threshold that guarantees all or most (e.g., 99%) of nodes are connected [164].\\r\\nThe so-called maximal spanning tree is an easy and classical method to automatically set the threshold by\\r\\nguaranteeing that the network is connected [100, 107], while at the same time avoiding the creation of edges that\\r\\nwould form loops (and are therefore unnecessary for connectedness). One adds the largest correlation node pairs as\\r\\nedges one by one under the condition that the generated network is the tree. In the end, the maximal spanning tree\\r\\ncontains all the N nodes, and the number of edges is N − 1. Thanks to the mapping from (large) sample correlation\\r\\ncoefficients to (small) Euclidean distances established by Eq. (6), the maximal (in the sense of correlation) spanning\\r\\ntree is sometimes called the minimum (in the sense of distance) spanning tree [100, 107] (MST). The MST can be\\r\\nviewed as the graph achieving the overall minimum length among all graphs that make the N nodes reachable from\\r\\none another. Here, the minimum length is defined as the sum of the lengths of its realized edges, and the length\\r\\nof an edge is the metric distance between its endpoints. The maximal spanning tree also allows a hierarchical tree\\r\\nrepresentation, which facilitates interpretation [107,165,166]. However, the generated network is extreme in the sense\\r\\nthat it is a most sparse network among all the connected networks on N nodes, without any triangles. A variant\\r\\nof the maximum spanning tree is to sequentially add edges with the largest correlation value under the constraint\\r\\nthat the generated network can be embedded on a surface of a prescribed genus value (roughly speaking, the given\\r\\nnumber of holes) without edge intersection [167]. If the genus is constrained to be zero, the resulting network is a\\r\\nplanar graph, called the planar maximally filtered graph (PMFG). The PMFG contains 3N − 6 edges. The PMFG\\r\\ncontains more information than the maximum spanning tree (which is in any case contained in the PMFG), such as\\r\\nsome cycles and their statistics. Note that these methods effectively produce a threshold for the correlation value\\r\\nto be retained, but on the other hand preserve only some of the edges that exceed the threshold. Indeed, they are\\r\\n(designed to be) irreducible to the mere identification of an overall threshold value, with their merit residing in the\\r\\nintroduction of higher-order geometric constraints guiding the dichotomization procedure.\\r\\nAnother related method is to use the k nearest neighbor graph of the correlation matrix, with which each ith node\\r\\n12\\nis connected at least to the k nodes with the highest correlation with the ith node [153]. Yet another choice, which is\\r\\ndesigned for general weighted networks, is the disparity filter, with which one preserves only statistically significant\\r\\nedges to generate the network backbone [168, 169]. Note that, with these methods as well, some lower-correlation\\r\\nnode pairs are retained as edges and some higher-correlation edges are discarded.\\r\\nApplication example: Wang et al. compared functional networks of the brain between children with attention\\x02deficit hyperactivity disorder (ADHD) and healthy controls using fMRI data [170]. The brain scan lasted for eight\\r\\nminutes in the resting state, and the brain image was acquired every two seconds. The authors used a previously\\r\\npublished popular brain atlas defined by three-dimensional coordinates of 90 anatomical regions of interest (45 per\\r\\nhemisphere), each of which defines a node of the network. After various steps of preprocessing the original data,\\r\\nthey computed the Pearson correlation coefficient between the time series at each pair of nodes, separately for each\\r\\nchild. Aware of the problem inherent in choosing a threshold value, which we discussed earlier in this section, the\\r\\nauthors examined dichotomized networks for a range of threshold values θ ∈ [0.05, 0.5]. As downstream analysis,\\r\\nthey measured the small-world-ness of networks in terms of the so-called global efficiency and local efficiency [163].\\r\\nA large global efficiency value and a small local efficiency value suggest that the network has a small-world property.\\r\\nThey found that, while the networks of both ADHD and healthy control children were small-world, those of children\\r\\nwith ADHD were somewhat less small-world than those of the controls across a wide range of θ; the difference was\\r\\nstatistically significant for the local efficiency in some range of θ.\\r\\n3.3 Persistent homology\\r\\nIn the previous section, we discussed the idea of integrating analysis of dichotomized networks over different threshold\\r\\nvalues to mitigate the effect of selecting a single threshold value. Topological data analysis, or more specifically,\\r\\npersistent homology, provides a systematic and mathematically founded suite of methods to do such analyses. In\\r\\ntopological analysis in general, one focuses on properties of shapes that do not change under continuous deformations\\r\\nof them, such as stretching and bending. Such topologically invariant properties include the numbers of connected\\r\\ncomponents and of holes, which can be calculated through the so-called homology group. Persistent homology\\r\\ncaptures the changes in the network structure over multiple scales, or over a range of threshold values, clarifying\\r\\ntopological features that are robust with respect to the threshold choice. In this broader perspective, networks are\\r\\nonly a particular instance of the type of topological object under major consideration in topological data analysis,\\r\\ncalled simplicial complexes, because networks only consider pairwise interactions. One may want to consider the\\r\\nclique complex, in which each k-clique (i.e., complete subgraph with k nodes) in the network is defined to be a\\r\\nhigher-order object called a (k − 1)-simplex and belongs to the simplicial complex of the given data. Note that the\\r\\nclique complex contains all the edges of the original network as well because an edge is a 2-clique by definition. For\\r\\nreviews of topological data analysis including persistent homology, see [171–175].\\r\\nTo analyze correlation matrix data using persistent homology, we start with a point cloud, with each point\\r\\ncorresponding to a node in a correlation network. Then, we introduce a distance between each pair of nodes, dij ,\\r\\nwhere 1 ≤ i, j ≤ N. By setting a threshold value θ\\r\\n′\\r\\n, we obtain a dichotomized network, or, e.g., a clique complex,\\r\\ndepending on the choice, denoted by Gθ\\r\\n′ . The simplicial complexes with varying θ\\r\\n′ values forms a filtration, i.e.,\\r\\nwith the nestedness property\\r\\nGθ\\r\\n′\\r\\n1\\r\\n⊆ Gθ\\r\\n′\\r\\n2\\r\\n⊆ Gθ\\r\\n′\\r\\n3\\r\\n⊆ · · · , for θ\\r\\n′\\r\\n1 ≤ θ\\r\\n′\\r\\n2 ≤ θ\\r\\n′\\r\\n3 ≤ · · · , (10)\\r\\nwith the inclusion relationship in Eq. (10) referring to that of the edge set and that of any higher-order simplex\\r\\nobject. The collection of clique complexes in Eq. (10) is called the Vietoris-Rips filtration. Simply put, with a\\r\\nlarger threshold on the distance between nodes, the generated simplicial complex has more edges (and higher-order\\r\\nsimplexes in the case of the clique complex). In practice, it suffices to consider the sequence of threshold values\\r\\n{θ\\r\\n′\\r\\n1\\r\\n, θ′\\r\\n2\\r\\n, . . .}, such that Gθ\\r\\n′\\r\\nm+1\\r\\n, contains just one more edge (and some higher-order simplexes containing that edge)\\r\\nthan Gθ\\r\\n′m\\r\\n; if there are multiple node pairs with exactly the same distance value, the corresponding multiple edges not\\r\\nin Gθ\\r\\n′m\\r\\nsimultaneously appear in Gθ\\r\\n′\\r\\nm+1\\r\\n. If θ\\r\\n′\\r\\nis taken to vary over all possible topologies, the simplicial complex Gθ\\r\\n′\\r\\n1\\r\\nis composed of N isolated nodes, while the last one in the nested sequence is the complete graph (that is, precisely,\\r\\nthe corresponding clique complex) of N nodes.\\r\\nA large correlation should correspond to a small dij . One can realize this by setting dij = f(ρ\\r\\nsam\\r\\nij ), where f is a\\r\\nmonotonically decreasing function. Then, the network interpretation of Eq. (10) simply states the nested relationship\\r\\nin which the edges existing in a dichotomized network are included in dichotomized networks with smaller thresholds.\\r\\nHowever, while this practice is common, dij is not mathematically guaranteed to be a distance metric, typically\\r\\nviolating the triangle inequality if we use an arbitrary monotonically decreasing function f. Therefore, one often\\r\\nuses f that makes dij a distance metric, such as Eq. (6) or variants thereof. Then, the ensuing topological data\\r\\nanalysis of correlation networks is underpinned by stronger mathematical foundations.\\r\\n13\\nThe next step is to calculate for each Gθ\\r\\n′ the homology groups or associated quantities, such as the kth Betti\\r\\nnumber. The zeroth and first Betti numbers are the numbers of connected components and essentially different cycles,\\r\\nrespectively. These and other topological features of Gθ\\r\\n′ depend on θ\\r\\n′\\r\\n. For example, each connected component and\\r\\ncycle may appear (through closing loops) and disappear (through mergers with others) as one gradually increases\\r\\nthe distance threshold θ\\r\\n′\\r\\nfrom θ\\r\\n′ = 0, at which all the nodes are isolated. One can precisely visualize the occurrence\\r\\nof the birth and death events of each component by the persistence barcodes or persistence diagrams. For example,\\r\\nthe persistence diagram represents each connected component, cycles, two-dimensional voids, etc. as a point (x, y) in\\r\\nthe two-dimensional space in which x represents the θ\\r\\n′ value at which, e.g., a cycle appears and y (≥ x) represents\\r\\nthe θ\\r\\n′ value at which the same cycle disappears. If y − x is large, that particular feature of the data is robust over\\r\\nchanging scales because it is independent of the specific value of θ\\r\\n′ within a relatively large range.\\r\\nWe are often motivated to compare different correlation matrices or networks, such as in the comparison of func\\x02tional brain networks between the patient group and control group. Quantitative comparison between two persistent\\r\\ndiagrams provides a threshold-free method to effectively compare dichotomized networks. A persistence diagram\\r\\nconsists of a set of two-dimensional points (x, y). The bottleneck and Wasserstein distance, between persistence\\r\\ndiagrams P1 and P2, which are commonly used, first consider the best matching between (x, y) in P1 and that in P2.\\r\\nFor the obtained best matching, both distance metrics measure the distance between (x, y) in P1 and that in P2 in\\r\\nthe Euclidean space and tally up the distance over all the points in different manners. For instance, the bottleneck\\r\\ndistance is given by d(P1, P2) = infγ sup(x,y)∈P1\\r\\n∥(x, y) − γ((x, y))∥∞, where γ is a matching between P1 and P2.\\r\\nPersistent homology has been applied to correlation networks in neuronal activity data [141, 144, 176, 177], gene\\r\\nco-expression data [173, 178, 179], financial data [180, 181], and to co-occurrence networks of characters in literature\\r\\nwork and in academic collaboration [182]. Scalability of persistent homology algorithms remains a concern. However,\\r\\nit may be of less concern for correlation network analysis because the number of nodes allowed for correlation networks\\r\\nis typically limited by the length of the data, not by the speed of algorithms (see section 3.1).\\r\\n3.4 Weighted networks\\r\\nA strategy for avoiding the arbitrariness in the choice of the threshold value and loss of information in dichotomizing\\r\\nis to use weighted networks, retaining the pairwise correlation value as the edge weight [73, 140]. Although there\\r\\nare numerous settings in network science where negative edge weights are considered, they are generally more\\r\\ndifficult to treat. (See section 3.5.) As such, two common methods to create positively weighted networks are\\r\\n(1) using the absolute value of the correlation coefficient as the edge weight and (2) ignoring negatively weighted\\r\\nedges and only using the positively weighted edges. Both methods dismiss some information contained in the original\\r\\ncorrelation matrix, i.e., the sign of the correlation or the magnitude of the negative pairwise correlation. Nonetheless,\\r\\nthese transformations are widely used because many methods are available for analyzing general positively weighted\\r\\nnetworks, many of which are extensions of the corresponding methods for unweighted networks. One can also use\\r\\nmethods that are specifically designed for weighted networks [183].\\r\\nIt should be noted that weighted networks share the problem of false positives due to indirect interaction between\\r\\nnodes with the unweighted networks created by dichotomization. We also note that, in contrast to thresholding\\r\\n(which may be followed by dichotomization), node pairs with any small correlation (i.e., correlation coefficient close\\r\\nto 0) are kept as edges in the case of the weighted network. This may increase the uncertainty of the generated\\r\\nnetwork and hence of the subsequent network analysis results.\\r\\nThresholding operations in statistics literature to increase the sparsity of the estimated covariance matrix often\\r\\nproduce weighted networks. This is in contrast to the dichotomization, which produces unweighted networks. Hard\\r\\nthresholding in statistics literature refers to coercing C\\r\\nsam\\r\\nij , with i ̸= j, to 0 if\\r\\n\\x0c\\r\\n\\x0cC\\r\\nsam\\r\\nij\\r\\n\\x0c\\r\\n\\x0c < θ and keep the original\\r\\nC\\r\\nsam\\r\\nij if\\r\\n\\x0c\\r\\n\\x0cC\\r\\nsam\\r\\nij\\r\\n\\x0c\\r\\n\\x0c ≥ θ [127, 184–186]. Soft thresholding [184, 186, 187] transforms C\\r\\nsam\\r\\nij by a continuous non-decreasing\\r\\nfunction of C\\r\\nsam\\r\\nij , denoted by f(C\\r\\nsam\\r\\nij ), such that\\r\\nf(x) =\\r\\n\\uf8f1\\r\\n\\uf8f4\\uf8f2\\r\\n\\uf8f4\\uf8f3\\r\\nx − θ if x ≥ θ,\\r\\n0 if − θ < x < θ,\\r\\nx + θ if x ≤ −θ.\\r\\n(11)\\r\\nThis assumption implies that, in contrast to hard thresholding, there is no discontinuous jump in the transformed edge\\r\\nweight at C\\r\\nsam\\r\\nij = ±θ. Both hard and soft thresholding, as well as a more generalized class of thresholding function\\r\\nf(x) [186], do not imply dichotomization and therefore generate weighted networks. In numerical simulations, all\\r\\nthese thresholding methods to generate weighted networks outperformed the sample covariance matrix in estimating\\r\\ntrue sparse covariance matrices [186]. The same study also found that there was no clear winner between hard or\\r\\nsoft thresholding, while combination of them tended to perform somewhat better than other types of thresholding.\\r\\n14\\nAdaptive thresholding refers to using threshold values that depend on (i, j). An example is to use Eq. (11), but\\r\\nwith an (i, j)-dependent threshold value, denoted by θij , in place of θ, with θij = c\\r\\np\\r\\nVarij · log N/L, where c is a\\r\\nconstant, and Varij is an estimate of the variance of (Xi−µi)(Xj−µj ) [188]. This adaptive thresholding theoretically\\r\\nconverges faster to the real covariance matrix and performs better numerically than universal thresholding schemes\\r\\n(i.e., using a threshold value independent of (i, j)).\\r\\nTapering estimators also threshold and suppress C\\r\\nsam\\r\\nij in an (i, j)-dependent manner. A tapering estimator for the\\r\\n(i, j) entry of the covariance matrix is given by f(i, j)·\\r\\nL−1\\r\\nL C\\r\\nsam\\r\\nij , where f(i, j) = 1 if |i−j| ≤ k/2, f(i, j) = 2−2|i−j|/k\\r\\nif k/2 < |i − j| < k, and f(i, j) = 0 if |i − j| ≥ k. Here, tapering parameter k is an even integer, and L−1\\r\\nL C\\r\\nsam\\r\\nij is\\r\\nthe maximum likelihood estimator of Cij . The tapering estimator more strongly suppresses the (i, j) entries that are\\r\\nfarther from the diagonal respecting the sparsity of the estimated covariance matrix. This estimator is optimal in\\r\\nterms of the rate of convergence to the true covariance matrix, and the value of k realizing the optimal rate differs\\r\\nbetween the two major matrix norms with which the estimation error is measured [189].\\r\\nApplication example: Chen et al. compared gene co-expression networks between people with schizophrenia and\\r\\nnon-schizophrenic controls [190]. They used the Weighted Correlation Network Analysis (WGCNA) software (see\\r\\nsection 6), which is frequently used in gene co-expression network analysis, to construct weighted networks of genes.\\r\\nIn short, it uses |ρij |\\r\\nβ\\r\\nas the weight of edge (i, j), where β is a parameter. Then, they compared gene modules, or\\r\\ncommunities of the network, between the schizophrenia and control groups. The module detection was carried out\\r\\nby an algorithm implemented in WGCNA. There was no significant difference between the schizophrenia and control\\r\\ngroups in terms of the module structure (i.e., which genes are in each module). However, the eigengene — that is,\\r\\nthe first principal component of the co-expression matrix within a module — of each of two modules was significantly\\r\\nassociated with schizophrenia compared to the control. The hub genes of each of these two modules, NOTCH2 and\\r\\nMT1X, were the largest contributor to the respective eigengenes. The authors further carried out biological analyses\\r\\nof these two genes to clarify where their expressions are upregulated and the functions in which these genes may be\\r\\ninvolved. The results were similar for a biopolar disorder data set.\\r\\n3.5 Negative weights\\r\\nCorrelation matrices have negative entries in general. In the case of both unweighted and weighted correlation\\r\\nnetworks, we often prohibit negative edges either by coercing negative entries of the correlation matrix to zero or by\\r\\ntaking the absolute value of the pairwise correlation before transforming the correlation matrix into a network. We\\r\\nprohibit negative edges for two main reasons. First, in some research areas, it is often difficult to interpret negative\\r\\nedges. In the case of multivariate financial time series, a negative edge implies that the price of the two assets tend\\r\\nto move in the opposite manner, which is not difficult to interpret. In contrast, when fMRI time series from two\\r\\nbrain regions are negatively correlated, it does not necessarily imply that these regions are connected by inhibitory\\r\\nsynapses, and it is not straightforward to interpret negative edges in brain dynamics data [24,191]. Second, compared\\r\\nto weighted networks and directed networks, we do not have many established tools for analyzing networks in which\\r\\npositive and negative edges are mixed, i.e., signed networks. Signed network analysis is still emerging [192]\\r\\nIn fact, negative edges may provide useful information. For example, they benefit community detection because,\\r\\nwhile many positive edges should be within a community, negative edges might ideally connect different communities\\r\\nrather than lie within a community. Some community detection algorithms for signed networks exploit this idea [192,\\r\\n193]. Another strategy for analyzing signed network data is to separately analyze the network composed of positive\\r\\nedges and that composed of negative edges and then combine the information obtained from the two analyses. For\\r\\nexample, the modularity, an objective function to be maximized for community detection, can be separately defined\\r\\nfor the positive network and the negative network originating from a single signed network and then combined to\\r\\ndefine a composite modularity to be maximized [140, 194]. While these methods are designed for general signed\\r\\nnetworks, they have been applied to brain correlation networks [140, 193].\\r\\nAnother type of approach to signed weighted networks is nonparametric weighted stochastic block models [195,\\r\\n196], which are useful for modeling correlation matrix data. Crucially, this method separately estimates the un\\x02weighted network structure and the weight of each edge but in a unified Bayesian framework. By imposing a\\r\\nmaximum-entropy principle with a fixed mean and variance on the edge weight, they assumed a normal distribution\\r\\nfor the signed edge weight. Because the edge weight in the case of correlation matrices, i.e., the correlation coefficient,\\r\\nis confined between −1 and 1, an ad-hoc transformation to map (−1, 1) to (−∞, ∞) such as y = 2 arctanh x = ln 1+x\\r\\n1−x\\r\\nis applied before fitting the model. One can assess the goodness of such an ad-hoc transformation by a posteriori\\r\\ncomparison with different forms of functions to transform x to y using Bayesian model selection [196]. In this way,\\r\\nthis stochastic block model can handle negative correlation values. With this method, one can determine community\\r\\nstructure (i.e., blocks) including its number and hierarchical structure.\\r\\n15\\n3.6 Partial correlation\\r\\nA natural method with which to avoid false positives due to indirect interaction effects in the Pearson correlation\\r\\nmatrix is to use the partial correlation coefficient (as in, e.g., [197–199]). This entails measuring the linear correlation\\r\\nbetween nodes i and j after partialing out the effect of the other N − 2 nodes. Specifically, to calculate the partial\\r\\ncorrelation between nodes i and j, we first compute the linear regression of Xi on {X1, . . . , XN } \\\\ {Xi, Xj}, which\\r\\nwe write as Xi ≈\\r\\nPN\\r\\nm=1;m̸=i,j βi,mXm, where βi,m is the coefficient of linear regression. Similarly, we regress Xj on\\r\\n{X1, . . . , XN } \\\\ {Xi, Xj}, which we write as Xj ≈\\r\\nPN\\r\\nm=1;m̸=i,j βj,mXm. The residuals for L samples are given by\\r\\nεi,ℓ = xiℓ −\\r\\nPN\\r\\nm=1;m̸=i,j βi,mxmℓ and εj,ℓ = xjℓ −\\r\\nPN\\r\\nm=1;m̸=i,j βj,mxmℓ, where ℓ ∈ {1, . . . , L}. The partial correlation\\r\\ncoefficient, denoted by ρ\\r\\npar\\r\\nij , is the Pearson correlation coefficient between {εi,1, . . . , εi,L} and {εj,1, . . . , εj,L}.\\r\\nIn fact, the partial correlation coefficient between i and j (j ̸= i) is given by\\r\\nρ\\r\\npar\\r\\nij = −p\\r\\nΩij\\r\\nΩiiΩjj\\r\\n, (12)\\r\\nwhere Ω = C\\r\\n−1\\r\\nis the precision matrix [133, 134]. Equation (12) implies that Ωij = 0 is equivalent to the lack of\\r\\npartial correlation, i.e., ρ\\r\\npar\\r\\nij = 0. This conditional independence property gives an interpretation of the precision\\r\\nmatrix, Ω. Equation (12) also implies that the partial correlation can be calculated only when C is of full rank,\\r\\nwhose necessary but not sufficient condition is L ≥ N. If C is rank-deficient, a natural estimator of the N × N\\r\\npartial correlation matrix ρ\\r\\npar = (ρ\\r\\npar\\r\\nij ) is a pseudoinverse of C. However, the standard Moore-Penrose pseudoinverse\\r\\nis known to be a suboptimal estimator in terms of approximation error [200, 201], while the pseudoinverse is useful\\r\\nfor screening for hubs in partial correlation networks [201]. If C is of full rank, Ω as well as C is positive definite.\\r\\nTherefore, although Eq. (12) only holds true for i ̸= j, if we denote the matrix defined by the right-hand side of\\r\\nEq. (12) including the diagonal entries by ˜ρ, then the diagonal entries of ˜ρ are equal to −1, and ˜ρ is negative definite.\\r\\nWe can verify this by rewriting Eq. (12) as ˜ρ = −D−1/2ΩD−1/2, where D = diag(Ω11, . . . , ΩNN ) is the diagonal\\r\\nmatrix whose diagonal entries are Ω11, . . ., ΩNN . If we consider matrix ρ\\r\\npar = 2I + ˜ρ, where I is the identity\\r\\nmatrix, as a partial correlation matrix to force its diagonal entries to 1 instead of −1, the eigenvalues of ρ\\r\\npar are\\r\\nupper-bounded by 2 [202].\\r\\nBy thresholding the partial correlation matrix, or using an alternative method, one obtains an unweighted or\\r\\nweighted partial correlation network, depending on whether we further dichotomize the thresholded matrix. Because\\r\\nthe partial correlation avoids the indirect interaction affect, the network created from random partial correlation\\r\\nmatrices yields, for example, smaller clustering coefficients [156] than if we had used the Pearson correlation matrix.\\r\\nWhile it apparently sounds reasonable to partial out the effect of the other nodes to determine a pairwise\\r\\ncorrelation between two nodes, it is not straightforward to determine when the partial correlation matrix is better\\r\\nthan the Pearson correlation one. First, Eq. (12) implies that extreme eigenvalues of ρ\\r\\npar are those of a normalized\\r\\nprecision matrix. Because the precision matrix is the inverse of the covariance matrix C, extreme eigenvalues of\\r\\nρ\\r\\npartial are derived from eigenvalues of C with small magnitudes. It is empirically known for, e.g., financial time\\r\\nseries, that small-magnitude eigenvalues of the covariance matrices are buried in noise, i.e., not distinguishable from\\r\\neigenvalues of random matrices [101, 102], as we discuss later in section 4. Therefore, the dominant eigenvalue of the\\r\\nprecision matrix is strongly affected by noise [203].\\r\\nSecond, the entries of ρ\\r\\npar are more variable than those of Pearson correlation matrices. Specifically, if (x1, . . . , xN )\\r\\nobeys a multivariate normal distribution, the Fisher-transformed partial correlation of a sample partial correlation,\\r\\ni.e.,\\r\\nzij =\\r\\n1\\r\\n2\\r\\nln \\r\\n1 + ρ\\r\\npar,sam\\r\\nij\\r\\n1 − ρ\\r\\npar,sam\\r\\nij !\\r\\n, (13)\\r\\nwhere ρ\\r\\npar,sam\\r\\nij is the sample partial correlation calculated through Eq. (12) with Ω = (C\\r\\nsam)\\r\\n−1\\r\\n, approximately\\r\\nobeys the normal distribution with mean 1\\r\\n2\\r\\nln \\x10\\r\\n1+ρ\\r\\npar\\r\\nij\\r\\n1−ρ\\r\\npar\\r\\nij \\x11\\r\\nand standard deviation [L − 3 − (N − 2)]−1/2. This result\\r\\ndates back to Fisher (see e.g., [204]). In contrast, the corresponding result for the Fisher transformation of the\\r\\nPearson correlation coefficient is that the transformed variable approximately obeys the normal distribution with\\r\\nmean 1\\r\\n2\\r\\nlog \\x10\\r\\n1+ρ\\r\\nsam\\r\\nij\\r\\n1−ρ\\r\\nsam\\r\\nij \\x11\\r\\nand standard deviation (L − 3)−1/2[204]. Therefore, the partial correlation has more sampling\\r\\nvariance than the Pearson correlation unless L ≫ N.\\r\\nThird, partial correlation matrices typically have more negative entries and smaller-magnitude entries than Pear\\x02son correlation matrices [204, 205]. Combined with the larger variation of the sample partial correlation than the\\r\\nsample Pearson correlation discussed just above, the tendency that ρ\\r\\npar\\r\\nij has a smaller magnitude than ρij poses a\\r\\nchallenge of statistically validating the estimated partial correlation networks [204].\\r\\n16\\nStudies in neuroscience have compared partial correlation networks with simple correlation networks and/or\\r\\nwith the corresponding underlying structural networks [32, 62, 198, 206–209] (see section 2.2). One study found that\\r\\nthe similarity between partial correlation networks and structural networks is higher than that between correlation\\r\\nnetworks and structural networks [62].\\r\\nApplication example: Wang, Xie, and Stanley analyzed correlation networks composed of stock market indices\\r\\nfrom 2005 to 2014 from 57 countries [210], widely covering the continents of the world, with each country corre\\x02sponding to a node. They computed Pearson and partial correlation coefficients for the time series of the logarithmic\\r\\nreturns, given by Eq. (1), between each pair of countries, converted the correlation coefficient value into a distance\\r\\n(see Eq. (6)), and constructed the minimal spanning trees, called the MST-Pearson and MST-Partial networks.\\r\\nThese networks appeared to be scale-free (i.e., with a power-law-like degree distribution) trees. Among other things,\\r\\nthey compared clusters and top centrality nodes (i.e., countries) between the MST-Pearson and MST-Partial. They\\r\\nobserved that the results from the MST-Partial networks are more reasonable than those from the MST-Pearson\\r\\nconstruction in light of our general understanding of world economics.\\r\\n3.7 Graphical lasso and variants\\r\\nEstimating a true correlation matrix, which contains N(N − 1)/2 unknowns, is an ill-founded problem unless the\\r\\nnumber of samples is sufficiently larger than N(N −1)/2. A strategy to overcome this problem is to impose sparsity of\\r\\nthe estimated correlation network. A sparsity constraint enforces zeros on a majority of matrix entries to suppress the\\r\\nnumber of unknowns to be estimated relative to the number of samples. Imposing sparsity on estimated correlation\\r\\nnetworks is a major form of covariance selection. Structural learning refers to estimation of an unknown network\\r\\nfrom data and usually assumes that the given data obey a multivariate normal distribution and that the estimated\\r\\nnetwork is sparse. For reviews with tutorials and examples on this topic, see [25, 211, 212].\\r\\nThe Gaussian graphical model assumes that the precision matrix from the data obeys a multivariate normal\\r\\ndistribution and usually imposes sparsity of the precision matrix [134]. In addition to reducing the number of\\r\\nunknowns to be estimated, a motivation behind estimating a sparse precision matrix is that Ωij = 0 is equivalent\\r\\nto the absence of conditional linear dependence of the signals at the ith and jth nodes given all the other N − 2\\r\\nvariables, which is easy to interpret. The graphical lasso is an algorithm for learning the structure of a Gaussian\\r\\ngraphical model [213–218]. The graphical lasso maximizes the likelihood of the multivariate normal distribution\\r\\nunder a lasso penalty (i.e., ℓ1 penalty), whose simplest version is of the form λ\\r\\nPN\\r\\ni,j=1 |Ωij |, where we recall that Ωij\\r\\nis the (i, j) entry of the precision matrix, and λ is a positive constant. This penalty term is added to the negative log\\r\\nlikelihood to be minimized. If λ is large, it strongly penalizes positive |Ωij |, and the minimization of the objective\\r\\nfunction yields many zeros of the estimated Ωij . The (i, j) pairs for which the estimated Ωij is nonzero form edges\\r\\nof the network; if there is no edge between i and j, they are conditionally independent, i.e., conditioned on the other\\r\\nN − 2 variables. This conditional independence is also referred to as the pairwise Markov property because the\\r\\ndistribution of Xi only depends on {Xj : (i, j) is an edge of the network}. The pairwise Markov property is a special\\r\\ncase of the global Markov property. The global Markov property dictates that {Xi: i ∈ A} and {Xi: i ∈ B} are\\r\\nconditionally independent given {Xi: i ∈ S} if S is a cutset of A and B, that is, any path in the graph connecting\\r\\na node in A and a node in B passes through S. The pairwise and global Markov properties are major cases of\\r\\nthe Markov random field, which is defined by a set of random variables having Markov property specified by an\\r\\nundirected graph [134, 219–222]. The network is sparse by design. One can extend the lasso penalty function in\\r\\nmultiple ways, for example, by allowing λ to depend on (i, j) and automatically determining λ using an information\\r\\ncriterion. Other ways to regularize the number of nonzero elements in the precision matrix than lasso penalty are\\r\\nalso possible. (See e.g., [223–225].)\\r\\nA neighborhood selection method is another algorithm to estimate a sparse Gaussian graphical model [226].\\r\\nWith this method, one first carries out lasso regression for each ith variable (i.e., node) to determine a tentative\\r\\nsmall set of i’s neighbors. Second, if j is a tentative neighbor of i, and i is a tentative neighbor of j, then they\\r\\nare connected by an undirected edge (i, j). The method named “space” (Sparse PArtial Correlation Estimation)\\r\\nadvances the aforementioned neighborhood selection method by proposing to minimize a single composite penalized\\r\\nloss function, not separately minimizing the penalized loss function for each variable [227]. The loss function of\\r\\n“space” is a weighted sum of the regression error (i.e., error in estimating Xiin terms of a linear combination of\\r\\nother Xj ’s) over all i’s plus the usual ℓ1 penalty term. These regression-based methods as well the graphical lasso\\r\\nalgorithms permit the case in which the number of observations, L, is smaller than the number of variables, N.\\r\\nBayesian variants of graphical lasso provide the level of undertainty in the estimated model. One such Bayesian\\r\\napproach assumes that we know node pairs that are not adjacent to each other, which is equivalent to imposing\\r\\nΩij = 0 for a given set of node pairs (i, j) [228]. Such a situation is possible when both the correlation matrix and\\r\\nstructural network are available, as is common in MRI experiments in the brain. The Bayesian method uses the\\r\\n17\\nG-Wishart distribution, which is the Wishart distribution constrained by Ωij = 0 for non-adjacent node pairs in\\r\\nthe given graph (i.e., structural network) G, as the prior distribution of the precision matrix, Ω [229, 230]. If G is\\r\\nthe complete graph, then the G-Wishart distribution is the Wishart distribution. Then, it uses data (xiℓ) ∈ R\\r\\nN×L\\r\\nto update the distribution using Bayes’ rule, obtaining the posterior distribution of Ω, which is again a G-Wishart\\r\\ndistribution but with updated parameter values. In other words, the G-Wishart distribution is a conjugate prior,\\r\\ngiving us good intuitive pictures of how the prior distribution is transformed to the posterior distribution and\\r\\nmitigating the computational burden of the Bayesian method when N is not small.\\r\\nGaussian graphical models assume normality. One approach to relax this assumption is to use a so-called non\\x02paranormal distribution [231]. The idea is to assume that the transformed random variables (f1(X1), . . . , fN (XN ))\\r\\nobey a multivariate normal distribution, where f1, . . ., fN are monotone and differentiable functions. In this case,\\r\\nwe say that the original variables (X1, . . . , XN ) have a nonparanormal distribution or a Gaussian copula (given by\\r\\nf1, . . ., fN , and the mean vector and the covariance matrix of the normal distribution after the transformation). The\\r\\nnonparanormal distribution is nonidentifiable. For example, if we replace f1(X1) by a new function ˜f1(X1) ≡ 2f1(X1)\\r\\nand appropriately scale the first row and column of the covariance matrix and the first entry of mean vector used\\r\\nfor the multivariate normal distribution that the transformed N variables obey, then one gets the same distribution\\r\\nof (X1, . . . , XN ). Therefore, we further impose that the transformations f1, . . ., fN conserve the mean and variance\\r\\nof the original X1, . . ., XN . Then, as in the case of Gaussian graphical models, Ωij = 0 implies that Xi and Xj are\\r\\nconditionally independent of each other given all the other N − 2 variables, in addition to that f(Xi) and f(Xj ) are\\r\\nconditionally independent of each other. There are methods to estimate from data the functions f1, . . ., fN as well\\r\\nas a sparse covariance matrix Ω, assuming a lasso penalty. Naturally, the nonparanormal distribution outperforms\\r\\nthe graphical lasso when the true distribution of the data is not multivaraite normal [231]. Later work proposed\\r\\nalgorithms for estimating the nonparanormal distribution with optimal convergence rate [232, 233]; the idea behind\\r\\nthe improved algorithms is to avoid explicitly calculating f1, . . ., fN and to exploit statistics of rank correlation\\r\\ncoefficient estimators. There are also other methods for estimating non-normal graphical models without relying on\\r\\ncopula [234, 235].\\r\\nExtreme cases of non-Gaussian distribution of the random variables are discrete multivariate distributions, in\\r\\nparticular when each Xiis binary (i.e., ∈ {−1, 1}.). In this case, the form of the distribution of (X1, . . ., XN )\\r\\nproportional to e\\r\\nxΩx\\r\\n⊤\\r\\n, where x = (X1 − µ1, . . . , XN − µN ), defines the Ising model. Note that the same form of\\r\\ndistribution for continuous variables is the multivariate normal distribution, which we discussed above for graphical\\r\\nlasso. There are many methods for inferring the Ising model from observed multivariate binary data, which is the\\r\\ntask referred to as the inverse Ising model and Boltzmann machine learning [236–240]. Although exact likelihood\\r\\nmaximization is available, it is practical only for small N. Therefore, various methods aim to overcome this limitation\\r\\nby allowing some approximation. Similar to graphical models, inference algorithms for the Ising model respecting\\r\\nthe sparsity of the underlying network have also been investigated. For example, ℓ1-regularized methods based on\\r\\npseudo-likelihood maximization were developed for estimating a sparse Ising Markov random field, or a graph in\\r\\nwhich the absence of the edge signifies the conditional independence between two nodes [241–243]. Decimation, or\\r\\nto recursively set the small edge weights to zero, combined with a stopping criterion and other heuristics, improves\\r\\nthe performance of psuedo-likelihood maximization [244].\\r\\nAn alternative to the graphical lasso is to estimate sparse covariance matrices rather than sparse precision matrices\\r\\nunder lasso penalty [245–250]. With this approach, the consequence of imposing sparsity, Cij = 0, corresponds to\\r\\nmarginal independence between Xi and Xj . Similar to the case of the graphical lasso, one regards (i, j) pairs for\\r\\nwhich the estimated Cij is nonzero as edges of the network.\\r\\nMost graphical lasso models and their variants do not model the estimation problem relative to a null model\\r\\ncorrelation matrix. However, by estimating a sparse correlation matrix that is different relative to a null model of\\r\\ncorrelation matrix (see section 4 for null models), it was found that the estimated correlation matrix gives a better\\r\\ndescription of the given financial correlation matrix data than the graphical lasso and that the choice of the null\\r\\nmodel also affects the performance [250]. By construction, this method infers a set of edges that are not expected\\r\\nfrom the so-called correlation matrix configuration model (see section 4 for details).\\r\\n3.8 Statistical significance of correlation edges\\r\\nA test of significance of an edge, run on each edge, may sound like a natural way to filter a network. However,\\r\\nthis idea is not easily feasible because multiple comparisons with N(N − 1)/2 estimates, each of whose significance\\r\\nwould have to be tested, is not practical given that N(N −1)/2 is usually large [25]. If we require a significance level\\r\\nof 0.05, then Bonferroni correction for multiple comparisons implies that the edge statistic has to be tested to be\\r\\nsignificant with the p value less than 0.05/[N(N −1)/2] for the edge to be actually significant at the 0.05 significance\\r\\nlevel. Unless N is small, this condition is usually too harsh. Furthermore, the different edges are correlated with\\r\\n18\\neach other, particularly if they share a node. In contrast, the Bonferroni correction assumes that the different tests\\r\\nare independent. Extensions of the Bonferroni corrections, such as the Sid´ak and Holm corrections, do not resolve ˇ\\r\\nthese issues.\\r\\nThe false discovery rate approach, more specifically, the Benjamini-Yekutieli procedure [251], provides a solution\\r\\nto these problems. This test is less stringent than family-wise error rate controls including the Bonferroni correction\\r\\nand allows dependency between different tests. To run this procedure to generate a correlation network, we first\\r\\ncalculate the p value for each node pair (i, j). Second, we arrange all the p values in ascending order, which we denote\\r\\nby p(1), p(2), . . ., p(N(N−1)/2). Then, we reject the null hypothesis (i.e., regard that the edge with the corresponding\\r\\np value is significant) for p(1), . . ., p(M), where\\r\\nM = max (i : p(i) <\\r\\n0.05i\\r\\nN(N−1)\\r\\n2\\r\\nPN(N−1)/2\\r\\ni\\r\\n′=1\\r\\n1\\r\\ni\\r\\n′\\r\\n)\\r\\n. (14)\\r\\nThis procedure is a thresholding method with an automatically set threshold value implied by the number of edges,\\r\\nM, determined by Eq. (14). We note that the type of correlation matrix (e.g., Pearson or partial correlation) does\\r\\nnot matter once p values have been obtained.\\r\\nIn the above procedure, what does it mean to calculate a p value for a node pair? The answer varies. If we are\\r\\ngiven just one correlation matrix, which we assume in most of this article, the p value can be that of the Pearson\\r\\ncorrelation coefficient in a standard textbook, if the Pearson correlation matrix is given as input [252] (see [253–255]\\r\\nfor different calculations of the p value when a single matrix is given as input). If we are given multiple correlation\\r\\nmatrices forming one group, the one-sample t-test provides a p value for each (i, j) pair, quantifying whether the (i, j)\\r\\ncorrelation for the group is different from 0 [198]. If correlation matrices from two groups need to be compared, the\\r\\ntwo-sample t-test provides a p value for each (i, j). In this case, the generated network will be a difference network,\\r\\nin which the edges represent node pairs for which the correlation is significantly different between the two groups.\\r\\nIn neuroimaging research, the network-based statistic (NBS) is popularly used for controlling for the same multiple\\r\\ncomparison problem [256]. In the NBS, one first calculates the p value for each (i, j), as in the Benjamini-Yekutieli\\r\\nprocedure. Then, we keep (i, j) whose p values are smaller than an arbitrary threshold. If we just use those (i, j) pairs\\r\\nto form a network, we would suffer from the aforementioned problems (i.e., multiple comparisons and dependencies\\r\\nbetween edges). Instead, the NBS focuses on the connected components induced by the surviving edges, which\\r\\nare small in number in general, and tests the significance of the sizes (i.e., the number of nodes) of the connected\\r\\ncomponents using a permutation test. The NBS has been extended to a threshold-free method [257]. NBS and many\\r\\nof its extensions are applicable to correlation matrix data beyond neuroscience. Note that the NBS is not a method\\r\\nto estimate a correlation network; it tactically avoids network estimation and the problem of multiple comparisons,\\r\\nwhile providing a statistically controlled downstream network analysis (i.e., test on the size of connected components).\\r\\nIn this sense, the NBS casts a key question: why do we need to estimate a network first of all? We will discuss this\\r\\ntopic in section 7.1.\\r\\nCovariance selection methods, such as the graphical lasso, do not explicitly test the significance of individual\\r\\nedges. The edges that survive these types of filtering methods should be regarded to be sufficiently strong to be\\r\\nincluded in the model [25].\\r\\n3.9 Temporal correlation networks\\r\\nMany empirical networks vary over time, including temporal correlation networks [258], and many methods have\\r\\nbeen developed for analyzing time-varying network data [110,258–260]. If the given data is a multivariate time series\\r\\nthat is non-stationary, then correlation matrices computed from the first 10% of the time points may be drastically\\r\\ndifferent from that computed from the last 10%. So, there is the possibility of greater adaptability and better\\r\\ngeneralizability when one uses a time series of correlation networks rather than just one. One can then apply various\\r\\ntemporal network analysis tools to the obtained temporal correlation networks.\\r\\nA simple method to create dynamic correlation networks from multivariate time series data is sliding-window\\r\\ncorrelation [261] (also called rolling-window correlation in the finance literature; see e.g. [262]). With this method,\\r\\none considers time windows within the entire observation time horizon, t = {1, . . . , tmax}. These time windows\\r\\nmay be overlapping or non-overlapping. Then, within each time window, one calculates the correlation matrix and\\r\\nnetwork. If there are 100 time windows within [1, tmax], then this method creates a temporal network of length 100.\\r\\nReliably computing a single correlation matrix and a static correlation network from multivariate time series requires\\r\\na reasonable length (i.e., the number of time points) of a time window. Generation of a reliable dynamic correlation\\r\\nnetwork requires longer data because one needs a multitude of such reasonably long time windows. A limitation of\\r\\nsliding-window correlations is that they are susceptible to large variability if the size of the time window is small,\\r\\nwhereas a large window size sacrifices sensitivity to temporal changes [263].\\r\\n19\\nEarly seminal reports analyzed temporal correlation networks of stock markets by tracking financial indices and\\r\\ncentral nodes of the static correlation network over more than a decade [166,264,265]. However, methods of dynamic\\r\\ncorrelation networks have been particularly developed in brain network analysis. In neuroimaging studies, in partic\\x02ular in fMRI studies, dynamic correlation networks are known as dynamic (or time-varying) functional connectivity\\r\\nnetworks [261, 266–269]. Temporal changes in functional (i.e., correlational) brain networks may represent neural\\r\\nsignaling, behavioral performance, or changes in the cognitive state, for example. Patterns of time-varying functional\\r\\nnetworks may alter under a disease. One can also analyze stability of community structure of temporal correlation\\r\\nnetworks over time [252, 270, 271]. (See also [103–106, 110] for temporal stability analyses of community detection\\r\\nin financial correlation networks.) Many variations of the method, such as on how to create sliding windows and\\r\\nhow to cluster time windows to define discrete-state transition dynamics, and change-point detection are available\\r\\nin the field (e.g., see [269]). Many of these methods should be applicable to multivariate time series data to generate\\r\\ntemporal correlation networks in other domains.\\r\\nThere are also methods for estimating dynamic precision matrices, proposed outside neuroscience [272–274]. For\\r\\nexample, the time-varying graphical lasso (TVGL) formulates the inference of discrete-time time-varying precision\\r\\nmatrix as a convex optimization problem [274]. The objective function is composed of maximization of the log\\r\\nlikelihood under a lasso sparsity constraint and the constraint that the precision matrix at adjacent time points does\\r\\nnot change too drastically, enforcing the temporal consistency.\\r\\n4 Null models and random matrices\\r\\nIn the analysis of (correlation) networks, a good practice to verify any structural or dynamical measurement α is to\\r\\ncompare the value of α in the given network with the value of α achieved in random networks. This allows one to\\r\\ndetermine whether the α value measured for the given network data is explainable purely by the gross structural\\r\\nproperties (e.g. edge density, degree distributions) of the random graph family (when the α value is similar between\\r\\nthe given and random networks) or it is the result of other distinctive features of the data (when the α value is\\r\\nstatistically different between the given and random networks). Many discoveries in network science owe to the\\r\\nfact that key analyses have prudently implemented this practice by using or inventing appropriate null models of\\r\\nnetworks. Already in one of the earliest seminal papers on small-world networks, Watts and Strogatz compared the\\r\\naverage path length and the clustering coefficient of empirical networks with those of the Erd˝os-R´enyi random graph\\r\\nhaving the same number of nodes and edges [275].\\r\\nIn this section, we report on similar concepts and results for correlation matrices. The idea is to introduce\\r\\nvarious null hypotheses that would imply certain properties for the sample correlation matrix, and compare the\\r\\nempirical matrix with the null model in order to extract statistically significant properties. This discussion will\\r\\nlead us to consider on one hand models defined in analogy with null models for networks, and on the other hand\\r\\ngenuine models for correlation matrices derived from RMT. Several papers have noted the need for proper null models\\r\\nspecifically for correlation networks [33,103,156,276,277]. We should use correlation networks derived from a random\\r\\ncorrelation matrix as a null model. We stress that a random correlation matrix is different from a random network\\r\\nmodel (e.g., Erd˝os-R´enyi model), because of the dependencies between entries. Similarly, many classes of random\\r\\nmatrices are not appropriate null models for correlation networks, either. For example, a symmetric matrix whose all\\r\\non-diagonal entries are 1 and off-diagonal entries are i.i.d. uniformly on (−1, 1) is almost never a correlation matrix\\r\\nunless N is small [278]. In this section, we introduce several null models of correlation matrices. All the null models\\r\\npresented give distributions over correlation matrices. Then, using any method introduced in previous sections (e.g.,\\r\\nby thresholding in various ways), one can define corresponding null models over correlation networks.\\r\\n4.1 Models based on shuffling\\r\\nA straightforward and traditional null model consists in shuffling the original data, {xiℓ}, based on which the\\r\\ncorrelation matrix is calculated. This method is especially typical for multivariate time series data. In the simplest\\r\\ncase, one randomizes all entries independently within each time series, thereby destroying all the cross-correlations\\r\\nwhile preserving the original values for each time series separately.\\r\\nAs a more constrained option, one preserves the power spectrum of the time series at each node while the time\\r\\nseries is otherwise randomized [156, 277, 279, 280]. More specifically, one Fourier transforms the time series at the\\r\\nith node, randomize the phase, and carry out the inverse Fourier transform. Then, for the randomized multivariate\\r\\ntime series, one calculates the correlation matrix, which is used as control.\\r\\nAnother method that preserves the full autocorrelation structure within each single time series, while randomizing\\r\\ncross-correlations among the N time series, has been proposed in [281, 282]. The method is called the rotational\\r\\nrandom shuffling (RRS) model because it first imposes periodic boundary conditions (i.e., ‘gluing’ the last timestep\\r\\n20\\nto the first one) to turn each time series into a ‘ring’, and then randomly rotates the N rings with respect to each\\r\\nother while keeping each ring internally intact.\\r\\nOne can impose additional constraints on the randomization of time series depending on the properties other\\r\\nthan the power spectrum that one wants to have the null model preserve [283].\\r\\n4.2 Models inspired by network analysis\\r\\nOther null models are explicitly inspired by null models that are routinely used for networks. For instance, the\\r\\nH-Q-S algorithm, invented by Hirschberger, Qi, and Steuer [284] is an equivalent of the Erd˝os-R´enyi random graph\\r\\nin general network analysis. Specifically, given the covariance matrix, C\\r\\nsam, the H-Q-S algorithm generates random\\r\\ncovariance matrices, C\\r\\nHQS, under the following constraints. First, the expectation of each on-diagonal entry of CHQS\\r\\nis equal to the average of the N on-diagonal entries of C\\r\\nsam, denoted by µon ≡ (1/N) ×\\r\\nPN\\r\\ni=1 C\\r\\nsam\\r\\nii . Second, the\\r\\nexpectation and variance of each off-diagonal entry of C\\r\\nHQS are equal to those of Csam calculated on the basis of\\r\\nall the off-diagonal entries, denoted by µoff and σ\\r\\n2\\r\\noff, respectively. Optionally, one can also constrain the variance of\\r\\nthe on-diagonal entries of C\\r\\nHQS [284] or use a fine-tuned heuristic variant of the algorithm [156]. To implement the\\r\\nmost basic H-Q-S algorithm without constraining the variance of the on-diagonal entries of C\\r\\nHQS, we set\\r\\nL\\r\\nHQS ≡ max \\r\\n2, ⌊\\r\\n\\r\\nµ\\r\\n2\\r\\non − µ\\r\\n2\\r\\noff\\x01\\r\\n/σ2\\r\\noff⌋\\r\\n\\x01\\r\\n, (15)\\r\\nwhere ⌊·⌋ is the largest integer that is not greater than the argument. Then, we draw N × L\\r\\nHQS variables, denoted\\r\\nby xiℓ (with i ∈ {1, . . . , N} and ℓ ∈ {1, . . . , LHQS\\r\\np\\r\\n}), independently from the identical normal distribution with mean\\r\\nµoff/LHQS and variance −µoff/LHQS+\\r\\np\\r\\nµ\\r\\n2\\r\\noff/(LHQS)\\r\\n2 + σ\\r\\n2\\r\\noff/LHQS. Then, the H-Q-S algorithm sets the covariance\\r\\nmatrix by\\r\\nC\\r\\nHQS\\r\\nij =\\r\\nL\\r\\nXHQS\\r\\nℓ=1\\r\\nxiℓxjℓ i, j ∈ {1, . . . , N}. (16)\\r\\nIt is known that ⟨C\\r\\nHQS⟩ij = δijµon + (1 − δij )µoff. Therefore, the expectation of the correlation matrix, ρHQS, is ap\\x02proximately given by ⟨ρ\\r\\nHQS⟩ij = δij+(1−δij )µoff/µon. This highlights the completely homogeneous nature of the null\\r\\nmodel. For instance, while the degree of empirical correlation networks is usually heterogeneously distributed [108],\\r\\nthis property is not captured by the H-Q-S algorithm [285].\\r\\nOne of the most popular heterogeneous null models for networks is the configuration model, i.e., a uniform\\r\\nensemble of networks under the constraint that the degree of each node is conserved [277, 286], either exactly\\r\\nor in expectation. By comparing given networks against a configuration model, one can reliably quantify and\\r\\ndiscuss various network properties such as network motifs [287], community structure [288], rich clubs [289], and\\r\\ncore-periphery structure [290]. The rationale behind the use of the configuration model is that the node’s degree\\r\\nis heterogeneously distributed for many empirical networks and that one generally wants to explore structural,\\r\\ndynamical, or other properties of networks that are not immediate outcomes of the heterogeneous degree distribution.\\r\\nOne can extend the configuration model by imposing additional constraints that the network under discussion is\\r\\nsupposed to satisfy, such as spatiality, i.e., the constraint that the nodes are embedded in a metric space and the\\r\\nprobability of an edge depends on the distance between the two nodes [291]. See [286, 292, 293] for reviews of\\r\\nconfiguration models and best practices for generating random realizations from such models.\\r\\nWe should similarly test properties found for given correlation networks against appropriate null models. However,\\r\\nthe usual configuration models are not appropriate as null models of correlation networks because they are significantly\\r\\ndifferent from correlation networks derived from purely random data [87, 103, 110, 156]. The expectation ⟨Aij ⟩ of the\\r\\n(i, j) entry of the adjacency matrix of the configuration model conditioned on the degrees of all nodes, at least in\\r\\nthe idealized and unrealistic regime of weak heterogeneity of the degrees [292, 293], is equal to\\r\\n⟨Aij ⟩ =\\r\\nkikj\\r\\nNk\\r\\n, (17)\\r\\nwhere ki =\\r\\nPN\\r\\nj=1 Aij is the degree of the ith node in the original network, Aij is the entry of the empirical adjacency\\r\\nmatrix of the original network (i.e., Aij = 1 if there is an edge between the ith and jth nodes, and Aij = 0 otherwise),\\r\\nand k is the average degree over all nodes. The above expected value also represents the probability of independently\\r\\nconnecting the ith and jth nodes in realizations of the configuration model for networks. Note that, one can indeed\\r\\nrealize graphs in the configuration model by sampling edges independently (at least if the constraint on the degree is\\r\\n‘soft’, i.e. realized only as an ensemble average over realizations [293]), correlation matrices cannot be generated with\\r\\nindependent entries, even in the null model of independent signals. This is because, even under the null hypothesis\\r\\n21\\nof independent realizations of the original time series, the correlation matrix constructed from such time series still\\r\\nobeys the ‘metric’ (or triangular) inequality in (9). We will elaborate more on this point below.\\r\\nTo see what Eq. (17) yields by merely replacing an empirical adjacency matrix with an empirical correlation\\r\\nmatrix ρij , we proceed as follows [103]. We assume that each empirical signal is standardized in advance such that\\r\\nVar(Xi) = 1, ∀i ∈ {1, . . . , N}. In this way, we do not need to distinguish between the correlation (i.e., ρij ) and\\r\\ncovariance (i.e., Cij ) matrices. We express the ‘degree’ as\\r\\nki =\\r\\nX\\r\\nN\\r\\nj=1\\r\\nρij =\\r\\nX\\r\\nN\\r\\nj=1\\r\\nCij =\\r\\nX\\r\\nN\\r\\nj=1\\r\\nCov(Xi, Xj ) = Cov(Xi, Xtot), (18)\\r\\nwhere Cov represents the covariance and Xtot =\\r\\nPN\\r\\ni=1 Xi\\r\\nis the ‘total’ signal. Then, we obtain\\r\\nNk =\\r\\nX\\r\\nN\\r\\ni=1\\r\\nki = Cov(Xtot, Xtot) = Var(Xtot), (19)\\r\\nwhere Var(Xtot) is the variance of Xtot. By inserting the above quantities into (17) with Aij replaced by ρij , we\\r\\nobtain for the expected correlation matrix\\r\\n⟨ρij ⟩ =\\r\\nCov(Xi, Xtot)Cov(Xj , Xtot)\\r\\nVar(Xtot)\\r\\n=\\r\\nCov(Xi, Xtot)\\r\\np\\r\\nVar(Xtot)\\r\\np\\r\\nVar(Xi)\\r\\n·\\r\\nCov(Xj , Xtot)\\r\\np\\r\\nVar(Xtot)\\r\\np\\r\\nVar(Xj )\\r\\n= ρ(Xi, Xtot)ρ(Xj , Xtot).\\r\\n(20)\\r\\nTechnically, this expected matrix is a covariance matrix because it is symmetric, rank 1, and the only nonzero\\r\\neigenvalue is positive [103, 294]. To interpret the meaning of the above expression for ⟨ρij ⟩, we recall the definition\\r\\nof the conditional three-way partial Pearson correlation coefficient [3, 133]:\\r\\nρ(Xi, Xj | Xtot) = ρ(Xi\\r\\n, Xj ) − ρ(Xi, Xtot)ρ(Xj , Xtot)\\r\\np\\r\\n1 − ρ(Xi, Xtot)\\r\\n2\\r\\np\\r\\n1 − ρ(Xj , Xtot)\\r\\n2\\r\\n. (21)\\r\\nWe therefore conclude that the expected correlation matrix in (20) is a correlation matrix of N signals that satisfy\\r\\nthe conditional independence relationship\\r\\nρ(Xi, Xj | Xtot) = 0 (22)\\r\\n∀i, j(̸= i) ∈ {1, . . . , N} [109, 110].\\r\\nHowever, when one generates a correlation network from the configuration model, i.e. from a correlation matrix\\r\\nobeying (22), the generated network is far from a typical correlation network generated by random data, due to the\\r\\ntriangular inequality mentioned above. To see an example of this, let us revisit the example briefly explained in\\r\\nsection 3.2. Let us consider purely random data in which we generate each sample xiℓ, i ∈ {1, . . . , N}, ℓ ∈ {1, . . . , L}\\r\\n(where we recall that L is the number of samples for each node) as i.i.d. random numbers obeying a given distribution.\\r\\nThen, we calculate the sample correlation matrix for {xiℓ} and then a sample correlation network. This procedure\\r\\nimmediately establishes a connection between null models for sample correlation matrices and RMT [7–9], some\\r\\nelements of which are discussed in section 4.3. For a broad class of methods, including dichotomizing, the generated\\r\\ncorrelation network has a high clustering coefficient [156], precisely because of the inequality (9). Therefore, high\\r\\nclustering coefficients in correlation networks should not come as a surprise. In contrast, networks generated by the\\r\\nordinary configuration model yield low clustering coefficients [295], disqualifying it as a null model for correlation\\r\\nnetworks [103]. If we use the usual configuration model as the null model, we would incorrectly conclude that a\\r\\ngiven correlation network has high clustering even if the network does not have particularly high clustering among\\r\\ncorrelation networks. The configuration model as null model also underperforms the simpler null model, called the\\r\\nuniform null model (which is analogous to the random regular graph and to the Hirschberger-Qi-Steuer (H-Q-S)\\r\\nalgorithm explained above) on benchmark problems of community detection when communities of different sizes are\\r\\npresent in single networks and those communities are detected by modularity maximization [110].\\r\\nA different configuration model, specifically designed for covariance matrices, can be defined as follows. This model\\r\\npreserves the expectation of each row sum excluding the diagonal entry, which is equivalent to each node’s degree in\\r\\nthe case of the adjacency matrix of a conventional network [276]. This algorithm, which we refer to as the correlation\\r\\nmatrix configuration model, preserves the expectation of each diagonal entry of C\\r\\nsam, or the variance of each variable,\\r\\nand the expectation of each row sum excluding the diagonal entry, i.e., PN\\r\\nj=1;j̸=i C\\r\\nsam\\r\\nij ∀i, corresponding to the degree\\r\\nof the ith node. Under these constraints, the correlation matrix configuration model uses the distribution of xiℓ,\\r\\ndetermined from the maximum entropy principle. In fact, each (x1ℓ, . . . , xNℓ)\\r\\n⊤, ℓ ∈ {1, . . . , L}, independently obeys\\r\\n22\\nan identical multivariate normal distribution whose mean is the zero matrix and covariance matrix is denoted by C\\r\\ncm.\\r\\nTherefore, the correlation matrix configuration model is the Wishart distribution WN (C, L) that we have introduced\\r\\nin section 3.1, with C = C\\r\\ncm. The matrix Ccm is of the form \\x02\\r\\n(C\\r\\ncm)−1\\r\\n\\x03\\r\\nij = − [δij · 2αi + (1 − δij )(βi + βj )], where\\r\\nαi and βi are parameters to be determined. One determines the values of αi and βi using a gradient descent\\r\\nalgorithm [276] or by reformulating the problem as a convex optimization problem and solving it [250].\\r\\n4.3 Models based on random matrix theory\\r\\nAnother class of null models is based on powerful estimates provided by RMT for the expected spectral properties\\r\\nof a random sample correlation matrix, rather than for the expected matrix itself [7–9]. Note that the expected\\r\\ncorrelation matrix, under the null hypothesis of N independent signals, is the identity matrix whose entries we\\r\\ndenote as ρ\\r\\nMG1\\r\\nij = δij (where δij is the Kronecker delta symbol) [103, 110]. Equivalently, ρ\\r\\nMG1 = I where I is the\\r\\nN × N identity matrix. This null model corresponds to an expectation under white noise signals {xiℓ} that are\\r\\nindependent for different nodes i ∈ {1, . . . , N} and samples ℓ ∈ {1, . . . , L}. However, the sample pairwise correlation\\r\\nρ\\r\\nsam\\r\\nij measured from the signals, even under the null hypothesis of independence, will be different from the identity\\r\\nmatrix unless L → ∞, assuming a finite N. To take into account the effects of finite L, it is convenient to look at the\\r\\ndistribution of the random sample correlation matrix. If we assume a standardized independent normal distribution\\r\\nfor xi ∀i, then ρ\\r\\nsam obeys the Wishart distribution with C = I, i.e. WN (I, L). Note that the expectation of\\r\\nWN (I, L) is I. The ensemble of Wishart matrices is a well studied topic in RMT. It turns out that, in addition to\\r\\nthe distribution WN (I, L) for the entire sample correlation matrix, one can accurately describe the limiting density\\r\\nof eigenvalues in the asymptotic limit N → ∞ and L → ∞, with L/N → Q > 1. Note that Q > 1 is a necessary\\r\\ncondition for the sample correlation matrix to be non-degenerate, as we already mentioned. The limiting eigenvalue\\r\\ndensity pQ(λ), known as the Marchenko-Pastur distribution, has the form\\r\\npQ(λ) = ( Q\\r\\n√\\r\\n(λ+−λ)(λ−λ−)\\r\\n2πλ for λ− < λ < λ+,\\r\\n0 otherwise,\\r\\n(23)\\r\\nwhere\\r\\nλ± =\\r\\n\\x10\\r\\n1 ±\\r\\np\\r\\nQ−1\\r\\n\\x112\\r\\n(24)\\r\\nare the expected minimum (λ−) and maximum (λ+) eigenvalues. As Q → ∞, which corresponds to an infinite\\r\\nnumber of samples per node, it holds true that λ± → 1. This result implies that all eigenvalues become unity, and\\r\\nit is because each sample correlation matrix becomes the identity matrix in that case, as we already mentioned;\\r\\nthen all eigenvalues are equal to 1. The fact that Q is necessarily finite for empirical correlation matrices makes\\r\\nEq. (23) particularly useful as a null model for the empirical eigenvalue distribution expected under the hypothesis of\\r\\nindependence of the N observed time series. In particular, early studies of financial multivariate time series data found\\r\\nthat only a small number of the largest eigenvalues of the empirical covariance matrix are found above λ+ [101, 102].\\r\\nIn other words, only a few leading eigenvalues and the associated eigenvectors outside the prediction of RMT are\\r\\nstatistically significant relative to the null model. As we discuss below, the role of the largest empirical eigenvalue is\\r\\nhowever different from that of the next largest ones. Specifically, the former encodes a system-wide, overall positive\\r\\ncorrelation, while the latter represent ‘mesoscopic’ information arising from the presence of internally coherent groups\\r\\nof time series. Based on this interpretation, RMT has provided useful null models for covariance/correlation matrix\\r\\ndata, in particular in financial time series data analysis; see [6] for a review. In neuroscience, RMT has been applied\\r\\nonly more recently (e.g., for noise-cleaning in the estimation of correlation matrices [65] as we describe later in this\\r\\nsection and for community detection [296] as we describe in section 5.2) and less systematically. Nonetheless, it holds\\r\\npromise as a general and powerful tool for the analysis of large data in virtually any field, especially in the modern\\r\\nera of data science [9]. Also see [130] for a review of random correlation as opposed to covariance matrices.\\r\\nAmong many possible specific choices of null models for correlation matrices based on RMT, here we consider\\r\\ntwo models, which we denote by ρ\\r\\nMG2 and ρMG3, proposed in [103, 296]. A given correlation matrix is symmetric\\r\\nand positive semidefinite and therefore can be decomposed as\\r\\nρ\\r\\nsam =\\r\\nX\\r\\nN\\r\\ni=1\\r\\nλiu(i)u\\r\\n⊤\\r\\n(i)\\r\\n, (25)\\r\\nwhere λi(≥ 0) is the ith eigenvalue of ρ\\r\\nsam, and u(i)\\r\\nis the associated normalized right eigenvector. The null model\\r\\ncorrelation matrix ρ\\r\\nMG2 preserves the contribution of small eigenvalues to Eq. (25), which are regarded to be noisy\\r\\n23\\nand described by RMT, and is given by\\r\\nρ\\r\\nMG2 =\\r\\nX\\r\\ni:λi≤λ+\\r\\nλiu(i)u\\r\\n⊤\\r\\n(i)\\r\\n. (26)\\r\\nThe boundary λ+ originates from the Marchenko-Pastur distribution given by Eq. (23) and, as we mentioned,\\r\\nrepresents the expected largest eigenvalue under the null hypothesis of independent signals. Matrix ρ\\r\\nMG2 is not a\\r\\ncorrelation matrix because its diagonal elements are not equal to 1. However, this does not affect most network\\r\\nanalyses because we usually ignore diagonal elements or self-loops in correlation networks. Matrix ρ\\r\\nMG2 represents\\r\\na null model constructed only from the eigenvalues of the empirical correlation matrix that are deemed to be\\r\\nnoisy. Therefore comparing the empirical matrix against ρ\\r\\nMG2 singles out properties that cannot be traced back to\\r\\nnoise [103]. Note that the difference\\r\\nρ\\r\\nsam − ρMG2 =\\r\\nX\\r\\ni:λi>λ+\\r\\nλiu(i)u\\r\\n⊤\\r\\n(i)\\r\\n(27)\\r\\nbetween the sample correlation matrix ρ\\r\\nsam and the null model ρMG2 is nothing but the sum of the dominant\\r\\neigencomponents of ρ\\r\\nsam. This matrix coincides with the output of the popular PCA technique, also called the\\r\\neigenvalue clipping. The main difference between the generic use of that technique and the one based on ρ\\r\\nMG2 in\\r\\nEq. (27) is the criterion (here based on RMT) for the selection of the number of principal components to retain.\\r\\nBy contrast, the matrix ρ\\r\\nMG3 also preserves the contribution of the largest eigenvalue in addition to that of\\r\\nthe noisy eigenvalues. The largest eigenvalue is useful to control for separately, if all the entries of the associated\\r\\neigenvector are positive. It is because, in that case, it represents the effect of a common trend in the original time\\r\\nseries, giving an uninformative all-positive overall contribution (also called global mode, or market mode in the\\r\\ncontext of financial time series) to the sample correlation matrix. The matrix ρ\\r\\nMG3 is given by\\r\\nρ\\r\\nMG3 = λmaxu(max)u⊤\\r\\n(max) +\\r\\nX\\r\\ni:λi≤λ˜+\\r\\nλiu(i)u\\r\\n⊤\\r\\n(i)\\r\\n, (28)\\r\\nwhere max is the index for the dominant eigenvalue of ρ\\r\\nsam, and λ+ has been replaced by\\r\\nλ˜+ ≡\\r\\n\\x12\\r\\n1 −\\r\\nλmax\\r\\nN\\r\\n\\x13\\r\\nλ+. (29)\\r\\nThe above modification in the value of the threshold eigenvalue, with respect to the previous null model in Eq. (24),\\r\\noriginates from the fact that sample correlation matrices always have trace equal to N, with all their diagonal entries\\r\\nbeing unity by construction. Therefore, the addition of a global mode represented by a large λmax has the unavoidable\\r\\neffect of proportionally reducing all the other eigenvalues in such a way that the trace is preserved [101, 296]. The\\r\\nvalue in Eq. (29) thus represents the expected largest eigenvalue under the null hypothesis of independent signals\\r\\nplus a global mode. Note that λ˜+ < λ+, which implies that any empirical eigenvalue λi smaller than λ+ but larger\\r\\nthan λ˜+, i.e. λ˜+ < λi < λ+, is interpreted as noisy (hence discarded) under the null model ρ\\r\\nMG2 and as informative\\r\\n(hence retained) under the null model ρ\\r\\nMG3 [296]. Also note that all the entries of the dominant eigenvector, u(max),\\r\\nare positive, which is a necessary condition for the null model ρ\\r\\nMG3 to have a clear interpretation, if there is a\\r\\nsufficiently strong common trend affecting all the N signals. If this common trend is so strong that all the entries\\r\\nof the correlation matrix are positive, then the Perron-Frobenius theorem ensures the positivity of the dominant\\r\\neigenvector. When this happens, the common trend obscures all the mutual correlations among the signals. Matrix\\r\\nρ\\r\\nMG3 deliberately removes this global trend in addition to the noise, to reveal the underlying structure. Therefore,\\r\\nproperties of correlation matrices or correlation networks that are not expected from ρ\\r\\nMG3 represent those not\\r\\nanticipated by the simultaneous presence of local noise and global trends. In other words, they reflect the presence\\r\\nof mesoscopic correlated structures such as correlation-induced communities [79, 103–106, 296]. As we will discuss in\\r\\nsection 5.2, one can indeed use matrix ρ\\r\\nMG3 to successfully detect communities of correlated signals.\\r\\nWe finally discuss another RMT-based model that, rather than representing a null model of the data, aims\\r\\nat providing the best fit, i.e., an optimal estimate, for the true (unobserved) correlation matrix ρ\\r\\ntrue, starting\\r\\nfrom the sample correlation matrix ρ\\r\\nsam. The model is called the optimal rotationally invariant estimator (RIE)\\r\\nfor the correlation matrix [6, 65]. Informally, the RIE estimator is the matrix ρ\\r\\nRIE that, among the correlation\\r\\nmatrices sharing the same eigenvectors with ρ\\r\\nsam, achieves the minimum Hilbert-Schmidt distance dHS(ρ, ρtrue) =\\r\\nTr[(ρ−ρ\\r\\ntrue)2\\r\\n] from the true population matrix ρ\\r\\ntrue. This task might seem impossible at first sight, because ρtrue is\\r\\nunobservable. However, it turns out that, for the minimization of dHS(ρ, ρtrue) for large enough L, it is sufficient to\\r\\nknow the spectral density p\\r\\ntrue(λ) of ρtrue, which can be obtained from the spectral density psam(λ) of ρsam, thanks\\r\\nto a type of self-averaging property [6]. The final ingredient consists in modifying the eigenvalues {λi}\\r\\nN\\r\\ni=1 of ρ\\r\\nsam to\\r\\nλ˜\\r\\ni ≡\\r\\nλi\\r\\n|1 − Q−1 + Q−1zis(zi)|\\r\\n2\\r\\ni = 1, N (30)\\r\\n24\\nwhere zi ≡ λi − iη is the complexification of λi\\r\\n; η is a small parameter that should vanish in the N → ∞ limit;\\r\\ns(z) ≡ Tr[(zI − ρ\\r\\nsam)−1\\r\\n]/N is the so-called Cauchy transform of the spectrum of ρ\\r\\nsam [6]. A convenient choice for\\r\\nη in the case of finite N is η = N −1/2[6]. (However, see [65] and the application example below for an improved\\r\\nvariant.) Using Eq. (30), we obtain the optimal RIE as\\r\\nρ\\r\\nRIE =\\r\\nX\\r\\nN\\r\\ni=1\\r\\nλ˜\\r\\niu(i)u\\r\\n⊤\\r\\n(i)\\r\\n. (31)\\r\\nFor large L, the optimal RIE is expected to outperform, in terms of dHS, any other estimator (including PCA,\\r\\nshrinkage, and other corrected sample estimators) that, like the RIE itself, modifies only the spectrum of ρ\\r\\nsam and\\r\\nnot its eigenvectors [6].\\r\\nApplication example. Ib`a˜nez-Berganza et al. [65] compared several noise-cleaning methods of estimation of covari\\x02ance and precision matrices from both human brain activity (fMRI) time series and synthetic data of size comparable\\r\\nwith that typically encountered in neuroscience. They assessed the reliability of each method via the test-set like\\x02lihood and, in the case of synthetic data, via the distance from the true precision matrix. The methods considered\\r\\ninclude the eigenvalue clipping (or PCA; see above), linear shrinkage (see section 3.1), graphical lasso (see section 3.7),\\r\\nFA [3, 4], early-stopping gradient ascent algorithms, the RMT-based optimal RIE given by Eq. (31), and a variant\\r\\nof the last one where the parameter η is optimized by cross-validation on a grid of values rather than being set to\\r\\nη = N −1/2\\r\\n(see above). Their cross-validated RIE outperformed all the other estimators in the severe undersampling\\r\\nregime (i.e., small Q) typical of fMRI time series, highlighting the power of RMT for the analysis of neuroscience data.\\r\\nNotably, the cross-validated RIE was the only method that improved upon the raw sample correlation matrix ρ\\r\\nsam in\\r\\nthe estimation of the true correlation matrix ρ\\r\\ntrue, in all the simulated regimes and especially for strongly correlated\\r\\nsynthetic data. They finally proposed a simple algorithm based on an iterative likelihood gradient ascent, leading to\\r\\naccurate estimations in weakly correlated synthetic data sets. A Python code implementing all the methods used in\\r\\nthe paper is available [297].\\r\\n5 Network-analysis inspired analysis directly applied to correlation ma\\x02trices\\r\\nAs we already mentioned, a straightforward way to use null models for correlation matrices as controls of correlation\\r\\nnetworks is to generate correlation networks from the correlation matrix generated by the null model or its distribu\\x02tion. In this section, we showcase another usage of null models for correlation matrices, which is to conduct analysis\\r\\ninspired by network analysis but directly on correlation matrix data with the help of null model correlation matrices.\\r\\nCrucially, this scenario does not involve transformation of a given correlation matrix into a correlation network.\\r\\nTo explain the idea, consider financial time series analysis using correlation matrices. Portfolio optimization and\\r\\nRMT directly applied to correlation matrix data are among powerful techniques to analyze such data [6, 8, 9]. These\\r\\nmethods do not suffer from difficulties in transforming correlation matrices into correlation networks because they\\r\\ndo not carry out such a transformation. In contrast, a motivation behind carrying out such a transformation is that\\r\\none can then use various network analysis methods. A strategy to take advantage of both approaches is to adapt\\r\\nnetwork analysis methods for conventional networks to the case of correlation matrix data.\\r\\n5.1 Degree\\r\\nMany empirical networks show heterogeneous degree distributions such as a power-law-like distribution [295, 298];\\r\\nsuch networks are called scale-free networks. The same holds true for the weighted degree of many networks [299].\\r\\nCorrelation networks are no exception, not much depending on how one constructs a network from correlation matrix\\r\\ndata [264, 276, 300–302].\\r\\nIf we do not transform the given correlation matrix into a network, the node’s weighted degree represents how\\r\\nthe node’s signal, Xi, is close to the signal averaged over all the nodes, Xtotal, as shown in Eq. (18). Previous\\r\\nresearch showed that the weighted degree calculated in this manner is heterogeneously distributed for some empirical\\r\\ndata, while the right tail of the distribution is not as fat as typical degree distributions for conventional empirical\\r\\nnetworks [276]. The results are qualitatively similar when one calculates the weighted degree of the ith node as\\r\\nPN\\r\\nj=1;j̸=i\\r\\n|Cij | or PN\\r\\nj=1;j̸=i;Cij>0 Cij . Therefore, heterogeneous degree distributions of the correlation network are\\r\\nnot an artifact of the thresholding or other operations for creating networks from correlation data, at least to some\\r\\nextent.\\r\\n25\\n5.2 Community detection\\r\\nCommunity structure in networks is a partition of the nodes into (generally non-overlapping) groups that are inter\\x02nally well connected and sparsely connected across. One can detect communities with many different algorithms,\\r\\nand one popular family of methods, despite some shortcomings [147], is modularity maximization [288, 303], which\\r\\naims at placing a higher-than-expected number of edges connecting nodes within the same community. Modularity\\r\\nwith a resolution parameter γ is defined by\\r\\nQ =\\r\\n1\\r\\nNk\\r\\nX\\r\\nN\\r\\ni,j=1\\r\\n\\x12\\r\\nAij − γ\\r\\nkikj\\r\\nNk\\r\\n\\x13\\r\\nδgi,gj, (32)\\r\\nwhere we remind that Aij is the entry of the empirical adjacency matrix, giis the community in which the ith node\\r\\nis placed by the current partition, and δ is again the Kronecker delta. Note the presence of the term kikj/Nk coming\\r\\nfrom Eq. (17), which signifies the use of the configuration model as standard null model in the ordinary modularity\\r\\nfor networks. Approximate maximization of Q by varying g1, . . ., gN for a given network identifies its community\\r\\nstructure.\\r\\nCommunity detection, in particular modularity maximization, is desirable for correlation network data, too.\\r\\nGiven that the original correlation matrix has both positive and negative entries in general, a possible variant of\\r\\nmodularity maximization for correlation networks is to maximize a modularity designed for signed networks. The\\r\\nmodularity for signed networks may be defined as a weighted difference of the modularity calculated for the positive\\r\\nnetwork (i.e., the weighted network only containing positively weighted edges) and the modularity calculated for\\r\\nthe negative network (i.e., the weighted network only containing negatively weighted edges with the edge weight\\r\\nbeing the absolute value of the correlation) [140]. However, this procedure assumes that, in the null model, edges\\r\\ncan be thought as independent (as in the model described by Eq. (17)) and that positive edges can be randomized\\r\\nindependently of negative edges. We have seen that both assumptions are clearly not valid for correlation matrices.\\r\\nOne can bypass the analogy with networks by directly computing and maximizing a modularity function that is\\r\\nappropriately defined for correlation matrices [103]. A viable redefinition of modularity for correlation matrices is\\r\\ngiven by\\r\\nQ\\r\\ncor =\\r\\n1\\r\\nN\\r\\nX\\r\\nN\\r\\ni,j=1\\r\\n\\r\\nρ\\r\\nsam\\r\\nij − ⟨ρij ⟩\\r\\n\\x01\\r\\nδgi,gj, (33)\\r\\nwhere N =\\r\\nPN\\r\\ni,j=1 ρ\\r\\nsam\\r\\nij is a normalization constant, which is inessential to the maximization problem as long as it\\r\\nis positive. Matrix ⟨ρ⟩ should be a proper null model for the correlation matrix. Approximate maximization of Qcor\\r\\nprovides optimal communities in correlation matrices. The crucial ingredient is the choice of the null model, ⟨ρij ⟩.\\r\\nDepending on what features of the original data one desires to preserve, the use of any of the models described in\\r\\nsection 4 is in principle legitimate, e.g., the white-noise (identity matrix) model ρ\\r\\nMG1\\r\\nij = δij , the noise-only model\\r\\nρ\\r\\nMG2\\r\\nij , the noise+global model ρ\\r\\nMG3\\r\\nij , or the correlation matrix configuration model C\\r\\ncm\\r\\nij . However, note that in the\\r\\nsummation in Eq. (33) some terms must be positive and some must be negative, but at the same time not dominated\\r\\nby noise, in order to identify a nontrivial community structure, i.e., one different from a single community enclosing\\r\\nall nodes. For example, if all the entries of the empirical correlation matrix, ρ\\r\\nsam, are positive, then the null model\\r\\nρ\\r\\nMG1 will keep all terms in the summation in Eq. (33) non-negative, and the resulting optimal partition will be a\\r\\nsingle community [103]. By contrast, the use of ρ\\r\\nMG2 removes the noisy component of the sample correlation matrix\\r\\nand allows to detect noise-filtered communities, unless a global trend is present. If a global trend is present, then all\\r\\nthe entries of the filtered matrix in Eq. (27) are positive, preventing communities from being detected. When all the\\r\\nterms in the summation in Eq. (33) are non-negative, the resulting optimal partition is a single community [103],\\r\\nincidentally showing the limitation of PCA for the community detection task. In presence of such a global trend,\\r\\none obtains the best results by using ρ\\r\\nMG3, which uncovers group-specific correlations [103]. Having maximized the\\r\\nmodularity guarantees that the identified community structure is ‘optimally contrasted’, with necessarily positive\\r\\noverall residual correlations (with respect to the null model) inside each community and necessarily negative overall\\r\\nresidual correlations across different communities. Modularity maximization using ρ\\r\\nMG3 has successfully revealed\\r\\nnontrivial community structure in time series of financial stock prices [103, 104, 106], credit default swaps [105],\\r\\nsingle-cell gene expression profiles [79], and neuronal activity [296]. The last example is expanded below.\\r\\nApplication example: Almog et al. [296] applied RMT-based community detection, defined via the maximization of\\r\\nthe modularity given by Eq. (33), to the empirical correlation matrix obtained from single-neuron time series of gene\\r\\nexpression in the biological clock of mice. The recording was made from the suprachiasmatic nucleus (SCN), located\\r\\nin the hypothalamus. The biological clock is a highly synchronized brain region that is yet adaptable, for example,\\r\\nto external light stimuli and their seasonal variations. Therefore, the research focus was on the identification of\\r\\n26\\nboth positive (excitatory, phase-coherent) and negative (inhibitory, phase-opposing) interactions among constituent\\r\\nneurons. They showed that methods based on dichotomization using a global threshold, as well as ‘naive’ community\\r\\ndetection methods using the ordinary network-based modularity (i.e., Eq. (32)), fail to identify groups of neurons\\r\\nthat are internally positively correlated, and negatively correlated across. On the other hand, the maximization of\\r\\nthe RMT-based modularity (i.e., Eq. (33)), with the null model given by ⟨ρ⟩ = ρ\\r\\nMG3 in Eq. (28), successfully found\\r\\ncommunity structure by filtering out both the neuron-specific noise and the system-wide dependencies that obfuscate\\r\\nthe presence of underlying modules in the SCN. Their study uncovered two otherwise undetectable, negatively\\r\\ncorrelated populations of neurons (specifically, a spatially inner population and an outer one, both with left-right\\r\\nsymmetry), whose relative size and mutual interaction strength were found to depend on the photoperiod [296].\\r\\nIn particular, the average residual intra-community correlation was significantly higher in short photoperiods (e.g.,\\r\\nwinter) than in long photoperiods (e.g., summer). In contrast, the residual inter-community correlation was lower\\r\\nin short photoperiods than in long photoperiods. A MATLAB package for the calculation of the null models used in\\r\\nthe paper is available [304].\\r\\n5.3 Clustering coefficient\\r\\nClustering coefficients measure the abundance of triangles in a network [295]. A dominant definition of clustering\\r\\ncoefficient for unweighted networks, denoted by C˜, is given by [275]\\r\\nC˜ =\\r\\n1\\r\\nN\\r\\nX\\r\\nN\\r\\ni=1\\r\\nC˜\\r\\ni\\r\\n, (34)\\r\\nwhere C˜\\r\\ni\\r\\nis the local clustering coefficient at the ith node given by\\r\\nC˜\\r\\ni =\\r\\n(number of triangles involving the ith node)\\r\\nki(ki − 1)/2\\r\\n. (35)\\r\\nThe denominator of the right-hand side of Eq. (35) is a normalization constant to ensure that 0 ≤ C˜\\r\\ni ≤ 1.\\r\\nOne often measures clustering coefficients for both unweighted and weighted correlation networks. There are vari\\x02ous definitions of weighted clustering coefficients [305,306]. One definition [301] is given by C˜wei,Z = N −1 PN\\r\\ni=1 C˜wei,Z\\r\\ni\\r\\n,\\r\\nwhere\\r\\nC˜wei,Z\\r\\ni =\\r\\n1\\r\\nmaxi\\r\\n′j′ wi\\r\\n′j′\\r\\nP\\r\\n1≤j,ℓ≤N\\r\\nj,ℓ̸=i\\r\\nwijwiℓwjℓ\\r\\nP\\r\\n1≤j,ℓ≤N\\r\\nj,ℓ̸=i;j̸=ℓ\\r\\nwijwiℓ\\r\\n, (36)\\r\\nand wij (= wji ≥ 0) is the weight of edge (i, j).\\r\\nMany empirical networks show large unweighted or weighted clustering coefficient values, and correlation networks\\r\\nare no exception. However, as we pointed out in section 3.2, a high clustering coefficient of the correlation network\\r\\nis at least partly due to pseudo correlation.\\r\\nGiven this background, clustering coefficients for correlation matrices were proposed using a similar idea to the\\r\\ncase of modularity directly defined for correlation matrices [307]. Because correlation matrices are naturally clustered\\r\\nif we dichotomize on the Pearson correlation matrix, the authors used the three-way partial correlation coefficient or\\r\\npartial mutual information to partial out the effect of a common neighbor of nodes j and ℓ (say i) to quantify partial\\r\\nconnection between j and ℓ. In other words, we measure the connectivity between neighbors of i by, for example,\\r\\nthe partial correlation coefficient ρ(Xj , Xℓ | Xi), which we abbreviate as ρjℓ|i; the partial correlation coefficient is\\r\\ndefined by Eq. (21). Because there is no clear notion of neighborhood for correlation matrices, we need to consider\\r\\nall triplets of different nodes, (i, j, ℓ). Then, as for the definition of the original clustering coefficient for networks,\\r\\nthey took the average of ρjℓ|i over the ith node’s neighbors j and ℓ to define a local clustering coefficient for i. For\\r\\nexample, we define a local clustering coefficient for node i as a weighted average by\\r\\nC\\r\\ncor,A\\r\\ni =\\r\\nP\\r\\n1≤j<ℓ≤N\\r\\nj,ℓ̸=i\\r\\n\\x0c\\r\\n\\x0cρijρiℓρjℓ|i\\r\\n\\x0c\\r\\n\\x0c\\r\\nP\\r\\n1≤j<ℓ≤N\\r\\nj,ℓ̸=i\\r\\n|ρijρiℓ|\\r\\n. (37)\\r\\nFinally, as in the case of the clustering coefficient for networks, we define the global clustering coefficient by C\\r\\nP\\r\\ncor,A =\\r\\nN\\r\\ni=1 C\\r\\ncor,A\\r\\ni\\r\\n/N. This method borrows the idea of clustering coefficient from complex network studies and tailors it for\\r\\ncorrelation matrix data. Clustering coefficients C\\r\\ncor,A\\r\\ni\\r\\nand C\\r\\ncor,A already partial out the effect of pseudo correlation\\r\\nbetween Xj and Xℓ due to Xi. However, we can still compare the observed clustering coefficient values against those\\r\\n27\\nfor null models to validate whether or not the clustering coefficient values for the given data are significantly different\\r\\nfrom those for the null model [276].\\r\\nApplication example: Masuda et al. searched for possible association of C\\r\\ncor,A\\r\\ni\\r\\n, C\\r\\ncor,A, and similar clustering\\r\\ncoefficients for correlation matrices with the age of human participants in fMRI experiments [307]. They used\\r\\npublicly available resting-state fMRI data from the brains of healthy adults with a wide range of ages. The nodes\\r\\nwere defined by a commonly used brain atlas consisting of N = 30 regions of interest. They found that the global\\r\\nclustering coefficients, such as C\\r\\ncor,A, declined with age. The correlation between the age and Ccor,A (and a variant\\r\\nof C\\r\\ncor,A) was stronger than that between the age and conventional clustering coefficients for general unweighted and\\r\\nweighted networks combined with both Pearson and partial correlation networks. Furthermore, the proposed local\\r\\nclustering coefficients were more strongly negatively correlated with age than the conventional clustering coefficients\\r\\nfor general networks.\\r\\n6 Software\\r\\nIn this section, we introduce freely available code useful for analyzing correlation networks. Obviously, one can\\r\\napply software for analyzing general unweighted or weighted networks after thresholding (and optionally further\\r\\ndichotomizing) the given correlation matrix data. There are numerous packages for unweighted and weighted network\\r\\nanalysis, which we do not mention here without a few exceptions.\\r\\nIn gene correlation network analysis, Weighted Correlation Network Analysis (WGCNA) is a popular freely avail\\x02able R software package [73]. WGCNA provides various outputs, such as community structure, weighted clustering\\r\\ncoefficients, and visualization. WGCNA transforms the original edge weight, denoted by wij for edge (i, j), by a\\r\\nso-called soft thresholding transformation, i.e., by |wij |\\r\\nβ\\r\\n, where β ≥ 1 is a parameter5, such that one obtains an\\r\\nunsigned weighted network. Phiclust [79] is another community-detection tool for correlations from single-cell gene\\r\\nexpression data, derived from RMT (i.e., Wishart ensemble as described in section 4). It can be used to identify\\r\\ncell clusters with non-random substructure, possibly leading to the discovery of previously overlooked phenotypes.\\r\\nPhiclust is written in R and is freely available at Github [308] and Zenodo under GNU General Public License\\r\\nV3.0 [309].\\r\\nThe Brain Connectivity Toolbox is a MATLAB toolbox for analyzing networks [310]. It is also implemented in\\r\\nPython. Despite the name, many of the functions provided by the Brain Connectivity Toolbox can be applied to\\r\\ngeneral network data, and many people outside neuroscience also use it. In relation to correlation networks, this\\r\\ntoolbox is particularly good at handling weighted and signed networks, such as their degree, clustering coefficients,\\r\\nand community structure.\\r\\nThe graph-tool module in Python provides powerful network analysis and visualization functionality [311]. In\\r\\nrelation to correlation networks, graph-tool is particularly strong at network inference based on stochastic block\\r\\nmodels.\\r\\nThe bootnet package in R can be used for estimating psychological networks using graphical lasso, with a unique\\r\\nfeature of being able to assess the accuracy of the estimated network [25]. This package can be used for other types\\r\\nof correlation matrix data as well. Also see [25, 312] for various R packages for sparse and related estimation of the\\r\\nprecision and covariance matrices. For example, the qgraph package can also generate networks using the graphical\\r\\nlasso and visualize Pearson and partial correlation matrices [313], with beginner-friendly tutorials (e.g., [314]).\\r\\nGraphical lasso is also implemented in Python, through the GraphicalLassoCV function in the scikit-learn pack\\x02age [315, 316]. The sklearn.covariance package also contains other functions for covariance estimation such as co\\x02variance shrinkage. Table 2 of [21] lists other code packages for estimating graphical models as well as different\\r\\nmodels.\\r\\nThe Covariance Estimators package in Python, developed by Lucibello and Ib`a˜nez-Berganza [297], contains\\r\\nseveral utilities for denoising empirical covariance matrices. In particular, it implements various variants of PCA,\\r\\nlinear shrinkage, graphical lasso, FA, early-stopping gradient ascent algorithms, and the RMT-based optimal RIE\\r\\nproposed in [65] (see section 4.3).\\r\\nPapers [103, 277] contain references to Python, MATLAB, and R codes for generating null models of correlation\\r\\nmatrices discussed in section 4. For example, the “spatiotemporal modeling tools for Python” contains functions to\\r\\ngenerate null model correlation matrices such as the H-Q-S model (named Zalesky matching model in their package)\\r\\nand methods through generating surrogate time series [283] (see section 4.1). Another package in the list is the Scola,\\r\\nin Python, which generates the H-Q-S model and the correlation matrix configuration model [250] (see section 4.2).\\r\\n5The soft thresholding here, coined in [73], is different from the same term defined in section 3.4. It also does not belong to thresholding\\r\\nin the sense used in this article (defined in section 3.2).\\r\\n28\\nFinally, a MATLAB package by MacMahon [304] implements the null models based on RMT, ρ\\r\\nMG2 and ρMG3\\r\\n,\\r\\nderived in [103] from the Wishart ensemble (see section 4.3).\\r\\n7 Outlook\\r\\n7.1 Recommended practices\\r\\nWe have reviewed various techniques to obtain and analyze networks generated from correlation matrix data, which\\r\\nnaturally arise across many domains. Sections 2 and 3 emphasize for readers that there is not a single dominant\\r\\nmethod. We also highlighted good practices and pitfalls of individual methods. Na¨ıve applications of network\\r\\ngeneration and analysis methods to correlation matrix data can easily yield flawed results. We should be careful\\r\\nabout both how to generate correlation networks and how to analyze them. We recommend the following practices.\\r\\nFirst, in resonance with previous reports, we explained that a simplistic dichotomizing, which is widely used, is\\r\\nproblematic for multiple reasons (see section 3.2). Therefore, if you threshold your correlation matrices to create net\\x02works, which may or may not be followed by dichotomization, do either of the following: (i) report your downstream\\r\\nnetwork analysis results across a wide range of the threshold value; (ii) use a method designed to investigate a range\\r\\nof thresholds altogether (e.g., persistent homology); (iii) devise and use a network index that is robust with respect\\r\\nto the choice of the threshold value; and/or (iv) still use the simplistic thresholding but combine it with a proper\\r\\nnull model. We showed an example of (i) at the end of section 3.2. All of these practices are heuristic, but they are\\r\\nsubstantially better than simply using a single threshold value, reporting network analysis results, and skipping the\\r\\nexamination of robustness.\\r\\nThat said, we do not have much knowledge about item (iii) regarding which network indices one should use.\\r\\nFor example, the values of many network indices probably depend on the threshold value, which directly affects the\\r\\nnumber of edges [156]. However, in most cases, we are interested in comparing network analysis results between\\r\\ndifferent cases, such as between disease and control groups, between empirical and randomized data, or between\\r\\nindividuals of different ages. Then, the group difference or ranking among individuals may be robust enough when\\r\\none varies the threshold value. Investigating robustness of correlation network analysis outcomes with respect to the\\r\\nthreshold warrants more work.\\r\\nTo illustrate item (iv), we recall that correlation networks have high clustering no matter what data we use.\\r\\nHowever, a proper null model (e.g., shuffling of {xiℓ}) will also produce dichotomized correlation networks with high\\r\\nclustering. Therefore, by comparing the results with those for the null model, one can avoid wrong conclusions such\\r\\nas that almost all empirical correlation networks have high clustering. We recall that null models for networks, most\\r\\nfamously the configuration model, are not a proper null model for correlation networks because they generally do\\r\\nnot originate from correlation matrices and do not generally match key properties of typical correlation matrices.\\r\\nSee section 4 for proper choices.\\r\\nOur second, alternative recommendation is to resort to other methods, such as weighted correlation networks,\\r\\ngraphical lasso (which is a partial correlation network method) and its variants, and covariance shrinkage, which\\r\\navoid thresholding. Nonetheless, these methods usually require some arbitrary decisions by the users, such as\\r\\nsetting hyperparameter values. Therefore, assessing robustness with respect to such choices remains important. For\\r\\nexample, with weighted correlation networks without thresholding, one usually chooses between whether to force\\r\\nnegative correlation values to zero, to keep them by taking the absolute value of the correlation, or to treat them\\r\\nas signed networks. Few papers have investigated different cases to check robustness of the subsequent network\\r\\nanalysis results. Furthermore, because these different operations have been main options for a long time, it may be\\r\\nbeneficial to pursue quantification of weighted networks that would provide results that are robust with respect to\\r\\nthis methodological choice.\\r\\nOur third recommendation is to avoid transforming correlation matrix data into networks but yet carry out\\r\\ndownstream analysis analogous to network analysis. However, we recommend doing so only for properties whose\\r\\ndefinition does not make (implicit) assumptions that are violated by correlation matrices, such as the assumption\\r\\nof independent matrix entries that the ordinary modularity function in Eq. (32) implicitly makes. In presence of\\r\\nsuch unverified assumptions, we recommend either appropriately revising the definition of the property, e.g. as in the\\r\\nmodified modularity in Eq. (33), or dismissing the property altogether. In section 5, we showcased some such methods\\r\\nincluding two example analyses. This type of analysis is available for at least the degree, modularity maximization,\\r\\nand clustering coefficients. Then, one can evade thresholding or thoroughly examining various threshold values.\\r\\nFuture work should generalize this approach to other structural properties and analysis methods formulated for\\r\\nnetworks. Examples include various node centrality measures, motifs, community detection methods apart from\\r\\nmodularity maximization, rich clubs, fractals, and simplicial complexes. In many cases, the configuration model is\\r\\na standard choice of null model when constructing a network algorithm, such as a community detection algorithm.\\r\\n29\\nHowever, while we now have a reasonably long list of null models for correlation matrices (see section 4), it is not\\r\\nknown whether the configuration model for covariance matrices or a different null model described in section 4\\r\\nshould be a standard choice when constructing an algorithm for correlation matrices inspired by network analysis.\\r\\nAnswering this question needs systematic comparison studies of downstream analyses across various null models for\\r\\ncorrelation matrices.\\r\\n7.2 Directions for future research\\r\\nIn this section, we identify some promising directions for future research.\\r\\nReproducibility of correlation networks arising from common approaches is a major practical issue, as has been\\r\\npointed out in the literature in psychology and psychotherapy (e.g. [51, 57]) and microbiome analysis [92]. In these\\r\\nresearch fields and others, it is often the case that only relatively few samples are available given the size of matrix\\r\\nand network, N, we wish to analyze. Especially if the number of samples, L, is smaller than or close to N, the\\r\\npartial correlation matrix and spectra of random matrices carry large variation, in part because they are inherently\\r\\nrank deficient. From the statistical inference point of view, it is not sound to try to infer many parameters, such\\r\\nas entries of the covariance matrix, from relatively few observations. The recognition of such phenomena led to\\r\\nthe idea of covariance selection and various other methods. The amount of data needed for reliably estimating\\r\\ncorrelation networks, i.e., power analysis in statistics, should be further pursued for various correlation matrix\\r\\ndata [25]. The development of methods to help practitioners use correlation networks better (e.g., by providing\\r\\nuncertainty quantification or clarifying the various noise trade-offs) can be transformational. Despite these challenges,\\r\\nthere is a pressing need to understand complex systems of a very high dimension (i.e., N ≫ L) with correlational\\r\\ndata. One approach to this problem is to formulate estimation of large correlation networks as a computational\\r\\nrather than statistical challenge, as a problem to be solved under runtime and memory constraints, and to search\\r\\nfeasible solutions in combination with machine learning [1]. How to reconsile the statistical and computational types\\r\\nof approach and deepen usage of machine learning and artificial intelligence techniques to correlation network analysis\\r\\nmay be a beneficial research direction.\\r\\nA key outstanding question is the treatment of low-correlation edges. On one hand, we have surveyed attempts\\r\\nto remove “noise” edges (for example by thresholding or graphical lasso), which is supposed to improve the overall\\r\\nsignal-to-noise ratio of the graph representation. Sparser models are more parsimonious, easier to process quickly\\r\\nand with a lower memory footprint, and amenable to a range of network science analysis tools. On the other hand,\\r\\none can argue that getting rid of low-correlation edges risks losing valuable information (see section 3.2). In fact,\\r\\nit has been shown in the neuroscience context that the low-correlation edges alone can have substantial predictive\\r\\npower [161, 317, 318].\\r\\nA related question is how to use a priori domain knowledge to choose appropriate preprocessing steps, such as\\r\\nwhat threshold to apply and whether or not to dichotomize. For example, dichotomizing may be more appropriate\\r\\nwhen the a priori belief is that nodes are either coordinating or not, with no appreciable difference in the degree of\\r\\ncoordination when two nodes coordinate. As another example, one could use RMT on domain-relevant distributions\\r\\nto compare the dominant eigenvectors or eigenvalues before and after thresholding. This exercise may provide\\r\\nguidance on when thresholding is unlikely to have adverse effects.\\r\\nAnother viable alternative to the current focus on trying to recover and analyze the most accurate possible\\r\\ncorrelation matrix may be to treat the constructed correlation network as inherently uncertain and to regard it as\\r\\na sample from a distribution of possible matrices as part of the analysis. For example, when assessing community\\r\\nstructure, it may make sense to focus on structures that appear consistently across samples of correlation networks\\r\\ndrawn from the estimated distribution, even when exact correlation values (perhaps especially the weaker correlations)\\r\\nconsiderably vary from sample to sample. Although there are established ways to do this for general networks [319],\\r\\nmodeling of correlation networks and their substructures by their probability distributions is still a new idea [320]\\r\\nand needs further development. Such approaches may leverage existing work on null models for correlation networks,\\r\\nfor example, as priors when forming a posterior distribution to sample from. On the other hand, some studies have\\r\\ndocumented the stability of the detected correlation-induced communities across time and their robustness under\\r\\nchange of temporal resolution [103–106, 296].\\r\\nThere are many multilayer network data sets, including multilayer correlation matrix data sets, and various\\r\\ndata-analysis methods for them [110, 321–325]. Examples include brain activity, where different layers of correlation\\r\\nmatrices correspond to, for example, different frequency bands [326–329], or brain activity during different tasks\\r\\n[330]. In gene co-expression networks, different layers correspond to, for example, different levels of co-expression\\r\\nbetween gene pairs [331] or different tissue types such as different organs [332]. Overlaying different methods to\\r\\nconstruct correlation networks from one data set in each layer is another method to construct multilayer correlation\\r\\nmatrices [333]. There are methods for analyzing multilayer correlation matrices and networks such as multilayer\\r\\n30\\ncommunity detection algorithms [110, 332]. However, methods that exploit the mathematical nature of correlation\\r\\nmatrices data are still scarce and left for future work. Furthermore, multilayer networks are a major representation\\r\\nof temporal network data, where each layer corresponds to one time window [110, 258, 334]. Therefore, methods for\\r\\nanalyzing multilayer correlation network data will also contribute to analysis of time-varying correlation network\\r\\ndata.\\r\\nSimilar to other studies, we emphasize the potentially negative effects of thresholding, motivating our explanation\\r\\nof other methods for constructing correlation networks. However, thresholding also has positive effects such as\\r\\nreducing false positives by discarding edges with small weights including the case of correlation networks. Such\\r\\npositive effects of thresholding may be manifested in multilayer data. For example, aggregating layers in a multilayer\\r\\nnetwork and dichotomizing the aggregated edge weight can enhance detectability of multilayer communities compared\\r\\nto no dichotomizing under certain conditions [335, 336]. Furthermore, some layers in a multilayer network may be\\r\\nmore informative than other layers. While these arguments are for general multilayer networks, many of them may\\r\\ndirectly apply to multilayer correlation matrices.\\r\\nFor a given N, the set of covariance matrices constitutes the positive semidefinite cone, which is convex. Similarly,\\r\\nthe set of full-rank correlation matrices, which is a strict subset of full-rank covariance matrices, is called the\\r\\nelliptope [131, 132]. Positive semidefinite cones and elliptopes are manifolds and have their own geometric structure,\\r\\nwhich have been suggested to be useful for measuring the similarity between pairs of covariance or correlation\\r\\nmatrices. Quantitative comparison of two covariance and correlation matrices is useful for various tasks such as\\r\\nfingerprinting of individuals, anomaly detection, and change-point detection in multivariate time series data. A\\r\\nstraightforward way to measure the distance between two covariance/correlation matrices is to use a common matrix\\r\\nnorm such as the Frobenius norm (i.e., r\\r\\nPN\\r\\ni=1\\r\\nPN\\r\\nj=1\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0cρ\\r\\n(1)\\r\\nij − ρ\\r\\n(2)\\r\\nij\\r\\n\\x0c\\r\\n\\x0c\\r\\n\\x0c\\r\\n2\\r\\nin the case of correlation matrices, where ρ\\r\\n(1)\\r\\nand ρ\\r\\n(2) are two correlation matrices). However, research (in particular in neuroimaging studies) has suggested that\\r\\ngeodesic distance measures respecting the structure of these manifolds is better at, for example, fingerprinting of\\r\\nindividuals from fMRI data [337–339]. In these geodesic distance measures, one considers the tangent space at a\\r\\ngiven point x on the manifold, which corresponds to a correlation/covariance matrix. The so-called exponential\\r\\nmap provides a one-to-one mapping from the straight line segment on the tangent space, which is essentially the\\r\\nEuclidean space, to the geodesic from x to y on the manifold. The logarithm map is the inverse of the exponential\\r\\nmap. The geodesic distance between x and y is the length of the geodesic and has a practical matrix algebraic\\r\\nformula. Multiple reasonable definitions of such geodesic distances exist [338]. See [338, 340, 341] for mathematical\\r\\nexpositions. Although these techniques are not for correlation networks but for matrices, they may potentially benefit\\r\\nunderstanding and algorithms for correlation networks. For example, can we understand null models of correlation\\r\\nmatrices as a projection onto a submanifold of the entire elliptope? What are geometric meanings of thresholding,\\r\\ndichotomizing, and other operations to create correlation networks? Do we benefit by measuring distances between\\r\\ncorrelation networks rather than between correlation matrices?\\r\\nWe mentioned examples of microbiome and bibliometric co-occurrence networks as variants of correlation networks\\r\\nin sections 2.5 and 2.8, respectively. For example, let xiℓ = 1 if the ith researcher is an author of the ℓth paper in the\\r\\ndatabase and xiℓ = 0 otherwise. Then, the number of papers that the ith and jth researchers have coauthored, which\\r\\nare co-occurrence events, is equal to PL\\r\\nℓ=1 xiℓxjℓ, where L is the number of papers in the entire data. This quantity is\\r\\na non-standardized covariance, which one can analyze as a correlation network. In fact, the original data, i.e., N ×L\\r\\nmatrix (xiℓ), is a bipartite network, in which one part consists of N nodes representing the researchers, and the other\\r\\npart consists of L nodes representing the papers. An edge exists between an author node and a paper node if and\\r\\nonly if xiℓ = 1. Then, we can interpret the N × N co-occurrence, or correlation, network whose (i, j) entry is given\\r\\nby PL\\r\\nℓ=1 xiℓxjℓ as a one-mode projection of the bipartite network. This viewpoint provides research opportunities.\\r\\nIt is known that one-mode projection introduces various biases [342]. For example, one-mode projection inflates\\r\\nthe clustering coefficient [343–345], which is in fact consistent with the finding that correlation networks even from\\r\\nrandom data would have a high clustering coefficient (section 3.2). One strategy to avoid such biases is to analyze\\r\\nthe data as a bipartite network [342]. Therefore, bipartite network analysis may be equally useful for understanding\\r\\ncorrelation structure of continuous-valued data, i.e., an N ×L matrix (xiℓ), xiℓ ∈ R, which we have mostly dealt with\\r\\nin the present article. Establishing mapping of the continuous-valued data to a bipartite network is a first natural\\r\\nstep toward this goal.\\r\\nOne complication that has not received enough attention is that many in-practice comparisons involve ensembles\\r\\nof observed networks rather than single networks. This is the case in most fMRI studies where networks are used.\\r\\nWhen working with an ensemble of networks, one must make various decisions, such as whether or not to ensure\\r\\nthat edge density is constant across networks (see section 3.2 for the absolute versus proportional threshold). The\\r\\ndevelopment of mathematical theories for how to construct correlation-based networks for ensembles may be helpful\\r\\nbecause most null models and other tools are only oriented toward single networks. Multilayer approach and geometric\\r\\n31\\napproaches to correlation networks and matrices, both of which cater to between-network/matrix comparisons, are\\r\\npromising paths towards this goal.\\r\\nA graphon is a symmetric measurable function W : [0, 1]2 → [0, 1]. Given W, we generate dense graphs in which\\r\\nthere is an edge between the ith and jth nodes with probability W(ui, uj ), where each ui ∀i independently obeys the\\r\\nuniform density on [0, 1] [346]. In network science, assigning a node weight, either from a probability distribution or\\r\\nempirical data, and connecting two nodes probabilistically as a function of the two node weights has been a major\\r\\nmethod to generate networks [347–353]. Basic correlation networks are equivalent to an extension of this class of\\r\\nnetwork models where uiis an L-dimensional vector of the ith feature from L samples, and W is a criterion with which\\r\\nto determine the edge. In fact, similar to the construction of correlation networks by dichotomizing, dichotomizing\\r\\nfunctions have been used as W to generate networks with power-law degree distributions from scalar node weights\\r\\nthat do not have to obey long-tailed distributions [350, 352, 354–356]. Importing mathematical frameworks and\\r\\nmethods from graphon-related models, such as the limit of a sequence of dense networks, to correlation network\\r\\nanalysis may be an interesting idea. Those frameworks may be able to provide null models for correlation networks\\r\\nor give more mathematical foundations of correlation networks.\\r\\n7.3 Final words\\r\\nWe have seen that there are various research fields in which they collect and analyze correlation networks. We have\\r\\nalso seen that some particular analysis techniques are heavily studied in one field, but others are preferred in different\\r\\nfields. For example, random matrices for sample correlation matrices [8, 9] have particularly been used in financial\\r\\ndata studies, including econophysics [6, 101–106], while they have been applied much less, and only more recently, in\\r\\nneuroscience [65, 296]. As another example, a majority of studies of temporal correlation networks has been done in\\r\\nnetwork neuroscience under the name of dynamic functional connectivity/networks. However, very often, methods\\r\\nfor analyzing correlation networks developed in one research field do not rely on particularities of the field and are\\r\\ntherefore transferable to other research fields. While such cross-fertilization has been ongoing and advocated [21], we\\r\\nemphasize that much more of it will be useful for furthering correlation network analysis and applications. By the\\r\\nsame token, studies directly comparing objectives and performances of methods used in different research domains\\r\\nwill be valuable.\\r\\nCross-fertilization is also desirable within theoretical fields. Statisticians and non-statisticians tend not to know\\r\\neach other’s work and publish in different types of journals. Statisticians tend to start with research questions that\\r\\nare ideally asked and answered by statistical hypothesis testing or Bayesian methods. Therefore, they would develop\\r\\nmethods for correlation networks with which the data analysis results can be statistically tested. In contrast, non\\x02statistically-focused researchers including many applied mathematicians, physicists, and computer scientists tend\\r\\nto focus on network analysis techniques which may be heuristic or reflect analogies to other processes, many of\\r\\nwhich have been proven to be powerful in different settings. Because there are already many useful network analysis\\r\\nmethods, if one can exploit them in correlation data analysis, these types of researchers, including the present authors,\\r\\nwould expect that it is a beneficial connection. We tried to cover both types of approaches to correlation networks as\\r\\nmuch as possible in this article. We believe that more discussion between these different perspectives on correlation\\r\\nnetworks will drive further developments of both types of approach. See for example [357] for related discussion.\\r\\nAcknowledgements\\r\\nWe thank Sarah Muldoon for discussion. N.M. acknowledges support from National Institute of General Medi\\x02cal Sciences (under grant no. R01GM148973), the Japan Science and Technology Agency (JST) Moonshot R&D\\r\\n(under grant no. JPMJMS2021), the National Science Foundation (under grant nos. 2052720 and 2204936), and\\r\\nJSPS KAKENHI (under grant nos. JP 21H04595 and 23H03414). P.J.M. and Z.M.B. acknowledge support from\\r\\nthe Army Research Office (under MURI award W911NF-18-1-0244). P.J.M. also acknowledges support from the\\r\\nNational Institute of Diabetes and Digestive and Kidney Diseases (under grant no. R01DK125860) and from the Na\\x02tional Science Foundation (under grant no. 2140024). Z.M.B. also acknowledges support from the National Science\\r\\nFoundation (under grant no. 2137511). D.G. acknowledges support by the European Union - NextGenerationEU -\\r\\nNational Recovery and Resilience Plan (Piano Nazionale di Ripresa e Resilienza, PNRR), project ‘SoBigData.it -\\r\\nStrengthening the Italian RI for Social Mining and Big Data Analytics’ - Grant IR0000013 (n. 3264, 28/12/2021)\\r\\n(https://pnrr.sobigdata.it/). The content is solely the responsibility of the authors and does not necessarily\\r\\nrepresent the official views of any agency supporting this work.\\r\\n32\\nReferences\\r\\n[1] M. Becker, H. Nassar, C. Espinosa, I. A. Stelzer, D. Feyaerts, E. Berson, Neda H. Bidoki, A. L. Chang,\\r\\nG. Saarunya, A. Culos, D. De Francesco, R. Fallahzadeh, Q. Liu, Y. Kim, I. Mari´c, S. J. Mataraso, S. N. Pay\\x02rovnaziri, T. Phongpreecha, N. G. Ravindra, N. Stanley, S. Shome, Y. Tan, M. Thuraiappah, M. Xenochristou,\\r\\nL. Xue, G. Shaw, D. Stevenson, M. S. Angst, B. Gaudilliere, and N. Aghaeepour. Large-scale correlation net\\x02work construction for unraveling the coordination of complex biological systems. Nat. Comput. Sci., 3:346–359,\\r\\n2023.\\r\\n[2] I. T. Jolliffe. Principal Component Analysis. Springer, New York, NY, second edition, 2002.\\r\\n[3] T. W. Anderson. An Introduction to Multivariate Statistical Analysis. John Wiley & Sons, Hoboken, NJ, third\\r\\nedition, 2003.\\r\\n[4] S. A. Mulaik. Foundations of Factor Analysis. Taylor & Francis, Boca Raton, FL, second edition, 2010.\\r\\n[5] H. Markowitz. Portfolio selection. J. Finance, 7:77–91, 1952.\\r\\n[6] J. Bun, J. P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools from Random Matrix\\r\\nTheory. Phys. Rep., 666:1–109, 2017.\\r\\n[7] M. L. Mehta. Random Matrices. Elsevier, San Diego, CA, third edition, 2004.\\r\\n[8] G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices. Springer, Cham, Switzerland, 2018.\\r\\n[9] M. Potters and J.-P. Bouchaud. A First Course in Random Matrix Theory. Cambridge University Press,\\r\\nCambridge, UK, 2021.\\r\\n[10] B. Podobnik and H. E. Stanley. Detrended cross-correlation analysis: A new method for analyzing two non\\x02stationary time series. Phys. Rev. Lett., 100:084102, 2008.\\r\\n[11] X.-Y. Qian, Y.-M. Liu, Z.-Q. Jiang, B. Podobnik, W.-X. Zhou, and H. E. Stanley. Detrended partial cross\\x02correlation analysis of two nonstationary time series influenced by common external forces. Phys. Rev. E,\\r\\n91:062816, 2015.\\r\\n[12] N. Yuan, Z. Fu, H. Zhang, L. Piao, E. Xoplaki, and J. Luterbacher. Detrended partial-cross-correlation analysis:\\r\\nA new method for analyzing correlations in complex system. Sci. Rep., 5:8143, 2015.\\r\\n[13] D. Y. Kenett, M. Tumminello, A. Madi, G. Gur-Gershgoren, R. N. Mantegna, and E. Ben-Jacob. Dominating\\r\\nclasp of the financial sector revealed by partial correlation analysis of the stock market. PLoS ONE, 5:e15032,\\r\\n2010.\\r\\n[14] D. Y. Kenett, T. Preis, G. Gur-Gershgoren, and E. Ben-Jacob. Dependency network and node influence:\\r\\nApplication to the study of financial markets. Int. J. Bifu. Chaos, 22:1250181, 2012.\\r\\n[15] D. Zhou, A. Gozolchiani, Y. Ashkenazy, and S. Havlin. Teleconnection paths via climate network direct link\\r\\ndetection. Phys. Rev. Lett., 115:268501, 2015.\\r\\n[16] L. Chen, R. Liu, Z.-P. Liu, M. Li, and K. Aihara. Detecting early-warning signals for sudden deterioration of\\r\\ncomplex diseases by dynamical network biomarkers. Sci. Rep., 2:342, 2012.\\r\\n[17] S. Chen, E. B. O’Dea, J. M. Drake, and B. I. Epureanu. Eigenvalues of the covariance matrix as early warning\\r\\nsignals for critical transitions in ecological systems. Sci. Rep., 9:2572, 2019.\\r\\n[18] N. G. MacLaren, P. Kundu, and N. Masuda. Early warnings for multi-stage transitions in dynamics on networks.\\r\\nJ. R. Soc. Interface, 20:20220743, 2023.\\r\\n[19] T. Watanabe, N. Masuda, F. Megumi, R. Kanai, and G. Rees. Energy landscape and dynamics of brain activity\\r\\nduring human bistable perception. Nat. Comm., 5:4765, 2014.\\r\\n[20] T. Ezaki, T. Watanabe, M. Ohzeki, and N. Masuda. Energy landscape analysis of neuroimaging data. Phil.\\r\\nTrans. R. Soc. A, 375:20160287, 2017.\\r\\n33\\n[21] D. Borsboom, M. K. Deserno, M. Rhemtulla, S. Epskamp, E. I. Fried, R. J. McNally, D. J. Robinaugh,\\r\\nM. Perugini, J. Dalege, G. Costantini, A.-M. Isvoranu, A. C. Wysocki, C. D. van Borkulo, R. van Bork, and\\r\\nL. J. Waldorp. Network analysis of multivariate data in psychological science. Nat. Rev. Methods Primers,\\r\\n1:58, 2021.\\r\\n[22] V. Kukreti, H. K. Pharasi, P. Gupta, and S. Kumar. A perspective on correlation-based financial networks\\r\\nand entropy measures. Front. Phys., 8:323, 2020.\\r\\n[23] M. Tumminello, F. Lillo, and R. N. Mantegna. Correlation, hierarchies, and networks in financial markets. J.\\r\\nEcon. Behav. Org., 75:40–58, 2010.\\r\\n[24] F. De Vico Fallani, J. Richiardi, M. Chavez, and S. Achard. Graph analysis of functional brain networks:\\r\\nPractical issues in translational neuroscience. Phil. Trans. R. Soc. B, 369:20130521, 2014.\\r\\n[25] S. Epskamp, D. Borsboom, and E. I. Fried. Estimating psychological networks and their accuracy: A tutorial\\r\\npaper. Behav. Res., 50:195–212, 2018.\\r\\n[26] M.-E. Lynall, D. S. Bassett, R. Kerwin, P. J. McKenna, M. Kitzbichler, U. Muller, and E. Bullmore. Functional\\r\\nconnectivity and brain networks in schizophrenia. J. Neurosci., 30:9477–9487, 2010.\\r\\n[27] X. Liu, Y. Liu, J. Chen, and X. Liu. PSCC-Net: Progressive spatio-channel correlation network for image\\r\\nmanipulation detection and localization. IEEE Trans. Circ. Syst. Video Technol., 32:7505–7517, 2022.\\r\\n[28] M. Lee, C. Park, S. Cho, and S. Lee. Superpixel group-correlation network for co-saliency detection. In\\r\\nProceedings of the International Conference on Image Processing (ICIP), pages 806–810, 2022.\\r\\n[29] Z. Tu, Z. Li, C. Li, and J. Tang. Weakly alignment-free RGBT salient object detection with deep correlation\\r\\nnetwork. IEEE Trans. Image Proc., 31:3752–3764, 2022.\\r\\n[30] U. von Luxburg. A tutorial on spectral clustering. Stat. Comput., 17:395–416, 2007.\\r\\n[31] M. Tumminello, C. Coronnello, F. Lillo, S. Micciche, and R. N. Mantegna. Spanning trees and bootstrap\\r\\nreliability estimation in correlation-based networks. Int. J. Bifu. Chaos, 17:2319–2329, 2007.\\r\\n[32] S. M. Smith, K. L. Miller, G. Salimi-Khorshidi, M. Webster, C. F. Beckmann, T. E. Nichols, J. D. Ramsey,\\r\\nand M. W. Woolrich. Network modelling methods for FMRI. NeuroImage, 54:875–891, 2011.\\r\\n[33] K. Faust and J. Raes. Microbial interactions: From networks to models. Nat. Rev. Microbiol., 10:538–550,\\r\\n2012.\\r\\n[34] Y. X. R. Wang and H. Huang. Review on statistical methods for gene network reconstruction using expression\\r\\ndata. J. Theor. Biol., 362:53–61, 2014.\\r\\n[35] A. Fukushima, M. Kusano, H. Redestig, M. Arita, and K. Saito. Metabolomic correlation-network modules in\\r\\nArabidopsis based on a graph-clustering approach. BMC Syst. Biol., 5:1, 2011.\\r\\n[36] T. Millington and M. Niranjan. Construction of minimum spanning trees from financial returns using rank\\r\\ncorrelation. Physica A, 566:125605, 2021.\\r\\n[37] A. J. Butte and I. S. Kohane. Mutual information relevance networks: Functional genomic clustering using\\r\\npairwise entropy measurements. In Pacific Symposium on Biocomputing, pages 418–429, Singapore, 2000.\\r\\nWorld Scientific.\\r\\n[38] P. E. Meyer, F. Lafitte, and G. Bontempi. minet: A r/bioconductor package for inferring large transcriptional\\r\\nnetworks using mutual information. BMC Bioinformatics, 9:461, 2008.\\r\\n[39] X. Guo, H. Zhang, and T. Tian. Development of stock correlation networks using mutual information and\\r\\nfinancial big data. PLoS ONE, 13:e0195941, 2018.\\r\\n[40] R. Salvador, J. Suckling, C. Schwarzbauer, and E. Bullmore. Undirected graphs of frequency-dependent func\\x02tional connectivity in whole brain networks. Phil. Trans. R. Soc. B, 360:937–946, 2005.\\r\\n[41] P. Fiedor. Partial mutual information analysis of financial networks. Acta Physica Polonica A, 127:863–867,\\r\\n2015.\\r\\n34\\n[42] J. Pearl. Causal inference. Journal of Machine Learning Research Workshop and Conference Proceedings,\\r\\n6:39–58, 2010.\\r\\n[43] G. Briganti, M. Scutari, and R. J. McNally. A tutorial on bayesian networks for psychopathology researchers.\\r\\nPsychol. methods, 28:947–961, 2023.\\r\\n[44] J. Runge, S. Bathiany, E. Bollt, G. Camps-Valls, D. Coumou, E. Deyle, C. Glymour, M. Kretschmer, M. D. Ma\\x02hecha, J. Mu˜noz-Mar´ı, E. H. van Nes, J. Peters, R. Quax, M. Reichstein, M. Scheffer, B. Sch¨olkopf, P. Spirtes,\\r\\nG. Sugihara, J. Sun, K. Zhang, and J. Zscheischler. Inferring causation from time series in Earth system\\r\\nsciences. Nat. Commun., 10:2553, 2019.\\r\\n[45] P. Maurage, A. Heeren, and M. Pesenti. Does chocolate consumption really boost nobel award chances? the\\r\\nperil of over-interpreting correlations in health studies. J. Nutrition, 143:931–933, 2013.\\r\\n[46] J. M. Rohrer. Thinking clearly about correlations and causation: Graphical causal models for observational\\r\\ndata. Advances in Methods and Practices in Psychological Science, 1:27–42, 2018.\\r\\n[47] O. Ryan, L. F. Bringmann, and N. K. Schuurman. The challenge of generating causal hypotheses using network\\r\\nmodels. Structural Equation Modeling, 29:953–970, 2022.\\r\\n[48] G. M. Harari, N. D. Lane, R. Wang, B. S. Crosier, A. T. Campbell, and S. D. Gosling. Using smartphones\\r\\nto collect behavioral data in psychological science: Opportunities, practical considerations, and challenges.\\r\\nPerspectives on Psychological Science, 11:838–854, 2016.\\r\\n[49] A. L. McGowan, F. Sayed, Z. M. Boyd, M. Jovanova, Y. Kang, M. E. Speer, D. Cosme, P. J. Mucha, K. N.\\r\\nOchsner, D. S. Bassett, E. B. Falk, and D. M. Lydon-Staley. Dense sampling approaches for psychiatry research:\\r\\nCombining scanners and smartphones. Biol. Psychiatry, 93:681–689, 2023.\\r\\n[50] D. Borsboom and A. O. J. Cramer. Network analysis: An integrative approach to the structure of psychopathol\\x02ogy. Annu. Rev. Clin. Psychol., 9:91–121, 2013.\\r\\n[51] E. I. Fried and A. O. J. Cramer. Moving forward: Challenges and directions for psychopathological network\\r\\ntheory and methodology. Pers. Psychol. Sci., 12:999–1020, 2017.\\r\\n[52] E. I. Fried, C. D. van Borkulo, A. O. J. Cramer, L. Boschloo, R. A. Schoevers, and D. Borsboom. Mental\\r\\ndisorders as networks of problems: A review of recent insights. Soc. Psychiatry Psychiatr. Epidemiol., 52:1–10,\\r\\n2017.\\r\\n[53] A. L. McGowan, Z. M. Boyd, Y. Kang, L. Bennett, P. J. Mucha, K. N. Ochsner, D. S. Bassett, E. B. Falk,\\r\\nand D. M. Lydon-Staley. Within-person temporal associations among self-reported physical activity, sleep, and\\r\\nwell-being in college students. Psychosomatic Medicine, 85:141–153, 2023.\\r\\n[54] S. Epskamp, L. J. Waldorp, R. M˜ottus, and D. Borsboom. The Gaussian graphical model in cross-sectional\\r\\nand time-series data. Multivar. Behav. Res., 53:453–480, 2018.\\r\\n[55] W. Lutz, B. Schwartz, S. G. Hofmann, A. J. Fisher, K. Husen, and J. A. Rubel. Using network analysis for\\r\\nthe prediction of treatment dropout in patients with mood and anxiety disorders: A methodological proof-of\\x02concept study. Sci. Rep., 8:7819, 2018.\\r\\n[56] D. C. R. van Zelst, E. M. Veltman, D. Rhebergen, P. Naarding, A. A. L. Kok, N. R. Ottenheim, and E. J.\\r\\nGiltay. Network structure of time-varying depressive symptoms through dynamic time warp analysis in late-life\\r\\ndepression. Int. J. Geriatr. Psychiatry, 37:1–12, 2022.\\r\\n[57] M. K. Forbes, A. G. C. Wright, K. E. Markon, and R. F. Krueger. Evidence that psychopathology symptom\\r\\nnetworks have limited replicability. J. Abnormal Psychol., 126:969–988, 2017.\\r\\n[58] E. Bullmore and O. Sporns. Complex brain networks: Graph theoretical analysis of structural and functional\\r\\nsystems. Nat. Rev. Neurosci., 10:186–198, 2009.\\r\\n[59] D. S. Bassett and O. Sporns. Network neuroscience. Nat. Neurosci., 20:353–364, 2017.\\r\\n[60] J. Chung, E. Bridgeford, J. Arroyo, B. D. Pedigo, A. Saad-Eldin, V. Gopalakrishnan, L. Xiang, C. E. Priebe,\\r\\nand J. T. Vogelstein. Statistical connectomics. Annu. Rev. Stat. Appl., 8:463–492, 2021.\\r\\n35\\n[61] H.-J. Park and K. Friston. Structural and functional brain networks: From connections to cognition. Science,\\r\\n342:1238411, 2013.\\r\\n[62] R. Li´egeois, A. Santos, V. Matta, D. Van De Ville, and A. H. Sayed. Revisiting correlation-based functional\\r\\nconnectivity and its relationship with structural connectivity. Network Neuroscience, 4:1235–1251, 2020.\\r\\n[63] R. C. Craddock, S. Jbabdi, C.-G. Yan, J. T. Vogelstein, F. X. Castellanos, A. Di Martino, C. Kelly, K. Heberlein,\\r\\nS. Colcombe, and M. P. Milham. Imaging human connectomes at the macroscale. Nat. Methods, 10:524–539,\\r\\n2013.\\r\\n[64] G. Varoquaux and R. C. Craddock. Learning and comparing functional connectomes across subjects. Neu\\x02roImage, 80:405–415, 2013.\\r\\n[65] M. Ib´a˜nez-Berganza, C. Lucibello, F. Santucci, T. Gili, and A. Gabrielli. Noise cleaning the precision matrix\\r\\nof short time series. Phys. Rev. E, 108:024313, 2023.\\r\\n[66] B. Biswal, F. Z. Yetkin, V. M. Haughton, and J. S. Hyde. Functional connectivity in the motor cortex of\\r\\nresting human brain using echo-planar MRI. Magnetic Resonance in Medicine, 34:537–541, 1995.\\r\\n[67] G. L. Colclough, M. W. Woolrich, P. K. Tewarie, M. J. Brookes, A. J. Quinn, and S. M. Smith. How reliable\\r\\nare MEG resting-state connectivity metrics? NeuroImage, 138:284–293, 2016.\\r\\n[68] A. Alexander-Bloch, J. N. Giedd, and E. Bullmore. Imaging structural co-variance between human brain\\r\\nregions. Nat. Rev. Neurosci., 14:322–336, 2013.\\r\\n[69] A. C. Evans. Networks of anatomical covariance. NeuroImage, 80:489–504, 2013.\\r\\n[70] J. Seidlitz, F. V´aˇsa, M. Shinn, R. Romero-Garcia, K. J. Whitaker, P. E. V´ertes, K. Wagstyl, P. Kirkpatrick\\r\\nReardon, L. Clasen, S. Liu, A. Messinger, D. A. Leopold, P. Fonagy, R. J. Dolan, P. B. Jones, I. M. Goodyer,\\r\\nthe NSPN Consortium, A. Raznahan, and E. T. Bullmore. Morphometric similarity networks detect microscale\\r\\ncortical organization and predict inter-individual cognitive variation. Neuron, 97:231–247, 2018.\\r\\n[71] J. Y. Hansen, G. Shafiei, R. D. Markello, K. Smart, S. M. L. Cox, M. Nørgaard, V. Beliveau, Y. Wu, J.-D.\\r\\nGallezot, E. Aumont, S. Servaes, S. G. Scala, J. M. DuBois, G. Wainstein, G. Bezgin, T. Funck, T. W. Schmitz, ´\\r\\nR. N. Spreng, M. Galovic, M. J. Koepp, J. S. Duncan, J. P. Coles, T. D. Fryer, F. I. Aigbirhio, C. J. McGinnity,\\r\\nA. Hammers, J.-P. Soucy, S. Baillet, S. Guimond, J. Hietala, M.-A. Bedard, M. Leyton, E. Kobayashi, P. Rosa\\x02Neto, M. Ganz, G. M. Knudsen, N. Palomero-Gallagher, J. M. Shine, R. E. Carson, L. Tuominen, A. Dagher,\\r\\nand B. Misic. Mapping neurotransmitter systems to the structural and functional organization of the human\\r\\nneocortex. Nat. Neurosci., 25:1569–1581, 2022.\\r\\n[72] S. Horvath and J. Dong. Geometric interpretation of gene coexpression network analysis. PLoS Comput. Biol.,\\r\\n4:e1000117, 2008.\\r\\n[73] P. Langfelder and S. Horvath. WGCNA: An R package for weighted correlation network analysis. BMC\\r\\nBioinformatics, 9:559, 2008.\\r\\n[74] B. H. Junker and F. Schreiber, editors. Analysis of Biological Networks. John Wiley & Sons, Inc., Hoboken,\\r\\nNJ, 2008.\\r\\n[75] M. Vidal, M. E. Cusick, and A.-L. Barab´asi. Interactome networks and human disease. Cell, 144:986–998,\\r\\n2011.\\r\\n[76] C. Gaiteri, Y. Ding, B. French, G. C. Tseng, and E. Sibille. Beyond modules and hubs: The potential of gene\\r\\ncoexpression networks for investigating molecular mechanisms of complex brain disorders. Genes Brain Behav.,\\r\\n13:13–24, 2014.\\r\\n[77] G. Fiscon, F. Conte, L. Farina, and P. Paci. Network-based approaches to explore complex biological systems\\r\\ntowards network medicine. Genes, 9:437, 2018.\\r\\n[78] S. van Dam, U. V˜osa, A. van der Graaf, L. Franke, and J. P. de Magalh˜aes. Gene co-expression analysis for\\r\\nfunctional classification and gene–disease predictions. Brief. Bioinfo., 19:575–592, 2018.\\r\\n36\\n[79] M. Mircea, M. Hochane, X. Fan, S. M. Chuva de Sousa Lopes, D. Garlaschelli, and S. Semrau. Phiclust: A\\r\\nclusterability measure for single-cell transcriptomics reveals phenotypic subpopulations. Genome Biol., 23:18,\\r\\n2022.\\r\\n[80] K. S. Parker, J. D. Wilson, J. Marschall, P. J. Mucha, and J. P. Henderson. Network analysis reveals sex\\x02and antibiotic resistance-associated antivirulence targets in clinical uropathogens. ACS Infectious Diseases,\\r\\n1:523–532, 2015.\\r\\n[81] Z. Zou, R. F. Potter, W. H. McCoy 4th, J. A. Wildenthal, G. L. Katumba, P. J. Mucha, G. Dantas, and J. P.\\r\\nHenderson. E. coli catheter-associated urinary tract infections are associated with distinctive virulence and\\r\\nbiofilm gene determinants. JCI Insight, 8:e161461, 2023.\\r\\n[82] W.-C. Chou, A.-L. Cheng, M. Brotto, and C.-Y. Chuang. Visual gene-network analysis reveals the cancer gene\\r\\nco-expression in human endometrial cancer. BMC Genomics, 15:300, 2014.\\r\\n[83] R. Dobrin, J. Zhu, C. Molony, C. Argman, M. L. Parrish, S. Carlson, M. F. Allan, D. Pomp, and E. E. Schadt.\\r\\nMulti-tissue coexpression networks reveal unexpected subnetworks associated with disease. Genome Biol.,\\r\\n10:R55, 2009.\\r\\n[84] Y. Xiang, J. Zhang, and K. Huang. Mining the tissue-tissue gene co-expression network for tumor microenvi\\x02ronment study and biomarker prediction. BMC Genomics, 14:S4, 2013.\\r\\n[85] L. J. A. Kogelman, J. Fu, L. Franke, J. W. Greve, M. Hofker, S. S. Rensen, and H. N. Kadarmideen. Inter\\x02tissue gene co-expression networks between metabolically healthy and unhealthy obese individuals. PLoS ONE,\\r\\n11:e0167519, 2016.\\r\\n[86] N. J. Hudson, A. Reverter, and B. P. Dalrymple. A differential wiring analysis of expression data correctly\\r\\nidentifies the gene containing the causal mutation. PLoS Comput. Biol., 5:e1000382, 2009.\\r\\n[87] R. Steuer. On the analysis and interpretation of correlations in metabolomic data. Brief. Bioinfo., 7:151–158,\\r\\n2006.\\r\\n[88] E. K. Silverman, H. H. H. W. Schmidt, E. Anastasiadou, L. Altucci, M. Angelini, L. Badimon, J.-L. Balligand,\\r\\nG. Benincasa, G. Capasso, F. Conte, A. Di Costanzo, L. Farina, G. Fiscon, L. Gatto, M. Gentili, J. Loscalzo,\\r\\nC. Marchese, C. Napoli, P. Paci, M. Petti, J. Quackenbush, P. Tieri, D. Viggiano, G. Vilahur, K. Glass, and\\r\\nJ. Baumbach. Molecular networks in Network Medicine: Development and applications. Wiley Interdisciplinary\\r\\nReviews: Syst. Biol. Med., 12:e1489, 2020.\\r\\n[89] D. Camacho, A. de la Fuente, and P. Mendes. The origin of correlations in metabolomics data. Metabolomics,\\r\\n1:53–63, 2005.\\r\\n[90] J. I. Robinson, W. H. Weir, J. R. Crowley, T. Hink, K. A. Reske, J. H. Kwon, C.-A. D. Burnham, E. R.\\r\\nDubberke, P. J. Mucha, and J. P. Henderson. Metabolomic networks connect host-microbiome processes to\\r\\nhuman Clostridioides difficile infections. J. Clin. Invest., 129:3792–3806, 2019.\\r\\n[91] H. Hirano and K. Takemoto. Difficulty in inferring microbial community structure based on co-occurrence\\r\\nnetwork approaches. BMC Bioinformatics, 20:329, 2019.\\r\\n[92] M. Goberna and M. Verd´u. Cautionary notes on the use of co-occurrence networks in soil ecology. Soil Biol.\\r\\nBiochem., 166:108534, 2022.\\r\\n[93] J. M. Diamond. Assembly of species communities. In M. L. Cody and J. M. Diamond, editors, Ecology and\\r\\nEvolution of Communities, pages 342–444. Harvard University Press, Cambridge, MA, 1975.\\r\\n[94] J. X. Hu, C. E. Thomas, and S. Brunak. Network biology concepts in complex disease comorbidities. Nat.\\r\\nRev. Genet., 17:615–629, 2016.\\r\\n[95] C. A. Hidalgo, N. Blumm, A.-L. Barab´asi, and N. A. Christakis. A dynamic network approach for the study\\r\\nof human phenotypes. PLoS Comput. Biol., 5:e1000353, 2009.\\r\\n[96] K.-I. Goh, M. E. Cusick, D. Valle, B. Childs, M. Vidal, and A.-L. Barab´asi. The human disease network. Proc.\\r\\nNatl. Acad. Sci. USA, 104:8685–8690, 2007.\\r\\n37\\n[97] A. Rzhetsky, D. Wajngurt, N. Park, and T. Zheng. Probing genetic overlap among complex human phenotypes.\\r\\nProc. Natl. Acad. Sci. USA, 104:11694–11699, 2007.\\r\\n[98] D.-S. Lee, J. Park, K. A. Kay, N. A. Christakis, Z. N. Oltvai, and A.-L. Barab´asi. The implications of human\\r\\nmetabolic network topology for disease comorbidity. Proc. Natl. Acad. Sci. USA, 105:9880–9885, 2008.\\r\\n[99] X. Zhou, J. Menche, A.-L. Barab´asi, and A. Sharma. Human symptoms–disease network. Nat. Commun.,\\r\\n5:4212, 2014.\\r\\n[100] R. N. Mantegna and H. E. Stanley. An Introduction to Econophysics. Cambridge University Press, Cambridge,\\r\\nUK, 1999.\\r\\n[101] L. Laloux, P. Cizeau, J.-P. Bouchaud, and M. Potters. Noise dressing of financial correlation matrices. Phys.\\r\\nRev. Lett., 83:1467–1470, 1999.\\r\\n[102] V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. Nunes Amaral, and H. E. Stanley. Universal and nonuniversal\\r\\nproperties of cross correlations in financial time series. Phys. Rev. Lett., 83:1471–1474, 1999.\\r\\n[103] M. MacMahon and D. Garlaschelli. Community detection for correlation matrices. Phys. Rev. X, 5:021006,\\r\\n2015.\\r\\n[104] Almog, A. and Besamusca, F. and MacMahon, M. and Garlaschelli, D. Mesoscopic community structure of\\r\\nfinancial markets revealed by price and sign fluctuations. PLoS ONE, 10:e0133679, 2015.\\r\\n[105] I. Anagnostou, T. Squartini, D. Kandhai, and D. Garlaschelli. Uncovering the mesoscale structure of the credit\\r\\ndefault swap market to improve portfolio risk modelling. Quantitative Finance, 21:1501–1518, 2021.\\r\\n[106] S. M. Zema, G. Fagiolo, T. Squartini, and D. Garlaschelli. Mesoscopic structure of the stock market and\\r\\nportfolio optimization. Journal of Economic Interaction and Coordination, 2024.\\r\\n[107] R. N. Mantegna. Hierarchical structure in financial markets. Eur. Phys. J. B, 11:193–197, 1999.\\r\\n[108] G. Bonanno, G. Caldarelli, F. Lillo, and R. N. Mantegna. Topology of correlation-based minimal spanning\\r\\ntrees in real and model markets. Phys. Rev. E, 68:046130, 2003.\\r\\n[109] D. J. Fenn, M. A. Porter, P. J. Mucha, M. McDonald, S. Williams, N. F. Johnson, and N. S. Jones. Dynamical\\r\\nclustering of exchange rates. Quantitative Finance, 12:1493–1520, 2012.\\r\\n[110] M. Bazzi, M. A. Porter, S. Williams, M. McDonald, D. J. Fenn, and S. D. Howison. Community detection in\\r\\ntemporal multilayer networks, with an application to correlation networks. Multiscale Model. Simul., 1:1–41,\\r\\n2016.\\r\\n[111] M. Tumminello, F. Lillo, J. Piilo, and R. N. Mantegna. Identification of clusters of investors from their real\\r\\ntrading activity in a financial market. New J. Phys., 14:013041, 2012.\\r\\n[112] S. Ranganathan, M. Kivel¨a, and J. Kanniainen. Dynamics of investor spanning trees around dot-com bubble.\\r\\nPLoS ONE, 13:e0198807, 2018.\\r\\n[113] D. Kercher. Inconsistent correlation and momenta: A new approach to portfolio allocation. MS Thesis. Brigham\\r\\nYoung University, 2023.\\r\\n[114] R. K. Y. Low and E. Tan. The role of analyst forecasts in the momentum effect. International Review of\\r\\nFinancial Analysis, 48:67–84, 2016.\\r\\n[115] E. Yan and Y. Ding. Scholarly network similarities: How bibliographic coupling networks, citation networks,\\r\\ncocitation networks, topical networks, coauthorship networks, and coword networks relate to each other. J.\\r\\nAmer. Soc. Info. Sci. Tech., 63:1313–1326, 2012.\\r\\n[116] J.-P. Qiu, K. Dong, and H.-Q. Yu. Comparative study on structure and correlation among author co-occurrence\\r\\nnetworks in bibliometrics. Scientometrics, 101:1345–1360, 2014.\\r\\n[117] M. E. J. Newman. Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality.\\r\\nPhys. Rev. E, 64:016132, 2001.\\r\\n38\\n[118] C. Cattuto, A. Barrat, A. Baldassarri, G. Schehr, and V. Loreto. Collective dynamics of social annotation.\\r\\nProc. Natl. Acad. Sci. USA, 106:10511–10515, 2009.\\r\\n[119] F. Luo, J. Z. Wang, and E. Promislow. Exploring local community structures in large networks. In Proceedings\\r\\nof 2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI’06), pages 233–239, 2006.\\r\\n[120] A. A. Tsonis and P. J. Roebber. The architecture of the climate network. Physica A, 333:497–504, 2004.\\r\\n[121] A. A. Tsonis, K. L. Swanson, and P. J. Roebber. What do networks have to do with climate? Bull. Amer.\\r\\nMeteorol. Soc., 87:585–595, 2006.\\r\\n[122] J. F. Donges, Y. Zou, N. Marwan, and J. Kurths. The backbone of the climate network. EPL, 87:48007, 2009.\\r\\n[123] J. Fan, J. Meng, J. Ludescher, X. Chen, Y. Ashkenazy, J. Kurths, S. Havlin, and H. J. Schellnhuber. Statistical\\r\\nphysics approaches to the complex Earth system. Phys. Rep., 896:1–84, 2021.\\r\\n[124] J. Heitzig, J. F. Donges, Y. Zou, N. Marwan, and J. Kurths. Node-weighted measures for complex networks\\r\\nwith spatially embedded, sampled, or differently sized nodes. Eur. Phys. J. B, 85:38, 2012.\\r\\n[125] S. Scarsoglio, F. Laio, and L. Ridolfi. Climate dynamics: A network-based approach for the analysis of global\\r\\nprecipitation. PLoS ONE, 8:e71129, 2013.\\r\\n[126] M. van der Mheen, H. A. Dijkstra, A. Gozolchiani, M. den Toom, Q. Feng, J. Kurths, and E. Hernandez\\x02Garcia. Interaction network based early warning indicators for the Atlantic MOC collapse. Geophys. Res.\\r\\nLett., 40:2714–2719, 2013.\\r\\n[127] P. J. Bickel and E. Levina. Covariance regularization by thresholding. Ann. Stat., 36:2577–2604, 2008.\\r\\n[128] M. Pourahmadi. Covariance estimation: The GLM and regularization perspectives. Statist. Sci., 26:369–387,\\r\\n2011.\\r\\n[129] J. Fan, Y. Liao, and H. Liu. An overview of the estimation of large covariance and precision matrices. Econo\\x02metrics J., 19:C1–C32, 2016.\\r\\n[130] R. B. Holmes. On random correlation matrices. SIAM J. Matrix Anal. Appl., 12:239–272, 1991.\\r\\n[131] J. A. Tropp. Simplicial faces of the set of correlation matrices. Discrete Comput. Geom., 60:512–529, 2018.\\r\\n[132] Y. Thanwerdas and X. Pennec. Geodesics and curvature of the quotient-affine metrics on full-rank correlation\\r\\nmatrices. Lect. Notes Comput. Sci., 12829:93–102, 2021.\\r\\n[133] J. Whittaker. Graphical Models in Applied Multivariate Statistics. John Wiley & Sons, Chichester, UK, 1990.\\r\\n[134] S. L. Lauritzen. Graphical Models. Clarendon Press, Oxford, UK, 1996.\\r\\n[135] A. P. Dempster. Covariance selection. Biometrics, 28:157–175, 1972.\\r\\n[136] O. Ledoit and M. Wolf. The power of (non-)linear shrinking: A review and guide to covariance matrix\\r\\nestimation. J. Financial Econometrics, 20:187–218, 2022.\\r\\n[137] O. Ledoit and M. Wolf. Improved estimation of the covariance matrix of stock returns with an application to\\r\\nportfolio selection. J. Empirical Finance, 10:603–621, 2003.\\r\\n[138] O. Ledoit and M. Wolf. A well-conditioned estimator for large-dimensional covariance matrices. J. Multivar.\\r\\nAnal., 88:365–411, 2004.\\r\\n[139] Y. Chen, A. Wiesel, Y. C. Eldar, and A. O. Hero. Shrinkage algorithms for MMSE covariance estimation.\\r\\nIEEE Trans. Signal Proc., 58:5016–5029, 2010.\\r\\n[140] M. Rubinov and O. Sporns. Weight-conserving characterization of complex functional brain networks. Neu\\x02roImage, 56:2068–2079, 2011.\\r\\n[141] H. Lee, H. Kang, M. K. Chung, B.-N. Kim, and D. S. Lee. Persistent brain network homology from the\\r\\nperspective of dendrogram. IEEE Trans. Med. Imaging, 31:2267–2277, 2012.\\r\\n39\\n[142] K. A. Garrison, D. Scheinost, E. S. Finn, X. Shen, and R. T. Constable. The (in)stability of functional brain\\r\\nnetwork measures across thresholds. NeuroImage, 118:651–661, 2015.\\r\\n[143] F. De Vico Fallani, V. Latora, and M. Chavez. A topological criterion for filtering information in complex\\r\\nbrain networks. PLoS Comput. Biol., 13:e1005305, 2017.\\r\\n[144] M. K. Chung, H. Lee, A. DiChristofano, H. Ombao, and V. Solo. Exact topological inference of the resting-state\\r\\nbrain networks in twins. Netw. Neurosci., 3:674–694, 2019.\\r\\n[145] M. W. Cole, S. Pathak, and W. Schneider. Identifying the brain’s most globally connected regions. NeuroImage,\\r\\n49:3132–3148, 2010.\\r\\n[146] D. Scheinost, J. Benjamin, C. M. Lacadie, B. Vohr, K. C. Schneider, L. R. Ment, X. Papademetris, and R. T.\\r\\nConstable. The intrinsic connectivity distribution: A novel contrast measure reflecting voxel level functional\\r\\nconnectivity. NeuroImage, 62:1510–1519, 2012.\\r\\n[147] L. Peel, T. P. Peixoto, and M. De Domenico. Statistical inference links data and theory in network science.\\r\\nNat. Commun., 13:6794, 2022.\\r\\n[148] M. P. van den Heuvel, S. C. de Lange, A. Zalesky, C. Seguin, B. T. T. Yeo, and R. Schmidt. Proportional thresh\\x02olding in resting-state fMRI functional connectivity networks and consequences for patient-control connectome\\r\\nstudies: Issues and recommendations. NeuroImage, 152:437–449, 2017.\\r\\n[149] S. Achard and E. Bullmore. Efficiency and cost of economical brain functional networks. PLoS Comput. Biol.,\\r\\n3:e17, 2007.\\r\\n[150] M. P. van den Heuvel, C. J. Stam, M. Boersma, and H. E. Hulshoff Pol. Small-world and scale-free organization\\r\\nof voxel-based resting-state functional connectivity in the human brain. NeuroImage, 43:528–539, 2008.\\r\\n[151] J. Wang, L. Wang, Y. Zang, H. Yang, H. Tang, Q. Gong, Z. Chen, C. Zhu, and Y. He. Parcellation-dependent\\r\\nsmall-world brain functional networks: A resting-state fMRI study. Human Brain Mapping, 30:1511–1523,\\r\\n2009.\\r\\n[152] B. C. M. van Wijk, C. J. Stam, and A. Daffertshofer. Comparing brain networks of different size and connectivity\\r\\ndensity using graph theory. PLoS ONE, 5:e13701, 2010.\\r\\n[153] A. F. Alexander-Bloch, N. Gogtay, D. Meunier, R. Birn, L. Clasen, F. Lalonde, R. Lenroot, J. Giedd, and\\r\\nE. T. Bullmore. Disrupted modularity and local connectivity of brain functional networks in childhood-onset\\r\\nschizophrenia. Front. Syst. Neurosci., 4:147, 2010.\\r\\n[154] E. Langford, N. Schwertman, and M. Owens. Is the property of being positively correlated transitive? Am.\\r\\nStat., 55:322–325, 2001.\\r\\n[155] J. Gillis and P. Pavlidis. The role of indirect connections in gene networks in predicting function. Bioinformatics,\\r\\n27:1860–1866, 2011.\\r\\n[156] A. Zalesky, A. Fornito, and E. Bullmore. On the use of correlation as a measure of network connectivity.\\r\\nNeuroImage, 60:2096–2106, 2012.\\r\\n[157] M. Barth´elemy. Spatial networks. Phys. Rep., 499:1–101, 2011.\\r\\n[158] M. Barthelemy. Morphogenesis of Spatial Networks. Springer, Cham, Switzerland, 2018.\\r\\n[159] N. Langer, A. Pedroni, and L. J¨ancke. The problem of thresholding in small-world network analysis. PLoS\\r\\nONE, 8:e53199, 2013.\\r\\n[160] C. E. Ginestet, T. E. Nichols, E. T. Bullmore, and A. Simmons. Brain network analysis: Separating cost from\\r\\ntopology using cost-integration. PLoS ONE, 6:e21570, 2011.\\r\\n[161] D. S. Bassett, B. G. Nelson, B. A. Mueller, J. Camchong, and K. O. Lim. Altered resting state complexity in\\r\\nschizophrenia. NeuroImage, 59:2196–2207, 2012.\\r\\n[162] S. M. H. Hosseini, F. Hoeft, and S. R. Kesler. GAT: A graph-theoretical analysis toolbox for analyzing\\r\\nbetween-group differences in large-scale structural and functional brain networks. PLoS ONE, 7:e40709, 2012.\\r\\n40\\n[163] V. Latora and M. Marchiori. Efficient behavior of small-world networks. Phys. Rev. Lett., 87:198701, 2001.\\r\\n[164] D. S. Bassett, A. Meyer-Lindenberg, S. Achard, T. Duke, and E. Bullmore. Adaptive reconfiguration of fractal\\r\\nsmall-world human brain functional networks. Proc. Natl. Acad. Sci. USA, 103:19518–19523, 2006.\\r\\n[165] N. Vandewalle, F. Brisbois, and X. Tordoir. Non-random topology of stock markets. Quant. Finance, 1:372–374,\\r\\n2001.\\r\\n[166] J.-P. Onnela, A. Chakraborti, K. Kaski, and J. Kert´esz. Dynamic asset trees and portfolio analysis. Eur. Phys.\\r\\nJ. B, 30:285–288, 2002.\\r\\n[167] M. Tumminello, T. Aste, T. Di Matteo, and R. N. Mantegna. A tool for filtering information in complex\\r\\nsystems. Proc. Natl. Acad. Sci. USA, 102:10421–10426, 2005.\\r\\n[168] M. A Serrano, M. Bogu˜n´a, and A. Vespignani. Extracting the multiscale backbone of complex weighted ´\\r\\nnetworks. Proc. Natl. Acad. Sci. USA, 106:6483–6488, 2009.\\r\\n[169] V. Gemmetto, A. Cardillo, and D. Garlaschelli. Irreducible network backbones: unbiased graph filtering via\\r\\nmaximum entropy. arXiv preprint arXiv:1706.00230, 2017.\\r\\n[170] L. Wang, C. Zhu, Y. He, Y. Zang, Q. Cao, H. Zhang, Q. Zhong, and Y. Wang. Altered small-world brain\\r\\nfunctional networks in children with attention-deficit/hyperactivity disorder. Human Brain Mapping, 30:638–\\r\\n649, 2009.\\r\\n[171] R. Ghrist. Barcodes: The persistent topology of data. Bull. Amer. Math. Soc., 45:61–75, 2008.\\r\\n[172] D. Horak, S. Maleti´c, and M. Rajkovi´c. Persistent homology of complex networks. J. Stat. Mech., 2009:P03034,\\r\\n2009.\\r\\n[173] N. Otter, M. A. Porter, U. Tillmann, P. Grindrod, and H. A. Harrington. A roadmap for the computation of\\r\\npersistent homology. EPJ Data Sci., 6:17, 2017.\\r\\n[174] M. E. Aktas, E. Akbas, and A. E. Fatmaoui. Persistence homology of networks: Methods and applications.\\r\\nAppl. Netw. Sci., 4:61, 2019.\\r\\n[175] A. E. Sizemore, J. E. Phillips-Cremins, R. Ghrist, and D. S. Bassett. The importance of the whole: Topological\\r\\ndata analysis for the network neuroscientist. Netw. Neurosci., 3:656–673, 2019.\\r\\n[176] G. Petri, P. Expert, F. Turkheimer, R. Carhart-Harris, D. Nutt, P. J. Hellyer, and F. Vaccarino. Homological\\r\\nscaffolds of brain functional networks. J. R. Soc. Interface, 11:20140873, 2014.\\r\\n[177] C. Giusti, E. Pastalkova, C. Curto, and V. Itskov. Clique topology reveals intrinsic geometric structure in\\r\\nneural correlations. Proc. Natl. Acad. Sci. USA, 112:13455–13460, 2015.\\r\\n[178] A. N. Duman and H. Pirim. Gene coexpression network comparison via persistent homology. Int. J. Genomics,\\r\\n2018:7329576, 2018.\\r\\n[179] D. Shnier, M. A. Voineagu, and I. Voineagu. Persistent homology analysis of brain transcriptome data in\\r\\nautism. J. R. Soc. Interface, 16:20190531, 2019.\\r\\n[180] G. Leibon, S. Pauls, D. Rockmore, and R. Savell. Topological structures in the equities market network. Proc.\\r\\nNatl. Acad. Sci. USA, 105:20589–20594, 2008.\\r\\n[181] M. Gidea. Topological data analysis of critical transitions in financial networks. In 3rd International Winter\\r\\nSchool and Conference on Network Science: NetSci-X 2017 3, pages 47–59. Springer, 2017.\\r\\n[182] B. Rieck, U. Fugacci, J. Lukasczyk, and H. Leitte. Clique community persistence: A topological visual analysis\\r\\napproach for complex networks. IEEE Trans. Vis. Comput. Graphics, 24:822–831, 2017.\\r\\n[183] S. Horvath. Weighted Network Analysis. Springer, New York, NY, 2011.\\r\\n[184] D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81:425–455,\\r\\n1994.\\r\\n41\\n[185] N. El Karoui. Operator norm consistent estimation of large-dimensional sparse covariance matrices. Ann. Stat.,\\r\\n36:2717–2756, 2008.\\r\\n[186] A. J. Rothman, E. Levina, and J. Zhu. Generalized thresholding of large covariance matrices. J. Am. Stat.\\r\\nAssoc., 104:177–186, 2009.\\r\\n[187] R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58:267–288, 1996.\\r\\n[188] T. Cai and W. Liu. Adaptive thresholding for sparse covariance matrix estimation. J. Am. Stat. Assoc.,\\r\\n106:672–684, 2011.\\r\\n[189] T. T. Cai, C.-H. Zhang, and H. H. Zhou. Optimal rates of convergence for covariance matrix estimation. Ann.\\r\\nStat., 38:2118–2144, 2010.\\r\\n[190] C. Chen, L. Cheng, K. Grennan, F. Pibiri, C. Zhang, J. A. Badner, Members of the Bipolar Disorder Genome\\r\\nStudy (BiGS) Consortium, E. S. Gershon, and C. Liu. Two gene co-expression modules differentiate psychotics\\r\\nand controls. Mol. Psychiatry, 18:1308–1314, 2013.\\r\\n[191] K. R. A. Van Dijk, T. Hedden, A. Venkataraman, K. C. Evans, S. W. Lazar, and R. L. Buckner. Intrinsic func\\x02tional connectivity as a tool for human connectomics: Theory, properties, and optimization. J. Neurophysiol.,\\r\\n103:297–321, 2010.\\r\\n[192] J. Tang, Y. Chang, C. Aggarwal, and H. Liu. A survey of signed network mining in social media. ACM Comput.\\r\\nSurv., 49:42, 2016.\\r\\n[193] L. Zhan, L. M. Jenkins, O. E. Wolfson, J. J. GadElkarim, K. Nocito, P. M. Thompson, O. A. Ajilore, M. K.\\r\\nChung, and A. D. Leow. The significance of negative correlations in brain connectivity. J. Comparative Neurol.,\\r\\n525:3251–3265, 2017.\\r\\n[194] S. G´omez, P. Jensen, and A. Arenas. Analysis of community structure in networks of correlated data. Phys.\\r\\nRev. E, 80:016114, 2009.\\r\\n[195] C. Aicher, A. Z. Jacobs, and A. Clauset. Learning latent block structure in weighted networks. J. Comp.\\r\\nNetw., 3:221–248, 2015.\\r\\n[196] T. P. Peixoto. Nonparametric weighted stochastic block models. Phys. Rev. E, 97:012306, 2018.\\r\\n[197] A. de la Fuente, N. Bing, I. Hoeschele, and P. Mendes. Discovery of meaningful associations in genomic data\\r\\nusing partial correlation coefficients. Bioinformatics, 20:3565–3574, 2004.\\r\\n[198] R. Salvador, J. Suckling, M. R. Coleman, J. D. Pickard, D. Menon, and E. Bullmore. Neurophysiological\\r\\narchitecture of functional magnetic resonance images of human brain. Cereb. Cortex, 15:1332–1342, 2005.\\r\\n[199] G. Marrelec, A. Krainik, H. Duffau, M. P´el´egrini-Issac, S. Leh´ericy, J. Doyon, and H. Benali. Partial correlation\\r\\nfor functional brain interactivity investigation in functional MRI. NeuroImage, 32:228–237, 2006.\\r\\n[200] M. Goldstein and A. F. M. Smith. Ridge-type estimators for regression analysis. J. R. Statist. Soc. Ser. B,\\r\\n36:284–291, 1974.\\r\\n[201] A. Hero and B. Rajaratnam. Hub discovery in partial correlation graphs. IEEE Trans. Info. Th., 58:6064–6078,\\r\\n2012.\\r\\n[202] R. Artner, P. P. Wellingerhof, G. Lafit, T. Loossens, W. Vanpaemel, and F. Tuerlinckx. The shape of partial\\r\\ncorrelation matrices. Commun. Stat., 51:4133–4150, 2022.\\r\\n[203] T. Aste and T. Di Matteo. Dynamical networks from correlations. Physica A, 370:156–161, 2006.\\r\\n[204] D. R. Williams. Learning to live with sampling variability: Expected replicability in partial correlation net\\x02works. Psychol. Methods, 27:606–621, 2022.\\r\\n[205] T. Millington and M. Niranjan. Partial correlation financial networks. Appl. Netw. Sci., 5:11, 2020.\\r\\n[206] G. Varoquaux, A. Gramfort, J.-B. Poline, and B. Thirion. Brain covariance selection: better individual\\r\\nfunctional connectivity models using population prior. Advances in Neural Information Processing Systems,\\r\\n23:2334–2342, 2010.\\r\\n42\\n[207] S. Ryali, T. Chen, K. Supekar, and V. Menon. Estimation of functional connectivity in fmri data using stability\\r\\nselection-based sparse partial correlation with elastic net penalty. NeuroImage, 59:3852–3861, 2012.\\r\\n[208] M. R. Brier, A. Mitra, J. E. McCarthy, B. M. Ances, and A. Z. Snyder. Partial covariance based functional\\r\\nconnectivity computation using Ledoit-Wolf covariance regularization. NeuroImage, 121:29–38, 2015.\\r\\n[209] U. Pervaiz, D. Vidaurre, M. W. Woolrich, and S. M. Smith. Optimising network modelling methods for fmri.\\r\\nNeuroImage, 211:116604, 2020.\\r\\n[210] G.-J. Wang, C. Xie, and H. E. Stanley. Correlation structure and evolution of world stock markets: Evidence\\r\\nfrom Pearson and partial correlation-based networks. Comput. Econ., 51:607–635, 2018.\\r\\n[211] M. Drton and M. H. Maathuis. Structure learning in graphical modeling. Annu. Rev. Stat. Appl., 4:365–393,\\r\\n2017.\\r\\n[212] S. Epskamp and E. I. Fried. A tutorial on regularized partial correlation networks. Psychol. Methods, 23:617–\\r\\n634, 2018.\\r\\n[213] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. Biometrika, 94:19–35,\\r\\n2007.\\r\\n[214] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood estimation\\r\\nfor multivariate Gaussian or binary data. J. Machine Learning Res., 9:485–516, 2008.\\r\\n[215] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.\\r\\nBiostatistics, 9:432–441, 2008.\\r\\n[216] J. Fan, Y. Feng, and Y. Wu. Network exploration via the adaptive LASSO and SCAD penalties. Ann. Appl.\\r\\nStat., 3:521–541, 2009.\\r\\n[217] R. Foygel and M. Drton. Extended Bayesian information criteria for Gaussian graphical models. Advances in\\r\\nNeural Information Processing Systems 23, 23:604–612, 2010.\\r\\n[218] T. Hastie, R. Tibshirani, and M. Wainwright. Statistical Learning with Sparsity: The Lasso and Generalizations.\\r\\nCRC Press, Boca Raton, FL, 2015.\\r\\n[219] R. Kindermann and J. L. Snell. Markov Random Fields and Their Applications. American Mathematial Society,\\r\\nProvidence, RI, 1980.\\r\\n[220] H. Rue and L. Held. Gaussian Markov Random Fields. Chapman & Hall/CRC, New York, NY, 2005.\\r\\n[221] S. Z. Li. Markov Random Field Modeling in Image Analysis. Springer-Verlag, London, UK, third edition, 2009.\\r\\n[222] C. Wang, N. Komodakis, and N. Paragios. Markov random field modeling, inference & learning in computer\\r\\nvision & image understanding: A survey. Comput. Vis. Image Understanding, 117:1610–1627, 2013.\\r\\n[223] J. Sch¨afer and K. Strimmer. An empirical Bayes approach to inferring large-scale gene association networks.\\r\\nBioinformatics, 21:754–764, 2005.\\r\\n[224] H. Li and J. Gui. Gradient directed regularization for sparse Gaussian concentration graphs, with applications\\r\\nto inference of genetic networks. Biostatistics, 7:302–317, 2006.\\r\\n[225] A. d’Aspremont, O. Banerjee, and L. El Ghaoui. First-order methods for sparse covariance selection. SIAM J.\\r\\nMatrix Anal. Appl., 30:56–66, 2008.\\r\\n[226] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the Lasso. Ann. Stat.,\\r\\n34:1436–1462, 2006.\\r\\n[227] J. Peng, P. Wang, N. Zhou, and J. Zhu. Partial correlation estimation by joint sparse regression models. J.\\r\\nAm. Stat. Assoc., 104:735–746, 2009.\\r\\n[228] M. Hinne, L. Ambrogioni, R. J. Janssen, T. Heskes, and M. A. J. van Gerven. Structurally-informed bayesian\\r\\nfunctional connectivity analysis. NeuroImage, 86:294–305, 2014.\\r\\n43\\n[229] A. Atay-Kayis and H. Massam. A Monte Carlo method for computing the marginal likelihood in nondecom\\x02posable Gaussian graphical models. Biometrika, 92:317–335, 2005.\\r\\n[230] A. Dobra, A. Lenkoski, and A. Rodriguez. Bayesian inference for general gaussian graphical models with\\r\\napplication to multivariate lattice data. J. Am. Stat. Assoc., 106:1418–1433, 2011.\\r\\n[231] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimensional\\r\\nundirected graphs. J. Machine Learning Res., 10:2295–2328, 2009.\\r\\n[232] H. Liu, F. Han, M. Yuan, J. Lafferty, and L. Wasserman. High-dimensional semiparametric gaussian copula\\r\\ngraphical models. Ann. Stat., 40:2293–2326, 2012.\\r\\n[233] L. Xue and H. Zou. Regularized rank-based estimation of high-dimensional nonparanormal graphical models.\\r\\nAnn. Stat., 40:2541–2571, 2012.\\r\\n[234] R. E. Morrison, R. Baptista, and Y. Marzouk. Beyond normality: Learning sparse probabilistic graphical\\r\\nmodels in the non-Gaussian setting. Advances in Neural Information Processing Systems, 31:2356–2366, 2017.\\r\\n[235] R. Baptista, R. Morrison, O. Zahm, and Y. Marzouk. Learning non-gaussian graphical models via hessian\\r\\nscores and triangular transport. J. Machine Learning Res., 25:85, 2024.\\r\\n[236] T. Mora and W. Bialek. Are biological systems poised at criticality? J. Stat. Phys., 144:268–302, 2011.\\r\\n[237] R. R. Stein, D. S. Marks, and C. Sander. Inferring pairwise interactions from biological data using maximum\\x02entropy probability models. PLoS Comput. Biol., 11:e1004182, 2015.\\r\\n[238] H. C. Nguyen, R. Zecchina, and J. Berg. Inverse statistical problems: from the inverse ising problem to data\\r\\nscience. Adv. Phys., 66:197–261, 2017.\\r\\n[239] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborov´a.\\r\\nMachine learning and the physical sciences. Rev. Mod. Phys., 91:045002, 2019.\\r\\n[240] P. Mehta, M. Bukov, C.-H. Wang, A. G. R. Day, C. Richardson, C. K. Fisher, and D. J. Schwab. A high-bias,\\r\\nlow-variance introduction to machine learning for physicists. Phys. Rep., 810:1–124, 2019.\\r\\n[241] J. Bento and A. Montanari. Which graphical models are difficult to learn? In Proceedings of the 22nd\\r\\nInternational Conference on Neural Information Processing Systems, pages 1303–1311, 2009.\\r\\n[242] P. Ravikumar, M. J. Wainwright, and J. D. Lafferty. High-dimensional ising model selection using ℓ1-regularized\\r\\nlogistic regression. Ann. Stat., 38:1287–1319, 2010.\\r\\n[243] E. Aurell and M. Ekeberg. Inverse Ising inference using all the data. Phys. Rev. Lett., 108:090201, 2012.\\r\\n[244] A. Decelle and F. Ricci-Tersenghi. Pseudolikelihood decimation algorithm improving the inference of the\\r\\ninteraction network in a general class of Ising models. Phys. Rev. Lett., 112:070603, 2014.\\r\\n[245] J. Z. Huang, N. Liu, M. Pourahmadi, and L. Liu. Covariance matrix selection and estimation via penalised\\r\\nnormal likelihood. Biometrika, 93:85–98, 2006.\\r\\n[246] J. Bien and R. J. Tibshirani. Sparse estimation of a covariance matrix. Biometrika, 98:807–820, 2011.\\r\\n[247] H. Liu, L. Wang, and T. Zhao. Sparse covariance matrix estimation with eigenvalue constraints. J. Comput.\\r\\nGraphical Stat., 23:439–459, 2014.\\r\\n[248] H. Wang. Coordinate descent algorithm for covariance graphical lasso. Stat. Comput., 24:521–529, 2014.\\r\\n[249] H. Wang. Scaling it up: Stochastic search structure learning in graphical models. Bayesian Analysis, 10:351–\\r\\n377, 2015.\\r\\n[250] S. Kojaku and N. Masuda. Constructing networks by filtering correlation matrices: A null model approach.\\r\\nProc. R Soc. A, 475:20190578, 2019.\\r\\n[251] Y. Benjamini and D. Yekutieli. The control of the false discovery rate in multiple testing under dependency.\\r\\nAnn. Stat., pages 1165–1188, 2001.\\r\\n44\\n[252] D. S. Bassett, N. F. Wymbs, M. A. Porter, P. J. Mucha, J. M. Carlson, and S. T. Grafton. Dynamic reconfig\\x02uration of human brain networks during learning. Proc. Natl. Acad. Sci. USA, 108:7641–7646, 2011.\\r\\n[253] S. Achard, R. Salvador, B. Whitcher, J. Suckling, and E. Bullmore. A resilient, low-frequency, small-world\\r\\nhuman brain functional network with highly connected association cortical hubs. J. Neurosci., 26:63–72, 2006.\\r\\n[254] J. Kang, F. D. Bowman, H. Mayberg, and H. Liu. A depression network of functionally connected regions\\r\\ndiscovered via multi-attribute canonical correlation graphs. NeuroImage, 141:431–441, 2016.\\r\\n[255] B. Ma, Y. Wang, S. Ye, S. Liu, E. Stirling, J. A. Gilbert, K. Faust, R. Knight, J. K. Jansson, C. Cardona,\\r\\nL. R¨ottjers, and J. Xu. Earth microbial co-occurrence network reveals interconnection pattern across micro\\x02biomes. Microbiome, 8:82, 2020.\\r\\n[256] A. Zalesky, A. Fornito, and E. T. Bullmore. Network-based statistic: Identifying differences in brain networks.\\r\\nNeuroImage, 53:1197–1207, 2010.\\r\\n[257] H. C. Baggio, A. Abos, B. Segura, A. Campabadal, A. Garcia-Diaz, C. Uribe, Y. Compta, M. J. Marti,\\r\\nF. Valldeoriola, and C. Junque. Statistical inference in brain graphs using threshold-free network-based statis\\x02tics. Human Brain Mapping, 39:2289–2302, 2018.\\r\\n[258] N. Masuda and R. Lambiotte. A Guide to Temporal Networks. World Scientific, Singapore, second edition,\\r\\n2020.\\r\\n[259] P. Holme and J. Saram¨aki. Temporal networks. Phys. Rep., 519:97–125, 2012.\\r\\n[260] P. Holme. Modern temporal network theory: A colloquium. Eur. Phys. J. B, 88:234, 2015.\\r\\n[261] R. Hindriks, M. H. Adhikari, Y. Murayama, M. Ganzetti, D. Mantini, N. K. Logothetis, and G. Deco.\\r\\nCan sliding-window correlations reveal dynamic functional connectivity in resting-state fMRI? NeuroImage,\\r\\n127:242–256, 2016.\\r\\n[262] Z. Adams, R. F¨uss, and T. Gl¨uck. Are correlations constant? Empirical and theoretical results on popular\\r\\ncorrelation models in finance. J. Banking Finance, 84:9–24, 2017.\\r\\n[263] M. A. Lindquist, Y. Xu, M. B. Nebel, and B. S. Caffo. Evaluating dynamic bivariate correlations in resting-state\\r\\nfMRI: A comparison study and a new approach. NeuroImage, 101:531–546, 2014.\\r\\n[264] J.-P. Onnela, A. Chakraborti, K. Kaski, J. Kert´esz, and A. Kanto. Dynamics of market correlations: Taxonomy\\r\\nand portfolio analysis. Phys. Rev. E, 68:056110, 2003.\\r\\n[265] J.-P. Onnela, K. Kaski, and J. Kert´esz. Clustering and information in correlation based financial networks.\\r\\nEur. Phys. J. B, 38:353–362, 2004.\\r\\n[266] R. M. Hutchison, T. Womelsdorf, E. A. Allen, P. A. Bandettini, V. D. Calhoun, M. Corbetta, S. Della Penna,\\r\\nJ. H. Duyn, G. H. Glover, J. Gonzalez-Castillo, D. A. Handwerker, S. Keilholz, V. Kiviniemi, D. A. Leopold,\\r\\nF. de Pasquale, O. Sporns, M. Walter, and C. Chang. Dynamic functional connectivity: Promise, issues, and\\r\\ninterpretations. NeuroImage, 80:360–378, 2013.\\r\\n[267] V. D. Calhoun, R. Miller, G. Pearlson, and T. Adalı. The chronnectome: Time-varying connectivity networks\\r\\nas the next frontier in fMRI data discovery. Neuron, 84:262–274, 2014.\\r\\n[268] M. Filippi, E. G. Spinelli, C. Cividini, and F. Agosta. Resting state dynamic functional connectivity in\\r\\nneurodegenerative conditions: A review of magnetic resonance imaging findings. Front. Neurosci., 13:657,\\r\\n2019.\\r\\n[269] D. J. Lurie, D. Kessler, D. S. Bassett, R. F. Betzel, M. Breakspear, S. Kheilholz, A. Kucyi, R. Li´egeois, M. A.\\r\\nLindquist, A. R. McIntosh, R. A. Poldrack, J. M. Shine, W. H. Thompson, N. Z. Bielczyk, L. Douw, D. Kraft,\\r\\nR. L. Miller, M. Muthuraman, L. Pasquini, A. Razi, D. Vidaurre, H. Xie, and V. D. Calhoun. Questions and\\r\\ncontroversies in the study of time-varying functional connectivity in resting fMRI. Netw. Neurosci., 4:30–69,\\r\\n2020.\\r\\n[270] D. S. Bassett, N. F. Wymbs, M. P. Rombach, M. A. Porter, P. J. Mucha, and S. T. Grafton. Task-based\\r\\ncore-periphery organization of human brain dynamics. PLoS Comput. Biol., 9:e1003171, 2013.\\r\\n45\\n[271] U. Braun, A. Sch¨afer, H. Walter, S. Erk, N. Romanczuk-Seiferth, L. Haddad, J. I. Schweiger, O. Grimm,\\r\\nA. Heinz, H. Tost, A. Meyer-Lindenberg, and D. S. Bassett. Dynamic reconfiguration of frontal brain networks\\r\\nduring executive cognition in humans. Proc. Natl. Acad. Sci. USA, 112:11678–11683, 2015.\\r\\n[272] J. Nakajima and M. West. Bayesian analysis of latent threshold dynamic models. J. Busi. Econ. Stat., 31:151–\\r\\n164, 2013.\\r\\n[273] J. Nakajima and M. West. Dynamic network signal processing using latent threshold models. Digital Signal\\r\\nProc., 47:5–16, 2015.\\r\\n[274] D. Hallac, Y. Park, S. Boyd, and J. Leskovec. Network inference via the time-varying graphical lasso. In\\r\\nProceedings of the 23th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\\r\\npages 205–213, 2017.\\r\\n[275] D. J. Watts and S. H. Strogatz. Collective dynamics of ‘small-world’ networks. Nature, 393:440–442, 1998.\\r\\n[276] N. Masuda, S. Kojaku, and Y. Sano. Configuration model for correlation matrices preserving the node strength.\\r\\nPhys. Rev. E, 98:012312, 2018.\\r\\n[277] F. V´aˇsa and B. Miˇsi´c. Null models in network neuroscience. Nat. Rev. Neurosci., 23:493–504, 2022.\\r\\n[278] W. B¨ohm and K. Hornik. Generating random correlation matrices by the simple rejection method: Why it\\r\\ndoes not work. Stat. Prob. Lett., 87:27–30, 2014.\\r\\n[279] J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, and J. D. Farmer. Testing for nonlinearity in time series:\\r\\nThe method of surrogate data. Physica D, 58:77–94, 1992.\\r\\n[280] T. Schreiber and A. Schmitz. Surrogate time series. Physica D, 142:346–382, 2000.\\r\\n[281] H. Iyetomi, Y. Nakayama, H. Yoshikawa, H. Aoyama, Y. Fujiwara, Y. Ikeda, and W. Souma. What causes\\r\\nbusiness cycles? analysis of the japanese industrial production data. Journal of the Japanese and International\\r\\nEconomies, 25:246–272, 2011.\\r\\n[282] H. Iyetomi, Y. Nakayama, H. Aoyama, Y. Fujiwara, Y. Ikeda, and W. Souma. Fluctuation-dissipation theory\\r\\nof input-output interindustrial relations. Phys. Rev. E, 83:016103, 2011.\\r\\n[283] M. Shinn, A. Hu, L. Turner, S. Noble, K. H. Preller, J. L. Ji, F. Moujaes, S. Achard, D. Scheinost, R. T.\\r\\nConstable, J. H. Krystal, F. X. Vollenweider, D. Lee, A. Anticevic, E. T. Bullmore, and J. D. Murray. Functional\\r\\nbrain networks reflect spatial and temporal autocorrelation. Nat. Neurosci., 26:867–878, 2023.\\r\\n[284] M. Hirschberger, Y. Qi, and R. E. Steuer. Randomly generating portfolio-selection covariance matrices with\\r\\nspecified distributional characteristics. Eur. J. Oper. Res., 177:1610–1625, 2007.\\r\\n[285] A. Fornito, A. Zalesky, and M. Breakspear. Graph analysis of the human connectome: Promise, progress, and\\r\\npitfalls. NeuroImage, 80:426–444, 2013.\\r\\n[286] B. K. Fosdick, D. B. Larremore, J. Nishimura, and J. Ugander. Configuring random graph models with fixed\\r\\ndegree sequences. SIAM Review, 60:315–355, 2018.\\r\\n[287] R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. Network motifs: simple building\\r\\nblocks of complex networks. Science, 298:824–827, 2002.\\r\\n[288] S. Fortunato. Community detection in graphs. Phys. Rep., 486:75–174, 2010.\\r\\n[289] V. Colizza, M. A. Serrano A. Flammini, and A. Vespignani. Detecting rich-club ordering in complex networks.\\r\\nNat. Phys., 2:110–115, 2006.\\r\\n[290] S. Kojaku and N. Masuda. Core-periphery structure requires something else in the network. New J. Phys.,\\r\\n20:043012, 2018.\\r\\n[291] P. Expert, T. S. Evans, V. D. Blondel, and R. Lambiotte. Uncovering space-independent communities in spatial\\r\\nnetworks. Proc. Natl. Acad. Sci. USA, 108:7663–7668, 2011.\\r\\n[292] T. Squartini, R. Mastrandrea, and D. Garlaschelli. Unbiased sampling of network ensembles. New J. Phys.,\\r\\n17:023052, 2015.\\r\\n46\\n[293] T. Squartini and D. Garlaschelli. Maximum-Entropy Networks. Springer, Cham, Switzerland, 2017.\\r\\n[294] E. Valdano and A. Arenas. Exact rank reduction of network models. Phys. Rev. X, 9:031050, 2019.\\r\\n[295] M. E. J. Newman. Networks. Oxford University Press, Oxford, UK, second edition, 2018.\\r\\n[296] A. Almog, M. R. Buijink, O. Roethler, S. Michel, J. H. Meijer, J. H. T. Rohling, and D. Garlaschelli. Uncovering\\r\\nfunctional signature in neural systems via random matrix theory. PLoS Comput. Biol., 15:e1006934, 2019.\\r\\n[297] C. Lucibello and M. Ib`a˜nez-Berganza. Covariance estimators. https://github.com/CarloLucibello/\\r\\ncovariance-estimators. GitHub. Accessed: September 24, 2024.\\r\\n[298] A. L. Barab´asi. Network Science. Cambridge University Press, Cambridge, UK, 2016.\\r\\n[299] A. Barrat, M. Barth´elemy, R. Pastor-Satorras, and A. Vespignani. The architecture of complex weighted\\r\\nnetworks. Proc. Natl. Acad. Sci. USA, 101:3747–3752, 2004.\\r\\n[300] V. M. Egu´ıluz, D. R. Chialvo, G. A. Cecchi, M. Baliki, and A. V. Apkarian. Scale-free brain functional\\r\\nnetworks. Phys. Rev. Lett., 94:018102, 2005.\\r\\n[301] B. Zhang and S. Horvath. A general framework for weighted gene co-expression network analysis. Stat. Appl.\\r\\nGenet. Mol. Biol., 4:17, 2005.\\r\\n[302] D. S. Bassett, E. Bullmore, B. A. Verchinski, V. S. Mattay, D. R. Weinberger, and A. Meyer-Lindenberg.\\r\\nHierarchical organization of human cortical networks in health and schizophrenia. J. Neurosci., 28:9239–9248,\\r\\n2008.\\r\\n[303] M. A. Porter, J.-P. Onnela, and P. J. Mucha. Communities in networks. Notices of the AMS, 56:1082–1097,\\r\\n1164–1166, 2009.\\r\\n[304] M. MacMahon. Random matrix theory (RMT) filtering of financial time series\\r\\nfor community detection. http://www.mathworks.com/matlabcentral/fileexchange/\\r\\n49011-random-matrix-theory-rmt-filtering-of-financial-time-series-for-community-detection.\\r\\nMATLAB Central File Exchange. Accessed: July 15, 2024.\\r\\n[305] J. Saram¨aki, M. Kivel¨a, J.-P. Onnela, K. Kaski, and J. Kert´esz. Generalizations of the clustering coefficient to\\r\\nweighted complex networks. Phys. Rev. E, 75:027105, 2007.\\r\\n[306] Y. Wang, E. Ghumare, R. Vandenberghe, and P. Dupont. Comparison of different generalizations of clustering\\r\\ncoefficient and local efficiency for weighted undirected graphs. Neural Comput., 29:313–331, 2017.\\r\\n[307] N. Masuda, M. Sakaki, T. Ezaki, and T. Watanabe. Clustering coefficients for correlation networks. Front.\\r\\nNeuroinfo., 12:7, 2018.\\r\\n[308] phiclust: A clusterability measure for scRNA-seq data. https://github.com/semraulab/phiclust. Accessed:\\r\\n13 November 2023.\\r\\n[309] Mircea, M. and Hochane, M. and Fan, X. and Chuva de Sousa Lopes, S. M. and Garlaschelli, D. and Semrau,\\r\\nS. Phiclust: A clusterability measure for single-cell transcriptomics reveals phenotypic subpopulations. https:\\r\\n//zenodo.org/record/5785793#.Ybs5wn3MK3I. Accessed: 13 November 2023.\\r\\n[310] M. Rubinov and O. Sporns. Complex network measures of brain connectivity: Uses and interpretations.\\r\\nNeuroImage, 52:1059–1069, 2010.\\r\\n[311] T. P. Peixoto. The graph-tool python library. http://figshare.com/articles/graph_tool/1164194. Ac\\x02cessed: 14 November 2023.\\r\\n[312] M. O. Kuismin and M. J. Sillanp¨a¨a. Estimation of covariance and precision matrix, network structure, and a\\r\\nview toward systems biology. WIREs Comput. Stat., 9:e1415, 2017.\\r\\n[313] S. Epskamp, A. O. J. Cramer, L. J. Waldorp, V. D. Schmittmann, and D. Borsboom. qgraph: Network\\r\\nvisualizations of relationships in psychometric data. J. Stat. Software, 48:1–18, 2012.\\r\\n[314] R. R. Gabriel. Estimating a psychometric network with qgraph. https://reisrgabriel.com/blog/\\r\\n2021-08-10-psych-network/. Accessed: 24 August 2023.\\r\\n47\\n[315] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\\r\\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. ´\\r\\nScikit-learn: Machine learning in Python. J. Mach. Learn. Res., 12:2825–2830, 2011.\\r\\n[316] Scikit-learn package. https://scikit-learn.org/stable/install.html. Accessed: 18 May 2022.\\r\\n[317] M. W. Cole, T. Yarkoni, G. Repovs, A. Anticevic, and T. S. Braver. Global connectivity of prefrontal cortex\\r\\npredicts cognitive control and intelligence. J. Neurosci., 32:8988–8999, 2012.\\r\\n[318] E. Santarnecchi, G. Galli, N. R. Polizzotto, A. Rossi, and S. Rossi. Efficiency of weak brain connections support\\r\\ngeneral cognitive functioning. Human Brain Mapping, 35:4566–4582, 2014.\\r\\n[319] J.-G. Young, G. T. Cantwell, and M. E. J. Newman. Bayesian inference of network structure from unreliable\\r\\ndata. J. Compl. Netw., 8:cnaa046, 2020.\\r\\n[320] S. Raimondo and M. De Domenico. Measuring topological descriptors of complex networks under uncertainty.\\r\\nPhys. Rev. E, 103:022311, 2021.\\r\\n[321] S. Boccaletti, G. Bianconi, R. Criado, C. I. del Genio, J. G´omez-Garde˜nes, M. Romance, I. Sendi˜na Nadal,\\r\\nZ. Wang, and M. Zanin. The structure and dynamics of multilayer networks. Phys. Rep., 544:1–122, 2014.\\r\\n[322] M. Kivel¨a, A. Arenas, M. Barthelemy, J. P. Gleeson, Y. Moreno, and M. A. Porter. Multilayer networks. J.\\r\\nComp. Netw., 2:203–271, 2014.\\r\\n[323] G. Bianconi. Multilayer Networks. Oxford University Press, Oxford, UK, 2018.\\r\\n[324] O. Artime, B. Benigni, G. Bertagnolli, V. d’Andrea, R. Gallotti, A. Ghavasieh, S. Raimondo, and M. De\\r\\nDomenico. Multilayer Network Science. Cambridge University Press, Cambridge, UK, 2022.\\r\\n[325] M. De Domenico. More is different in real-world multilayer networks. Nat. Phys., 19:1247–1262, 2023.\\r\\n[326] M. J. Brookes, P. K. Tewarie, B. A. E. Hunt, S. E. Robson, L. E. Gascoyne, E. B. Liddle, P. F. Liddle, and\\r\\nP. G. Morris. A multi-layer network approach to MEG connectivity analysis. NeuroImage, 132:425–438, 2016.\\r\\n[327] P. Tewarie, A. Hillebrand, B. W. van Dijk, C. J. Stam, G. C. O’Neill, P. Van Mieghem, J. M. Meier, M. W.\\r\\nWoolrich, P. G. Morris, and M. J. Brookes. Integrating cross-frequency and within band functional networks\\r\\nin resting-state MEG: A multi-layer network approach. NeuroImage, 142:324–336, 2016.\\r\\n[328] J. M. Buld´u and M. A. Porter. Frequency-based brain networks: From a multiplex framework to a full multilayer\\r\\ndescription. Netw. Neurosci., 2:418–441, 2017.\\r\\n[329] M. De Domenico, S. Sasai, and A. Arenas. Mapping multiplex hubs in human functional brain networks. Front.\\r\\nNeurosci., 10:326, 2016.\\r\\n[330] M. De Domenico. Multilayer modeling and analysis of human brain networks. GigaScience, 6:gix004, 2017.\\r\\n[331] R. Dorantes-Gilardi, D. Garc´ıa-Cort´es, E. Hern´andez-Lemus, and J. Espinal-Enr´ıquez. Multilayer approach\\r\\nreveals organizational principles disrupted in breast cancer co-expression networks. Appl. Netw. Sci., 5:47,\\r\\n2020.\\r\\n[332] M. Russell, A. Aqil, M. Saitou, O. Gokcumen, and N. Masuda. Gene communities in co-expression networks\\r\\nacross different tissues. PLoS Comput. Biol., 00:in press, 2023.\\r\\n[333] N. Musmeci, V. Nicosia, T. Aste, T. Di Matteo, and V. Latora. The multiplex dependency structure of financial\\r\\nmarkets. Complexity, 2017:9586064, 2017.\\r\\n[334] P. J. Mucha, T. Richardson, K. Macon, M. A. Porter, and J.-P. Onnela. Community structure in time\\x02dependent, multiscale, and multiplex networks. Science, 328:876–878, 2010.\\r\\n[335] D. Taylor, S. Shai, N. Stanley, and P. J. Mucha. Enhanced detectability of community structure in multilayer\\r\\nnetworks through layer aggregation. Phys. Rev. Lett., 116:228301, 2016.\\r\\n[336] D. Taylor, R. S. Caceres, and P. J. Mucha. Super-resolution community detection for layer-aggregated multi\\x02layer networks. Phys. Rev. X, 7:031056, 2017.\\r\\n48\\n[337] M. Venkatesh, J. Jaja, and L. Pessoa. Comparing functional connectivity matrices: A geometry-aware approach\\r\\napplied to participant identification. NeuroImage, 207:116398, 2020.\\r\\n[338] K. You and H.-J. Park. Re-visiting Riemannian geometry of symmetric positive definite matrices for the\\r\\nanalysis of functional connectivity. NeuroImage, 225:117464, 2021.\\r\\n[339] K. Abbas, M. Liu, M. Venkatesh, E. Amico, A. D. Kaplan, M. Ventresca, L. Pessoa, J. Harezlak, and J. Go˜ni.\\r\\nGeodesic distance on optimally regularized functional connectomes uncovers individual fingerprints. Brain\\r\\nConn., 11:333–348, 2021.\\r\\n[340] X. Pennec, P. Fillard, and N. Ayache. A Riemannian framework for tensor computing. Int. J. Comput. Vis.,\\r\\n66:41–66, 2006.\\r\\n[341] M. Rahim, B. Thirion, and G. Varoquaux. Population shrinkage of covariance (PoSCE) for better individual\\r\\nbrain functional-connectivity estimation. Med. Image Anal., 54:138–148, 2019.\\r\\n[342] M. Latapy, C. Magnien, and N. Del Vecchio. Basic notions for the analysis of large two-mode networks. Soc.\\r\\nNetw., 30:31–48, 2008.\\r\\n[343] M. E. J. Newman. Scientific collaboration networks. i. network construction and fundamental results. Phys.\\r\\nRev. E, 64:016131, 2001.\\r\\n[344] J.-L. Guillaume and M. Latapy. Bipartite structure of all complex networks. Info. Proc. Lett., 90:215–221,\\r\\n2004.\\r\\n[345] J. J. Ramasco, S. N. Dorogovtsev, and R. Pastor-Satorras. Self-organization of collaboration networks. Phys.\\r\\nRev. E., 70:036106, 2004.\\r\\n[346] L. Lov´asz. Large Networks and Graph Limits. American Mathematical Society, Providence, RI, 2012.\\r\\n[347] G. Bianconi and A.-L. Barab´asi. Bose-Einstein condensation in complex networks. Phys. Rev. Lett., 86:5632–\\r\\n5635, 2001.\\r\\n[348] G. Bianconi and A.-L. Barab´asi. Competition and multiscaling in evolving networks. Europhys. Lett., 54:436–\\r\\n442, 2001.\\r\\n[349] K.-I. Goh, B. Kahng, and D. Kim. Universal behavior of load distribution in scale-free networks. Phys. Rev.\\r\\nLett., 87:278701, 2001.\\r\\n[350] G. Caldarelli, A. Capocci, P. De Los Rios, and M. A. Mu˜noz. Scale-free networks from varying vertex intrinsic\\r\\nfitness. Phys. Rev. Lett., 89:258702, 2002.\\r\\n[351] M. Boguna and R. Pastor-Satorras. Class of correlated random networks with hidden variables. Phys. Rev. E,\\r\\n68:036112, 2003.\\r\\n[352] N. Masuda, H. Miwa, and N. Konno. Analysis of scale-free networks based on a threshold graph with intrinsic\\r\\nvertex weights. Phys. Rev. E, 70:036124, 2004.\\r\\n[353] N. Perra, B. Gon¸calves, R. Pastor-Satorras, and A. Vespignani. Activity driven modeling of time varying\\r\\nnetworks. Sci. Rep., 2:469, 2012.\\r\\n[354] N. Masuda, H. Miwa, and N. Konno. Geographical threshold graphs with small-world and scale-free properties.\\r\\nPhys. Rev. E, 71:036108, 2005.\\r\\n[355] N. Masuda and N. Konno. VIP-club phenomenon: Emergence of elites and masterminds in social networks.\\r\\nSoc. Netw., 28:297–309, 2006.\\r\\n[356] G. T. Cantwell, Y. Liu, B. F. Maier, A. C. Schwarze, C. A. Serv´an, J. Snyder, and G. St-Onge. Thresholding\\r\\nnormally distributed data creates complex networks. Phys. Rev. E, 101:062302, 2020.\\r\\n[357] M. A. Porter and S. D. Howison. The role of network analysis in industrial and applied mathematics. Preprint\\r\\nhttps://arxiv.org/abs/1703.06843v3, 2018.\\r\\n49'},\n",
       " {'name': '2007.02673v1.pdf',\n",
       "  'content': '1\\r\\nImpact of COVID-19 on Forecasting Stock Prices: An Integration \\r\\nof Stationary Wavelet Transform and Bidirectional Long Short\\x02Term Memory\\r\\nDaniel Štifanić1, Jelena Musulin2, Adrijana Miočević3, Sandi Baressi Šegota4, Roman Šubić5,\\r\\nZlatan Car6\\r\\n1\\r\\ndstifanic@riteh.hr\\r\\n2\\r\\njmusulin@riteh.hr\\r\\n3\\r\\nadrijana.miocevic@uniri.hr\\r\\n4\\r\\nsbaressisegota@riteh.hr\\r\\n5\\r\\nroman.subic@hnb.hr\\r\\n6\\r\\ncar@riteh.hr\\r\\n1,2,4,6Faculty of Engineering Rijeka, University of Rijeka; Vukovarska 58, 51000 Rijeka, Croatia\\r\\n3University of Rijeka; Trg braće Mažuranića 10, 51000 Rijeka, Croatia\\r\\n5Croatian National Bank; Trg hrvatskih velikana 3, 10000 Zagreb, Croatia\\r\\nAbstract\\r\\nCOVID-19 is an infectious disease that mostly affects the respiratory system. At the time of \\r\\nthis research being performed, there were more than 1.4 million cases of COVID-19, and one \\r\\nof the biggest anxieties is not just our health, but our livelihoods, too. In this research, authors \\r\\ninvestigate the impact of COVID-19 on the global economy, more specifically, the impact of \\r\\nCOVID-19 on financial movement of Crude Oil price and three U.S. stock indexes: DJI, S&P \\r\\n500 and NASDAQ Composite. The proposed system for predicting commodity and stock\\r\\nprices integrates the Stationary Wavelet Transform (SWT) and Bidirectional Long Short\\x02Term Memory (BDLSTM) networks. Firstly, SWT is used to decompose the data into \\r\\napproximation and detail coefficients. After decomposition, data of Crude Oil price and stock \\r\\nmarket indexes along with COVID-19 confirmed cases were used as input variables for future \\r\\nprice movement forecasting. As a result, the proposed system BDLSTM+WT-ADA achieved \\r\\nsatisfactory results in terms of five-day Crude Oil price forecast.\\r\\nKeywords: COVID-19, Commodity Price Movement, Stock Market Movement, Artificial \\r\\nIntelligence, Stationary Wavelet Transform, Bidirectional-LSTM.\\n2\\r\\n1. Introduction\\r\\nInfectious diseases have always been a threat to humanity, especially those about which little \\r\\nor nothing is known. World Health Organization (WHO) describes pandemic as “the \\r\\nworldwide spread of a new disease” and although in such times the greatest concern is how to \\r\\nsave human lives, the first following objective is how to save the economy and preserve the \\r\\nwell-being [1]. In recent history, it is possible to observe the impact of Spanish flu (1918-\\r\\n1919) on the economy. According to Centers for Disease Control and Prevention (CDC)\\r\\nestimates, roughly 500 million people were taken ill with the disease, which ultimately took \\r\\nthe lives of about 50 million worldwide [2]. Even though the economic data from the early \\r\\n20th century is rare, it has been noted that the impact of business closures has led to \\r\\nunemployment, and businesses that have survived have suffered huge losses. The comparison \\r\\ncan be drawn with the pandemic from the recent past, too. During the 2003 SARS (Severe \\r\\nAcute Respiratory Syndrome), which lasted less than a year, business saw enormous revenue \\r\\nplunge. Similar scenario happened in 2009 when expansion the H1N1 flu triggered numerous \\r\\nconsequences [3][4]. Pandemic like COVID-19 will surely have a significant influence on the \\r\\nglobal economy, as well as impact on the financial markets. From 24 to 28 February 2020, \\r\\nstock markets worldwide reported their largest one-week declines since the 2008 financial \\r\\ncrisis. Traders began to sell shares out of fear, and as a result, a market-wide circuit breaker \\r\\nwas triggered four times in March [5][6]. The breaks were made for 15 minutes each in the \\r\\nhope that the situation would calm down. Every pandemic is unique and it is unlikely to \\r\\nexpect the same results, but direction and movement can be predicted which is important for a \\r\\ntimely response. A recent occurrence of pandemic has created a supply and a demand shock \\r\\nwhich is significantly different in comparison with other crises. Starting with the supply-side \\r\\nreductions due to the astonishing closures of factories and labor shortages, the global \\r\\neconomy was simultaneously affected by the demand-side shock with immediate reduction in \\r\\nconsumer spending. These shocks have ultimately resulted in shifting aggregate supply and \\r\\naggregate demand downward and, consequently, in reducing national and global gross \\r\\ndomestic products.\\r\\nForecasting stock prices has always been considered a challenging task due to the fact that \\r\\nstock market tends to be non-stationary, non-linear and highly noisy [7]. Artificial \\r\\nIntelligence (AI) algorithms have been proven successful in solving problems such as \\r\\npredicting stock prices [8] as well as other various fields of science, technology and medicine \\r\\n[9-11]. Numerous factors influence financial market performance, and even financial experts \\r\\nfind it complicated to make accurate predictions. The algorithm that may be efficient in \\n3\\r\\ncommodity and stock market forecasting is a Bidirectional Long Short-Term Memory \\r\\n(BDLSTM) [12]. This algorithm is a combination of Bidirectional Recurrent Network \\r\\n(BDRNN) and Long Short-Term Memory (LSTM) cells. Such combination causes the \\r\\nBDLSTM to have the advantage of LSTM with feedback for the next layer [13]. \\r\\nAlthelaya et al. (2018) demonstrate the use of BDLSTM for the most challenging real-world \\r\\napplication for time-series prediction [14]. Jia et al. (2019) show the use of Bidirectional \\r\\nLSTM to predict the accuracy of GREE stock price, and achieve good results [15]. Eapen et \\r\\nal. (2019) offer a view into a combination of multiple pipelines of convolutional neural \\r\\nnetwork and bidirectional long short term memory units and its use for stock market index \\r\\nprediction [16].\\r\\nIn order to decompose high complexity data of commodity and stock market indexes and in \\r\\nthe same time retain translation invariance, Stationary Wavelet Transform (SWT) was \\r\\nutilized. Since SWT is shift-invariant and non-decimated, it can be used for feature extraction, \\r\\nchange detection and pattern recognition. SWT can be described as follows: at each level, \\r\\nafter the signal is convolved with high-pass and low-pass filters, resulting sequences have the \\r\\nsame number of samples as the original signal [17]. \\r\\nBai et al. (2016) demonstrate the successful use of SWT and backpropagation neural network \\r\\n(BPNN) to forecast daily air pollutants concentrations and the results show that the SWT\\x02BPNN model has better forecasting performance for the three air pollutants than BPNN \\r\\nmodel without SWT [18]. Supratid et al. (2017) show development of a reservoir inflow \\r\\nintegrated forecasting model, relying on SWT and nonlinear autoregressive neural network \\r\\nwith exogenous input (NARX), and achieve good results with relatively accurate predictions \\r\\n[19].\\r\\nThree major U.S. indexes: Dow Jones Industrial Average, S&P 500 and NASDAQ Composite \\r\\nalong with Crude Oil price, are chosen as the research objects. Sample data is selected from \\r\\nMarch 22, 2000 to April 7, 2020. Wang et al. (2012) show the cross-correlations between \\r\\nCrude Oil market and Dow Jones Industrial Average, S&P 500 and NASDAQ Composite \\r\\nstock market from the perspective of econophysics, and they found that cross-correlated \\r\\nbehavior between Crude Oil market and other three U.S. stock market is nonlinear and \\r\\nmultifractal [20].\\r\\nDatasets for each stock market index (Dow Jones Industrial Average, S&P 500 and NASDAQ \\r\\nComposite) along with Crude Oil price were obtained from Yahoo finance website [21] while \\r\\nthe data of COVID-19 confirmed cases was obtained from the Johns Hopkins University \\r\\nCenter for Systems Science and Engineering (JHU CSSE) [22]. At the time, when this \\n4\\r\\nresearch was performed, data of commodity and each stock market index consisted of 4992\\r\\ndata-points, which were split into the training and testing sets.\\r\\nThe aim of this research is to integrate SWT with BDLSTM in order to predict the movement \\r\\nof aforementioned commodity and stock market indexes during the COVID-19 outbreak.\\r\\nCOVID-19 caused huge shock to the global economy including commodity prices as well as \\r\\nstock market [23]. With implementation and forecasting price movement, it is expected to \\r\\nmake a prompt and significant contribution in terms of understanding and responding to \\r\\nimpact of COVID-19 pandemic on the global economy. This approach allows more effective \\r\\npredictions during pandemic and it will help with lowering the negative impact of COVID-19 \\r\\non financial market by providing experts with additional information and tools in their \\r\\ndecision making. Integration of SWT with BDLSTM should help not only in the current \\r\\nsituation but also in the future situations similar to COVID-19 in order to be able to react in \\r\\ntime and prevent a financial crisis.\\r\\nFirst, original data of each dataset will be used as input variable in order to forecast future\\r\\nprice movement by utilizing BDLSTM. Second, commodity and each of stock market index\\r\\ndata will be decomposed by using SWT in order to obtain approximation and detail \\r\\ncoefficients which will be used to train the BDLSTM model. Afterwards, the obtained results \\r\\nfor each configuration system will be compared. Third, the impact of confirmed cases detail\\r\\ncoefficients on forecasting accuracy will be examined. Lastly, the best performing system \\r\\nconfiguration will be used in order to show the forecasted movement of Crude Oil price for\\r\\nthe next five days with 128 observation days. The overview of the proposed system is given \\r\\nin Figure 1.\\r\\nFigure 1. Framework of the proposed system for commodity and stock price prediction during \\r\\nCOVID-19 pandemic (SWT – Stationary Wavelet Transform, cA – Approximation coefficients, \\r\\ncD – Detail coefficients, BDLSTM – Bidirectional Long Short-Term Memory network)\\n5\\r\\n2. Materials and Methods\\r\\nThis section provides a detailed description of datasets used for forecasting price movements\\r\\nas well as a brief overview and mathematical description of Stationary Wavelet Transform, \\r\\nBidirectional Recurrent Neural Network and Bidirectional Long Short-Term Memory \\r\\nnetwork. In last two subsections, grid search algorithm and evaluation criteria are described.\\r\\n2.1. Dataset description\\r\\nIn order to create the dataset used in this research, historical data of West Texas Intermediate\\r\\n(WTI) Crude Oil price and three stock indexes along with the number of COVID-19 \\r\\nconfirmed cases are used. WTI can be defined as the main oil benchmark for North America\\r\\nand moreover is the most liquid crude oil benchmark [24]. In the oil market, benchmarks \\r\\nserve as a pricing reference for Crude Oil. Existence of different Crude Oil grades and \\r\\nvarieties led to the use of benchmarks as a gauge in order to compare one type of Crude Oil \\r\\nwith others. WTI is considered as light sweet oil since it has a sulfur content of 0.24%, \\r\\ntherefore it is ideal for gasoline [25].\\r\\nThe stock indexes are Dow Jones Industrial Average, S&P 500, and NASDAQ Composite. \\r\\nThe data of these indexes and Crude Oil price for the time period from March 22, 2000 to \\r\\nApril 07, 2020 are publicly available and obtained from the Yahoo finance website [21].\\r\\nCrude Oil commodity and each stock market index contain the data of volume and open, high, \\r\\nlow, close prices for each day when the financial market was open. For the purpose of this \\r\\nresearch, only the closing price is used. The data of COVID-19 confirmed cases is publicly \\r\\navailable and operated by the Johns Hopkins University Center for Systems Science and \\r\\nEngineering (JHU CSSE) and supported by ESRI Living Atlas Team and the Johns Hopkins \\r\\nUniversity Applied Physics Lab (JHU APL) [22]. Obtained data contains the number of \\r\\nconfirmed cases (infected patients) for each day since the start of the COVID-19 pandemic \\r\\nJanuary 22, 2020 until April 07, 2020. Datasets are organized in a way that column represents\\r\\nclosing price, while rows represent the date of data collection. Furthermore, for each date after \\r\\nJanuary 22, 2020, the number of confirmed cases is added in additional column. Datasets with \\r\\nclosing prices of aforementioned indexes, Crude Oil and COVID-19 confirmed cases are\\r\\norganized as multivariate time-series data and used in order to build an efficient deep learning \\r\\nmodel. Before implementation of the AI algorithms, signal decomposition using Wavelet \\r\\nTransform (WT) was utilized.\\n6\\r\\nDescriptive Statistics of commodity, stock market indexes and COVID-19 confirmed cases\\r\\nare provided in Table 1. With these statistics, the features of each dataset can be described\\r\\n[26]. Descriptive Statistics used in this research are: mean, maximum, minimum, standard \\r\\ndeviation, kurtosis and skewness. The total number of data-points i.e. observations in each of \\r\\nthe aforementioned datasets is 4992, which were split into two parts. First part (80% of the \\r\\ntotal number) is used for model training, while the second part (20% of the total number) is \\r\\nused in order to evaluate the performance of the trained models.\\r\\nTable 1. Descriptive Statistics of commodity, stock market indexes and COVID-19 confirmed \\r\\ncases.\\r\\nStatistic Commodity Stock market indexes COVID-19\\r\\nCrude Oil DJI S&P 500 NASDAQ confirmed cases\\r\\nComposite\\r\\nMean 62.04356 14240.89 1596.151 3528.579 2665.000\\r\\nMaximum 145.1800 29551.42 3386.150 9817.180 1430981\\r\\nMinimum 17.45000 6547.050 676.5300 1114.110 0\\r\\nSt.Dev. 26.05498 5376.658 609.2196 1989.950 44701.96\\r\\nKurtosis 2.310495 3.163203 2.912266 3.200213 601.9265\\r\\nSkewness 0.382736 1.104382 0.980250 1.127060 23.19243\\r\\nObservation 4992 4992 4992 4992 4992\\r\\nAdditionally, each dataset is tested for stationarity using Augmented Dickey-Fuller (ADF) \\r\\nand Phillips-Perron (PP) Unit Root Tests. The results for Level and 1st Difference are \\r\\nobtained with intercept and with trend and intercept for both ADF and PP tests as shown in \\r\\nTable 2. In order to select optimal lag length in the ADF test, the Schwarz Information \\r\\nCriterion (SIC) was utilized with maximum lags of 31. On the other hand, in the PP test \\r\\nBartlett kernel was used as Spectral estimation method along with the Newey-West automatic \\r\\nbandwidth selection. The value of optimal lag length (ADF test) and optimal bandwidth (PP \\r\\ntest) for each dataset is enclosed in parentheses () and given in Table 2. The critical values for \\r\\nADF and PP tests with intercept are: -3.431479, -2.861924 and -2.567017 for 1%, 5% and \\r\\n10%, while the critical values for the same tests but with trend and intercept are: -3.959877, -\\r\\n3.410705 and -3.127138 for 1%, 5% and 10%.\\n7\\r\\nTable 2. Results of Augmented Dickey-Fuller and Phillips-Perron Unit Root Tests for each \\r\\nstock market index (Dow Jones Industrial Average, S&P 500 and NASDAQ Composite), \\r\\nCrude Oil price and COVID-19 confirmed cases.\\r\\nVariables\\r\\nADF test (with \\r\\nintercept)\\r\\nADF test (with trend \\r\\nand intercept)\\r\\nPP test (with \\r\\nintercept)\\r\\nPP test (with trend \\r\\nand intercept)\\r\\nLevel 1\\r\\nst Difference Level 1st Difference Level 1st Difference Level 1st Difference\\r\\nCrude Oil -1.813368 \\r\\n(1)\\r\\n-74.18251\\r\\n(0)\\r\\n-1.451348 \\r\\n(1)\\r\\n-74.19782 \\r\\n(0)\\r\\n-1.851205 \\r\\n(10)\\r\\n-74.15676 \\r\\n(10)\\r\\n-1.487574 \\r\\n(9)\\r\\n-74.17632 \\r\\n(10)\\r\\nDJI\\r\\n-0.589078 \\r\\n(9)\\r\\n-21.95618\\r\\n(8)\\r\\n-2.502465 \\r\\n(9)\\r\\n-21.96959 \\r\\n(8)\\r\\n-0.561936 \\r\\n(22)\\r\\n-82.15463 \\r\\n(21)\\r\\n-2.445644 \\r\\n(23)\\r\\n-82.15622 \\r\\n(20)\\r\\nS&P 500\\r\\n-0.300921 \\r\\n(9)\\r\\n-22.33898\\r\\n(8)\\r\\n-2.541671 \\r\\n(9)\\r\\n-22.38022 \\r\\n(8)\\r\\n-0.304412 \\r\\n(20)\\r\\n-82.90155\\r\\n(19)\\r\\n-2.578981 \\r\\n(20)\\r\\n-82.96314 \\r\\n(18)\\r\\nNASDAQ \\r\\nComposite\\r\\n0.340195\\r\\n(9)\\r\\n-22.89305 \\r\\n(8)\\r\\n-2.983702 \\r\\n(9)\\r\\n-23.02818 \\r\\n(8)\\r\\n0.172873 \\r\\n(17)\\r\\n-81.56604 \\r\\n(16)\\r\\n-3.619394\\r\\n(16)\\r\\n-81.75234 \\r\\n(14)\\r\\nCOVID 19\\r\\ncases\\r\\n40.89298 \\r\\n(30)\\r\\n51.75262\\r\\n(31)\\r\\n41.05363 \\r\\n(30)\\r\\n51.64084\\r\\n(31)\\r\\n264.3043 \\r\\n(42)\\r\\n-34.20378 \\r\\n(34)\\r\\n262.4940 \\r\\n(42)\\r\\n-34.44749 \\r\\n(34)\\r\\nFrom the results of Unit Root Tests, it can be concluded that the series of commodity and \\r\\nthree U.S. stock market indexes do not reject the null hypothesis and can be considered as \\r\\nnon-stationary at the level except for NASDAQ Composite, where PP test with trend and \\r\\nintercept shows the value of -3.619394. If test critical value of 5% (-3.410705) is chosen, the \\r\\nnull hypothesis can be rejected, and the series of NASDAQ Composite is stationary.\\r\\nFurthermore, results point out that commodity and three stock market indexes are stationary at \\r\\ntheir 1st difference form.\\r\\nIn the case of COVID-19 confirmed cases, the results reveal that the series reject the null \\r\\nhypothesis in the PP test with intercept and with trend and intercept at the 1st difference and \\r\\ncan be considered as stationary.\\r\\n2.2. Data decomposition with Wavelet Transform\\r\\nThe Wavelet Transform (WT) is a powerful mathematical tool for signal processing [27]. \\r\\nApplying WT, a signal can be decomposed into many frequency bands, which can simplify \\r\\nthe analysis process. The Fourier Transform (FT) major drawback is losing time information, \\r\\nwhile preciseness of Short Time Fourier Transform (STFT) largely depends on its window \\r\\nsize and shape. Unlike FT and STFT, WT preserves precise information about time and \\r\\nfrequency. Since the characteristics of the stock market are non-stationary, non-linear and \\r\\nnoisy, and considering the aforementioned drawbacks of FT and STFT, WT can be \\n8\\r\\nappropriate approach when dealing with economic and financial time-series analysis. Wavelet \\r\\ntransform of signal x(t) can be calculated as:\\r\\n ( ) \\r\\n \\r\\n√| |\\r\\n∫ ( ) \\r\\n (\\r\\n \\r\\n \\r\\n) \\r\\n \\r\\n \\r\\n (1)\\r\\nwhere ψ represents the analyzing wavelet, * stands for complex conjugate, a represents a time \\r\\ndilation and τ represents time translation [28]. Therefore, the Discrete Wavelet Transform \\r\\n(DWT) of signal x[m] can be defined as [29]:\\r\\n [ ] \\r\\n \\r\\n \\r\\n ∑ [ ] [ \\r\\n ]\\r\\n \\r\\n \\r\\n (2)\\r\\nTo obtain approximation coefficients cA and detail coefficients cD from the original signal \\r\\nx[m], DWT needs to be performed. After DWT decomposition process, approximation \\r\\ncontains the low-frequency components while the detail contains the high-frequency \\r\\ncomponents of the original signal. In the case of conventional DWT, after each decomposition \\r\\nlevel signal is decimated. Because of decimation, the DWT is not a time-invariant transform \\r\\nand it is not suitable for data preprocessing in this research. This drawback can be overcome \\r\\nby using one of the DWT’s extensions, such as, Stationary Wavelet Transform (SWT) which \\r\\nsolves the problem of shift-invariance. SWT is feasible for feature extraction, change \\r\\ndetection and pattern recognition due to shift-invariant and non-decimated properties [30]. In \\r\\nSWT, after the signal is convolved with high and low pass filters, no decimation is performed,\\r\\nthereby the number of obtained coefficients cA and cD at each decomposition level is the \\r\\nsame as the number of samples in the original signal. Five-level SWT decomposition of an \\r\\ninput signal x[n] is shown in Figure 2.\\n9\\r\\nFigure 2. Signal decomposition using SWT at level five (hi – high level features at \\r\\ndecomposition level i, li – low level features at decomposition level i, cDi – Detail coefficients \\r\\nat decomposition level i, cA5 – Approximation coefficients at decomposition level five)\\r\\nIn order to obtain a good decomposition of the original signal, discrete Meyer wavelet is \\r\\nutilized. The Meyer wavelet is linear-phase, orthogonal wavelet, and it is defined in the \\r\\nfrequency domain as follows [31]:\\r\\n ( ) \\r\\n{\\r\\n \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n√ \\r\\n (\\r\\n \\r\\n \\r\\n (\\r\\n | |\\r\\n \\r\\n )) \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n | | \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n√ \\r\\n (\\r\\n \\r\\n \\r\\n (\\r\\n | |\\r\\n \\r\\n )) \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n | | \\r\\n \\r\\n \\r\\n \\r\\n \\r\\n (3)\\r\\nwhere ν is an auxiliary function that can be defined as\\r\\n ( ) \\r\\n \\r\\n( \\r\\n  \\r\\n) [ ] (4)\\r\\n2.3. Bidirectional Recurrent Neural Network\\r\\nRecurrent Neural Networks (RNNs) are a class of Artificial Neural Networks (ANNs) with \\r\\nfeedback connection [32]. Between units are connections by which a directed cycle is formed. \\r\\nTherefore in RNN model, a signal can travel both forward and backward. In such network, \\n10\\r\\nknowledge can be represented with the values of synaptic connections between input, hidden \\r\\nand output layers of neurons. The main idea behind RNNs is to use sequential data as input. \\r\\nThe RNN model can be simplified by unfolding the RNN architecture over the input sequence \\r\\nof data as is shown in Figure 3.\\r\\nFigure 3. Folded RNN architecture and the process of unfolding with T time-steps (x – input \\r\\nvector, wxh – weight matrix between input and hidden layer, h – hidden state, whh – weight \\r\\nmatrix between two hidden states, why – weight matrix between hidden and output layer, y –\\r\\noutput vector)\\r\\nConventional feedback neural networks process the data in one direction only, but in certain \\r\\nareas, past and future information is desirable. Therefore, in 1997, Schuster and Paliwal \\r\\nintroduced Bidirectional Recurrent Neural Network (BRNN), whose basic idea was to extend \\r\\nthe RNN architecture by introducing additional hidden layers where data were placed in the \\r\\nopposite, negative direction. The hidden layer maintains a hidden state which can be defined \\r\\nas:\\r\\n ⃗ \\r\\n ( ⃗ ⃗ ⃗ ⃗  ⃗ ) (5)\\r\\nfor the positive direction, and\\r\\n ⃖⃗\\r\\n ( ⃖⃗ ⃖⃗ ⃖⃗ ⃖⃗ ⃖⃗) (6)\\n11\\r\\nfor the negative direction [33]. represents the weight matrix between input and hidden \\r\\nlayer, represents the input vector, represents the weight matrix between two hidden \\r\\nstates, represents the bias of the hidden layer and represents the activation function. The \\r\\noutput layer can be defined as:\\r\\n ( ⃗  ⃗ \\r\\n ⃖⃗ \\r\\n ⃖⃗\\r\\n ) (7)\\r\\nwhere ⃗ represents the weight matrix between hidden and output layer, while ⃖⃗ \\r\\nrepresents the same but in other direction, is the bias of the output layer [33]. As a major \\r\\ndrawback, BRNN in its basic form cannot model a complex time dynamics and it can suffer \\r\\nfrom the vanishing or exploding gradients.\\r\\n2.3. Bidirectional Long Short-Term Memory\\r\\nOne of the solutions to overcome aforementioned problems is to use Bidirectional Long \\r\\nShort-Term Memory (BDLSTM) architecture. Such architecture differs from the RNN\\r\\narchitecture in terms of hidden layers. BDLSTM has a LSTM cell as hidden layer, which \\r\\nconsists of three gates: an input gate, forget gate, and an output gate. LSTM cell can be \\r\\nmathematically defined as follows [34]:\\r\\n ( ) (8)\\r\\n ( \\r\\n) (9)\\r\\n ( \\r\\n) (10)\\r\\n ( ) and (11)\\r\\n ( \\r\\n) (12)\\r\\nIn Eq. (8) – (12)   represent forget, input, and output gate, and represent the \\r\\nweight matrices, b is a bias vector, is a sigmoid activation function, tanh is the hyperbolic \\r\\ntangent function, is the cell output state, is the layer output, and operator ʘ is the \\n12\\r\\nelement-wise product of the vectors. By using Eq. (5) – (11), forward and backward layer \\r\\noutputs can be calculated. The result of combining BRNN with LSTM cells is a BDLSTM \\r\\nnetwork, which can model more complex time dynamics and deal with long-term \\r\\ndependencies [35]. The architecture of an unfolded BDLSTM is shown in Figure 4.\\r\\nFigure 4. The architecture of an unfolded BDLSTM with three steps (x – input vector, ⃗ –\\r\\nhidden state for the positive direction, ⃖⃗ – hidden state for the negative direction, σ – function \\r\\nused to combine two output sequences, y – output vector)\\r\\nBy using inputs in a positive sequence, the forward layer output sequence is calculated, and \\r\\nby using reversed inputs, the backward layer output sequence is calculated. Each element in \\r\\noutput vector of BDLSTM layer can be calculated as:\\r\\n ( \\r\\n⃗⃗⃗ ⃖⃗⃗⃗) (13)\\r\\nwhere two output sequences are combined by utilizing the σ function [35]. In many studies, \\r\\nbidirectional networks have been proven to be significantly better than unidirectional \\r\\nnetworks in various fields, such as speech recognition [36], classification problems [37], and \\r\\nalso in stock price prediction [38]. In this research, BDLSTM is trained in order to predict \\r\\nprice movement for the time period where the impact of COVID-19 on the global economy is \\r\\nrelatively high. In the output vector of a BDLSTM layer, the last element is predicted value\\r\\nfor the next time iteration. Furthermore, to prevent the network from overfitting, dropout can \\r\\nbe implemented on hidden layers [39].\\n13\\r\\n2.4. Hyperparameter Optimization\\r\\nIn order to determine optimal hyperparameters of the ANN, the grid search algorithm has \\r\\nbeen used. This algorithm can be described as an exhaustive search through a set of manually \\r\\nspecified parameters [40]. Therefore it iterates through every possible parameter combination, \\r\\ntrains the network and finally stores the result for each combination. Hyperparameters can be \\r\\ndescribed as follows [41]: \\r\\n\\uf0b7 hidden layer size is defined with two integers where first one represents the number of \\r\\nhidden layers and the other one defines the number of hidden neurons in that layer,\\r\\n\\uf0b7 activation function determines the output value behavior of each neuron based on its \\r\\ninput values,\\r\\n\\uf0b7 optimizer is used for minimizing the value of cost function in order to improve metric \\r\\nimportant for the research,\\r\\n\\uf0b7 learning rate can be considered as a hyperparameter that regulates the weight \\r\\nadjustment,\\r\\n\\uf0b7 learning rate decay is a technique where the training process starts with a large \\r\\nlearning rate, and then decays it with the time, and\\r\\n\\uf0b7 regularization parameter L2 forces the weights to decay towards zero but does not \\r\\nmake them zero in order to limit the influence of input parameters.\\r\\nThis way the algorithm can find the optimal hyperparameters of the model that achieve the \\r\\nmost accurate predictions. The hyperparameters adjusted in this research are: number of \\r\\nBDLSTM hidden layers and neurons, number of fully-connected (FC) hidden layers,\\r\\nactivation function, optimizer, learning rate, learning rate decay, and regularization parameter \\r\\nL2. Subset of hyperparameter space is shown in Table 3.\\n14\\r\\nTable 3. Hyperparameter values used in model training process. First column represents \\r\\nhyperparameter name, and in the second column possible parameters are shown.\\r\\nHyperparameter Possible parameters\\r\\nBDLSTM - hidden layer size (32), (64), (32, 32), (64, 32), (64, 64), (32, 32, 32), \\r\\n(64, 32, 32), (64, 32, 64), (64, 64, 64)\\r\\nFC – hidden layer size (12), (24), (12, 12), (24, 12), (24, 24), (12, 12, 12),\\r\\n(24, 12, 12), (24, 12, 24), (24, 24, 24)\\r\\nActivation function ReLU, ELU, Tanh, Identity\\r\\nOptimizer Adam, RMSprop\\r\\nLearning rate 0.0001, 0.001, 0.01\\r\\nLearning rate decay 1e-7, 1e-6, 1e-5\\r\\nRegularization parameter - L2 0.0001, 0.001, 0.01\\r\\n2.5. Evaluation criteria\\r\\nIn order to evaluate the performance of the implemented model, two evaluation criteria can be \\r\\nused as accuracy measures. These performance measures are Mean Absolute Error (MAE) \\r\\nand Root Mean Square Error (RMSE), and can be calculated as follows [42]:\\r\\n \\r\\n \\r\\n \\r\\n∑| ̂ \\r\\n|\\r\\n \\r\\n \\r\\nand (14)\\r\\n √\\r\\n \\r\\n \\r\\n∑( ̂ \\r\\n)\\r\\n \\r\\n \\r\\n \\r\\n (15)\\r\\nwith being the true signal, and ̂ being forecasted signal. Smaller values of performance\\r\\nmeasures defined by Eq. (14) and Eq. (15) mean the better forecasting performance of the \\r\\nmodel and vice-versa.\\n15\\r\\n3. Results\\r\\nThe forecasting results are obtained for Crude Oil commodity and Dow Jones Industrial \\r\\nAverage, S&P 500, NASDAQ Composite indexes. For each dataset, SWT is performed in \\r\\norder to obtain their approximation and detail coefficients at five decomposition levels using \\r\\ndiscrete Meyer wavelet function. For example, such decomposed signal of Crude Oil price is \\r\\nshown in Figure 5. where s is stock closing price for time period from March 22, 2000 to \\r\\nApril 07, 2020, cA and cD are approximation and detail coefficients.\\r\\nFigure 5. Five-level decomposition of Crude Oil closing price using SWT (s – input data, cAi\\r\\n– Approximation coefficients at decomposition level i, cDi – Detail coefficients at \\r\\ndecomposition level i)\\r\\nThree main system configurations were examined in order to achieve high-quality regression \\r\\nand small values of performance measures. In the first configuration, non-preprocessed data is \\r\\nused to train the BDLSTM model, in second the BDLSTM model is trained by using both \\r\\napproximation and detail coefficients (AD). Finally in the last configuration, the data contain \\r\\napproximation and detail coefficients for commodity and stock index price, but only the \\n16\\r\\napproximation for COVID-19 confirmed cases (ADA). The values of performance measures \\r\\nfor Crude Oil and stock market indexes with system configurations are shown in Table 4.\\r\\nTable 4. Simulation results and performance comparison of three configuration systems for \\r\\nCrude Oil commodity and Dow Jones Industrial Average, S&P 500, NASDAQ Composite\\r\\nindexes. As evaluation criteria, RMSE and MAE are used.\\r\\n RMSE MAE\\r\\nCRUDE OIL BDLSTM 0.04083 0.03051\\r\\nBDLSTM+WT-AD 0.03685 0.02962\\r\\nBDLSTM+WT-ADA 0.02911 0.02315\\r\\nDJI BDLSTM 0.04557 0.02753\\r\\nBDLSTM+WT-AD 0.02436 0.01743\\r\\nBDLSTM+WT-ADA 0.01450 0.01014\\r\\nS&P 500 BDLSTM 0.04391 0.02627\\r\\nBDLSTM+WT-AD 0.03670 0.02883\\r\\nBDLSTM+WT-ADA 0.02389 0.01734\\r\\nNASDAQ BDLSTM 0.03632 0.02502\\r\\nBDLSTM+WT-AD 0.02610 0.01754\\r\\nBDLSTM+WT-ADA 0.02597 0.01339\\r\\nBDLSTM model that achieves the best results have the same architecture for commodity and \\r\\nstock market indexes. Such architecture consists of three hidden layers, where the first two are \\r\\nBDLSTM layers with 64 hidden neurons each and the last one is FC layer with 12 hidden \\r\\nneurons. Additionally, dropout is applied on BDLSTM hidden layers with the value of 0.2 for \\r\\nthe first, and 0.1 for the second layer. All of the hidden layers use tanh activation function, \\r\\nand Adam optimizer. The best model has a learning rate of 0.001, learning rate decay of 1e-6 \\r\\nand a regularization parameter of 0.0001.\\r\\nDuring the COVID-19 pandemic, the correlation between Crude Oil price and other stock \\r\\nmarket indexes used in this research exist, and all of the data can be rearranged and used as \\r\\nmultivariate time-series data. This way, important features of Crude Oil commodity and three\\r\\nstock market indexes can be captured in order to predict movement of one price more \\r\\nprecisely.\\r\\nBy using data of Crude Oil commodity, three stock indexes and information of COVID-19 \\r\\nconfirmed cases in the past 128 days, predictions were made for Crude Oil price for the next \\n17\\r\\nfive days, as shown in Figure 6. The values of performance measures for Crude Oil with \\r\\nBDLSTM+WT-ADA system configuration are shown in Table 5.\\r\\nTable 5. Simulation results obtained with the best configuration system for Crude Oil price. \\r\\nAs evaluation criteria, RMSE and MAE are used.\\r\\nRMSE MAE\\r\\nCRUDE OIL BDLSTM+WT-ADA 0.02116 0.01701\\r\\n(a) Time period July 16, 2019 – (b) Time period August 06, 2019 –\\r\\n January 27, 2020 February 18, 2020\\r\\n(c) Time period August 27, 2019 – (d) Time period September 18, 2019 –\\r\\n March 10, 2020 March 31, 2020\\r\\nFigure 6. Prediction of Crude Oil price movement based on observations in the past 128 days\\n18\\r\\n4. Discussion\\r\\nThis research proposes an integrated system, BDLSTM+WT-ADA, for commodity and stock \\r\\nprice movement prediction during current pandemic. In order to validate the feasibility, the \\r\\nproposed system is compared with other approaches presented in the literature [43, 44] for \\r\\nprediction of stock prices. When all results are summed up, it can be seen that the minimal \\r\\nvalues of RMSE and MAE are achieved using BDLSTM+WD-ADA system configuration. If \\r\\nforecasting performances of three system configurations are compared, it can be seen that all \\r\\nof the configurations achieved RMSE value of 0.04557 or smaller and MAE value 0.03051 or \\r\\nsmaller. These results are satisfactory in terms of forecasting commodity and stock market \\r\\nprice. Furthermore, it can be seen that impact of approximation and detail coefficients\\r\\nmanifest through simulation results. For example, the worst results for each of the stock \\r\\nmarket index and Crude Oil commodity are achieved by using original, non-preprocessed \\r\\ndata. Moreover, the best results are achieved using the proposed system configuration \\r\\nBDLSTM+WD-ADA with the lowest RMSE value (0.01450) and MAE value (0.01014) for \\r\\nDow Jones Industrial Average index. Such configuration (ADA) uses five-level \\r\\ndecomposition utilizing the SWT with discrete Meyer wavelet function.\\r\\nCrude Oil is globally the most important commodity and is driven by supply and demand as \\r\\nany other good, but has a tendency to fluctuate more in price than, for example, stocks and \\r\\nbonds on financial markets. As Crude Oil prices rise, so do other fuel prices, which increase\\r\\nproduction prices in general. Rising production prices lead to higher prices of food and \\r\\nindustrial products thus generating inflation. The reduced demand for Crude Oil caused by \\r\\nvarious impacts, in this case the global pandemic, results in Crude Oil price disruption and, as \\r\\nmentioned, has a profound effect on the economy in general. For this reason, Crude Oil price\\r\\nwas selected for five-day prediction that can be extremely useful for foreseeing the events that \\r\\nfollow.\\r\\nThe relationship between the COVID-19 confirmed cases and the crude oil price is \\r\\nsignificant. With an increase in the number of cases, measures are being taken to slow down \\r\\nfurther spread. Some of them are closing factories, offices and shops and restricting the \\r\\nmovement. Consequently, much less fuel is needed for vehicles, machinery, etc. If demand \\r\\ndecreases and supply remains unchanged, this leads to lower commodity prices and crude oil \\r\\nprices fall [45]. The same goes for the stock market. If companies on the stock market reduce \\r\\nor close their operations, shareholders become nervous and fear what will happen to the value \\r\\nof that company’s shares in the future and whether it will decline. They start selling stocks \\r\\nthus increasing the supply in the market. As the number of confirmed cases increases and \\n19\\r\\nmeasures become more stringent, other buyers are not interested in buying. If there are more \\r\\nparticipants in the market looking to sell a stock than there is demand to acquire the stock, the \\r\\nstock price will fall. Therefore, the inclusion of a large amount of data (confirmed cases for \\r\\neach day) allows us to have more accurate information and a more credible result.\\r\\nFrom obtained results of forecasting Crude Oil price movement it can be seen that proposed \\r\\nsystem configuration is capable for accurate five-day prediction based on observations in the \\r\\npast 128 days. In the middle of February, the first signs of decline in Crude Oil price were \\r\\nobserved and for that time period, BDLSTM+WD-ADA system configuration successfully \\r\\npredicted the price movement. Expectations about future events are extremely important in \\r\\ntimes of crisis in order to adequately respond and initiate measures and mechanisms for the \\r\\npreservation and stability of the economy. However, because of the global role of Crude Oil \\r\\nas still irreplaceable source of energy, it has the direct effect on the geopolitical trends. The \\r\\nprice of Crude Oil is, from the economic aspect, hard to predict precisely due to political \\r\\nrelationships in the triangle: USA-OPEC countries-Russia, which are rarely stable and will \\r\\nalways disturb the economic model of supply and demand in a competitive market.\\r\\nConsequently, prediction models are valuable and can be used to foresee the sequence of \\r\\nevents, but factors like political interference, that cannot be included in the model, also affect \\r\\nthe price and must be emphasized.\\r\\n4.1. Result comparison\\r\\nThe obtained results demonstrate the connection between the crude oil price and the number \\r\\nof active cases of COVID-19. Most of the research performed in the area of economic impact \\r\\nof COVID-19 concludes that the rising number of active COVID-19 cases has a large \\r\\nnegative impact on global markets, as shown. Baker et al. (2020) use disaster modeling \\r\\ntechniques which predict a GDP contraction in the USA – with as much as 20 percent \\r\\ncontraction being predicted with 90 percent confidence interval [46]. Toda (2020) shows the \\r\\npossibility of a temporary 50 percent stock price decrease using classic asset pricing modeling\\r\\n[47]. Baldwin and Tomiura (2020) conclude that there is danger of permanent damage to the \\r\\ntrade system, depending on the policies implemented [48]. Atkeson (2020) uses as SJR \\r\\nMarkov chain based model to determine the spread and comments the possibility of key \\r\\nfinancial and economic infrastructure being affected temporarily and permanently due to \\r\\npossible extreme staff shortages, in case where the number of active cases exceeds 10 percent \\r\\nof population [49]. Albulescu (2020) investigates the impact of COVID-19 on oil pricing, due \\n20\\r\\nto the initial 20 percent drop caused by the market being flooded with oil [50]. \\r\\nAutoRegressive Distributed Lag (ARDL) estimation performed by the author demonstrates \\r\\nthat daily new infections have only a marginal impact, but a larger indirect impact is caused \\r\\ndue to the amplification of financial market volatility, falling in line with prediction made in \\r\\nthis paper. McKibbin and Fernando (2020) observe seven different scenarios in regard to the \\r\\nglobal macroeconomic impact of COVID-19, concluding that even a small, contained, impact \\r\\ncan have a large negative influence on the global markets [51]. Fernandes (2020) analyzes the \\r\\nreports from 30 countries under varying scenarios and concludes that the possible impact of \\r\\nCOVID-19 on the world economy is being underestimated, especially in heavily service \\r\\noriented countries [23]. Fernandes discusses that one of the possible problems is \\r\\nunderestimation of impact due to modeling based on previous SARS infections – showing the \\r\\nneed for newer, fast modeling techniques, which can, as shown in this and other papers be AI \\r\\nbased [52, 53].\\r\\n5. Conclusion\\r\\nThe goal of this research was to generate a forecasting model that integrates Stationary \\r\\nWavelet Transform and Bidirectional Long Short-Term Memory networks in order to predict \\r\\ncommodity and stock price movement during the COVID-19 pandemic. The results obtained \\r\\nusing proposed BDLSTM+WT-ADA configuration system show that, in addition to tradition \\r\\nstatistical models, Artificial Intelligence algorithms can be used to predict the movements of \\r\\nfinancial markets. The peculiarity of this paper is that information of COVID-19 confirmed \\r\\ncases is used as input data in parallel with three leading U.S. stock market indexes along with \\r\\nCrude Oil commodity. For the normal functioning of the global economy, it is very important \\r\\nthat Crude Oil has stable and secured delivery to the market. The global economy has been \\r\\nslowly recovering since the financial crisis of 2007-2008, but COVID-19 outbreak already \\r\\nshowed a huge impact on energy prices as well as stock market. Our proposed system shows a \\r\\ndecline in Crude Oil price. In addition to predicting future events through the methods that are \\r\\npresented, it is important to note that the geopolitical aspect is indirectly included in presented \\r\\nmodel through the input data. There-fore it is not possible to clearly define the impact of \\r\\ngeopolitical aspects in here presented model. It can be assumed that the geopolitical aspect in \\r\\nthis model is negligible, but it has a significant impact on the global economy.\\r\\nThe observed period used in analysis was marked by the extreme increase in oil stocks on the \\r\\nmarket. Due to this over-supply from the most important exporting countries (e.g. OPEC \\n21\\r\\ncountries) and geopolitical issues between the major players on the market, prices were \\r\\nconsequently slumping. Following the trends after the research was conducted, it is concluded \\r\\nthat despite the increase in the number of COVID-19 confirmed cases, the market is gradually \\r\\nadjusting oil prices due to the fact of joint agreement on production cuts (lowering the supply \\r\\nside) and on the other hand the gradual opening of markets and recovery of demand. The \\r\\nlogical consequence is the growing demand on a global level simultaneously followed by the \\r\\nimprovement of relations between oil exporters, which contributes to the temporary market \\r\\nstability.\\r\\nUnexpected situations such as a pandemic can have a significant effect on market \\r\\nfundamentals in the short term, and there have been correlations with indexes and oil. Due to \\r\\nfurther observation, in a period of several months and through the gradual opening of \\r\\neconomies, there is a stabilization of supply and demand, which has a positive effect on the \\r\\nformation of market equilibrium. The continued movement of stock indexes, especially this \\r\\npositive movement, does not reflect the real situation in the economy but is primarily based \\r\\non expectations and is further stimulated by monetary and fiscal incentives (e.g. cut of interest \\r\\nrates, reduction of taxes) from national governments.\\r\\nThe main contribution and novelty of the presented research is not only demonstrating the \\r\\nexistence of a link between the COVID-19 infections and commodity prices along with stock \\r\\nmarket prices but showing that modeling of the same can be achieved using data driven, \\r\\nartificial based, modeling methods.\\r\\nFuture work should use datasets with more data-points i.e. long time historical intraday data \\r\\nin order to achieve more precise forecasting. Also, apply more AI algorithms such as \\r\\nDynamic Programming (DP), Genetic Programming (GP) and combination of Convolutional \\r\\nNeural Networks (CNNs) with LSTM network in attempt to find more robust systems. The \\r\\nmain idea of using such algorithms will be to develop an advanced automatic forecasting \\r\\nsystem with capability of recognizing the positive correlation between financial markets.\\n22\\r\\nData Availability\\r\\nThis research uses a publicly available financial market data published by Yahoo finance \\r\\nwebsite and a publicly available dataset “2019 Novel Coronavirus Data Repository” \\r\\npublished by Johns Hopkins University Center for Systems Science and Engineering (JHU \\r\\nCSSE).\\r\\nConflicts of Interest\\r\\nThe authors declare that there is no conflict of interest regarding the publication of this paper.\\r\\nFunding Statement\\r\\nThis research has been (partly) supported by the CEEPUS network CIII-HR-0108, European \\r\\nRegional Development Fund under the grant KK.01.1.1.01.0009 (DATACROSS), project \\r\\nCEKOM under the grant KK.01.2.2.03.0004 and University of Rijeka scientific grant uniri\\x02tehnic-18-275-1447.\\r\\nReferences\\r\\n[1] World Health Organisation. What is a pandemic? (2010), obtained from: \\r\\nhttps://www.who.int/csr/disease/swineflu/frequently_asked_questions/pandemic/en/ \\r\\n[2] Morens, David M., Gregory K. Folkers, and Anthony S. Fauci. \"What is a pandemic?.\" \\r\\n(2009): 1018-1021.\\r\\n[3] Pine, Ray, and Bob McKercher. \"The impact of SARS on Hong Kong’s tourism \\r\\nindustry.\" International Journal of Contemporary Hospitality Management (2004).\\r\\n[4] Hunter, Murray. \"A Short History of Business and Entrepreneurable Evolution during \\r\\nthe 20th Century: Trends for the New Millenium.\" Geopolitics, History, and \\r\\nInternational Relations 5.1 (2013): 44-98.\\r\\n[5] Ozili, Peterson K., and Thankom Arun. \"Spillover of COVID-19: impact on the Global \\r\\nEconomy.\" Available at SSRN 3562570 (2020).\\n23\\r\\n[6] Lai, Rose Neng, and Yang Zhang. \"Spillover and Profitability of Intraday Herding on \\r\\nCross-Listed Stocks.\" The Chinese Economy 53.1 (2020): 25-61.\\r\\n[7] Huang, Wei, Yoshiteru Nakamori, and Shou-Yang Wang. \"Forecasting stock market \\r\\nmovement direction with support vector machine.\" Computers & operations \\r\\nresearch 32.10 (2005): 2513-2522.\\r\\n[8] Gao, Zhao. \"The application of artificial intelligence in stock investment.\" Journal of \\r\\nPhysics: Conference Series. Vol. 1453. 2020.\\r\\n[9] Jin, Long, et al. \"Robot manipulator control using neural networks: A survey.\" \\r\\nNeurocomputing 285 (2018): 23-34.\\r\\n[10] Lorencin, Ivan, et al. \"Genetic Algorithm Approach to Design of Multi-Layer \\r\\nPerceptron for Combined Cycle Power Plant Electrical Power Output \\r\\nEstimation.\" Energies 12.22 (2019): 4352.\\r\\n[11] Lorencin, Ivan, et al. \"Using multi-layer perceptron with Laplacian edge detector for \\r\\nbladder cancer diagnosis.\" Artificial Intelligence in Medicine 102 (2020): 101746.\\r\\n[12] Joo, Il-Taeck, and Seung-Ho Choi. \"Stock Prediction Model based on Bidirectional \\r\\nLSTM Recurrent Neural Network.\" The Journal of Korea Institute of Information, \\r\\nElectronics, and Communication Technology 11.2 (2018): 204-208.\\r\\n[13] Yulita, Intan Nurma, Mohamad Ivan Fanany, and Aniati Murni Arymuthy. \"Bi\\x02directional long short-term memory using quantized data of deep belief networks for \\r\\nsleep stage classification.\" Procedia computer science 116 (2017): 530-538.\\r\\n[14] Althelaya, Khaled A., El-Sayed M. El-Alfy, and Salahadin Mohammed. \"Evaluation \\r\\nof bidirectional lstm for short-and long-term stock market prediction.\" 2018 9th \\r\\ninternational conference on information and communication systems (ICICS). IEEE, \\r\\n2018.\\r\\n[15] Jia, Mingzhu, et al. \"Analysis and Research on Stock Price of LSTM and Bidirectional \\r\\nLSTM Neural Network.\" 3rd International Conference on Computer Engineering, \\r\\nInformation Science & Application Technology (ICCIA 2019). Atlantis Press, 2019.\\r\\n[16] Eapen, Jithin, Doina Bein, and Abhishek Verma. \"Novel deep learning model with \\r\\nCNN and bi-directional LSTM for improved stock market index prediction.\" 2019 \\r\\nIEEE 9th annual computing and communication workshop and conference (CCWC). \\r\\nIEEE, 2019.\\r\\n[17] Wang, X. H., Robert SH Istepanian, and Yong Hua Song. \"Microarray image \\r\\nenhancement by denoising using stationary wavelet transform.\" IEEE Transactions on \\r\\nNanobioscience 2.4 (2003): 184-189.\\n24\\r\\n[18] Bai, Yun, et al. \"Air pollutants concentrations forecasting using back propagation \\r\\nneural network based on wavelet decomposition with meteorological \\r\\nconditions.\" Atmospheric pollution research 7.3 (2016): 557-566.\\r\\n[19] Supratid, Siriporn, Thannob Aribarg, and Seree Supharatid. \"An integration of \\r\\nstationary wavelet transform and nonlinear autoregressive neural network with \\r\\nexogenous input for baseline and future forecasting of reservoir inflow.\" Water \\r\\nresources management 31.12 (2017): 4023-4043.\\r\\n[20] Wang, Gang-Jin, and Chi Xie. \"Cross-correlations between WTI Crude Oil market and \\r\\nUS stock market: A perspective from econophysics.\" Acta Physica Polonica B 43.10 \\r\\n(2012).\\r\\n[21] Yahoo Finance (7 April 2020). Obtained from: https://finance.yahoo.com/\\r\\n[22] Johns Hopkins CSSE (7 April 2020), \"Novel Coronavirus (COVID-19) Cases\", \\r\\nobtained from: https://github.com/CSSEGISandData/COVID-19\\r\\n[23] McKibbin, Warwick J., and Roshen Fernando. \"The global macroeconomic impacts of \\r\\nCOVID-19: Seven scenarios.\" (2020).\\r\\n[24] Zhang, Yue-Jun, and Jing Wang. \"Exploring the WTI crude oil price bubble process \\r\\nusing the Markov regime switching model.\" Physica A: Statistical Mechanics and its \\r\\nApplications 421 (2015): 377-387.\\r\\n[25] Chiroma, Haruna, et al. \"Bio-inspired algorithm optimization of neural network for the \\r\\nprediction of Dubai crude oil price.\" Proceedings of the International Conference on \\r\\nData Engineering 2015 (DaEng-2015). Springer, Singapore, 2019.\\r\\n[26] Khuntia, Sashikanta, and J. K. Pattanayak. \"Adaptive market hypothesis and evolving \\r\\npredictability of bitcoin.\" Economics Letters 167 (2018): 26-28.\\r\\n[27] Rioul, Olivier, and Martin Vetterli. \"Wavelets and signal processing.\" IEEE signal \\r\\nprocessing magazine 8.4 (1991): 14-38.\\r\\n[28] Samiee, Kaveh, Peter Kovacs, and Moncef Gabbouj. \"Epileptic seizure classification \\r\\nof EEG time-series using rational discrete short-time Fourier transform.\" IEEE \\r\\ntransactions on Biomedical Engineering 62.2 (2014): 541-552.\\r\\n[29] Tzanetakis, George, Georg Essl, and Perry Cook. \"Audio analysis using the discrete \\r\\nwavelet transform.\" Proc. Conf. in Acoustics and Music Theory Applications. Vol. 66. \\r\\n2001.\\r\\n[30] Bessec, Marie, and Julien Fouquau. \"Short-run electricity load forecasting with \\r\\ncombinations of stationary wavelet transforms.\" European Journal of Operational \\r\\nResearch 264.1 (2018): 149-164.\\n25\\r\\n[31] Bhati, Dinesh, et al. \"Design of time–frequency-localized two-band orthogonal \\r\\nwavelet filter banks.\" Circuits, Systems, and Signal Processing 37.8 (2018): 3295-\\r\\n3312.\\r\\n[32] Jarrah, Mutasem, and Naomie Salim. \"A recurrent neural network and a discrete \\r\\nwavelet transform to predict the Saudi stock price trends.\" International Journal of \\r\\nAdvanced Computer Science and Applications 10.4 (2019): 155-162.\\r\\n[33] Schuster, Mike, and Kuldip K. Paliwal. \"Bidirectional recurrent neural \\r\\nnetworks.\" IEEE transactions on Signal Processing 45.11 (1997): 2673-2681.\\r\\n[34] Liu, Yao, et al. \"Wind power short-term prediction based on LSTM and discrete \\r\\nwavelet transform.\" Applied Sciences 9.6 (2019): 1108.\\r\\n[35] Cui, Zhiyong, et al. \"Deep bidirectional and unidirectional LSTM recurrent neural \\r\\nnetwork for network-wide traffic speed prediction.\" arXiv preprint \\r\\narXiv:1801.02143 (2018).\\r\\n[36] Graves, Alex, Navdeep Jaitly, and Abdel-rahman Mohamed. \"Hybrid speech \\r\\nrecognition with deep bidirectional LSTM.\" 2013 IEEE workshop on automatic \\r\\nspeech recognition and understanding. IEEE, 2013.\\r\\n[37] Yildirim, Özal. \"A novel wavelet sequence based on deep bidirectional LSTM \\r\\nnetwork model for ECG signal classification.\" Computers in biology and medicine 96 \\r\\n(2018): 189-202.\\r\\n[38] Zhuge, Qun, Lingyu Xu, and Gaowei Zhang. \"LSTM Neural Network with Emotional \\r\\nAnalysis for Prediction of Stock Price.\" Engineering letters 25.2 (2017).\\r\\n[39] Gal, Yarin, and Zoubin Ghahramani. \"A theoretically grounded application of dropout \\r\\nin recurrent neural networks.\" Advances in neural information processing systems. \\r\\n2016.\\r\\n[40] Pontes, Fabrício José, et al. \"Design of experiments and focused grid search for neural \\r\\nnetwork parameter optimization.\" Neurocomputing 186 (2016): 22-34.\\r\\n[41] Buitinck, Lars, et al. \"API design for machine learning software: experiences from the \\r\\nscikit-learn project.\" arXiv preprint arXiv:1309.0238 (2013).\\r\\n[42] Brassington, Gary. \"Mean absolute error and root mean square error: which is the \\r\\nbetter metric for assessing model performance?.\" EGU General Assembly Conference \\r\\nAbstracts. Vol. 19. 2017.\\r\\n[43] Hsieh, Tsung-Jung, Hsiao-Fen Hsiao, and Wei-Chang Yeh. \"Forecasting stock \\r\\nmarkets using wavelet transforms and recurrent neural networks: An integrated system \\r\\nbased on artificial bee colony algorithm.\" Applied soft computing 11.2 (2011): 2510-\\r\\n2525.\\n26\\r\\n[44] Lahmiri, Salim. \"Wavelet low-and high-frequency components as features for \\r\\npredicting stock prices with backpropagation neural networks.\" Journal of King Saud \\r\\nUniversity-Computer and Information Sciences 26.2 (2014): 218-227.\\r\\n[45] Madhumathi, R. Derivatives and Risk Management. Pearson Education India, 2014.\\r\\n[46] Baker, Scott R., et al. Covid-induced economic uncertainty. No. w26983. National \\r\\nBureau of Economic Research, 2020.\\r\\n[47] Toda, Alexis Akira. \"Susceptible-infected-recovered (sir) dynamics of covid-19 and \\r\\neconomic impact.\" arXiv preprint arXiv:2003.11221 (2020).\\r\\n[48] Baldwin, Richard, and Eiichi Tomiura. \"Thinking ahead about the trade impact of \\r\\nCOVID-19.\" Economics in the Time of COVID-19 59 (2020).\\r\\n[49] Atkeson, Andrew. What will be the economic impact of covid-19 in the us? rough \\r\\nestimates of disease scenarios. No. w26867. National Bureau of Economic Research, \\r\\n2020.\\r\\n[50] Albulescu, Claudiu. \"Coronavirus and oil price crash.\" Available at SSRN 3553452 \\r\\n(2020).\\r\\n[51] Fernandes, Nuno. \"Economic effects of coronavirus outbreak (COVID-19) on the \\r\\nworld economy.\" Available at SSRN 3557504 (2020).\\r\\n[52] Car, Zlatan, et al. \"Modeling the Spread of COVID-19 Infection Using a Multilayer \\r\\nPerceptron.\" Computational and Mathematical Methods in Medicine 2020.\\r\\n[53] McCall, Becky. \"COVID-19 and artificial intelligence: protecting health-care workers \\r\\nand curbing the spread.\" The Lancet Digital Health 2.4 (2020): e166-e167.'},\n",
       " {'name': '2306.13776v1.pdf',\n",
       "  'content': 'Swin-Free: Achieving Better Cross-Window Attention and Efficiency with\\r\\nSize-varying Window\\r\\nJinkyu Koo, John Yang, Le An, Gwenaelle Cunha Sergio, and Su Inn Park\\r\\nNVIDIA, 2788 San Tomas Expy, Santa Clara, CA 95051\\r\\n{jinkyuk, johnyang, lean, gcunhasergio, joshp}@nvidia.com\\r\\nAbstract\\r\\nTransformer models have shown great potential in\\r\\ncomputer vision, following their success in language\\r\\ntasks. Swin Transformer is one of them that outperforms\\r\\nconvolution-based architectures in terms of accuracy, while\\r\\nimproving efficiency when compared to Vision Transformer\\r\\n(ViT) and its variants, which have quadratic complexity with\\r\\nrespect to the input size. Swin Transformer features shifting\\r\\nwindows that allows cross-window connection while lim\\x02iting self-attention computation to non-overlapping local\\r\\nwindows. However, shifting windows introduces memory\\r\\ncopy operations, which account for a significant portion of\\r\\nits runtime. To mitigate this issue, we propose Swin-Free in\\r\\nwhich we apply size-varying windows across stages, instead\\r\\nof shifting windows, to achieve cross-connection among lo\\x02cal windows. With this simple design change, Swin-Free\\r\\nruns faster than the Swin Transformer at inference with bet\\x02ter accuracy. Furthermore, we also propose a few of Swin\\x02Free variants that are faster than their Swin Transformer\\r\\ncounterparts.\\r\\n1. Introduction\\r\\nUntil recently, convolutional neural network (CNN) had\\r\\nbeen leading the remarkable innovations in computer vi\\x02sion tasks which had otherwise been considered too diffi\\x02cult in the past, such as autonomous driving [2–7]. How\\x02ever, the leading role of CNNs is recently being transferred\\r\\nto Transformer-based networks [1, 8, 9]. The Transformer\\r\\nmodel was first proposed for natural language processing\\r\\n(NLP) tasks, such as text classification and machine trans\\x02lation, and it has demonstrated great success [10–12]. Such\\r\\na breakthrough in the language domain has sparked great in\\x02terest in the computer vision community and recently lead\\r\\nto promising results on various tasks such as image classifi\\x02cation [1, 8] and semantic segmentation [13].\\r\\nThe key component in Transformer architecture is the\\r\\nself-attention module, which learns the relevance of one el\\x02ement to the other elements of a sequence. Unlike recur\\x02rent networks, such as LSTM [14], that can only attend\\r\\nto context within a limited scope, the self-attention mech\\x02anism explicitly models the interactions among all entities\\r\\nof a sequence. This allows Transformers to learn global\\r\\ncontext at once, resulting in their success in many applica\\x02tions [8, 12, 15]. A drawback is, however, that computation\\r\\ncomplexity of the self-attention increases quadratically with\\r\\nrespect to the length of an input sequence. This can be a crit\\x02ical problem especially in computer vision tasks, since the\\r\\nsequence length, often determined by the image resolution,\\r\\ncan be intractably large.\\r\\nSwin Transformer [1] mitigates the quadratic complexity\\r\\nissue by partitioning an image into non-overlapping win\\x02dows and computing self-attention within the local win\\x02dows. To bridge the non-overlapping windows, Swin Trans\\x02former features shifting the window partition between con\\x02secutive self-attention layers, providing cross-connections\\r\\namong local windows. While this design choice leads to\\r\\nimproved efficiency and accuracy, the operations for shift\\x02ing windows incur data movement in memory. In fact, as\\r\\nshown in Table 1, shifting windows account for about 8.7%\\r\\nof the total runtime for a Swin Transformer model, when\\r\\ninference is performed with NVIDIA TensorRT [16].\\r\\nTo mitigate this shortcoming of Swin Transformer, we\\r\\npropose Swin-Free, which does not shift local windows in\\r\\norder to reduce data movement. Instead, to achieve cross\\x02connection among non-overlapping windows, Swin-Free\\r\\nvaries the size of windows across different stages (see Table\\r\\n2). For example, Swin-Free may double the window size\\r\\nat a stage in order to model cross-attention among smaller\\r\\nlocal windows of the previous stage.\\r\\nExperimental results show that Swin-Free featuring the\\r\\nsize-varying windows reduces the model runtime signifi\\x02cantly as compared to Swin Transformer, mainly thanks\\r\\nto avoiding shifting windows and being able to leverage\\r\\nfaster matrix multiplication with larger inputs. Note that\\r\\non modern GPUs, efficient implementations of math op\\x02erations such as convolution with large kernels are widely\\r\\navailable. In Swin-Free, a larger portion of its runtime is\\r\\n1\\r\\narXiv:2306.13776v1 [cs.CV] 23 Jun 2023\\n(a) Conventional window-shifting block structures of Swin transformers [1]\\r\\n(b) Our proposed block structures with varying window sizes\\r\\nFigure 1. Comparison in functional blocks between Swin and Swin-Free. Note that in Swin-Free, shifting windows is removed and the\\r\\nsize of the local window varies across stages.\\r\\nTable 1. Operation profile of a Swin Transformer model (Swin-B)\\r\\non NVIDIA RTX 3080 GPU.\\r\\nOperation Percentage (%) in runtime\\r\\nTensorRT\\r\\n(FP16)\\r\\nPyTorch\\r\\n(FP32)\\r\\nShifting windows 8.74 4.39\\r\\nLayerNorm 10.11 9.63\\r\\nGELU 13.46 3.15\\r\\nspent on computation rather than memory copy, indicating\\r\\na better GPU utilization. At the same time, Swin-Free im\\x02proves the classification accuracy as well, implying that the\\r\\nsize-varying windows can provide better modeling power\\r\\nthan shifting windows with a constant window size.\\r\\nWe also propose several variants of Swin-Free that pri\\x02oritize latency over accuracy. In other words, with on par\\r\\naccuracy, a variant of Swin-Free is designed to be faster\\r\\nthan its Swin Transformer counterpart. In addition, we fur\\x02ther simplify Swin-Free with more efficient layers such as\\r\\nBatchNorm and ReLU, instead of more commonly used but\\r\\nexpensive LayerNorm and GELU layers, which also ac\\x02count for significant part of the runtime (see Table 1). With\\r\\nthose design elements, we were able to improve the latency\\r\\nby 19% compared to Swin-B. In addition, we also show that\\r\\nby utilizing the improved modeling power of Swin-Free, we\\r\\ncan further reduce the depth of our model. For example, a\\r\\nvariant of Swin-Free is faster than Swin by about 33% with\\x02out loss of accuracy (see Table 6).\\r\\n2. Related Work\\r\\nConvolutional Neural Network (CNN): Over the past\\r\\ndecade, CNNs have been the de facto standard in computer\\r\\nvision, and keep improving accuracy with innovations in ar\\x02chitecture design [2–5]. In parallel, a lot of efforts have also\\r\\nbeen made to reduce the complexity of CNN models for ef\\x02ficiency. Such directions include model compression, quan\\x02tization, and low cost operations such as depth-wise con\\x02volution [6, 7]. Although CNNs are still dominant in com\\x02puter vision tasks, many recent works have demonstrated\\r\\nthat Transformer-based models outperform the state-of-the\\x02art CNN-based models [1, 8, 9]. Arguably, we are about to\\r\\nsee a paradigm shift in computer vision from CNN to Trans\\x022\\nformer.\\r\\nTransformer Architectures: Introduced in a pioneer\\r\\nwork [17] for machine translation tasks, Transformers have\\r\\nbecome the state-of-the-art models for NLP tasks, replacing\\r\\nmost of the LSTM-based sequence-to-sequence approaches\\r\\n[10–12, 18, 19]. As opposed to recurrent networks that pro\\x02cess short-term context recursively, Transformer architec\\x02tures are based on the attention mechanism, which explic\\x02itly models the relative importance among all elements of a\\r\\nsequence, thereby learning sequence-wide relationships. In\\r\\nother words, Transformers process a sequence as a whole\\r\\nand recursion is totally avoided.\\r\\nTransformer in vision: With minimal vision-specific\\r\\nmodifications, ViT [8] applies the attention mechanism to\\r\\nimage classification tasks. As the counterpart of input token\\r\\nembeddings, ViT divides the images into patch embedding\\r\\nsequences and feeds them into a standard Transformer. ViT\\r\\noutperforms CNNs in image classifications, but it has been\\r\\noften reported to be difficult to train compared to CNNs.\\r\\nSince the computational complexity of the attention oper\\x02ation is quadratically proportional to the input size, ViT\\r\\nhas challenges to take high-resolution images in as inputs.\\r\\nOther Transformer-based vision models such as DETR [20]\\r\\nand SETR [21] also hold such a quadratic complexity issue.\\r\\n3. Preliminary: Swin Transformer\\r\\nSwin Transformer [1] leverages a multi-stage hierarchi\\x02cal architecture, where the input image is first divided into\\r\\nsmall-sized patches and feature maps are gradually merged\\r\\nwith neighboring patches along the stages. With these hi\\x02erarchical representations, Swin Transformer can easily be\\r\\napplied to dense prediction tasks such as object detection\\r\\nand segmentation. Swin Transformer achieves a linear com\\x02putational complexity by computing self-attention within\\r\\nnon-overlapping local windows. To capture interactions be\\x02tween local windows, the shifted window scheme that al\\x02ternates between two window configurations in consecutive\\r\\nTransformer blocks is employed.\\r\\nShifting windows plays a critical role in achieving Swin\\r\\nTransformer’s claimed accuracy, but also introduces a lot of\\r\\nmemory movements. As shown in Table 1, the shifting win\\x02dow operations in Swin-B (one of Swin Transformer vari\\x02ants) account for 8.7% of the total runtime with NVIDIA\\r\\nTensorRT (FP16 precision) and 4.4% with PyTorch (FP32\\r\\nprecision). This suggests that there is room for latency im\\x02provement if memory movements can be minimized.\\r\\nIn addition, LayerNorm and GELU used in Swin Tran\\x02former are also responsible for a significant portion of the\\r\\nruntime as shown in Table 1. Taking a look at those two op\\x02erations in ONNX representation [22] in Figure 2, a cascade\\r\\nof math operations can be identified to fulfill those two lay\\x02ers. Previous study has suggested that by strategically us\\x02ing BatchNorm and ReLU layers, the accuracy of a Trans-\\r\\n(a) LayerNorm (b) GELU\\r\\nFigure 2. Examples of ONNX representations of LayerNorm and\\r\\nGELU.\\r\\nformer model will not be degraded much [23]. In this paper,\\r\\nwe attempt to improve on top of Swin Transformer for both\\r\\naccuracy and runtime, and propose Swin-Free, which will\\r\\nbe explained in the following section.\\r\\n4. Method\\r\\n4.1. Overview of Swin-Free\\r\\nOur baseline architecture shown in Figure 3 is similar to\\r\\nSwin Transformer [1], except that it does not use the shifted\\r\\nwindows. The input image is first patchified. Each stage\\r\\napplies a number of Swin-style Transformer blocks for the\\r\\npatches, where the self-attention computation is done within\\r\\neach of non-overlapping local windows. Here, the local\\r\\nwindow operates on an M × M patch. Like in Swin Trans\\x02former, the number of patches are reduced by half at each\\r\\nstage by the patch merging layer. The only difference from\\r\\nSwin Transformer is that we do not shift the local windows.\\r\\nInstead, we choose to vary the size of the local window (i.e.,\\r\\nM) at each stage, which will be explained in more detail in\\r\\nSection 4.2.\\r\\nThe difference between Swin and Swin-Free for input\\r\\nsize 224 × 224 is summarized in Table 2. Note that in stage\\r\\n2 and 3, Swin-Free uses a larger window size than Swin,\\r\\nand therefore the number of non-overlapping windows in\\r\\n3\\nFigure 3. Overall architecture of Swin-Free.\\r\\nTable 2. Comparison between Swin and Swin-Free for the input\\r\\nsize 224×224. Here, P means the number of patches at the be\\x02ginning of a stage. The values of M and N denote the size of a\\r\\nlocal window and the number of non-overlapping windows in a\\r\\nstage, respectively. Note that Swin Transformer applies shifting\\r\\nwindows in every other Transformer block, while Swin-Free does\\r\\nnot shift windows.\\r\\nStage Swin Swin-Free\\r\\n1\\r\\nP = 56 × 56\\r\\nM = 7\\r\\nN = 64\\r\\nP = 56 × 56\\r\\nM = 7\\r\\nN = 64\\r\\n2\\r\\nP = 28 × 28\\r\\nM = 7\\r\\nN = 16\\r\\nP = 28 × 28\\r\\nM = 14\\r\\nN = 4\\r\\n3\\r\\nP = 14 × 14\\r\\nM = 7\\r\\nN = 4\\r\\nP = 14 × 14\\r\\nM = 14\\r\\nN = 1\\r\\n4\\r\\nP = 7 × 7\\r\\nM = 7\\r\\nN = 1\\r\\nP = 7 × 7\\r\\nM = 7\\r\\nN = 1\\r\\nSwin-Free is smaller at those stages than in Swin. Figure 1\\r\\nalso shows how Swin-Free is different from Swin in detail\\r\\nat the block level. In Swin-Free, shifting windows and its\\r\\nreverse operation used in Swin Transformer are removed,\\r\\nand the size of the window changes with each stage.\\r\\n4.2. Size-Varying Windows\\r\\nShifting the local windows in Swin Transformer is an ef\\x02fective way to achieve cross-connection among windows,\\r\\nbut it requires moving data in memory. This is typically\\r\\nmore costly than math computation on GPUs, and can there\\x02fore negatively impact the model efficiency. In fact, as\\r\\nshown in Table 1, shifting windows takes a considerable\\r\\nportion of the total runtime.\\r\\nTo avoid using the shifted windows, we enable cross\\x02connection between non-overlapping windows by changing\\r\\nthe size of the local windows at each stage. Recall that M is\\r\\nthe size of the local window. As Table 2 shows, in our im\\x02plementations for the input size 224×224, we vary the value\\r\\nof M as M = 7, 14, 14, 7 for the four stages. From this\\r\\nsetup, we consider the cross-connection among four neigh\\x02boring 7×7 local windows at stages 2 and 3, i.e., a 14×14\\r\\nlocal window in the current stage effectively includes four\\r\\nof 7×7 local windows in the previous stage.\\r\\nThe above changes may increase GPU computation load\\r\\nof a single local window due to the enlarged window size in\\r\\nthe attention block. However, note in Table 2 that the num\\x02ber of non-overlapping local windows (i.e., N) in stages 2\\r\\nand 3 of Swin-Free becomes one fourth of that in Swin. In\\r\\nother words, in the matrix multiplication of Swin-Free, the\\r\\nmatrices’ size is larger, but the number of matrices to be\\r\\nprocessed is smaller. We have observed that processing a\\r\\n14×14 local window does not increase the latency as com\\x02pared to processing four of 7×7 local windows on GPU, but\\r\\nrather decreased the latency, thanks to their massive paral\\x02lel computing capability. We will discuss this point in more\\r\\ndetail in Section 5.\\r\\n4.3. Further Optimization\\r\\nReplacement of LayerNorm and GELU: As shown in\\r\\nFigure 2, LayerNorm and GELU are composed of multiple\\r\\nmath layers, which require more computation as compared\\r\\nto the commonly used BatchNorm and ReLU layers. In Ta\\x02ble 1, it is observed that LayerNorm and GELU account for\\r\\nabout 24% of the total runtime of a Swin Transformer model\\r\\nwhen running with TensorRT. Thus, when the latency is also\\r\\ncritical in an application, we replace them with BatchNorm\\r\\nand ReLU without significant accuracy degradation [23]. It\\r\\ncan be seen in Section 5 that such modification allows Swin\\x02Free to run even faster while still surpassing Swin Trans\\x02former in terms of accuracy.\\r\\nDepth reduction: Another way to prioritize latency is\\r\\nto reduce the depth of a model. Specifically, we consider\\r\\nreducing the number of Transformer blocks at stage 3. For\\r\\nexample, compared to Swin-B, where stage 3 consists of 18\\r\\nTransformer blocks, we may consider using 14 blocks only.\\r\\nWe will see in Section 5 that this variant of Swin-Free can\\r\\nstill achieve better accuracy than Swin Transformer with\\r\\nsignificant improvement in latency.\\r\\n4\\n5. Experiments\\r\\nOur focus of experiments is to compare Swin-Free with\\r\\nSwin Transformer in terms of both latency and accuracy\\r\\nin classification tasks. All latency results are measured us\\x02ing NVIDIA RTX 3080, PyTorch 1.13, and TensorRT 8.5.3\\r\\nwith CUDA 11.8. Evaluations are done with the ImageNet\\r\\ndataset [24] with 1K classes and input shape 224×224. We\\r\\nconsider the same variant models as in Swin Transformer,\\r\\nshown in Table 3a. Note that we do not consider the Large\\r\\n(L) variant with embedding dimension 192 used in Swin,\\r\\nsince it requires what is called the fall11 version of the 22K\\x02class dataset that is no longer available. Like Swin-B, we\\r\\nadd a post-fix to a model name to indicate its variant (e.g.,\\r\\nSwin-Free-B). Additionally, we also consider other variants\\r\\nresulting from the modification in Table 3b, mentioned in\\r\\nSection 4.3. These additional optimizations enhance the la\\x02tency of a model, possibly at the cost of reduced accuracy.\\r\\nThe abbreviated symbols of these variants (i.e., BR or DRx)\\r\\nare also added as a post-fix to a model name.\\r\\n5.1. Shifted windows of Swin\\r\\nBefore going into the evaluation of Swin-Free, we first\\r\\nwant to understand the importance of the shifted window\\r\\nin each stage of Swin-B. Table 4 shows the top-1 accuracy\\r\\nof Swin-B depending on which stage has shifting windows\\r\\nenabled or disabled. Note that Case 1 uses the shifted win\\x02dows for all stages, and thus it is exactly the same as Swin\\x02B.1 We can first see from Case 8 that without the shifted\\r\\nwindows, it is even difficult to successfully complete train\\x02ing, and thus the shifted windows is indeed critical in Swin.\\r\\nWe can also see from Cases 4 to 7 that stage 3 is a critical\\r\\nstage to use the shifted windows. This is, to some extent, not\\r\\nsurprising, since stage 3 is a dominant portion of Swin-B.\\r\\nHowever, we can also see from Cases 1 to 3 that selectively\\r\\nusing the shifted windows over each stage marginally helps\\r\\nin increasing accuracy. Thus, it is important to apply them\\r\\nto all stages of Swin-B.\\r\\n5.2. Windows size of Swin-Free\\r\\nIn this section, we show which window size configura\\x02tions are better suited for each stage of Swin-Free. To en\\x02sure fair comparison with Swin, we assume that the input\\r\\nsize is 224×224 and the smallest windows size is 7. For\\r\\nthis reason, there are only two options for the window size\\r\\nat stages 1 to 3, which are 7 and 14, whereas stage 4 should\\r\\nalways have 7 as the window size. With that in mind, Table\\r\\n5 shows the latency and accuracy for all possible configu\\x02rations that we can have from Swin-B with no shifted win\\x02dows. It is worth mentioning that Case 1, with configuration\\r\\n1Even though we used the same training configuration, our Swin-B’s\\r\\ntop-1 accuracy, trained from scratch, is 83.4%, which is slightly lower than\\r\\n83.5% reported in [1].\\r\\n‘7, 7, 7, 7’, is the same as Swin-B without shifted windows,\\r\\nwhich is the same as Case 8 of Table 4.\\r\\nWe can first notice from Cases 2 to 4 in Table 5 that the\\r\\nmost effective stage to use 14 as the window size is stage\\r\\n3. Increasing the window size to 14 at stage 3 leads to the\\r\\nbest latency and accuracy compared to using the window\\r\\nsize of 14 at stage 1 or 2. This would again come from the\\r\\nfact that the stage 3 is the dominant part of Swin-B in terms\\r\\nof depth. Using a 14×14 local window at stage 3, we take\\r\\ncross-connection into account among four neighboring 7×7\\r\\nlocal windows at stage 2. Note that using the larger window\\r\\nsize means that we need to handle larger-kernel matrix mul\\x02tiplications, but the number of such matrix multiplications\\r\\n(i.e., the number of non-overlapping windows) gets smaller\\r\\n(refer to Table 2). Comparing latency results between Cases\\r\\n1 and 2, this rather helps reducing the latency. We may\\r\\nclaim the same improvement in latency at stage 1 or 2 by\\r\\nusing a window size of 14, but considering that those stages\\r\\nare of only depth two, we could not observe meaningful\\r\\nspeed-up there. See that Cases 3 and 4 get the same latency\\r\\nas Case 1 up to the first decimal point.\\r\\nIn Cases 5 to 7, we use the 14×14 local window at two\\r\\nstages at the same time. We see that not using the 14×14\\r\\nlocal window at stage 3 degrades both accuracy and latency,\\r\\nemphasizing the importance of stage 3 once again. We can\\r\\nalso see from Cases 5 and 6 that using the 14×14 local\\r\\nwindow at stage 1 or 2 in addition to stage 3 meaningfully\\r\\nimproves latency over Case 2, resulting in them being the\\r\\nfastest variants.\\r\\nLooking at Case 8, using a window size of 14 at stages 1\\r\\nto 3 does not further improve the latency over Case 5 or 6.\\r\\nThe accuracy rather slightly decreases. The reason may be\\r\\nthat the modeling of cross-window connection is less effec\\x02tive at early stages. From this study, we chose the configu\\x02ration of Swin-Free as Case 5 (as shown in Table 2), which\\r\\nwas one of the best ones in both accuracy and latency.\\r\\n5.3. Comparison between Swin and Swin-Free\\r\\nTable 6 lists all variants of Swin-Free and some of Swin\\r\\nfamily that we trained from scratch. First, from Cases 1\\r\\nand 6, we can compare Swin-Free with Swin for the Base\\r\\n(B) variant. Although Swin-Free-B has more FLOPs and\\r\\nparameters than Swin-B, we can see that Swin-Free-B is\\r\\nfaster than Swin-B at inference using either PyTorch (12.6\\r\\nms vs. 14.3 ms) or TensorRT (2.0ms vs. 2.1ms). From the\\r\\nstudy in Table 5, we understand this happens because Swin\\x02Free-B has a smaller number of non-overlapping windows\\r\\nat stages 2 and 3, although each window is larger in Swin\\x02Free-B.\\r\\nWe can also note that Swin-Free-B achieves better ac\\x02curacy than Swin-B. This implies that even without us\\x02ing the shifted windows, changing the size of the local\\r\\nwindow at certain stages can well model cross-connection\\r\\n5\\nTable 3. Model variants: (a) We consider variants by changing hyper-parameters of a given architecture. (b) We apply architectural\\r\\nmodification to a given model. The abbreviated symbol of each variant is added to a model name as a postfix.\\r\\n(a) Variants by hyper-parameters.\\r\\nVariant Embedding dimension per patch # of blocks at a stage (depth)\\r\\nTiny (T) 96 {2,2,6,2}\\r\\nSmall (S) 96 {2,2,18,2}\\r\\nBase (B) 128 {2,2,18,2}\\r\\n(b) Variants by modification.\\r\\nVariant Modification\\r\\nBatchNorm/ReLU (BR) Replace LayerNorm with BatchNorm and GELU with ReLU.\\r\\nDepth reduction to x (DRx) Reduce the number of Transformer blocks at stage 3 to x.\\r\\nTable 4. Turning on/off shifting windows in Swin-B at each stage:\\r\\n1 means ‘on’. For example, ‘1, 1, 1, 1’ implies that all stages\\r\\nuse the shifted windows, meaning exactly Swin-B. The symbol ‘-’\\r\\nmeans that training could not finish successfully (i.e., diverged).\\r\\nCase On/off on cyclic shift Top-1 accuracy (%)\\r\\n1 1, 1, 1, 1 83.4\\r\\n2 0, 1, 1, 1 82.3\\r\\n3 0, 0, 1, 1 82.3\\r\\n4 0, 0, 0, 1 -\\r\\n5 0, 0, 1, 0 82.2\\r\\n6 0, 1, 0, 0 -\\r\\n7 1, 0, 0, 0 -\\r\\n8 0, 0, 0, 0 -\\r\\namong neighboring windows. Consistently, Swin-Free-T\\r\\nand Swin-Free-S in Cases 5 and 6 also achieve better ac\\x02curacy than Swin’s corresponding variants (not shown here;\\r\\nRefer to [1]).\\r\\nWe also observed that for an input size of 224×224,\\r\\nSwinV2-B [9] gets the same accuracy as Swin-Free-B,\\r\\nbut its latency is significantly slower. Thus, for latency\\x02critical applications, Swin-Free would be a better choice\\r\\nthan SwinV2.\\r\\n5.4. BatchNorm/ReLU (BR) variants\\r\\nReplacing LayerNorm and GELU in Swin-Free with\\r\\nBatchNorm and ReLU, respectively, we get the variants in\\r\\nCases 7 to 9 in Table 6. We first notice that the accuracy\\r\\ndegradation that occurs with these replacements is trivial.\\r\\nNamely, only Swin-Free-B-BR has slightly lower accuracy\\r\\nthan Swin-Free-B, while others hold the same accuracy as\\r\\ntheir corresponding models. In regards to latency, BR vari\\x02ants achieve meaningful speed gain in TensorRT, although\\r\\nnot in Pytorch. Nonetheless, considering that TensorRT is\\r\\na de facto standard for deploying a deep learning model,\\r\\nBR variants would be good alternatives in case of latency\\x02critical applications. It is also worth noting from Case 2\\r\\nthat simply applying BR modification to the original Swin\\x02B does not yield similar accuracy or latency as compared to\\r\\nSwin-Free-B-BR.\\r\\n5.5. Depth reduction (DRx) variants\\r\\nCases 10 to 13 in Table 6 show the DRx variants of Swin\\x02Free-B. Not to mention, D10, D12, D14, and D16 variants\\r\\nof Swin-Free-B reduce FLOPs and the number of parame\\x02ters, thereby improving the latency from Swin-Free-B. See\\r\\nthat in Case 11, Swin-Free-B-DR12 has even lower FLOPs\\r\\nthan Swin-B and its TensorRT runtime is reduced from 2.0\\r\\nms to 1.5 ms when compared to Swin-Free-B. In regards to\\r\\naccuracy, we can see that it stays the same as Swin-Free-B.\\r\\nThis implies that with our size-varying window, we may not\\r\\nneed such deep depth of Swin at stage 3.\\r\\nFrom Cases 14 to 16, we can also see that the combi\\x02nation of BR and DRx can still result in superior accuracy\\r\\ncompared to Swin-B, while improving latency further. For\\r\\nexample, Swin-Free-B-BR-DR14 has an accuracy of 83.7%\\r\\nand latency of 1.4 ms, compared to 83.4% and 2.1 ms from\\r\\nSwin-B. Note in Cases 1 and 14 that by sacrificing a little\\r\\nbit of accuracy (from 83.4% to 83.3%), Swin-Free-B-BR\\x02DR12 can achieve significant reduction in latency (from 2.1\\r\\nms to 1.3 ms, which is about 38% reduction from Swin-B).\\r\\nThese kinds of Swin-Free variants could be attractive alter\\x02natives for Swin in situations where latency is more impor\\x02tant than accuracy.\\r\\n6. Conclusion\\r\\nThis paper presents Swin-Free, which attempts to im\\x02prove latency over Swin Transformer by reducing memory\\r\\ntraffic incurred by shifted window scheme. Instead, Swin\\x02Free varies the size of windows over stages, which mimics\\r\\nthe mechanism of the shifted windows. This simple tech\\x02nique is shown to offer reduced latency and better accuracy\\r\\ncompared to its Swin counterpart. We also show that fur\\x026\\nTable 5. Latency and accuracy according to the variation in window size at each stage of Swin-B without using cyclic shift. For example,\\r\\n‘7, 7, 14, 7’ means that stage 3 uses 14 as the window size, while stages 1, 2, and 4 use 7. The symbol ‘-’ means that training could not\\r\\nfinish successfully (i.e., diverged).\\r\\nCase Window size at a stage Top-1 accuracy (%) Latency in PyTorch (FP32) (ms)\\r\\n1 7, 7, 7, 7 - 13.7\\r\\n2 7, 7, 14, 7 83.8 12.7\\r\\n3 7, 14, 7, 7 81.1 13.7\\r\\n4 14, 7, 7, 7 81.2 13.7\\r\\n5 7, 14, 14, 7 83.8 12.6\\r\\n6 14, 7, 14, 7 83.8 12.6\\r\\n7 14, 14, 7, 7 81.2 13.8\\r\\n8 14, 14, 14, 7 83.7 12.6\\r\\nTable 6. Models trained with ImageNet-1K from scratch. FLOP and parameter counts are measured by [25]. SwinV2 did not work with\\r\\nthis tool so we mark it with ‘-’ here.\\r\\nCase Model FLOPs # of parameters Top-1 accuracy (%) Latency (ms)\\r\\nTensorRT (FP16) PyTorch (FP32)\\r\\n1 Swin-B 15.9G 88.7M 83.4 2.1 14.3\\r\\n2 Swin-B-BR 15.6G 88.7M 83.2 1.8 15.3\\r\\n3 SwinV2-B - - 83.8 3.5 21.5\\r\\n4 Swin-Free-B 16.8G 99.4M 83.8 2.0 12.6\\r\\n5 Swin-Free-T 5.0G 31.6M 82.1 0.9 6.7\\r\\n6 Swin-Free-S 9.7G 58.3M 83.6 1.7 12.6\\r\\n7 Swin-Free-T-BR 4.8G 31.6M 82.1 0.8 7.0\\r\\n8 Swin-Free-S-BR 9.5G 58.3M 83.6 1.4 13.2\\r\\n9 Swin-Free-B-BR 16.4G 99.4M 83.7 1.7 13.2\\r\\n10 Swin-Free-B-DR10 11.3G 69.3M 83.5 1.4 9.3\\r\\n11 Swin-Free-B-DR12 12.7G 76.8M 83.8 1.5 9.7\\r\\n12 Swin-Free-B-DR14 14.0G 84.4M 83.8 1.7 10.7\\r\\n13 Swin-Free-B-DR16 15.4G 91.9M 83.8 1.9 11.6\\r\\n14 Swin-Free-B-BR-DR12 12.4G 76.9M 83.3 1.3 10.1\\r\\n15 Swin-Free-B-BR-DR14 13.7G 84.4M 83.7 1.4 11.2\\r\\n16 Swin-Free-B-BR-DR16 15.1G 91.9M 83.8 1.6 12.2\\r\\nther speedup can be achieved by using simpler operations\\r\\nand shallower blocks without accuracy loss. Therefore, the\\r\\nproposed model is particularly suitable for deployment in\\r\\nproduction with improved efficiency.\\r\\nIn future work, we plan on applying Swin-Free to other\\r\\nvision tasks such as object detection and semantic segmen\\x02tation with larger input resolution. More optimizations,\\r\\nsuch as dynamic window size across different stages, will\\r\\nalso be investigated to further improve GPU utilization for\\r\\ninference.\\r\\nReferences\\r\\n[1] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\\r\\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans\\x02former: Hierarchical vision transformer using shifted win\\x02dows. In Proceedings of the IEEE/CVF International Con\\x02ference on Computer Vision (ICCV), pages 10012–10022,\\r\\nOctober 2021. 1, 2, 3, 5, 6\\r\\n[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\r\\nImagenet classification with deep convolutional neural net\\x02works. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein\\x02berger, editors, Advances in Neural Information Processing\\r\\nSystems, volume 25. Curran Associates, Inc., 2012. 1, 2\\r\\n[3] Karen Simonyan and Andrew Zisserman. Very deep convo\\x02lutional networks for large-scale image recognition. CoRR,\\r\\nabs/1409.1556, 2014. 1, 2\\r\\n[4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\r\\nScott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\r\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\r\\nconvolutions. CoRR, abs/1409.4842, 2014. 1, 2\\r\\n7\\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\r\\nSun. Deep residual learning for image recognition. CoRR,\\r\\nabs/1512.03385, 2015. 1, 2\\r\\n[6] Song Han, Huizi Mao, and William J. Dally. Deep com\\x02pression: Compressing deep neural network with pruning,\\r\\ntrained quantization and huffman coding. In Yoshua Ben\\x02gio and Yann LeCun, editors, 4th International Conference\\r\\non Learning Representations, ICLR 2016, San Juan, Puerto\\r\\nRico, May 2-4, 2016, Conference Track Proceedings, 2016.\\r\\n1, 2\\r\\n[7] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\\r\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An\\x02dreetto, and Hartwig Adam. Mobilenets: Efficient convolu\\x02tional neural networks for mobile vision applications. CoRR,\\r\\nabs/1704.04861, 2017. 1, 2\\r\\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\r\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\r\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl\\x02vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\r\\nworth 16x16 words: Transformers for image recognition at\\r\\nscale. In International Conference on Learning Representa\\x02tions, 2021. 1, 2, 3\\r\\n[9] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\r\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu\\r\\nWei, and Baining Guo. Swin transformer V2: scaling up\\r\\ncapacity and resolution. CoRR, abs/2111.09883, 2021. 1, 2,\\r\\n6\\r\\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\r\\nToutanova. BERT: pre-training of deep bidirectional trans\\x02formers for language understanding. CoRR, abs/1810.04805,\\r\\n2018. 1, 3\\r\\n[11] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\\r\\nSutskever. Improving language understanding by generative\\r\\npre-training. 2018. 1, 3\\r\\n[12] Tom B. Brown et al. Language models are few-shot learners.\\r\\nCoRR, abs/2005.14165, 2020. 1, 3\\r\\n[13] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\\r\\nJose M. Alvarez, and Ping Luo. SegFormer: Simple and\\r\\nefficient design for semantic segmentation with transformers.\\r\\nIn Advances in Neural Information Processing Systems 34\\r\\npre-proceedings (NeurIPS), 2021. 1\\r\\n[14] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term ¨\\r\\nmemory. Neural Computation, 9(8):1735–1780, 1997. 1\\r\\n[15] Aakanksha Chowdhery et al. Palm: Scaling language mod\\x02eling with pathways, 2022. 1\\r\\n[16] NVIDIA TensorRT. https://developer.nvidia.com/tensorrt. 1\\r\\n[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko\\x02reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\r\\nPolosukhin. Attention is all you need. In Advances in neural\\r\\ninformation processing systems, pages 5998–6008, 2017. 3\\r\\n[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\\r\\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\r\\nPeter J. Liu. Exploring the limits of transfer learning with\\r\\na unified text-to-text transformer. CoRR, abs/1910.10683,\\r\\n2019. 3\\r\\n[19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell,\\r\\nQuoc V. Le, and Ruslan Salakhutdinov. Transformer-xl:\\r\\nAttentive language models beyond a fixed-length context.\\r\\nCoRR, abs/1901.02860, 2019. 3\\r\\n[20] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico\\x02las Usunier, Alexander Kirillov, and Sergey Zagoruyko.\\r\\nEnd-to-end object detection with transformers. CoRR,\\r\\nabs/2005.12872, 2020. 3\\r\\n[21] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\\r\\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\\r\\nXiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic\\r\\nsegmentation from a sequence-to-sequence perspective with\\r\\ntransformers. CoRR, abs/2012.15840, 2020. 3\\r\\n[22] Junjie Bai, Fang Lu, Ke Zhang, et al. ONNX: Open neu\\x02ral network exchange. https://github.com/onnx/\\r\\nonnx, 2019. 3\\r\\n[23] John Yang, Le An, Anurag Dixit, Jinkyu Koo, and Su Inn\\r\\nPark. Depth estimation with simplified transformer, 2022. 3,\\r\\n4\\r\\n[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\r\\ndatabase. In 2009 IEEE conference on computer vision and\\r\\npattern recognition, pages 248–255, 2009. 5\\r\\n[25] ThanatosShinji. onnx-tool. https://github.com/\\r\\nThanatosShinji/onnx-tool, 2023. 7\\r\\n8'},\n",
       " {'name': '1802.07007v3.pdf',\n",
       "  'content': 'Abstract— Traffic forecasting is a particularly challenging \\r\\napplication of spatiotemporal forecasting, due to the time-varying \\r\\ntraffic patterns and the complicated spatial dependencies on road\\r\\nnetworks. To address this challenge, we learn the traffic network \\r\\nas a graph and propose a novel deep learning framework, Traffic \\r\\nGraph Convolutional Long Short-Term Memory Neural Network \\r\\n(TGC-LSTM), to learn the interactions between roadways in the \\r\\ntraffic network and forecast the network-wide traffic state. We \\r\\ndefine the traffic graph convolution based on the physical network \\r\\ntopology. The relationship between the proposed traffic graph \\r\\nconvolution and the spectral graph convolution is also discussed. \\r\\nAn L1-norm on graph convolution weights and an L2-norm on \\r\\ngraph convolution features are added to the model’s loss function \\r\\nto enhance the interpretability of the proposed model. \\r\\nExperimental results show that the proposed model outperforms \\r\\nbaseline methods on two real-world traffic state datasets. The \\r\\nvisualization of the graph convolution weights indicates that the \\r\\nproposed framework can recognize the most influential road\\r\\nsegments in real-world traffic networks.\\r\\nIndex Terms— Traffic forecasting, Spatial-temporal, Graph \\r\\nconvolution, LSTM, Recurrent neural network\\r\\nI. INTRODUCTION\\r\\nRAFFIC forecasting is one of the most challenging \\r\\ncomponents of Intelligent Transportation Systems (ITS). \\r\\nThe goal of traffic forecasting is to predict future traffic states \\r\\nin the traffic network given a sequence of historical traffic states \\r\\nand the physical roadway network. Since the volume and \\r\\nvariety of traffic data has been increasing in recent years, data\\x02driven traffic forecasting methods have shown considerable \\r\\npromise in their ability to outperform conventional and \\r\\nsimulation-based methods [1]. \\r\\nPrevious work [2][3][4][5] on this topic roughly categorizes \\r\\nexisting models into two categories: classical statistical \\r\\nmethods and machine learning models. Most of the studies\\r\\nfocusing on traffic forecasting using statistical methods were\\r\\ndeveloped when traffic systems were less complex, and the \\r\\nsizes of traffic datasets were relatively small. However, \\r\\nstatistical models’ capability of handling high dimensional time \\r\\nseries data is quite limited. With the more recent rapid \\r\\nZ. Cui, R. Ke, Z. Pu, and Y. Wang are with the Department of Civil and\\r\\nEnvironmental Engineering, University of Washington, Seattle WA 98195,\\r\\nUSA (e-mail: zhiyongc@uw.edu; ker27@uw.edu; ziyuanpu@uw.edu;\\r\\nyinhai@uw.edu)\\r\\ndevelopment in computational power, as well as growth in \\r\\ntraffic data volume, much of the more recent work on this topic \\r\\nfocuses on machine learning methods for traffic forecasting. \\r\\nMachine learning methods with the capability of capturing \\r\\ncomplex non-linear relationships, like support vector regression \\r\\n(SVR) [6], tend to outperform the statistical methods, such as \\r\\nautoregressive integrated moving average (ARIMA) [7] and its \\r\\nvariants, with respect to handling complex traffic forecasting \\r\\nproblems [8]. However, the full potential of artificial \\r\\nintelligence approaches to traffic forecasting was not exploited \\r\\nuntil the rise of deep neural network (NN) models (also referred \\r\\nto as deep learning models). Following early works [2], [9]\\r\\napplying NNs to the traffic prediction problem, many NN-based \\r\\nmethods have been adopted for traffic forecasting.\\r\\nDeep learning models for traffic forecasting, such as deep \\r\\nbelief networks (DBN) [10] and stacked auto-encoders [11], \\r\\ncan effectively learn high dimensional features and achieve \\r\\ngood forecasting performance. Recurrent neural network \\r\\n(RNN) and its variants, including long short-term memory \\r\\n(LSTM) [12] and gated recurrent unit (GRU) [13] networks,\\r\\nhave also shown great potential for solving traffic forecasting\\r\\nproblems [8], [14], [15], [16]. Although RNN-based methods \\r\\ncan learn the spatial dependencies, they tend to be over\\x02complex and inevitably capture a certain amount of noise and \\r\\nspurious relationships which likely do not represent the true \\r\\ncausal structure in a physical traffic network. Moreover, \\r\\ninterpreting the network parameters in terms of real-world \\r\\nspatial dependencies is most often impossible. To address this, \\r\\nother works [5], [17], [18] attempt to model spatial \\r\\ndependencies with convolutional neural network (CNN). \\r\\nHowever, conventional CNNs are most appropriate for spatial \\r\\nrelationships in the Euclidean space as represented by two\\x02dimensional (2D) matrices or images. Thus, spatial features \\r\\nlearned in CNN are not optimal for representing the traffic \\r\\nnetwork structure [19][20].\\r\\nRecently, substantial research has focused on extending the \\r\\nconvolution operator to more general, graph-structured data, \\r\\nwhich can be applied to capture the spatial relationships present \\r\\nin a traffic network. There are two primary ways to conduct\\r\\ngraph convolution. The first class of methods [21], [22], [23], \\r\\nK. Henrickson is with the INRIX, Inc., Kirkland WA 98033, USA (e-mail:\\r\\nKristian.Henrickson@inrix.com)\\r\\nTraffic Graph Convolutional Recurrent Neural \\r\\nNetwork: A Deep Learning Framework for \\r\\nNetwork-Scale Traffic Learning and Forecasting\\r\\nZhiyong Cui, Kristian Henrickson, Ruimin Ke, Ziyuan Pu, and Yinhai Wang*\\r\\nT\\n[24] makes use of spectral graph theory, by designing spectral \\r\\nfilter/convolutions based on the graph Laplacian matrix. \\r\\nSpectral-based graph convolution has been adopted and \\r\\ncombined with RNN [20] and CNN [1] to forecast traffic states. \\r\\nThese models successfully apply convolution to graph\\x02structured data, but they do not fully capture the unique \\r\\nproperties of graphs [25], like traffic networks. These models \\r\\n[23], [26] usually adopt multiple graph convolution layers, and \\r\\nthus, their learned spatial dependencies are hard to interpret.\\r\\nThe other form of graph convolution proposed in several newly\\x02published studies is conducted on graph data dynamically, for \\r\\nexample, the dynamic edge-conditioned filters in graph \\r\\nconvolution [27], the high-order adaptive graph convolutional \\r\\nnetwork [25][28]. Still, these methods are not capable of fully \\r\\naccommodating the physical specialties of traffic networks.\\r\\nOne of the deficiencies of the previous graph convolution\\x02based models is that the receptive field of the convolution \\r\\noperators is not confined in the graph according to the real \\r\\nstructure of the traffic network. The traffic states of two \\r\\nlocations far apart from each other in the traffic network should\\r\\nnot be influenced by each other in a short time period. Though \\r\\nthe spectral graph convolution models [20],[23] can capture \\r\\nfeatures from K-localized neighbors of a vertex in the graph, \\r\\nhow to choose the value of K and whether the localized \\r\\nneighbors truly affect the vertex are still questions to be \\r\\nanswered. Thus, we propose a free-flow reachable matrix based \\r\\non the free-flow speed of the real traffic and apply it on the \\r\\ngraph convolution operator to learn features from the truly \\r\\ninfluential neighborhood in the traffic network. \\r\\nIn this study, we learn the traffic network as a graph and \\r\\nconduct convolution on the traffic network-based graph. To \\r\\nlearn localized features and incorporate roadway physical \\r\\ncharacteristics, we proposed a traffic graph convolution \\r\\noperator. Base on this operator, we propose a traffic graph \\r\\nconvolutional LSTM (TGC-LSTM) to model the dynamics of \\r\\nthe traffic flow and capture the spatial dependencies. Evaluation \\r\\nresults show that the proposed TGC-LSTM outperforms \\r\\nmultiple state-of-the-art traffic forecasting baselines. More \\r\\nimportantly, the proposed model turns out to be capable of \\r\\nidentifying the most influential roadway segments in the real\\x02world traffic networks. The main contributions of our work \\r\\ninclude:\\r\\n1. A traffic graph convolution operator is proposed to \\r\\naccommodate physical specialties of traffic networks \\r\\nand extract comprehensive features.\\r\\n2. A traffic graph convolutional LSTM neural network is \\r\\nproposed to learn the complex spatial and dynamic \\r\\ntemporal dependencies presented in traffic data.\\r\\n3. To make learned localized graph convolution features \\r\\nmore consistent and interpretable, we proposed two \\r\\nregularization terms, including an L1-norm on traffic the \\r\\ngraph convolution weights and an L2-norm on the traffic \\r\\ngraph convolution features, that can be optionally added \\r\\nto the model’s loss function.\\r\\n1\\r\\nhttps://github.com/zhiyongc/Seattle-Loop-Data\\r\\n4. The real-world traffic speed data, including the graph \\r\\nstructure of the traffic network, used in this study is \\r\\npublished via a publicly available website1to facilitate \\r\\nfurther research on this problem.\\r\\nII. LITERATURE REVIEW\\r\\nA. Deep Learning based Traffic Forecasting\\r\\nDeep learning models have shown their superior capabilities of \\r\\ncapturing nonlinear spatiotemporal effects for traffic \\r\\nforecasting [29]. Ever since the precursory study [30] using the \\r\\nfeed-forward NN for vehicle travel time estimation was \\r\\nproposed, many other NN-based models, including fuzzy NN \\r\\n[31], recurrent NN [9], convolution NN [5][18], deep belief \\r\\nnetworks [10][32], auto-encoders [11][33], generative \\r\\nadversarial networks [34][35], and combinations of these \\r\\nmodels have been applied to forecast traffic states. With the \\r\\ncapability of capturing temporal dependencies, the recurrent \\r\\nNN or its variants, like LSTM [12] and GRU [13], was widely\\r\\nadopted as a component of a traffic forecasting model to \\r\\nforecast traffic speed [8], travel time [36], and traffic flow [37].\\r\\nFurther, in most recent years, various novel deep learning\\x02based traffic forecasting models have been proposed through \\r\\nadjusting classical neural network model, combining existing \\r\\nmethods, and incorporating auxiliary data. Multiple novel\\r\\nLSTM based models, such as bidirectional LSTM [14], deep \\r\\nLSTM [15], shared hidden LSTM [38], and nested LSTM [39], \\r\\nhave been designed via reorganizing and combing single LSTM \\r\\nmodels and applied to capture comprehensive temporal \\r\\ndependencies for traffic prediction. In addition, sequence-to\\x02sequence (seq2seq) architecture based models [20],[33] have \\r\\nalso been used for traffic state sequence forecasting. To deal \\r\\nwith different types of features, multi-stream deep learning \\r\\nmodels [15][40][41][42] have also been well studied and tested \\r\\nfor traffic forecasting problems. To improve the prediction \\r\\nperformance, multiple deep learning based models also \\r\\nincorporate various traffic-related auxiliary data, including \\r\\nroadway geographical attribute data [33], accident data [15], \\r\\nand weather data [43].\\r\\nTo capture spatial relationships present in traffic networks, \\r\\nmany forecasting models [5], [44] incorporating CNNs to \\r\\nextract spatial features from 2D spatial-temporal traffic data.\\r\\nDue to the traffic structure is hard to be depicted by 2D spatial\\x02temporal data, studies [18] tried to convert traffic network \\r\\nstructures to images and use CNNs to learn spatial features.\\r\\nHowever, these converted images have a certain amount of \\r\\nnoise, inevitably resulting in spurious spatial relationships \\r\\ncaptured by CNNs. Recent studies [42][45][46] also attempted \\r\\nto convert traffic state data into three-dimensional (3D) \\r\\nmatrices and use the 3D convolutional network to extract more \\r\\neffective features. However, conventional CNN based methods \\r\\nstill cannot inherently deal with the topological structure and \\r\\nthe physical attributes of the traffic network. To solve this \\r\\nproblem, studies [1], [20] attempted to learn the traffic network\\r\\nas a graph and adopt the graph-based convolution operator to\\r\\nextract features from the graph-structured traffic network.\\nB. Graph Convolution Networks\\r\\nTraffic networks have already been analyzed as graphs for \\r\\ndynamic shortest path routing [47], traffic congestion analysis\\r\\n[48], and dynamic traffic assignment [49]. In the last couple of \\r\\nyears, many studies attempt to generalize neural networks to \\r\\nwork on arbitrarily structured graphs by designing graph \\r\\nconvolutional networks. Generally, the graph convolutional \\r\\nnetworks utilize the adjacency matrix or the Laplacian matrix \\r\\nto depict the structure of a graph. The Laplacian matrix based \\r\\ngraph convolution [22], [26] are designed based on the spectral \\r\\ngraph theory [50]. As an extension, a localized spectral graph \\r\\nconvolution [23] is also proposed to reduce the learning \\r\\ncomplexity. The adjacency matrix based graph convolution\\r\\nneural networks [24], [25] incorporate the adjacency matrix and \\r\\ntheir network structures are more flexible. The traffic network \\r\\ncan be considered as a graph consisting of nodes and edges, and \\r\\nthus, several graph convolution neural network based models, \\r\\nincluding the spectral graph convolution [1] and the diffusion \\r\\ngraph convolution [21], are proposed to fulfill network-wide \\r\\ntraffic forecasting. Several studies [51][52] also incorporated \\r\\nmulti-scale graph convolution operations into their proposed \\r\\nmodels to learn traffic features. Although these existing \\r\\nmethods can extract spatial features from neighborhoods in the \\r\\ntraffic network, the physical specialties of roadways, like \\r\\nlength, speed limits, and the number of lanes, are normally \\r\\nneglected.\\r\\nIII. METHODOLOGY\\r\\nA. Notions\\r\\n1) Traffic Network based Graph\\r\\nNormally, a graph consists of nodes (vertices) and edges. The \\r\\ngraph representing a traffic network is distinct from social \\r\\nnetwork graphs, document citation graphs, or molecule graphs, \\r\\nin several respects: 1) there are no isolated nodes/edges in \\r\\ntraffic network based graphs and the traffic network structure \\r\\nseldom changes; 2) the traffic status of each road in a traffic \\r\\nnetwork varies over time; and 3) the roads in a traffic network \\r\\nhave meaningful physical characteristics, such as the length, \\r\\ntype, speed limit, and lane numbers of a road. Further, traffic \\r\\nstate data is collected by different types of sensors such that \\r\\nsome types of data detect location-based traffic states, but \\r\\nothers may measure road segment based averaged traffic states. \\r\\nDue to traffic states vary over time, it is better to let the graph \\r\\nnodes possess the varying traffic states and keep the graph \\r\\nstructure fixed. Thus, to ensure the consistency of the definition \\r\\nin a graph, we use nodes to represent the traffic sensing \\r\\nlocations, which can be sensor stations or road segments. Then, \\r\\nthe edges in a graph represent the intersections or road \\r\\nsegments connecting those traffic sensing locations. \\r\\nThe traffic network and the relationship between traffic \\r\\nlocations can be represented by an undirected graph 𝒢 where \\r\\n𝒢 = (𝒱, ℰ) with 𝑁 nodes 𝑣𝑖 ∈ 𝒱 and edges (𝑣𝑖, 𝑣𝑗) ∈ ℰ. Even \\r\\nthough some roads are directed in the reality, due to the impact \\r\\nof traffic congestions occurring on these roads will be bi\\x02directionally propagated to upstream and downstream roads \\r\\n[14], we take the bidirectional impact into account and thus let \\r\\n𝒢 be an undirected graph.\\r\\n2) Adjacency Matrix and Neighborhood Matrix\\r\\nThe connectedness of nodes in 𝒢 is represented by an \\r\\nadjacency matrix 𝐴 ∈ ℝ\\r\\n𝑁×𝑁, in which each element 𝐴𝑖,𝑗 = 1 if \\r\\nthere is an edge connecting node 𝑖 and node 𝑗 and 𝐴𝑖,𝑗 = 0\\r\\notherwise (𝐴𝑖,𝑖 = 0). Based on the adjacency matrix, the degree \\r\\nmatrix of 𝒢, which measures the number of edges attached to \\r\\neach vertex, can be defined as 𝐷 ∈ ℝ\\r\\n𝑁×𝑁 in which 𝐷𝑖𝑖 =\\r\\n∑𝑗 𝐴𝑖𝑗. 𝐷 is a diagonal matrix and all non-diagonal elements are \\r\\nzeros.\\r\\nBased on the adjacency matrix, an edge counting function \\r\\n𝑑(𝑣𝑖, 𝑣𝑗) can be defined as counting the minimum number of \\r\\nedges traversed from node 𝑖 to node 𝑗. Then, the set of 𝑘-hop \\r\\n(𝑘-th order) neighborhood of each node 𝑖, including node 𝑖\\r\\nitself, can be defined as {𝑣𝑗 ∈ 𝒱|𝑑(𝑣𝑖, 𝑣𝑗) ≤ 𝑘}. However, \\r\\nsince the traffic states are time series data and the current traffic \\r\\nstate on a road will definitely influence the future state, we \\r\\nconsider the all roads are self-influenced. Thus, we consider the \\r\\nneighborhood of a node contains the node itself and a \\r\\nneighborhood matrix to characterize the one-hop neighborhood \\r\\nrelationship of the whole graph, denoted as \\r\\n𝐴̃ = 𝐴 +𝐼 (1)\\r\\nwhere 𝐼 is the identity matrix. Then, the 𝑘-hop neighborhood \\r\\nrelationship of the graph nodes can be characterized by\\r\\n(𝐴 +𝐼)\\r\\n𝑘\\r\\n. However, some elements in (𝐴 +𝐼)\\r\\n𝑘 will inevitably \\r\\nexceed one. Owing to the 𝑘-hop neighborhood of a node is only \\r\\nused for describing the existence of all the 𝑘-hop neighbors, it \\r\\nis not necessary to make a node’s 𝑘-hop neighbors weighted by \\r\\nthe number of hops. Thus, we clip the values of all elements in\\r\\n(𝐴 +𝐼)\\r\\n𝑘\\r\\nto be in {0,1} and define a new 𝑘-hop neighborhood \\r\\nmatrix 𝐴̃𝑘, in which each element 𝐴̃\\r\\n𝑖,𝑗\\r\\n𝑘\\r\\nsatisfies \\r\\n𝐴̃\\r\\n𝑖,𝑗\\r\\n𝑘 = min((𝐴 + 𝐼)𝑖,𝑗𝑘\\r\\n, 1) (2)\\r\\nwhere min refers to minimum. In this case, 𝐴̃1 = 𝐴\\r\\n1 = 𝐴. An \\r\\nintuitive example of 𝑘-hop neighborhood with respect to a node \\r\\n(a red star) is illustrated by blue points on the left side of Fig. 1.\\r\\n3) Free-Flow Reachable Matrix\\r\\nBased on the length of each road in the traffic network, we \\r\\ndefine a distance matrix 𝐷𝑖𝑠𝑡 ∈ ℝ\\r\\n𝑁×𝑁 , where each element \\r\\n𝐷𝑖𝑠𝑡𝑖,𝑗represents the real roadway distance from node 𝑖 to 𝑗\\r\\n(𝐷𝑖𝑠𝑡𝑖,𝑖 = 0). When taking the underlying physics of vehicle \\r\\ntraffic on a road network into consideration, we need to \\r\\nunderstand that the impact of a roadway segment on adjacent \\r\\nsegments is transmitted in two primary ways: 1) slowdowns \\r\\nand/or blockages propagating upstream; and 2) driver behavior \\r\\nand vehicle characteristics associated with a particular group of \\r\\nvehicles traveling downstream. Thus, for a traffic network\\x02based graph or other similar graphs, the traffic impact \\r\\ntransmission between non-adjacent nodes cannot bypass the \\r\\nintermediate node/nodes, and thus, we need to consider the \\r\\nreachability of the impact between adjacent and nearby node \\npairs. To ensure the traffic impact transmission between k-hop \\r\\nadjacent nodes follow the established traffic flow theory [53], \\r\\nwe define a free-flow reachable matrix, ℱℱ𝑅 ∈ ℝ\\r\\n𝑁×𝑁, that\\r\\nℱℱ𝑅𝑖,𝑗 = {\\r\\n1, 𝑆𝑖,𝑗\\r\\nℱℱ𝑚∆𝑡 −𝐷𝑖𝑠𝑡𝑖,𝑗 ≥ 0\\r\\n0, otherwise \\r\\n, ∀ 𝑣𝑖, 𝑣𝑗 ∈ 𝒱 (3)\\r\\nwhere 𝑆𝑖,𝑗\\r\\nℱℱ is the free-flow speed between node 𝑖 and 𝑗, and \\r\\nfree-flow speed [54] refers to the average speed that a motorist \\r\\nwould travel if there were no congestion or other adverse \\r\\nconditions (such as severe weather). ∆𝑡 is the duration of time \\r\\nquantum and 𝑚 is a number counting how many time intervals\\r\\nare considered to calculate the distance travelled under free\\x02flow speed. Thus, 𝑚 determines the temporal influence of \\r\\nformulating the ℱℱ𝑅 . Each element ℱℱ𝑅𝑖,𝑗 equals one if \\r\\nvehicles can traverse from node 𝑖 to 𝑗 in 𝑚 time-step, 𝑚 ∙ ∆𝑡, \\r\\nwith free-flow speed, and ℱℱ𝑅𝑖,𝑗 = 0 otherwise. Intuitively, \\r\\nthe ℱℱ𝑅𝑖,𝑗 measures whether a vehicle can travel from node 𝑖\\r\\nto node 𝑗 with the free-flow speed under a specific time interval. \\r\\nWe consider each road is self-reachable, and thus, all diagonal \\r\\nvalues of ℱℱ𝑅 are set as one. An example ℱℱ𝑅 with respect to \\r\\na node (a red star) is shown by green lines on the left side of \\r\\nFig. 1.\\r\\nB. Traffic Forecasting Problem\\r\\nTraffic forecasting refers to predicting future traffic states, \\r\\nsuch as traffic speed, travel time, or volume, given previously \\r\\nobserved traffic states from a road network. In this study, the \\r\\ntraffic network is converted into a graph consisting of all 𝑁\\r\\nnodes, representing 𝑁 traffic sensing locations, and a set of \\r\\nedges. During a period of time 𝑡, the signals of these nodes\\r\\nrepresenting the collected traffic states, can be denoted as 𝑥𝑡 ∈\\r\\nℝ\\r\\n𝑁.\\r\\nTo formulate the traffic forecasting problem, the main \\r\\naforementioned notations are summarized in the following list:\\r\\n𝒢 Traffic network-based graph 𝒢 = (𝒱, ℰ)\\r\\n𝒱 Set of vertices in 𝒢 with the size of |𝒱| = 𝑁\\r\\nℰ Set of edges in 𝒢 with the size of |ℰ|\\r\\n𝐴 ∈ ℝ\\r\\n𝑁×𝑁 Adjacency matrix of 𝒢\\r\\n𝐷 ∈ ℝ\\r\\n𝑁×𝑁 Degree matrix of 𝒢\\r\\n𝐴̃ ∈ ℝ\\r\\n𝑁×𝑁 Neighborhood matrix defined by (1)\\r\\n𝐴̃𝑘 ∈ ℝ\\r\\n𝑁×𝑁 𝑘-hop neighborhood matrix defined by (2)\\r\\n𝐷𝑖𝑠𝑡 ∈ ℝ\\r\\n𝑁×𝑁 Distance matrix \\r\\nℱℱ𝑅 ∈ ℝ\\r\\n𝑁×𝑁 Free-flow reachable matrix by (3)\\r\\n𝑥𝑡 ∈ ℝ\\r\\n𝑁 Vector of speed of all graph nodes at time 𝑡\\r\\nThe short-term traffic forecasting problem aims to learn a \\r\\nfunction 𝐹(∙) to map 𝑇 time steps of historical graph signals, \\r\\ni.e. 𝑿𝑇 = [𝑥1, … , 𝑥𝑡, … , 𝑥𝑇\\r\\n] , to the graph signals in the \\r\\nsubsequent one or multiple time steps. In this study, the \\r\\nfunction attempts to forecast the graph signals in the subsequent \\r\\none step, i.e. 𝑥𝑇+1, and the formulation of 𝐹(∙) is defined as \\r\\n𝐹 ([𝑥1, … , 𝑥𝑡, … , 𝑥𝑇\\r\\n]; 𝐺(𝒱 , ℰ , 𝐴̃𝑘\\r\\n,ℱℱ𝑅)) = 𝑥𝑇+1\\r\\n(4)\\r\\nFurther, another goal of this study is to learn the traffic \\r\\nimpact transmission between adjacent and neighboring nodes in \\r\\na traffic network-based graph by learning the weight parameters \\r\\nin the function 𝐹(∙). \\r\\nC. Traffic Graph Convolution\\r\\nPrevious work [24][25][28] has defined the graph \\r\\nconvolution based the adjacency matrix. The core idea of a \\r\\nconvolution layer in a neural network is to extract localized \\r\\nfeatures from input data in a 2D or 3D matrices structure. The \\r\\nlocalized region of the input space which affects the \\r\\nconvolution operation results is called receptive field. \\r\\nAnalogously, the core idea of a graph convolution layer is to \\r\\nextract localized features from input data in a graph structure. \\r\\nThus, the product of the neighborhood matrix 𝐴̃, the input data \\r\\n𝑥𝑡, and a trainable weight matrix 𝑊 , i.e. 𝐴̃𝑥𝑡𝑊 , can be \\r\\nconsidered as a graph convolution operation to extract features \\r\\nfrom one-hop neighborhood [24][25]. Then, the receptive field \\r\\nof the graph convolution operation on a node is the one-hop \\r\\nneighborhood.\\r\\nHowever, in this way, the receptive field is confined, and it \\r\\nonly concentrates on one-hop neighboring nodes. To overcome \\r\\nthis shortcoming, we extend the receptive field of graph \\r\\nconvolution by replacing the one-hop neighborhood matrix 𝐴̃\\r\\nwith the 𝑘-hop neighborhood matrix 𝐴̃𝑘. Meanwhile, existing\\r\\nstudies either neglect the properties of the edges in a graph, such \\r\\nas the distances between different sensing locations (the lengths \\r\\nof the graph edges) and the free-flow reachability defined in (3), \\r\\nor fail to consider high-order neighborhood of nodes in the \\r\\ngraph. Hence, to comprehensively solve the network-wide \\r\\nforecasting problem, we consider both graph edge properties \\r\\nand high-order neighborhood in the traffic network-based graph. \\r\\nHence, we define the 𝑘 -order ( 𝑘 -hop) Traffic Graph \\r\\nConvolution (TGC) operation as\\r\\n𝐺𝐶𝑡\\r\\n𝑘 = (𝑊𝑔𝑐𝑘⨀𝐴̃𝑘⨀ℱℱ𝑅)𝑥𝑡\\r\\n(5)\\r\\nwhere ⨀ is the Hadamard product operator, i.e. the element\\x02wise matrix multiplication operator, and 𝑥𝑡 ∈ ℝ\\r\\n𝑁 is the vector \\r\\nof traffic states (speed) of all nodes at time 𝑡. The 𝑊𝑔𝑐_𝑘 ∈\\r\\nℝ\\r\\n𝑁×𝑁 is a trainable weight matrix for the 𝑘-order traffic graph \\r\\nconvolution and the 𝐺𝐶𝑘 ∈ ℝ\\r\\n𝑁 is the extracted 𝑘-order traffic \\r\\ngraph convolution feature. Due to 𝐴̃𝑘and ℱℱ𝑅 are both sparse \\r\\nmatrices only containing 0 and 1 elements, the result of \\r\\n𝑊𝑔𝑐𝑘⨀𝐴̃𝑘⨀ℱℱ𝑅 is also sparse. Further, the trained weight \\r\\n𝑊𝑔𝑐_𝑘 has the potential to measure the interactive influence \\r\\nbetween graph nodes, and thus, enhance the interpretability of \\r\\nthe model.\\r\\nIn Equation (5), 𝑘 should be a positive integer. The larger the \\r\\norder 𝑘 is, the larger the size of the receptive field of the TGC\\r\\nis, and then the more neighborhood-based features can be \\r\\nextracted from the graph. However, 𝑘 is not infinite, and it can \\r\\nbe easily proved that, for a specific graph, when increasing the \\r\\nvalue of 𝑘, 𝐴̃𝑘⨀ℱℱ𝑅 will eventually converge to ℱℱ𝑅 such \\r\\nthat 𝑘 = 𝐾𝑚𝑎𝑥 and 𝐴̃𝐾𝑚𝑎𝑥⨀ℱℱ𝑅 = ℱℱ𝑅. It should be noted \\r\\nthat, while extracting traffic graph convolution features to solve\\r\\nreal traffic prediction problems, it is not necessary to set 𝑘 as\\r\\nthe max value 𝐾𝑚𝑎𝑥 . The trade-off between the prediction \\naccuracy and the feature richness, which is directly related to \\r\\nthe computational cost, should be considered and balanced.\\r\\nLet 𝐾 ≤ 𝐾𝑚𝑎𝑥 denote the largest hop for traffic graph \\r\\nconvolution in this study, and the corresponding traffic graph \\r\\nconvolution feature is 𝐺𝐶𝑡\\r\\n𝐾 with respect to input data 𝑥𝑡\\r\\n. \\r\\nDifferent hops of neighborhood in TGC will result in different\\r\\nextracted features. To enrich the feature space, the features \\r\\nextracted from different orders (from 1 to 𝐾) of traffic graph \\r\\nconvolution with respect to 𝑋𝑡 are concatenated together as a \\r\\nvector defined as follows\\r\\n𝑮𝑪𝑡\\r\\n{𝐾} = [𝐺𝐶𝑡\\r\\n1\\r\\n, 𝐺𝐶𝑡\\r\\n2\\r\\n, … , 𝐺𝐶𝑡\\r\\n𝐾] (6)\\r\\nThe 𝑮𝑪𝑡\\r\\n{𝐾}\\r\\n∈ ℝ\\r\\n𝑁×𝐾 contains all the 𝐾 orders of traffic graph \\r\\nconvolutional features, as intuitively shown in the left part of \\r\\nFig. 1. In this study, after operating the TGC on input data 𝑥𝑡, \\r\\nthe generated 𝑮𝑪𝑡\\r\\n{𝐾} will be fed into the following layer in the \\r\\nproposed neural network structure described in the following \\r\\nsection. \\r\\nD. Comparing TGC with Spectral Graph Convolution\\r\\nThe proposed traffic graph convolution is based on adjacency \\r\\nmatrix 𝐴, but the spectral graph convolution (SGC) is defined \\r\\nin the Fourier domain [50] based on the Laplacian matrix 𝐿, \\r\\nwhich equals \\r\\n𝐿 = 𝐷 − 𝐴 (7)\\r\\nwhere 𝐷 is the degree matrix as introduced in Section III.A.2. \\r\\nThe Laplacian matrix 𝐿 is symmetric positive semi-definite \\r\\nsuch that it can be diagonalized via eigen-decomposition as\\r\\n𝐿 = 𝑈Λ𝑈\\r\\n𝑇\\r\\n(8)\\r\\nwhere Λ is a diagonal matrix containing the eigenvalues, 𝑈\\r\\nconsists of the eigenvectors, and 𝑈\\r\\n𝑇\\r\\nis the transpose of 𝑈. \\r\\nThe spectral convolution on graph is defined as the \\r\\nmultiplication of a signal 𝑥𝑡 ∈ ℝ\\r\\n𝑁 with a filter ℎ𝜃 = diag(𝜃)\\r\\nparameterized by 𝜃 ∈ ℝ\\r\\n𝑁[24]. The diag(𝜃) is the diagonalized \\r\\nmatrix given 𝜃. The spectral graph convolution operation can \\r\\nbe described as\\r\\nℎ𝜃 ∗𝒢 𝑥𝑡 = 𝑈ℎ𝜃𝑈\\r\\n𝑇𝑥𝑡 = 𝑈diag(𝜃)𝑈𝑇𝑥𝑡\\r\\n(9)\\r\\nwhere ∗𝒢is the spectral graph convolution operator. The filter \\r\\nℎ𝜃 that can be considered as a learnable convolutional kernel\\r\\nweight. \\r\\nFurther, for saving computational cost, the localized spectral \\r\\ngraph convolution (LSGC) is proposed by employing a \\r\\npolynomial filter ℎ𝜃\\r\\n′ = ∑ 𝜃𝑗\\r\\n𝑘−1 ′\\r\\n𝑗=0 Λ\\r\\n𝑗\\r\\n[23] and the learnable\\r\\nparameter 𝜃\\r\\n′ ∈ ℝ𝐾 . Then 𝐾 -hop localized spectral graph \\r\\nconvolution can be formulated as:\\r\\nℎ𝜃\\r\\n′ ∗𝒢 𝑥𝑡 = 𝑈 ∑ 𝜃𝑗\\r\\n′\\r\\n𝐾−1\\r\\n𝑗=0\\r\\nΛ\\r\\n𝑗𝑈𝑇𝑥𝑡 = ∑ 𝜃𝑗\\r\\n′\\r\\n𝐾−1\\r\\n𝑗=0\\r\\n𝐿\\r\\n𝑗𝑥𝑡\\r\\n(10)\\r\\nThe advantages of the LSGC is that it only has 𝐾 parameters \\r\\nand does not need eigen-decomposition. It is well spatial \\r\\nlocalized and each convolution operation on a centered vertex \\r\\nextracts the summed weighted feature of the vertex’s 𝐾-hop \\r\\nneighbors. The details of SGC and LSGC can be found in the \\r\\nliterature [22][23][24].\\r\\nThe comparison between TGC, SGC, and LSGC in terms of \\r\\nthe number of parameters, computational time, and localized \\r\\nfeature extraction, is shown in TABLE I. Comparing to SGC \\r\\nand LSGC, the TGC is better in terms of spatial localization\\r\\nbecause it can extract local features based on physical \\r\\nproperties of roadways by incorporating the ℱℱ𝑅. TGC with \\r\\nmore parameters has better capabilities of representing the \\r\\nrelationships between connected nodes in the graph. Further, \\r\\nSGC and LSGC normally need multiple convolutional layers, \\r\\nwhich leads the SGC and LSGC to lose their interpretability. \\r\\nHowever, TGC only needs one convolution layer and its \\r\\nparameters can be better interpreted.\\r\\nE. Traffic Graph Convolutional LSTM\\r\\nWe propose a Traffic Graph Convolutional LSTM (TGC\\x02LSTM) recurrent neural network, as shown on the right side of \\r\\nthe Fig. 1, which learns both the complex spatial dependencies \\r\\nand the dynamic temporal dependencies presented in traffic \\r\\ndata. In this model, the gates structure in the vanilla LSTM [12]\\r\\nand the hidden state are unchanged, but the input is replaced by \\r\\nthe graph convolution features, which are reshaped into a vector \\r\\n𝑮𝑪{𝐾} ∈ ℝ\\r\\n𝐾𝑁. The forget gate f𝑡\\r\\n, the input gate i𝑡, the output \\r\\ngate o𝑡, and the input cell state 𝐶̃\\r\\n𝑡\\r\\nin terms of time step 𝑡 are \\r\\ndefined as follows\\r\\nf𝑡 = 𝜎𝑔(𝑊𝑓 ∙ 𝑮𝑪𝑡\\r\\n{𝐾} + 𝑈𝑓 ∙ ℎ𝑡−1 + 𝑏𝑓) (11)\\r\\nTABLE I\\r\\nCOMPARISON BETWEEN TGC, SGC, AND LSGC\\r\\nGraph convolution definition 𝐾-hop TGC SGC 𝐾-hop LSGC\\r\\nGraph convolution on signal 𝑥𝑡\\r\\n(𝑊𝑔𝑐𝑘⨀𝐴̃𝐾⨀ℱℱ𝑅) 𝑈diag(𝜃)𝑈\\r\\n𝑇 ∑ 𝜃𝑗\\r\\n′𝐿𝐾−1 𝑗\\r\\n𝑗=0\\r\\nWeight parameters 𝑊𝑔𝑐𝑘\\r\\n∈ ℝ\\r\\n𝑁×𝑁\\r\\n𝜃 ∈ ℝ\\r\\n𝑁 𝜃′ ∈ ℝ𝐾\\r\\nComputational time complexity O(𝑁\\r\\n2\\r\\n) O(𝑁\\r\\n2\\r\\n) [23] O(𝐾|ℰ|) [23]\\r\\nExtract Localized features\\r\\nYes. It is 𝑘-localized \\r\\nincorporating roadway \\r\\nphysical properties.\\r\\nNo Yes. It is exactly 𝑘-localized.\\ni𝑡 = 𝜎𝑔(𝑊𝑖\\r\\n∙ 𝑮𝑪𝑡\\r\\n{𝐾} + 𝑈𝑖\\r\\n∙ ℎ𝑡−1 + 𝑏𝑖) (12)\\r\\no𝑡 = 𝜎𝑔(𝑊𝑜 ∙ 𝑮𝑪𝑡\\r\\n{𝐾} + 𝑈𝑜 ∙ ℎ𝑡−1 +𝑏𝑜) (13)\\r\\n𝐶̃\\r\\n𝑡 = 𝑡𝑎𝑛ℎ(𝑊𝐶 ∙ 𝑮𝑪𝑡\\r\\n{𝐾} + 𝑈𝐶 ∙ ℎ𝑡−1 +𝑏𝐶) (14)\\r\\nwhere ∙ is the matrix multiplication operator. 𝑊𝑓, 𝑊𝑖, 𝑊𝑜, and \\r\\n𝑊𝐶 ∈ ℝ\\r\\n𝑁×𝐾𝑁 are the weight matrices, mapping the input to the \\r\\nthree gates and the input cell state, while 𝑈𝑓, 𝑈𝑖, 𝑈𝑜, and 𝑈𝐶 ∈\\r\\nℝ\\r\\n𝑁×𝑁 are the weight matrices for the preceding hidden state. \\r\\n𝑏𝑓, 𝑏𝑖, 𝑏𝑜, and 𝑏𝐶 ∈ ℝ\\r\\n𝑁 are four bias vectors. The 𝜎𝑔 is the gate \\r\\nactivation function, which typically is the sigmoid function, and \\r\\ntanh is the hyperbolic tangent function. \\r\\nDue to each node in a traffic network graph is influenced by \\r\\nthe preceding states of itself and its neighboring nodes, the \\r\\nLSTM cell state of each node in the graph should also be \\r\\naffected by neighboring cell states. Thus, a cell state gate is \\r\\ndesigned and added in the LSTM cell. The cell state gate, as \\r\\nshown in Fig. 1, is defined as follows \\r\\n𝐶𝑡−1\\r\\n∗ = 𝑊𝒩⨀(𝐴̃𝐾⨀ℱℱ𝑅) ∙ 𝐶𝑡−1\\r\\n(15)\\r\\nwhere 𝑊𝒩 is a weight matrix to measure the contributions of \\r\\nneighboring cell states. To correctly reflect the traffic network \\r\\nstructure, the 𝑊𝒩 is constrained by multiplying a ℱℱ𝑅 based \\r\\n𝐾 -hop adjacency matrix, 𝐴̃𝐾⨀ℱℱ𝑅 . With this gate, the \\r\\ninfluence of neighboring cell states will be considered when the \\r\\ncell state is recurrently input to the subsequent time step. Then, \\r\\nthe final cell state and the hidden state are calculated as follows\\r\\n𝐶𝑡 = f𝑡⨀𝐶𝑡−1\\r\\n∗ + i𝑡⨀𝐶̃\\r\\n𝑡\\r\\n(16)\\r\\nℎ𝑡 = o𝑡⨀ tanh(𝐶𝑡\\r\\n) (17)\\r\\nAt the final time step 𝑇, the hidden state ℎ𝑇 is the output of \\r\\nTGC-LSTM, namely the predicted value 𝑦̂𝑇 = ℎ𝑇 . Let 𝑦𝑇 ∈\\r\\nℝ\\r\\n𝑁 denote the label of the input data 𝑿𝑇 ∈ ℝ𝑁×𝑁 . For the \\r\\nsequence prediction problem in this study, the label of time step \\r\\n𝑇 is the input of the next time step (𝑇 + 1) such that 𝑦𝑇 = 𝑥𝑇+1. \\r\\nThen the loss during the training process is defined as \\r\\n𝐿𝑜𝑠𝑠 = L(𝑦𝑇, 𝑦̂𝑇\\r\\n) = L(𝑥𝑇+1\\r\\n, ℎ𝑇\\r\\n) (18)\\r\\nwhere L(∙) is a function to calculate the residual between the \\r\\npredicted value 𝑦̂𝑇 and the true value 𝑦𝑇 . Normally, the L(∙)\\r\\nfunction is a Mean Squared Error (MSE) function for predicting \\r\\ncontinuous values.\\r\\nTo explain the proposed method in a clearer way, a pseudo\\x02code of the TGC-LSTM algorithm is presented in Algorithm 1. \\r\\nGiven the traffic state data 𝑿𝑇 and the graph related matrices as \\r\\ninput, the pseudo-code mainly describes the process of \\r\\ngenerating the final output ℎ𝑇 after 𝑇 steps of iteration. For \\r\\nsimplicity, the pseudo-code does not include the mini-batch \\r\\ngradient descent process and the backpropagation-based \\r\\nparameter updating process. In Algorithm 1, Eq. is short for \\r\\nEquation and the function TGC-LSTM(⋅) refers to the whole \\r\\ncalculation process described in Equation (11-17) in this \\r\\nsection. \\r\\nF. Traffic Graph Convolution Regularization\\r\\nSince the proposed model contains a traffic graph \\r\\nconvolution operation, the generated set of TGC features 𝑮𝑪𝑡\\r\\n{𝐾}\\r\\nand the learned TGC weights {𝑊𝑔𝑐1, … , 𝑊𝑔𝑐𝐾} provide an \\r\\nopportunity to make the proposed model interpretable via \\r\\nanalyzing the learned TGC weights. To confine the graph \\r\\nconvolution features within a reasonable scale and make the \\r\\nlearned weights more stable and interpretable, we propose two \\r\\nFig. 1. The architecture of the proposed Traffic Graph Convolution LSTM is shown on the right side. The traffic graph convolution (TGC) as a component of the \\r\\nproposed model is shown on the left side in detail by unfolding the traffic graph convolution at time 𝒕, in which 𝑨෩𝒌s and 𝓕𝓕𝑹 with respect to a red star node are \\r\\ndemonstrated.\\r\\nAlgorithm 1 Calculation the output of the TGC-LSTM layer\\r\\nInputs: 𝑿𝑻 = [𝑥1, … , 𝑥𝑇\\r\\n], {𝐴̃1\\r\\n, … , 𝐴̃𝐾}, ℱℱ𝑅\\r\\nParameters: {𝑊𝑔𝑐1, … , 𝑊𝑔𝑐𝐾}, 𝑊s, 𝑈s, and 𝑏s in Eq. (11-14)\\r\\n𝑊𝒩 in Eq. (15)\\r\\nInitialize: ℎ0 = 𝟎 ∈ ℝ𝑁, 𝐶0 = 𝟎 ∈ ℝ𝑁\\r\\nfor 𝑡 = 1 to 𝑇 do\\r\\nfor 𝑘 = 1 to 𝐾 do\\r\\n𝐺𝐶𝑡\\r\\n𝑘 ← (𝑊𝑔𝑐𝑘⨀𝐴̃𝑘⨀ℱℱ𝑅)𝑥𝑡\\r\\n \\r\\nend for\\r\\n𝑮𝑪𝑡\\r\\n{𝐾} ← [𝐺𝐶𝑡\\r\\n1\\r\\n, 𝐺𝐶𝑡\\r\\n2\\r\\n, … , 𝐺𝐶𝑡\\r\\n𝐾]\\r\\nℎ𝑡, 𝐶𝑡 = TGC-LSTM( 𝑥𝑡, 𝑮𝑪𝑡\\r\\n{𝐾}\\r\\n, ℎ𝑡−1, 𝐶𝑡−1 ) \\r\\nend for\\r\\nReturn: ℎ𝑇\\noptional regularization terms that can be added to the loss \\r\\nfunction described in Equation (18).\\r\\n1) Regularization on Graph Convolution weights\\r\\nBecause the graph convolution weights are not confined to be \\r\\npositive and each node’s extracted features are influenced by \\r\\nmultiple neighboring nodes, the graph convolution weights can \\r\\nvary a lot while training. Ideally, the convolution weights would \\r\\nbe themselves informative, so that the relationships between \\r\\ndifferent nodes in the network could be interpreted and \\r\\nvisualized by plotting the convolution weights. This is not \\r\\nlikely to be possible without regularization, because very high \\r\\nor low weights tend to appear somewhat randomly, with the \\r\\nresult that high/low weights tend to cancel each other out. In \\r\\ncombination, such weights can still represent informative \\r\\nfeatures for the network, but they cannot reflect the true \\r\\nrelationship between nodes in the graph. Thus, we add L1-norm \\r\\nof the graph convolution weight matrices to the loss function as \\r\\na regularization term to make these weight matrices as sparse \\r\\nas possible. The L1 regularization term is defined as follows\\r\\n𝑅\\r\\n{1} = ‖𝑾𝑔𝑐‖\\r\\n1\\r\\n= ∑ |𝑊𝑔𝑐𝑖|\\r\\n𝐾\\r\\n𝑖=1\\r\\n(19)\\r\\nIn this way, the trained graph convolution weight can be \\r\\nsparse and stable, and thus, it will be more intuitive to \\r\\ndistinguish which neighboring node or group of nodes \\r\\ncontribute most.\\r\\n2) Regularization on Graph Convolution features\\r\\nConsidering that the impact of neighboring nodes with respect \\r\\nto a specific node must be transmitted through all nodes \\r\\nbetween the node of interest and the influencing node, features \\r\\nextracted from different hops in the graph convolution should \\r\\nnot vary dramatically. Thus, to restrict the difference between \\r\\nfeatures extracted from adjacent hops of graph convolution, an \\r\\nL2-norm based TGC feature regularization term is added on the \\r\\nloss function at each time step. The regularization term is \\r\\ndefined as follows\\r\\n2 https://github.com/zhiyongc/Seattle-Loop-Data\\r\\n𝑅\\r\\n{2} = ‖𝑮𝑪𝑻\\r\\n{𝐾}\\r\\n‖\\r\\n2\\r\\n= √∑ (𝐺𝐶𝑇\\r\\n𝑖 −𝐺𝐶𝑇𝑖+1\\r\\n)\\r\\n2\\r\\n𝐾−1\\r\\n𝑖=1\\r\\n(20)\\r\\nIn this way, the features extracted from adjacent hops of \\r\\ngraph convolution should not differ dramatically, and thus, the \\r\\ngraph convolution operator should be more in keeping with the \\r\\nphysical realities of the relationships present in a traffic \\r\\nnetwork.\\r\\nThen, the total loss function at time 𝑡 can be defined as \\r\\nfollows\\r\\n𝐿𝑜𝑠𝑠 = L(ℎ𝑇 −𝑥𝑇+1\\r\\n)+ 𝜆1𝑅\\r\\n{1} + 𝜆2𝑅{2}\\r\\n(21)\\r\\nwhere 𝜆1 and 𝜆2 are penalty terms to control the weight \\r\\nmagnitude of the regularization terms on graph convolution \\r\\nweights and features.\\r\\nIV. EXPERIMENTS\\r\\nA. Dataset Description\\r\\nIn this study, two real-world network-scale traffic speed \\r\\ndatasets are utilized. The first contains data collected from \\r\\ninductive loop detectors deployed on four connected freeways \\r\\n(I-5, I-405, I-90, and SR-520) in the Greater Seattle Area, \\r\\nshown in Fig. 2 (a). This dataset, which is publicly accessible2, \\r\\ncontains traffic state data from 323 sensor stations over the \\r\\nentirety of 2015 at 5-minute intervals. The second contains road \\r\\nlink-level traffic speeds aggregated from GPS probe data \\r\\ncollected by commercial vehicle fleets and mobile apps \\r\\nprovided by the company INRIX. The INRIX traffic network \\r\\ncovers the Seattle downtown area, shown in Fig. 2 (b). This \\r\\ndataset describes the traffic state at 5-minute intervals for 1014 \\r\\nroad segments and covers the entire year of 2012. We use \\r\\nLOOP data and INRIX data to denote these two datasets, \\r\\nrespectively, in this study.\\r\\nWe adopt the speed limit as the free-flow speed, which for \\r\\nthe segments in the LOOP traffic network is 60mph in all cases. \\r\\nThe INRIX traffic network contains freeways, ramps, arterials,\\r\\nand urban corridors, and so the free-flow speeds of INRIX \\r\\ntraffic network range from 20mph to 60mph. The distance \\r\\nadjacency matrices 𝐷𝑖𝑠𝑡 and free-flow reachable matrices \\r\\nℱℱ𝑅 for both datasets are calculated based on the roadway \\r\\ncharacteristics and topology.\\r\\nB. Experimental Settings\\r\\n1) Baselines\\r\\nWe compare TGC-LSTM with the following baseline models: \\r\\n(1) ARIMA: Auto-Regressive Integrated Moving Average \\r\\nmodel [7]; (2) SVR: Support Vector Regression [6]; (3) FNN: \\r\\nFeed forward neural network with two hidden layers, i.e. the \\r\\nmultilayer perceptron, whose hidden layer size is N; (4) LSTM: \\r\\nLong Short-Term Memory recurrent neural network [12]; (5) \\r\\nDiffGRU [20]: an adjusted version of diffusion convolutional \\r\\ngated recurrent network [20] whose gate units are defined based \\r\\non diffusion convolution. Since the graph is undirected in this \\r\\nstudy, we replace the diffusion convolution with spectral graph \\r\\nconvolution in DiffGRU; (6) Conv+LSTM: a one-dimensional \\r\\n(1D) convolution layer with two channels followed by an\\r\\nFig. 2. (a) LOOP dataset covering the freeway network in Seattle area; (b) \\r\\nINRIX dataset covering the downtown Seattle area, where traffic segments are \\r\\nplotted with colors.\\nLSTM layer, the 1D CNN is conducted on 𝑥𝑡 with two output \\r\\nchannels (kernel size=5 and stride=2); (7) SGC+LSTM: \\r\\nstacking a one-layer spectral graph convolution layer [26] with \\r\\nan LSTM layer; (8) LSGC+LSTM: stacking a one-layer \\r\\nlocalized spectral graph convolution layer [23] whose 𝐾=3 and\\r\\nan LSTM layer. All the LSTM/GRU layers have the same \\r\\nweight dimensions. The baseline models do not include auto\\x02encoder based models and pure CNN based models, due to the \\r\\ncore ideas of these methodologies are totally different from the \\r\\ntested baseline models which are mostly single RNN layer\\x02based models. All the neural networks are implemented based \\r\\non PyTorch 1.0.1 and they are trained and evaluated on a single \\r\\nNVIDIA GeForce GTX 1080 Ti with 11GB memory. \\r\\n2) TGC-LSTM Model\\r\\nFor both datasets, the dimensions of the hidden states of the \\r\\nTGC-LSTM are set as the amount of the nodes in the traffic \\r\\nnetwork graphs. The size of hops in the graph convolution can \\r\\nvary, but we set it as 3, 𝐾 = 3, for the model evaluation and \\r\\ncomparison in this experiment. In this case, the ℱℱ𝑅 is \\r\\ncalculated based on three time steps. The two regularization \\r\\nterms (𝑅\\r\\n{1}\\r\\nand 𝑅\\r\\n{2}\\r\\n) can not only confine the learnt graph \\r\\nconvolution weights, they also can avoid overfitting causing the \\r\\ndecrease of the prediction accuracy. Thus, there is a trade-off \\r\\nbetween the prediction accuracy and the scale of the penalty \\r\\nterms ( 𝜆1 and 𝜆2). Based on empirically adjusting the \\r\\nregularization rates, the values of the 𝜆1 and 𝜆2 are both set as \\r\\n0.01. We train our model by minimizing the mean square error \\r\\nwith the batch size of 10 and the initial learning rate of 10−5. \\r\\nSince the RMSProp [55] can solve the gradient exploding and \\r\\nvanishing problems, it is used as the gradient descent optimizer \\r\\nwhose alpha (smoothing constant) is set as 0.99 and epsilon (the \\r\\nterm added to the denominator to improve numerical stability) \\r\\nis set as 10−8.\\r\\n3) Evaluation\\r\\nIn this study, the samples of the input are traffic time series data \\r\\nwith 10 time steps. The output/label is the next subsequent data \\r\\nof the input sequence. The performance of the proposed and the \\r\\ncompared models are evaluated by three commonly used \\r\\nmetrics in traffic forecasting, including 1) Mean Absolute Error \\r\\n(MAE), 2) Mean Absolute Percentage Error (MAPE), and 3) \\r\\nRoot Mean Squared Error (RMSE). \\r\\n𝑀𝐴𝐸 =\\r\\n1\\r\\n𝑛\\r\\n∑ |𝑦𝑇 − 𝑦̂𝑇\\r\\n|\\r\\n𝑛\\r\\n𝑖=1\\r\\n(23)\\r\\n𝑀𝐴𝑃𝐸 =\\r\\n1\\r\\n𝑛\\r\\n∑ |\\r\\n𝑦𝑇 −𝑦̂𝑇\\r\\n𝑌𝑇\\r\\n| ∗ 100%\\r\\n𝑛\\r\\n𝑖=1\\r\\n(24)\\r\\n𝑅𝑀𝑆𝐸 = √\\r\\n1\\r\\n𝑛\\r\\n∑ (𝑦𝑇 − 𝑦̂𝑇\\r\\n)\\r\\n2\\r\\n𝑛\\r\\n𝑖=1\\r\\n(25)\\r\\nC. Experimental Results\\r\\nTABLE II demonstrates the results of the TGC-LSTM and \\r\\nother baseline models on the two datasets. The proposed \\r\\nmethod outperforms other models with all the three metrics on \\r\\nthe two datasets. The ARIMA and SVR cannot compete with \\r\\nother methods, which suggest that non-neural-network \\r\\napproaches are less appropriate for this network-wide \\r\\nprediction task, due to the complex spatiotemporal \\r\\ndependencies and the high dimension features in the datasets. \\r\\nThe basic FNN does not perform well on predicting spatial\\x02temporal sequence. The DiffGRU performs nearly the same as \\r\\nTABLE II\\r\\nPERFORMANCE COMPARISON OF DIFFERENT APPROACHES. (THE NUMBER OF HOPS K IS SET AS 3 IN THE GRAPH CONVOLUTION RELATED MODEL)\\r\\nModel\\r\\nLOOPf Data INRIX Data\\r\\nMAE \\r\\n(mph)±STD MAPE RMSE MAE (mph)±STD MAPE RMSE\\r\\nARIMA 6.10± 1.09 13.85% 10.65 4.80 ± 0.32 13.51% 10.85\\r\\nSVR 6.85± 1.17 14.39% 11.12 4.78 ± 0.37 13.37% 10.44\\r\\nFNN 4.45± 0.81 10.19% 7.83 2.31 ± 0.17 8.35% 5.92\\r\\nLSTM 2.70± 0.18 6.83% 4.97 1.14 ± 0.09 3.88% 2.43\\r\\nDiffGRU 4.64±0.38 11.18% 8.22 2.44 ± 0.09 8.91% 6.34\\r\\nConv+LSTM 2.71±0.12 6.79% 5.02 1.13 ± 0.08 3.80% 2.37\\r\\nLSGC+LSTM 3.16± 0.23 7.51% 6.18 1.38 ± 0.12 4.54% 2.82\\r\\nSGC+LSTM 2.64± 0.12 6.52% 4.80 1.07 ± 0.08 3.74% 2.28\\r\\nTGC-LSTM 2.57± 0.10 6.01% 4.63 1.02 ± 0.07 3.28% 2.18\\r\\nFig. 3. Histogram of performance comparison for the influence of orders \\r\\n(hops) of graph convolution in the TGC-LSTM on INRIX and LOOP datasets.\\nthe FNN. The reason might be that GRU has the no cell state to \\r\\nstore historical information in its gate units comparing to \\r\\nLSTM. This can reduce the prediction capability of DiffGRU. \\r\\nBoth LSTM and Conv+LSTM work well and they have similar \\r\\nperformance. The SGC+LSTM performs better than vanilla \\r\\nLSTM, which demonstrates the feature extraction by using \\r\\nspectral graph convolution is beneficial for traffic forecasting. \\r\\nHowever, the LSGC+LSTM does not outperform LSTM \\r\\nresulting from utilizing one-layer LSGC, whose parameters is \\r\\nnot enough for representing the network features. The proposed \\r\\nTGC-LSTM, which capture graph-based features while \\r\\naccommodating the physical specialties of traffic networks, \\r\\nperforms better than all other approaches. It should be noted \\r\\nthat, for the INRIX data, during the nighttime or off-peak hours \\r\\nwhen there are no observed speed values on specific roads, the \\r\\nmissing speed values are comprehensively imputed by the data \\r\\nprovider. Thus, there are few variations at the non-peak hours \\r\\nin the INRIX data. Further, the speed values in the INRIX data \\r\\nare all integers. Therefore, the calculated errors of the INRIX \\r\\ndata is less than that of the LOOP data and the evaluated \\r\\nperformance on INRIX data is inflated somewhat.\\r\\nFig. 3 shows a histogram of performance comparison on the \\r\\neffects of orders (hops) of the graph convolution in the TGC\\x02LSTM. The model performance is improved when the value of \\r\\n𝐾 increases. For the LOOP data, the performance improves \\r\\nslightly when 𝐾 is gradually increased. But for the INRIX data, \\r\\nthere is a big improvement in when 𝐾 increases to two from one. \\r\\nThe complex structure and the various road types in the INRIX \\r\\ntraffic network could be the main reason for this performing \\r\\ndifference. Further, when 𝐾 is larger than two, the \\r\\nimprovement of the prediction is quite limited. This is also the \\r\\nreason why we choose 𝐾=3 in the model comparison part, as \\r\\nshown in TABLE II.\\r\\nD. Training Efficiency\\r\\nIn this subsection, we compare the training efficiency of the \\r\\nproposed model and other LSTM-based models. Fig. 4 (a) \\r\\nshows the validation loss curves versus the training epoch. Due \\r\\nto the early stopping mechanism is used in the training process, \\r\\nthe numbers of training epochs are different. The TGC-LSTM \\r\\nneeds less epochs to converge than the SGC+LSTM and the \\r\\nLSGC+LSTM. In addition, the loss of the TGC-LSTM \\r\\ndecreases fastest among the compared models. Fig. 4 (b) shows \\r\\nthe comparison of the training time per epoch of different \\r\\nmodels. The training cost of Conv+LSTM is between that of \\r\\nLSTM and SGC+LSTM. TGC-LSTM costs twice as much as \\r\\nLSTM does. The time required for SGC+LSTM is less than that \\r\\nfor TGC-LSTM, while LSGC+LSTM costs slightly more than \\r\\nTGC-LSTM. Fig. 4 (c) shows the training losses of TGC-LSTM \\r\\nwith different hops of graph convolution components. The rate \\r\\nof convergence increases when increasing the number of hops, \\r\\n𝑘. In our experiments, when 𝑘 is larger than 3, the training and \\r\\nvalidation results improve only marginally for both INRIX and \\r\\nLOOP datasets.\\r\\nE. Effect of Regularization\\r\\nThe model’s loss function can add regularization terms to avoid \\r\\noverfitting. The proposed L1-norm on the graph convolution \\r\\nweights and L2-norm on the graph convolutional features can \\r\\nfurther help the model to confine the learned weights and \\r\\nfeatures. However, there is a trade-off between the prediction \\r\\naccuracy and the scale of the penalty terms (𝜆1 and 𝜆2). As \\r\\ntested, by adding the regularization terms to the loss function \\r\\nwith the penalty rates setting as 0.01, the MAEs of the proposed\\r\\nmodel tested on the two datasets increase around 0.02, which \\r\\nare still superior to baseline models. Meanwhile, the TGC \\r\\nweight sparsity is increased and the value of the feature \\r\\nregularization 𝑅\\r\\n{2}\\r\\nis lower than that of the proposed model \\r\\nwithout regularization terms in the loss function, which means \\r\\nthe TGC features’ consistency is enhanced. Thus, it is worth \\r\\nadding these regularization terms to the loss function to help the \\r\\ntrained model to be more interpretable. Fig. 5 (a) and (b) show \\r\\nportions of the averaged graph convolution weight matrices for \\r\\nthe INRIX data and the LOOP data, respectively, where 𝐾 = 3\\r\\nand the average weight is calculated by 1\\r\\n𝐾\\r\\n∑ 𝑊𝑖⨀𝐴̃\\r\\n𝑖⨀ℱℱ𝑅\\r\\n𝐾\\r\\n𝑖=1\\r\\n. \\r\\nThe road segment names, which are not displayed, are aligned \\r\\non the vertical and horizontal axes with the same order in each \\r\\nfigure. The colored dots in the matrices in Fig. 5 (a) and (b) \\r\\nillustrate the weight of the contribution of a single node to its \\r\\nneighboring nodes. Since we align the traffic states of roadway \\r\\nsegments based on their interconnectedness in the training data, \\r\\nmost of the weights are distributed around the diagonal line of \\r\\nthe weight matrix. The INRIX network is more complex and \\r\\nFig. 4. (a) Validation loss versus training epoch (batch size = 40 and early stopping patience = 10 epochs). (b) Histogram of model’s training time per epochs. (c) \\r\\nCompare training efficiency with different 𝐾 hops of TGC: training loss versus training iteration (batch size = 40). (The figures are generated based on the LOOP \\r\\ndata.)\\nthe average degree of nodes in the INRIX graph is higher than \\r\\nthat in the LOOP graph. Hence, the dots in the average weight \\r\\nmatrix of the INRIX graph convolution are more scattered. But \\r\\nthese dots still form multiple clusters demonstrating the weights \\r\\nof several nearby or connected road segments. Considering \\r\\nroadway segments are influenced by their neighboring or \\r\\nnearby connected segments, the nodes with the large absolute \\r\\nweight in a cluster are very likely to be key road segments in \\r\\nthe local traffic network. In this way, we can infer the \\r\\nbottlenecks of the traffic network from the traffic graph \\r\\nconvolution weight matrices. \\r\\nF. Model Interpretation and Visualization\\r\\nTo better understand the contribution of the graph convolution \\r\\nweight, we mark seven groups of representative weights in Fig.\\r\\n5 (a) and (b) and visualize their physical locations on the real \\r\\nmap in Fig. 5 (c) and (d), by highlighting them with Roman \\r\\nnumerals and red boxes. The influence of these marked weights \\r\\non neighboring nodes in the INRIX and LOOP data are \\r\\nvisualized by lines and circles, respectively, considering the \\r\\nINRIX traffic network is too dense to use circles. The darkness \\r\\nof the green and pink colors and the sizes of the circles represent \\r\\nthe magnitude of influence. It should be noted that the darkness \\r\\nof colors on lines on the INRIX map and the size of the circles \\r\\non the LOOP map will change when the model is trained with \\r\\ndifferent scales of regularization terms (𝜆1 and 𝜆2).\\r\\nFrom Fig. 5 (c), we can find the marked areas with dark \\r\\ncolors in the INRIX GC weight matrix, (I), (II), and (III), are all \\r\\nlocated at very busy and congested freeway entrance and exit \\r\\nramps in Seattle downtown area. In Fig. 5 (d), the area tagged \\r\\nwith (IV) is quite representative because the two groups of \\r\\ncircles are located at the intersections between freeways and \\r\\ntwo main corridors that represent the entrances to an island \\r\\n(Mercer Island). Areas (V) and (VI) are the intersections \\r\\nbetween I-90 and I-405 and between I-5 and SR-520, \\r\\nrespectively. The VII area located on SR-520 contains a \\r\\nfrequent-congested ramp connecting to the city of Bellevue, the \\r\\nlocation of which is highlighted by the biggest green circle. \\r\\nAdditionally, there are many other representative areas in the \\r\\ngraph convolution weight matrix, but we cannot show all of \\r\\nthem due to space limits. By comparing the weight matrix with \\r\\nFig. 5. (a) Visualization of a proportion of the INRIX GC weight matrix, in which three representative weight areas are tagged. (b) Visualization of a proportion \\r\\nof the LOOP GC weight matrix, in which four representative weight areas are tagged. (c) Visualization of the INRIX graph convolution weight on the real traffic \\r\\nnetwork using colored lines. (d) Visualization of the four tagged weight areas in the LOOP graph convolution weight on the Seattle freeway network using colorful \\r\\ncircles.\\nthe physical realities of the traffic network, it can be shown that \\r\\nthe proposed method effectively captures spatial dependencies \\r\\nand helps to identify the most influential points/segments in the \\r\\ntraffic network.\\r\\nFig. 6 visualizes the predicted traffic speed sequences and the \\r\\nground truth of two locations selected from the LOOP and \\r\\nINRIX dataset. Though the traffic networks of the two datasets \\r\\nare very different, the curves demonstrate that the trends of the \\r\\ntraffic speed are predicted well at both peak traffic and off-peak \\r\\nhours. \\r\\nV. CONCLUSION\\r\\nIn this paper, we learn the traffic network as a graph and \\r\\ndefine a traffic graph convolution operation to capture spatial \\r\\nfeatures from the traffic network. The traffic graph convolution \\r\\nincorporates the adjacency matrix and the proposed free-flow \\r\\nreachable matrix to extract localized features from the graph. \\r\\nWe propose a traffic graph convolutional LSTM neural network \\r\\nto forecast network-wide traffic states. We also design two \\r\\nregularization terms on the TGC weights and TGC features, \\r\\nrespectively, that can be added to the model’s loss function to \\r\\nhelp the learned TGC weight to be more stable and \\r\\ninterpretable. By evaluating on two real-world traffic datasets, \\r\\nour approach is proved to be superior to the compared baseline \\r\\nmodels. In addition, the learned TGC weight can help to \\r\\nidentify the most influential roadways, and thus, enhance the \\r\\ninterpretability of the proposed model. \\r\\nFor future work, we will move forward to improve the \\r\\nmodel’s prediction performance in terms of accuracy and \\r\\nrobustness, and further investigate how to conduct the \\r\\nconvolution on both spatial and temporal dimensions to make \\r\\nthe neural network more interpretable.\\r\\nREFERENCES\\r\\n[1] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph \\r\\nconvolutional neural network: A deep learning \\r\\nframework for traffic forecasting,” arXiv Prepr. \\r\\narXiv1709.04875, 2017.\\r\\n[2] D. Park and L. R. Rilett, “Forecasting freeway link \\r\\ntravel times with a multilayer feedforward neural \\r\\nnetwork,” Comput. Civ. Infrastruct. Eng., vol. 14, no. \\r\\n5, pp. 357–367, 1999.\\r\\n[3] E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, \\r\\n“Short-term traffic forecasting: Where we are and \\r\\nwhere we’re going,” Transp. Res. Part C Emerg. \\r\\nTechnol., vol. 43, pp. 3–19, Jun. 2014.\\r\\n[4] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, “Long \\r\\nshort-term memory neural network for traffic speed \\r\\nprediction using remote microwave sensor data,”\\r\\nTransp. Res. Part C Emerg. Technol., vol. 54, pp. \\r\\n187–197, May 2015.\\r\\n[5] X. Ma, Z. Dai, Z. He, J. Ma, Y. Wang, and Y. Wang, \\r\\n“Learning traffic as images: a deep convolutional \\r\\nneural network for large-scale transportation network \\r\\nspeed prediction,” Sensors, vol. 17, no. 4, p. 818, \\r\\n2017.\\r\\n[6] A. J. Smola and B. Schölkopf, “A tutorial on support \\r\\nvector regression,” Stat. Comput., vol. 14, no. 3, pp. \\r\\n199–222, 2004.\\r\\n[7] M. M. Hamed, H. R. Al-Masaeid, and Z. M. B. Said, \\r\\n“Short-term prediction of traffic volume in urban \\r\\narterials,” J. Transp. Eng., vol. 121, no. 3, pp. 249–\\r\\n254, 1995.\\r\\n[8] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, “Long \\r\\nshort-term memory neural network for traffic speed \\r\\nprediction using remote microwave sensor data,”\\r\\nTransp. Res. Part C Emerg. Technol., vol. 54, pp. \\r\\n187–197, 2015.\\r\\n[9] J. Van Lint, S. Hoogendoorn, and H. Van Zuylen, \\r\\n“Freeway travel time prediction with state-space \\r\\nneural networks: modeling state-space dynamics with \\r\\nrecurrent neural networks,” Transp. Res. Rec. J. \\r\\nTransp. Res. Board, no. 1811, pp. 30–39, 2002.\\r\\n[10] W. Huang, G. Song, H. Hong, and K. Xie, “Deep \\r\\nArchitecture for Traffic Flow Prediction: Deep Belief \\r\\nNetworks With Multitask Learning.,” IEEE Trans. \\r\\nIntell. Transp. Syst., vol. 15, no. 5, pp. 2191–2201, \\r\\n2014.\\r\\n[11] Y. Lv, Y. Duan, W. Kang, Z. Li, F.-Y. Wang, and \\r\\nothers, “Traffic flow prediction with big data: A deep \\r\\nlearning approach.,” IEEE Trans. Intell. Transp. Syst., \\r\\nvol. 16, no. 2, pp. 865–873, 2015.\\r\\n[12] S. Hochreiter and J. Schmidhuber, “Long Short-Term \\r\\nMemory,” Neural Comput., vol. 9, no. 8, pp. 1735–\\r\\n1780, Nov. 1997.\\r\\n[13] K. Cho et al., “Learning phrase representations using \\r\\nRNN encoder-decoder for statistical machine \\r\\ntranslation,” arXiv Prepr. arXiv1406.1078, 2014.\\r\\n[14] Z. Cui, R. Ke, and Y. Wang, “Deep Stacked \\r\\nBidirectional and Unidirectional LSTM Recurrent \\r\\nNeural Network for Network-wide Traffic Speed \\r\\nPrediction,” in 6th International Workshop on Urban \\r\\nComputing (UrbComp 2017), 2016.\\r\\n[15] R. Yu, Y. Li, C. Shahabi, U. Demiryurek, and Y. Liu, \\r\\n“Deep learning: A generic approach for extreme \\r\\ncondition traffic forecasting,” in Proceedings of the \\r\\n2017 SIAM International Conference on Data Mining, \\r\\nFig. 6. Traffic time series forecasting visualization for LOOP and INRIX \\r\\ndatasets on two randomly selected days. \\r\\n(a) Sensor ID: d005es16756 in LOOP dataset on 2015-01-04.\\r\\n(b) Sensor ID: 114P13774 in INRIX dataset on 2012-01-03.\\n2017, pp. 777–785.\\r\\n[16] F. Kong, J. Li, B. Jiang, T. Zhang, and H. Song, “Big \\r\\ndata-driven machine learning-enabled traffic flow \\r\\nprediction,” Trans. Emerg. Telecommun. Technol., p. \\r\\ne3482.\\r\\n[17] J. Zhang, Y. Zheng, and D. Qi, “Deep Spatio\\x02Temporal Residual Networks for Citywide Crowd \\r\\nFlows Prediction.,” in AAAI, 2017, pp. 1655–1661.\\r\\n[18] H. Yu, Z. Wu, S. Wang, Y. Wang, and X. Ma, \\r\\n“Spatiotemporal recurrent convolutional networks for \\r\\ntraffic prediction in transportation networks,” Sensors, \\r\\nvol. 17, no. 7, p. 1501, 2017.\\r\\n[19] B. Yu, M. Li, J. Zhang, and Z. Zhu, “3D Graph \\r\\nConvolutional Networks with Temporal Graphs: A \\r\\nSpatial Information Free Framework For Traffic \\r\\nForecasting,” Mar. 2019.\\r\\n[20] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion \\r\\nConvolutional Recurrent Neural Network: Data\\x02Driven Traffic Forecasting,” in International \\r\\nConference on Learning Representations (ICLR ’18), \\r\\n2018.\\r\\n[21] J. Atwood and D. Towsley, “Diffusion-convolutional \\r\\nneural networks,” in Advances in Neural Information \\r\\nProcessing Systems, 2016, pp. 1993–2001.\\r\\n[22] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, \\r\\n“Spectral Networks and Locally Connected Networks \\r\\non Graphs,” arXiv Prepr. arXiv1312.6203, Dec. 2013.\\r\\n[23] M. Defferrard, X. Bresson, and P. Vandergheynst, \\r\\n“Convolutional Neural Networks on Graphs with Fast \\r\\nLocalized Spectral Filtering,” in Advances in Neural \\r\\nInformation Processing Systems, 2016, pp. 3844–\\r\\n3852.\\r\\n[24] T. N. Kipf and M. Welling, “Semi-supervised \\r\\nclassification with graph convolutional networks,” in \\r\\nInternational Conference on Learning Representations \\r\\n(ICLR ’17), 2017.\\r\\n[25] Z. Zhou and X. Li, “Convolution on Graph: A High\\x02Order and Adaptive Approach,” arXiv Prepr. \\r\\narXiv1706.09916, 2018.\\r\\n[26] M. Henaff, J. Bruna, and Y. LeCun, “Deep \\r\\nConvolutional Networks on Graph-Structured Data,”\\r\\narXiv Prepr. arXiv1506.05163, Jun. 2015.\\r\\n[27] M. Simonovsky and N. Komodakis, “Dynamic \\r\\nedgeconditioned filters in convolutional neural \\r\\nnetworks on graphs,” in Proc. CVPR, 2017.\\r\\n[28] S. Abu-El-Haija et al., “MixHop: Higher-Order Graph \\r\\nConvolution Architectures via Sparsified \\r\\nNeighborhood Mixing,” Int. Conf. Mach. Learn., \\r\\n2019.\\r\\n[29] N. G. Polson and V. O. Sokolov, “Deep learning for \\r\\nshort-term traffic flow prediction,” Transp. Res. Part \\r\\nC Emerg. Technol., vol. 79, pp. 1–17, 2017.\\r\\n[30] J. Hua and A. Faghri, “Applcations of Artificial \\r\\nNeural Networks to Intelligent Vehicle-Highway \\r\\nSystems,” Record (Washington)., vol. 1453, p. 83, \\r\\n1994.\\r\\n[31] H. Yin, S. Wong, J. Xu, and C. K. Wong, “Urban \\r\\ntraffic flow prediction using a fuzzy-neural approach,”\\r\\nTransp. Res. Part C Emerg. Technol., vol. 10, no. 2, \\r\\npp. 85–98, 2002.\\r\\n[32] F. Kong, J. Li, B. Jiang, and H. Song, “Short-term \\r\\ntraffic flow prediction in smart multimedia system for \\r\\nInternet of Vehicles based on deep belief network,”\\r\\nFutur. Gener. Comput. Syst., vol. 93, pp. 460–472, \\r\\n2019.\\r\\n[33] B. Liao et al., “Deep Sequence Learning with \\r\\nAuxiliary Information for Traffic Prediction,” in \\r\\nProceedings of the 24th ACM SIGKDD International \\r\\nConference on Knowledge Discovery & Data Mining, \\r\\n2018, pp. 537–546.\\r\\n[34] Y. Liang, Z. Cui, Y. Tian, H. Chen, and Y. Wang, “A \\r\\nDeep Generative Adversarial Architecture for \\r\\nNetwork-Wide Spatial-Temporal Traffic-State \\r\\nEstimation,” Transp. Res. Rec. J. Transp. Res. Board, \\r\\np. 036119811879873, Oct. 2018.\\r\\n[35] Y. Lin, X. Dai, L. Li, and F.-Y. Wang, “Pattern \\r\\nSensitive Prediction of Traffic Flow Based on \\r\\nGenerative Adversarial Framework,” IEEE Trans. \\r\\nIntell. Transp. Syst., no. 99, pp. 1–6, 2018.\\r\\n[36] Y. Duan, Y. Lv, and F.-Y. Wang, “Travel time \\r\\nprediction with LSTM neural network,” in Intelligent \\r\\nTransportation Systems (ITSC), 2016 IEEE 19th \\r\\nInternational Conference on, 2016, pp. 1053–1058.\\r\\n[37] Z. Zhao, W. Chen, X. Wu, P. C. Y. Chen, and J. Liu, \\r\\n“LSTM network: a deep learning approach for short\\x02term traffic forecast,” IET Intell. Transp. Syst., vol. 11, \\r\\nno. 2, pp. 68–75, 2017.\\r\\n[38] X. Song, H. Kanasugi, and R. Shibasaki, \\r\\n“DeepTransport: Prediction and Simulation of Human \\r\\nMobility and Transportation Mode at a Citywide \\r\\nLevel.,” in IJCAI, 2016, pp. 2618–2624.\\r\\n[39] X. Ma, Y. Li, Z. Cui, and Y. Wang, “Forecasting \\r\\nTransportation Network Speed Using Deep Capsule \\r\\nNetworks with Nested LSTM Models,” arXiv Prepr. \\r\\narXiv1811.04745, 2018.\\r\\n[40] Y. Wu, H. Tan, L. Qin, B. Ran, and Z. Jiang, “A \\r\\nhybrid deep learning based traffic flow prediction \\r\\nmethod and its understanding,” Transp. Res. Part C \\r\\nEmerg. Technol., vol. 90, pp. 166–180, May 2018.\\r\\n[41] R. Ke, W. Li, Z. Cui, and Y. Wang, “Two-Stream \\r\\nMulti-Channel Convolutional Neural Network (TM\\x02CNN) for Multi-Lane Traffic Speed Prediction \\r\\nConsidering Traffic Volume Impact,” arXiv Prepr. \\r\\narXiv1903.01678, 2019.\\r\\n[42] C. Zhang and P. Patras, “Long-term mobile traffic \\r\\nforecasting using deep spatio-temporal neural \\r\\nnetworks,” in Proceedings of the Eighteenth ACM \\r\\nInternational Symposium on Mobile Ad Hoc \\r\\nNetworking and Computing, 2018, pp. 231–240.\\r\\n[43] D. Zhang and M. R. Kabuka, “Combining weather \\r\\ncondition data to predict traffic flow: a GRU-based \\r\\ndeep learning approach,” IET Intell. Transp. Syst., vol. \\r\\n12, no. 7, pp. 578–585, 2018.\\r\\n[44] W. Jin, Y. Lin, Z. Wu, and H. Wan, “Spatio-Temporal \\r\\nRecurrent Convolutional Networks for Citywide \\r\\nShort-term Crowd Flows Prediction,” in Proceedings \\r\\nof the 2nd International Conference on Compute and \\r\\nData Analysis, 2018, pp. 28–35.\\r\\n[45] C. Chen et al., “Exploiting Spatio-Temporal \\r\\nCorrelations with Multiple 3D Convolutional Neural \\nNetworks for Citywide Vehicle Flow Prediction,” in \\r\\n2018 IEEE International Conference on Data Mining \\r\\n(ICDM), 2018, pp. 893–898.\\r\\n[46] S. Guo, Y. Lin, S. Li, Z. Chen, and H. Wan, “Deep \\r\\nSpatial-Temporal 3D Convolutional Neural Networks \\r\\nfor Traffic Data Forecasting,” IEEE Trans. Intell. \\r\\nTransp. Syst., 2019.\\r\\n[47] Y. Sun, X. Yu, R. Bie, and H. Song, “Discovering \\r\\ntime-dependent shortest path on traffic graph for \\r\\ndrivers towards green driving,” J. Netw. Comput. \\r\\nAppl., vol. 83, pp. 204–212, 2017.\\r\\n[48] H. Sun, J. Wu, D. Ma, and J. Long, “Spatial \\r\\ndistribution complexities of traffic congestion and \\r\\nbottlenecks in different network topologies,” Appl. \\r\\nMath. Model., vol. 38, no. 2, pp. 496–505, 2014.\\r\\n[49] G. Kalafatas and S. Peeta, “An exact graph structure \\r\\nfor dynamic traffic assignment: Formulation, \\r\\nproperties, and computational experience,” 2007.\\r\\n[50] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, \\r\\nand P. Vandergheynst, “The emerging field of signal \\r\\nprocessing on graphs: Extending high-dimensional \\r\\ndata analysis to networks and other irregular \\r\\ndomains,” IEEE Signal Process. Mag., vol. 30, no. 3, \\r\\npp. 83–98, 2013.\\r\\n[51] X. Geng et al., “Spatiotemporal multi-graph \\r\\nconvolution network for ride-hailing demand \\r\\nforecasting,” in 2019 AAAI Conference on Artificial \\r\\nIntelligence (AAAI’19), 2019.\\r\\n[52] Q. Zhang, Q. Jin, J. Chang, S. Xiang, and C. Pan, \\r\\n“Kernel-Weighted Graph Convolutional Network: A \\r\\nDeep Learning Approach for Traffic Forecasting,” in \\r\\n2018 24th International Conference on Pattern \\r\\nRecognition (ICPR), 2018, pp. 1018–1023.\\r\\n[53] C. F. Daganzo, “The cell transmission model: A \\r\\ndynamic representation of highway traffic consistent \\r\\nwith the hydrodynamic theory,” Transp. Res. Part B \\r\\nMethodol., vol. 28, no. 4, pp. 269–287, 1994.\\r\\n[54] K. Hunter-Zaworski et al., “Transportation \\r\\nengineering online lab manual.” 2003.\\r\\n[55] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop: \\r\\nDivide the gradient by a running average of its recent \\r\\nmagnitude,” COURSERA Neural networks Mach. \\r\\nLearn., vol. 4, no. 2, pp. 26–31, 2012.'},\n",
       " {'name': '2312.15235v1.pdf',\n",
       "  'content': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\\r\\nTong Li1*, Zhaoyang Liu2, Yanyan Shen1†, Xue Wang2, Haokun Chen2, Sen Huang2\\r\\n1 Shanghai Jiao Tong University, 2 Alibaba Group\\r\\n{2017lt, shenyy}@sjtu.edu.cn, {jingmu.lzy, xue.w, hankel.chk, huangsen.huang}@alibaba-inc.com\\r\\nAbstract\\r\\nStock price forecasting has remained an extremely challeng\\x02ing problem for many decades due to the high volatility of the\\r\\nstock market. Recent efforts have been devoted to modeling\\r\\ncomplex stock correlations toward joint stock price forecast\\x02ing. Existing works share a common neural architecture that\\r\\nlearns temporal patterns from individual stock series and then\\r\\nmixes up temporal representations to establish stock correla\\x02tions. However, they only consider time-aligned stock cor\\x02relations stemming from all the input stock features, which\\r\\nsuffer from two limitations. First, stock correlations often oc\\x02cur momentarily and in a cross-time manner. Second, the fea\\x02ture effectiveness is dynamic with market variation, which af\\x02fects both the stock sequential patterns and their correlations.\\r\\nTo address the limitations, this paper introduces MASTER, a\\r\\nMArkert-Guided Stock TransformER, which models the mo\\x02mentary and cross-time stock correlation and leverages mar\\x02ket information for automatic feature selection. MASTER el\\x02egantly tackles the complex stock correlation by alternatively\\r\\nengaging in intra-stock and inter-stock information aggrega\\x02tion. Experiments show the superiority of MASTER com\\x02pared with previous works and visualize the captured realistic\\r\\nstock correlation to provide valuable insights.\\r\\nIntroduction\\r\\nStock price forecasting, which utilizes historical data col\\x02lected from the stock market to predict future trends, is a\\r\\nvital technique for profitable stock investment. Unlike sta\\x02tionary time series that often exhibit regular patterns such\\r\\nas periodicity and steady trends, the dynamics in the stock\\r\\nprice series are intricate because stock prices fluctuate sub\\x02ject to multiple factors, including macroeconomic factors,\\r\\ncapital flows, investor sentiments, and events. The mixing\\r\\nof factors interweaves the stock market as a correlated net\\x02work, making it difficult to precisely predict the individual\\r\\nbehavior of stocks without taking other stocks into account.\\r\\nMost previous works (Feng et al. 2019; Xu et al. 2021;\\r\\nWang et al. 2021, 2022; Wang, Qu, and Chen 2022) in the\\r\\nfield of stock correlation have relied on predefined concepts,\\r\\nrelationships, or rules and established a static correlation\\r\\ngraph, e.g., stocks in the same industry are connected to each\\r\\n*This work was done during her internship at Alibaba Group.\\r\\n†Corresponding author.\\r\\nCopyright © 2024, Association for the Advancement of Artificial\\r\\nIntelligence (www.aaai.org). All rights reserved.\\r\\ntime\\r\\nstock sequences representations predictions\\r\\nattention map\\r\\nor \\r\\ngraph\\r\\ncorrelation module \\r\\nstocks Figure 1: The framework of existing works. The dashed lines\\r\\nrepresent the underlying momentary and cross-time stock\\r\\ncorrelations, which reside between some (stock1, time1),\\r\\n(stock2, time2) pairs.\\r\\nother. While these methods provide insights into the rela\\x02tions between stocks, they do not account for the real-time\\r\\ncorrelation of stocks. For example, different stocks within\\r\\nthe same industry can experience opposite price movements\\r\\non a particular day. Additionally, the pre-defined relation\\x02ships may not be generalizable to new stocks in an evolv\\x02ing market where events such as company listing, delist\\x02ing, or changes in the main business happen normally. An\\x02other line of research (Yoo et al. 2021) follows the Trans\\x02former architecture (Vaswani et al. 2017), and use the self\\x02attention mechanism to compute dynamic stock correlations.\\r\\nThis data-driven manner is more flexible and applicable to\\r\\nthe time-varying stock sets in the market. Despite differ\\x02ent schemes for establishing stock correlations, the exist\\x02ing methods generally follow a common two-step compu\\x02tation flow. As depicted in Figure 1, the first step is using a\\r\\nsequential encoder to summarize the historical sequence of\\r\\nstock features, and obtain stock representation, and the sec\\x02ond step is to refine each stock representation by aggregating\\r\\ninformation from correlated stocks using graph encoders or\\r\\nattention mechanism. However, such a flow suffers from two\\r\\nlimitations.\\r\\nFirst, existing works distill an overall stock representation\\r\\nand blur the time-specific details of stock sequence, lead\\x02ing to weakness in modeling the de-facto stock correlations,\\r\\nwhich often occurs momentarily and in a cross-time man\\x02ner (Bennett, Cucuringu, and Reinert 2022). To be specific,\\r\\nthe stock correlation is highly dynamic and may reside in\\r\\nmisaligned time steps rather than holding true through the\\r\\nwhole lookback period. This is because the dominating fac\\x02tors of stock prices constantly change, and different stocks\\r\\nmay react to the same factors with different delays. For in\\x02arXiv:2312.15235v1 [cs.CE] 23 Dec 2023\\nstance, upstream companies’ stock prices may react faster to\\r\\na shortage of raw materials than those of downstream com\\x02panies, and individual stocks exhibit a lot of catch-up and\\r\\nfall-behind behaviors.\\r\\nSince the stock correlation may underlie between every\\r\\nstock pair and time pair, a straightforward way to simu\\x02late the momentary and cross-time correlation is to gather\\r\\nthe τ × |S| feature vectors for pair-wise attention computa\\x02tion, where τ is the lookback window length and S is the\\r\\nstock set. However, in addition to the increased computa\\x02tional complexity, this approach faces practical difficulties\\r\\nbecause the stock forecasting task is in intense data hunger.\\r\\nIntuitively, there are only around 250 trading days per year,\\r\\nproducing limited observations on stocks. When the model\\r\\nadopts such a large attention field with insufficient training\\r\\nsamples, it often struggles to optimize and may even fall\\r\\ninto suboptimal solutions. Although clustering approaches\\r\\nlike local sensitive hashing (Kitaev, Kaiser, and Levskaya\\r\\n2020) have been proposed to reduce the size of the attention\\r\\nfield, they are sensitive to initialization, which is a fatal issue\\r\\nin a data-hungry domain like stock forecasting. To address\\r\\nthese challenges, we propose a novel stock transformer ar\\x02chitecture specifically designed for stock price forecasting.\\r\\nRather than directly modeling the τ × |S| attention field or\\r\\nusing clustering-based approximation methods, our model\\r\\naggregates information from different time steps and differ\\x02ent stocks alternately to model realistic stock correlation and\\r\\nfacilitate model learning.\\r\\nAnother limitation of existing works is that they ignore\\r\\nthe impact of varying market status. In long-term practice\\r\\nwith the market variation, one essential observation by in\\x02vestors is that the features come into effect and expire dy\\x02namically. The effectiveness of features has an influence on\\r\\nboth the intra-stock sequential pattern and the stock correla\\x02tion. For instance, in a bull market, the correlations among\\r\\nstocks are more significant due to the investors’ optimism.\\r\\nTraditional investors repeatedly conduct statistical examina\\x02tion on to select effective feature, which is exhaustive and\\r\\nface a gap when integrated with learning-based methods.\\r\\nTo save the human efforts, we are motivated to equip our\\r\\nstock transformer with a novel gating mechanism, which in\\x02corporates the market information to perform automatically\\r\\nfeature selection. We name the proposed method MASTER,\\r\\nstanding for MArket-Guided Stock TransformER. To sum\\x02marize, our main contributions are as follows.\\r\\n• We propose a novel stock transformer for stock price\\r\\nforecasting to effectively capture the stock correlation. To\\r\\nthe best of our knowledge, we are the first to mine the\\r\\nmomentary and cross-time stock correlation with learning\\x02based methods.\\r\\n• We introduce a novel gating mechanism that integrates\\r\\nmarket information to automatically select relevant features\\r\\nand adapt to varying market scenarios.\\r\\n• We conducted experiments to validate the designs of our\\r\\nproposed method and demonstrated its superiority compared\\r\\nto baselines. The visualization results provided valuable in\\x02sights into the real-time dynamics of stock correlations.\\r\\nMethodology\\r\\nProblem Formulation\\r\\nThe indicators of each stock u ∈ S are collected at every\\r\\ntime step t ∈ [1, τ ] to form the feature vector xu,t ∈ RF .\\r\\nFollowing existing works on stock market analysis (Feng\\r\\net al. 2018; Sawhney et al. 2020; Huynh et al. 2023), we\\r\\nfocus on the prediction of the change in stock price rather\\r\\nthan the absolute value. The return ratio, which is the rel\\x02ative close price change in d days, is r˜u = (cu,τ+d −\\r\\ncu,τ+1)/cu,τ+1, where cu,t is the closing price of stock u\\r\\nat time step t, and d represents the predetermined prediction\\r\\ninterval. The return ratio normalizes the market price vari\\x02ety between different stocks in comparison to the absolute\\r\\nprice change. Since stock investment is to rank and select\\r\\nthe most profitable stocks, we perform daily Z-score normal\\x02ization of return ratio to encode the label with the rankings,\\r\\nru = NormS (˜ru), as in previous work (Yang et al. 2020).\\r\\nDefinition 1 (Stock Price Forecasting) Given stock fea\\x02tures {xu,t}u∈S,t∈[1,τ]\\r\\n, the stock price forecasting is to\\r\\njointly predict the future normalized return ratio {ru}u∈S .\\r\\nOverview\\r\\nFigure 2 depicts the architecture of our proposed method\\r\\nMASTER, which consists of five steps. (1) Market-Guided\\r\\nGating. We construct a vector representing the current mar\\x02ket status mτ and leverage it to rescale feature vectors by\\r\\na gating mechanism, achieving market-guided feature selec\\x02tion. (2) Intra-Stock Aggregation. Within the sequence of\\r\\neach stock, at each time step, we aggregate information from\\r\\nother time steps to generate a local embedding that preserves\\r\\nthe temporal local details of the stock while collecting all\\r\\nimportant signals along the time axis. The local embedding\\r\\nhu,t will serve as relays and transport the collected signals\\r\\nto other stocks in subsequent modules. (3) Inter-Stock Ag\\x02gregation. At each time step, we compute stock correlation\\r\\nwith attention mechanism, and each stock further aggregates\\r\\nthe local embedding of other stocks. The aggregated infor\\x02mation zu,t, which we refer to as temporal embedding, con\\x02tains not only the information of the momentarily correlated\\r\\nstocks at t, but also preserves the personal information of\\r\\nu. (4) Temporal Aggregation. For each stock, the last tem\\x02poral embedding queries from all historical temporal em\\x02bedding and produce a comprehensive stock embedding eu.\\r\\n(5) Prediction. The comprehensive stock embedding is sent\\r\\nto prediction layers for label prediction. We elaborate on\\r\\nthe details of MASTER step by step in the following sub\\x02sections.\\r\\nMarket-Guided Gating\\r\\nMarket Status Representation First, we propose to com\\x02bine information from two aspects into a vector mτ to give\\r\\nan abundant description of the current market status. (1)\\r\\nMarket index price. The market index price is a weighted\\r\\naverage of the prices of a group of stocks S\\r\\n′ by their share\\r\\nof market capitalization. S\\r\\n′\\r\\nis typically composed of top\\r\\ncompanies with the most market capitalization, represent\\x02ing a particular market or sector, and may differ from user-\\n𝑦1,1 𝑦1,𝜏\\r\\ntime\\r\\n𝑦1,2 …\\r\\nℎ1,1\\r\\nstocks\\r\\n𝑧1,1 𝑧1,2 𝑧1,𝜏\\r\\n𝑒𝑢\\r\\n𝑒1\\r\\n4. Temporal Aggregation\\r\\n2. Intra-Stock Aggregation\\r\\n5. Prediction\\r\\nℎ𝑢,1\\r\\nℎ1,2\\r\\nℎ𝑢,2\\r\\nℎ1,𝜏\\r\\nℎ𝑢,𝜏\\r\\n𝑢\\r\\n𝑣\\r\\n𝑖\\r\\n𝑗\\r\\n𝑧𝑢,𝑖\\r\\nℎ𝑣,𝑖\\r\\ny𝑣,𝑗\\r\\nTime Step 𝒊\\r\\nStock 𝒗\\r\\ncross-time correlation 𝑟𝑢,𝜏\\r\\n𝑟1,𝜏\\r\\n3. Inter-Stock Aggregation\\r\\n1. Market-Guided Gating\\r\\n𝑚𝜏 Gate\\r\\n𝑥\\u0de41,1\\r\\n𝑥1,1\\r\\nFeature Layer\\r\\nFigure 2: Overview of the MASTER framework.\\r\\ninterested stocks in investing S. We include both the cur\\x02rent market index price at τ and the historical market index\\r\\nprices, which is described by the average and standard de\\x02viation in the past d\\r\\n′ days to reveal the price fluctuations.\\r\\nHere, d\\r\\n′\\r\\nspecifies the referable interval length to introduce\\r\\nhistorical market information in applications. (2) Market in\\x02dex trading volume. The trading volumes of S\\r\\n′\\r\\nreveals the\\r\\ninvestors involvement, reflecting the activity of the market.\\r\\nWe include the average and standard deviation of market in\\x02dex trading volume in the past d\\r\\n′ days, to reveal the actual\\r\\nsize of the market. S\\r\\n′\\r\\nand d\\r\\n′\\r\\nare identical to the aforemen\\x02tioned definitions. Now we present the market-guided stock\\r\\nprice forecasting task.\\r\\nDefinition 2 (Market-Guided Stock Price Forecasting)\\r\\nGiven {xu,t}u∈S,t∈[1,τ] and the constructed market status\\r\\nvector mτ , market-guided stock price forecasting is to\\r\\njointly predict the future normalized return ratio {ru}u∈S .\\r\\nGating Mechanism The gating mechanism generates one\\r\\nscaling coefficient for each feature dimension to enlarge or\\r\\nshrink the magnitude of the feature, thereby emphasizing\\r\\nor diminishing the amount of information from the feature\\r\\nflowing to the subsequent modules. The gating mechanism\\r\\nis learned by the model training, and the coefficient is opti\\x02mized by how much the feature contributes to improve fore\\x02casting performance, thus reflect the feature effectiveness.\\r\\nGiven the market status representation mτ , |mτ | = F\\r\\n′\\r\\n,\\r\\nwe first use a single linear layer to transform mτ into the\\r\\nfeature dimension F = |xu,t|. Then, we perform Softmax\\r\\nalong the feature dimension to obtain a distribution.\\r\\nα(mτ ) = F · softmaxβ(Wαmτ + bα),\\r\\nwhere Wα, bα are learnable matrix and bias, β is the temper\\x02ature hyperparameter controlling the sharpness of the output\\r\\ndistribution. Softmax compels a competition among features\\r\\nto distinguish the effective ones and ineffective ones. Here,\\r\\na smaller temperature β encourages the distribution to focus\\r\\non certain dimension and the gating effect is stronger while a\\r\\nlarger β makes the distribution incline to even and the gating\\r\\neffect is weaker. Note that we enlarge the value at each di\\x02mension by F times as the scaling coefficient. This operation\\r\\ncompare the generated distribution with a uniform distribu\\x02tion where each dimension is 1/F, to determine whether to\\r\\nenlarge or shrink the value. The intuition to generate coef\\x02ficients from mτ is that the effectiveness of features are in\\x02fluenced by market status. For example, if the model learns\\r\\nmoving average (MA) factor is useful during volatile mar\\x02ket periods, it will emphasize MA when the market becomes\\r\\nvolatile again. Under the same mτ , α are shared for {xu,t},\\r\\nu ∈ S, t ∈ [1, τ ], in that we incorporate the most recent mar\\x02ket status to perform unified feature selection. The rescaled\\r\\nfeature vectors are x˜u,t = α(mτ ) ◦ xu,t, where ◦ is the\\r\\nHadamard product.\\r\\nIntra-Stock Aggregation\\r\\nIn MASTER, we use intra-stock aggregation followed by\\r\\ninter-stock aggregation to break down the large and complex\\r\\nattention field. Although the entire market is complicated\\r\\nwith diverse behaviours of individual stocks, the patterns of\\r\\na specific stock tend to be relatively continuous. Therefore,\\r\\nwe perform intra-stock aggregation first due to its smaller at\\x02tention field and simpler distribution. In our proposed intra\\x02stock aggregation, the feature at each time step aggregate in\\x02formation from other time steps and form a local embedding.\\r\\nCompared with existing works which initially mix the fea\\x02ture sequence into one representation (Yoo et al. 2021), we\\r\\nmaintain a sequence of local embedding which are advised\\r\\nwith the important signals in sequence through intra-stock\\r\\naggregation while reserve the local details.\\r\\nWe first send the rescaled feature vectors to a feature en\\x02coder and transform them into the embedding space, yu,t =\\r\\nf(˜xu,t), |yu,t| = D. We simply use a single linear layer as\\r\\nf(·). Then, we apply a bi-directional sequential encoder to\\r\\nobtain the local output at each time step t. Inspired by the\\r\\nsuccess of transformer-based models in modeling sequential\\r\\npatterns, we instantiate the sequential encoder with a single\\x02layer transformer encoder (Vaswani et al. 2017). Each fea\\x02ture vector at a particular time step is treated as a token, and\\r\\nwe add a fixed D-dimensional sinusoidal positional encod\\x02ing pt to mark the chronically order in the look back window.\\r\\nYu = ||t∈[1,τ]LN(f(˜xu,t) + pt),\\r\\nwhere || denotes the concatenation of vectors and LN the\\nlayer normalization. Then, the feature embedding at each\\r\\ntime step queries from all time steps in the stock sequence.\\r\\nWe introduce multi-head attention mechanisms, denoted as\\r\\nMHA(·), with N1 heads to perform different aggregations in\\r\\nparallel. We also utilize feed forward layers, FFN(·), to fuse\\r\\nthe information obtained from the multi-head attention.\\r\\nQ\\r\\n1\\r\\nu = W1QYu, K1u = W1KYu, V 1u = W1V Yu,\\r\\nH1\\r\\nu = ||t∈[1,τ]hu,t = FFN1\\r\\n(MHA1(Q\\r\\n1\\r\\nu\\r\\n, K1\\r\\nu\\r\\n, V 1\\r\\nu\\r\\n) + Yu),\\r\\nwhere FFN is a two-layer MLP with ReLU activation and\\r\\nresidual connection. As a result, the local embedding hu,t\\r\\nboth reserve the local details and encode indicative signals\\r\\nfrom other time steps.\\r\\nInter-Stock Aggregation\\r\\nThen, we consider aggregating information from correlated\\r\\nstocks. Compared with existing works that distill an over\\x02all stock correlation, we establish a series of momentary\\r\\nstock correlation corresponding to every time step. Instead\\r\\nof using pre-defined relationships that face a mismatch with\\r\\nthe proximity of real-time stock movements, we propose to\\r\\nmine the asymmetric and dynamic inter-stock correlation\\r\\nvia attention-mechanism. The quality of the correlation will\\r\\nbe measured by its contribution to improving the forecast\\x02ing performance, and automatically optimized by the model\\r\\ntraining process.\\r\\nSpecifically, at each time step, we gather the local embed\\x02ding of all stocks H2\\r\\nt = ||u∈S hu,t and perform multi-head\\r\\nattention mechanism with N2 heads.\\r\\nQ\\r\\n2\\r\\nt = W2\\r\\nQH2\\r\\nt\\r\\n, K2\\r\\nt = W2\\r\\nKH2\\r\\nt\\r\\n, V 2\\r\\nt = W2\\r\\nV H2\\r\\nt\\r\\n,\\r\\nZt = ||u∈S zu,t = FFN2(MHA2(Q\\r\\n2\\r\\nt\\r\\n, K2\\r\\nt\\r\\n, V 2\\r\\nt\\r\\n) + H2\\r\\nt\\r\\n).\\r\\nWith the residual connection of FFN, the temporal embed\\x02ding zu,t is encoded with both the information from momen\\x02tarily correlated stocks and the personal information of stock\\r\\nu itself. Our stock transformer is able to model the cross\\x02time correlation of stocks, as shown in Figure 2 (Right). The\\r\\nlocal details of yv,j can first be conveyed to hv,i by the intra\\x02stock aggregation of stock v, and then transmitted to zu,i\\r\\nby inter-stock aggregation at time step i, hence modeling\\r\\nthe correlation from any (v, j) to (u, i). We further visualize\\r\\nand explain the captured cross-time correlation in the exper\\x02iments section.\\r\\nTemporal Aggregation\\r\\nIn contrast with existing works which obtain one embedding\\r\\nfor each stock after modeling stock correlation (Feng et al.\\r\\n2019), our approach involves producing a series of temporal\\r\\nembedding zu,t, t ∈ [1, τ ]. Each zu,t is encoded with in\\x02formation from stocks that are momentarily correlated with\\r\\n(u, t). To summarize the obtained temporal embeddings and\\r\\nobtain a comprehensive stock embedding eu, we employ a\\r\\ntemporal attention layer along the time axis. We use the lat\\x02est temporal embedding zu,τ as the query vector, and com\\x02pute the attention score λu,t in a hidden space with transfor\\x02mation matrix Wλ,\\r\\nλu,t =\\r\\nexp(z\\r\\nT\\r\\nu,tWλzu,τ )\\r\\nP\\r\\ni∈[1,τ]\\r\\nexp(z\\r\\nT\\r\\nu,iWλzu,τ )\\r\\n, eu =\\r\\nX\\r\\nt∈[1,τ]\\r\\nλu,tzu,t.\\r\\nPrediction and Training\\r\\nFinally, the stock embedding eu is fed into a predictor g(·)\\r\\nfor label regression. We use a single linear layer as the pre\\x02dictor, and the forecasting quality is measured by the MSE\\r\\nloss. In each batch, MASTER is jointly optimized for all\\r\\nu ∈ S on a particular prediction date. And a training epoch\\r\\nis composed of multiple batches correspond to different pre\\x02diction dates in the training set.\\r\\nrˆu = g(eu), L =\\r\\nX\\r\\nu∈S\\r\\nMSE(ru, rˆu).\\r\\nDiscussions\\r\\nRelationships with Existing Works Modeling stock cor\\x02relations has long been an indispensable research direc\\x02tion for stock price prediction. Today, many researchers\\r\\nand quantitative analysts, still opt for linear models, sup\\x02port vector machines, and tree-based methods for stock price\\r\\nforecasting (Nugroho, Adji, and Fauziati 2014; Chen and\\r\\nGuestrin 2016; Kamble 2017; Xie et al. 2013; Li et al. 2015;\\r\\nPiccolo 1990). The aggregation of correlation information\\r\\nwithin and between stocks is often achieved through fea\\x02ture engineering, which relies heavily on manual expertise\\r\\nand constantly faces the risk of factor decay. Inspired by\\r\\nthe success of neural sequential data analysis, researchers\\r\\nare driven to take into account the stock feature sequences\\r\\nand learn the temporal correlation automatically. They de\\x02sign various sequential models, such as RNN-based (Feng\\r\\net al. 2019; Sawhney et al. 2021; Yoo et al. 2021; Huynh\\r\\net al. 2023), CNN-based (Wang et al. 2021), and attention\\x02based models(Liu et al. 2019; Ding et al. 2020), to mine the\\r\\ninternal temporal dynamics of a stock. Recent research focus\\r\\non the modeling of stock correlation, which add a correlation\\r\\nmodule in posterior to the sequential model as illustrated in\\r\\nFigure 1. They propose to use graph-based (Feng et al. 2019;\\r\\nXu et al. 2021; Wang et al. 2021, 2022), hypergraph-based\\r\\n(Sawhney et al. 2021; Huynh et al. 2023) and attention\\x02based (Yoo et al. 2021; Xiang et al. 2022) modules to build\\r\\nthe overall stock correlation and perform joint prediction.\\r\\nOur MASTER is dedicated to momentary and cross-time\\r\\nstock correlation mining. To do so, we develop a novel\\r\\nmodel architecture as in Figure 2 that is genuinely different\\r\\nfrom all existing methods. Furthermore, MASTER is spe\\x02cialized for stock price forecasting, which is distinct in data\\r\\nform and task properties from existing transformer-based\\r\\nmodels in spatial-temporal data (Bulat et al. 2021; Cong\\r\\net al. 2021; Xu et al. 2020; Li et al. 2023) or multivariate\\r\\ntime series domains (Zhang and Yan 2022; Nie et al. 2022).\\r\\nComplexity Analysis We now analyze the computation\\r\\ncomplexity of our proposed method. Let M = |S|, the\\r\\nmarket-guided gating rescale M × τ feature vectors of\\r\\ndimension F. In intra-stock aggregation, the calculation\\r\\namount of pair-wise attention is τ\\r\\n2\\r\\nfor each stock at\\r\\neach attention head. In inter-stock aggregation, the calcu\\x02lation amount is M2\\r\\nat each time step and each atten\\x02tion head. In temporal aggregation, we compute τ atten\\x02tion scores for each stock. The overall computation com\\x02plexity is O(FMτ + N1Mτ 2D2 + N2M2\\r\\nτD2 + MτD2),\\nwhere M ≫ τ . Therefore, MASTER is of O(N2M2\\r\\nτD2)\\r\\ntime complexity. Compared with directly operating on the\\r\\nM × τ attention field with N attention heads, which is\\r\\nin O(NM2τ\\r\\n2D2\\r\\n), we reduce the computation cost by\\r\\nabout τ times and achieve modeling cross-time correla\\x02tions between stocks more efficiently. The overall param\\x02eters to be trained in MASTER are transformation matri\\x02ces W1\\r\\nQ, W1K, W1V\\r\\n, W2\\r\\nQ, W2K, W2V\\r\\n, Wλ, which is in shape\\r\\nD × D, and parameters in MLP layers α, f, FFN1, FFN2\\r\\nand g.\\r\\nExperiments\\r\\nIn this section, we conduct experiments to answer the fol\\x02lowing four research questions:\\r\\n• RQ1 How is the overall performance of MASTER com\\x02pared with state-of-the-art methods?\\r\\n• RQ2 Is the proposed stock transformer architecture ef\\x02fective for stock price forecasting?\\r\\n• RQ3 How do hyper-parameter configurations affect the\\r\\nperformance of MASTER?\\r\\n• RQ4 What insights on the stock correlation can we get\\r\\nthrough visualizing the attention map?\\r\\nDatasets We evaluate our framework on the Chinese stock\\r\\nmarket with CSI300 and CSI800 stock sets. CSI300 and\\r\\nCSI800 are two stock sets containing 300 and 800 stocks\\r\\nwith the highest capital value on the Shanghai Stock Ex\\x02change and the Shenzhen Stock Exchange. The dataset\\r\\ncontains daily information ranging from 2008 to 2022 of\\r\\nCSI300 and CSI800. We use the data from Q1 2008 to Q1\\r\\n2020 as the training set, data in Q2 2020 as the validation\\r\\nset, and the last ten quarters, i.e., Q3 2020 to Q4 2022, are\\r\\nreserved as the test set. We apply the public Alpha158 in\\x02dicators (Yang et al. 2020) to extract stock features from\\r\\nthe collected data. The lookback window length τ and pre\\x02diction interval d are set as 8 and 5 respectively. For mar\\x02ket representation, we constructed 63 features with CSI300,\\r\\nCSI500 and CSI800 market indices, and refereable interval\\r\\nd\\r\\n′ = 5, 10, 20, 30, 60.\\r\\nBaselines We compare the performance of MASTER with\\r\\nseveral stock price forecasting baselines from different cate\\x02gories. • XGBoost (Chen and Guestrin 2016): A decision\\x02tree based method. According to the leaderboard of Qlib\\r\\nplatform (Yang et al. 2020), it is one of the strongest base\\x02lines. • LSTM (Graves and Graves 2012), GRU (Cho et al.\\r\\n2014), TCN (Bai, Kolter, and Koltun 2018), and Trans\\x02former (Vaswani et al. 2017): Sequential baselines that lever\\x02age vanilla LSTM/GRU/Temporal convolutional network/-\\r\\nTransformer along the time axis for stock price forecasting.\\r\\n• GAT (Velickovi ˇ c et al. 2017): A graph-based baseline, ´\\r\\nwhich first use sequential encoder to gain stock presenta\\x02tion and then aggregate information by graph attention net\\x02works1\\r\\n. • DTML (Yoo et al. 2021): A state-of-the-art stock\\r\\ncorrelation mining method, which follows the framework in\\r\\nFigure 1. DTML adopts the attention-mechanism to mine\\r\\n1More discussion is provided in the supplementary materials.\\r\\nthe dynamic correlation among stocks and also incorporates\\r\\nthe market information into the modeling.\\r\\nEvaluation We adopt both ranking metrics and portfolio\\x02based metrics to give a thorough evaluation of the model\\r\\nperformance. Four ranking metrics, Information Coefficient\\r\\n(IC), Rank Information Coefficient (RankIC), Information\\r\\nRatio based IC (ICIR) and Information Ratio based RankIC\\r\\n(RankICIR) are considered. IC and RankIC are the Pearson\\r\\ncoefficient and Spearman coefficient averaged at a daily fre\\x02quency. ICIR and RankICIR are normalized metrics of IC\\r\\nand RankIC by dividing the standard deviation. Those met\\x02rics are commonly used in literature (e.g., Xu et al. 2021 and\\r\\nYang et al. 2020) to describe the performance of the forecast\\x02ing results from the value and rank perspectives. Further\\x02more, we employ two portfolio-based metrics to compare\\r\\nthe investment profit and risk of each method. We simulate\\r\\ndaily trading using a simple strategy that selects the top 30\\r\\nstocks with the highest return ratio and reports the Excess\\r\\nAnnualized Return (AR) and Information Ratio (IR) met\\x02rics. AR measures the annual expected excess return gener\\x02ated by the investment, while IR measures the risk-adjusted\\r\\nperformance of an investment.\\r\\nImplementation We implemented MASTER2 with Py\\x02Torch and build our methods based on the open-source quan\\x02titative investment platform Qlib (Yang et al. 2020). For\\r\\nDTML, we implement it based on the original paper since\\r\\nthere is no official implementation publicly. For other base\\x02lines, we use their Qlib implementations. For hyperparame\\x02ters of each baseline method, the layer number and model\\r\\nsize are tuned from {1, 2, 3} and {128, 256, 512} respec\\x02tively. The learning rate lr is tuned among {10−i}i∈{3,4,5,6},\\r\\nand we selected the best hyperparameters based on the IC\\r\\nperformance in the validation stage. For hyperparameters of\\r\\nMASTER, we tune the model size D and learning rate lr\\r\\namong the same range as the baselines, and the final selec\\x02tion is D=256, lr=10−5\\r\\nfor all datasets; we set N1=4, N2=2\\r\\nfor all datasets and β=5 and β=2 for CSI300 and CSI800\\r\\nrespectively. More implementation details of baseline meth\\x02ods are summarized in the supplementary materials. Each\\r\\nmodel is trained for at most 40 epochs with early stopping.\\r\\nAll the experiments are conducted on a server equipped with\\r\\nIntel(R) Xeon(R) Platinum 8163 CPU, 128GB Memory, and\\r\\na Tesla V100-SXM2 GPU (16GB Memory). Each experi\\x02ment was repeated 5 times with random initialization and\\r\\nthe average performance was reported.\\r\\nOverall Performance (RQ1)\\r\\nThe overall performance are reported in Table 1 MAS\\x02TER achieves the best results on 6/8 of the ranking met\\x02rics, and consistently outperforms all benchmarks in the\\r\\nportfolio-based metrics. In particular, MASTER achieve\\r\\n13% improvements in ranking metrics and 47% improve\\x02ments in portfolio-based metrics compared to the second\\x02best results on the average sense. Note that ranking matrics\\r\\nare computed with the whole set and portfolio-based metrics\\r\\n2Code and supplementary materials are at https://github.com/\\r\\nSJTU-Quant/MASTER\\nDataset Model IC ICIR RankIC RankICIR AR IR\\r\\nCSI300\\r\\nXGBoost 0.051 ± 0.001 0.37 ± 0.01 0.050 ± 0.001 0.36 ± 0.01 0.23 ± 0.03 1.9 ± 0.3\\r\\nLSTM 0.049 ± 0.001 0.41 ± 0.01 0.051 ± 0.002 0.41 ± 0.03 0.20 ± 0.04 2.0 ± 0.4\\r\\nGRU 0.052 ± 0.004 0.35 ± 0.04 0.052 ± 0.005 0.34 ± 0.04 0.19 ± 0.04 1.5 ± 0.3\\r\\nTCN 0.050 ± 0.002 0.33 ± 0.04 0.049 ± 0.002 0.31 ± 0.04 0.18 ± 0.05 1.4 ± 0.5\\r\\nTransformer 0.047 ± 0.007 0.39 ± 0.04 0.051 ± 0.002 0.42 ± 0.04 0.22 ± 0.06 2.0 ± 0.4\\r\\nGAT 0.054 ± 0.002 0.36 ± 0.02 0.041 ± 0.002 0.25 ± 0.02 0.19 ± 0.03 1.3 ± 0.3\\r\\nDTML 0.049 ± 0.006 0.33 ± 0.04 0.052 ± 0.005 0.33 ± 0.04 0.21 ± 0.03 1.7 ± 0.3\\r\\nMASTER 0.064∗ ± 0.006 0.42 ± 0.04 0.076∗ ± 0.005 0.49 ± 0.04 0.27 ± 0.05 2.4 ± 0.4\\r\\nCSI800\\r\\nXGBoost 0.040 ± 0.000 0.37 ± 0.01 0.047 ± 0.000 0.42 ± 0.01 0.08 ± 0.02 0.6 ± 0.2\\r\\nLSTM 0.028 ± 0.002 0.32 ± 0.02 0.039 ± 0.002 0.41 ± 0.03 0.09 ± 0.02 0.9 ± 0.2\\r\\nGRU 0.039 ± 0.002 0.36 ± 0.05 0.044 ± 0.003 0.39 ± 0.07 0.07 ± 0.04 0.6 ± 0.3\\r\\nTCN 0.038 ± 0.002 0.33 ± 0.04 0.045 ± 0.002 0.38 ± 0.05 0.05 ± 0.04 0.4 ± 0.3\\r\\nTransformer 0.040 ± 0.003 0.43 ± 0.03 0.048 ± 0.003 0.51 ± 0.05 0.13 ± 0.04 1.1 ± 0.3\\r\\nGAT 0.043 ± 0.002 0.39 ± 0.02 0.042 ± 0.002 0.35 ± 0.02 0.10 ± 0.04 0.7 ± 0.3\\r\\nDTML 0.039 ± 0.004 0.29 ± 0.03 0.053 ± 0.008 0.37 ± 0.06 0.16 ± 0.03 1.3 ± 0.2\\r\\nMASTER 0.052∗ ± 0.006 0.40 ± 0.06 0.066 ± 0.007 0.48 ± 0.06 0.28∗ ± 0.02 2.3\\r\\n∗ ± 0.3\\r\\nTable 1: Overall performance comparison. The best results are in bold and the second-best results are underlined. And * denotes\\r\\nstatistically significant improvement (measured by t-test with p-value < 0.01) over all baselines.\\r\\nModel IC ICIR RankIC RankICIR AR IR\\r\\n(MA)STER 0.064 ± 0.003 0.43 ± 0.02 0.074 ± 0.004 0.48 ± 0.04 0.25 ± 0.03 2.1 ± 0.3\\r\\n(MA)STER-Bi 0.058 ± 0.005 0.38 ± 0.04 0.066 ± 0.008 0.41 ± 0.05 0.19 ± 0.03 1.6 ± 0.2\\r\\nNaive 0.041 ± 0.008 0.30 ± 0.05 0.046 ± 0.007 0.32 ± 0.04 0.18 ± 0.05 1.6 ± 0.6\\r\\nClustering 0.044 ± 0.003 0.36 ± 0.02 0.049 ± 0.005 0.39 ± 0.04 0.18 ± 0.04 1.7 ± 0.3\\r\\nTable 2: Experiments on CSI300 to validate the effectiveness of proposed stock transformer architecture. The best results are in\\r\\nbold and the second-best results are underlined.\\r\\nmostly consider the 30 top-performed stocks. The achieve\\x02ments in both types of metrics imply that MASTER is of\\r\\ngood predicting ability on the whole stock set without sacri\\x02ficing the accuracy of the important stocks. The significant\\r\\nimprovements cast light on the importance of stock correla\\x02tion modeling, so each stock can also benefit from the his\\x02torical signals of other momentarily correlated stocks. We\\r\\nalso observe all methods gain better performance on CSI300\\r\\nover CSI800. We believe it is because CSI300 consists of\\r\\ncompanies with larger capitalization whose stock prices are\\r\\nmore predictable. When compared to the existing stock cor\\x02relation method (i.e., DTML), MASTER outperforms in all\\r\\n6 metrics, which tells our proposed Market-Guided Gat\\x02ing and aggregation techniques are more efficient in mining\\r\\ncross-stock information than existing literature.\\r\\nStock Transformer Architecture (RQ2)\\r\\nWe validate the effectiveness of our specialized stock trans\\x02former architecture by experiments on four settings. (1)\\r\\n(MA)STER, which is our stock transformer without the gat\\x02ing. (2) (MA)STER-Bi, in which we substitute the single\\x02layer transformer encoder with a bi-directional LSTM to\\r\\nevince that the effectiveness of our proposed architecture\\r\\nis not coupled with strong sequential encoders. (3) Naive,\\r\\nwhich directly performs information aggregation among τ ×\\r\\n|S| tokens. (4) Clustering, in which we adapt the Local Sen\\x02sitive Hashing (Kitaev, Kaiser, and Levskaya 2020) to allo\\x02cate all tokens into 10 buckets by similarity and perform ag\\x02gregation within each group, which is a classic task-agnostic\\r\\ntechnique to reduce the scale of the attention field. For a fair\\r\\ncomparison, in (3) and (4), we first use the same transformer\\r\\nencoder to extract token embedding and then use the same\\r\\nmulti-head attention mechanism as in our stock transformer,\\r\\nso the only difference is the attention field. Due to resource\\r\\nlimits, we only conduct experiment on CSI300 dataset. The\\r\\nresults in Table 2 illustrate the efficacy of our tailored stock\\r\\ntransformer architecture, which performs intra-stock aggre\\x02gation and inter-stock aggregation alternatively.\\r\\nAblation Study (RQ3)\\r\\nFirst, we conduct ablation study on (N1, N2) combination.\\r\\nThe results of CSI300 are shown in Figure 3 and the results\\r\\non CSI800 are similar. The difference among head combina\\x02tions is not significant compared with the inherent variance\\r\\nunder each setting. In the studied range, most settings con\\x02sistently performed better than the baselines.\\r\\nSecond, we study the influence of temperature β in the\\r\\ngating mechanism. As explained before, a smaller β forces\\r\\na stronger feature selection while a larger β turns off the gat\\x02ing effect. Figure 4 shows the performance with varying β.\\r\\nThe CSI300 is a relatively easier dataset where most fea\\x02tures are quite effective, so the temperature is expected to\\r\\nbe larger to relax the feature selection, while more powerful\\r\\nfeature selection intervention is needed for the sophisticated\\nN\\r\\n1 1\\r\\n2\\r\\n4\\r\\nN2\\r\\n1\\r\\n2\\r\\n4\\r\\n0.056\\r\\n0.064\\r\\nIC\\r\\nN\\r\\n1 1\\r\\n2\\r\\n4\\r\\nN2\\r\\n1\\r\\n2\\r\\n4\\r\\n0.40\\r\\n0.44\\r\\nICIR\\r\\nN\\r\\n1 1\\r\\n2\\r\\n4\\r\\nN2\\r\\n1\\r\\n2\\r\\n4\\r\\n0.06\\r\\n0.07\\r\\n0.08\\r\\nRankIC\\r\\nN\\r\\n1 1\\r\\n2\\r\\n4\\r\\nN2\\r\\n1\\r\\n2\\r\\n4\\r\\n0.40\\r\\n0.48\\r\\nRankICIR\\r\\nN\\r\\n1 1\\r\\n2\\r\\n4\\r\\nN2\\r\\n1\\r\\n2\\r\\n4\\r\\n0.18\\r\\n0.24\\r\\n0.30\\r\\nAR\\r\\nN\\r\\n1 1\\r\\n2\\r\\n4\\r\\nN2\\r\\n1\\r\\n2\\r\\n4\\r\\n1.6\\r\\n2.0\\r\\n2.4\\r\\nIR\\r\\nFigure 3: The average and standard deviation of metrics with different (N1, N2) combinations on CSI300.\\r\\n1 2 5 10 20\\r\\n0.040\\r\\n0.048\\r\\n0.056\\r\\n0.064\\r\\nIC\\r\\n1 2 5 10 20\\r\\n0.32\\r\\n0.36\\r\\n0.40\\r\\n0.44\\r\\nICIR\\r\\n1 2 5 10 20\\r\\n0.056\\r\\n0.064\\r\\n0.072\\r\\n0.080\\r\\nRankIC\\r\\n1 2 5 10 20\\r\\n0.40\\r\\n0.44\\r\\n0.48\\r\\n0.52\\r\\nRankICIR\\r\\n1 2 5 10 20\\r\\n0.15\\r\\n0.20\\r\\n0.25\\r\\n0.30\\r\\nAR\\r\\n1 2 5 10 20\\r\\n1.2\\r\\n1.6\\r\\n2.0\\r\\n2.4\\r\\n2.8\\r\\nIR\\r\\nCSI300\\r\\nCSI800\\r\\nFigure 4: MASTER performance with varying β. The horizontal dash lines are performance without market-guided gating.\\r\\n1\\r\\n8\\r\\nCNPC(SH601857)\\r\\n1\\r\\n8\\r\\nICBC(SH601398)\\r\\n1\\r\\n8\\r\\nCATL(SZ300750)\\r\\nAvg.\\r\\n0.01\\r\\n0.02\\r\\n0.03\\r\\nFigure 5: The correlation towards three target stocks on Aug\\r\\n19th, 2022. The y-axis is time steps in the lookback win\\x02dow and the x-axis is source stocks. Avg. denotes the evenly\\r\\ndistributed value.\\r\\nCSI800 dataset whose β of the best performance is smaller.\\r\\nVisualization of Attention Maps (RQ4)\\r\\nWe show how MASTER captures the momentary and cross\\x02time stock correlation that previous methods are not ex\\x02pressive enough to model. Figure 5 shows the inter-stock\\r\\nattention map at different time steps in the lookback win\\x02dow. We choose three representative stocks as the target\\r\\nand sample 100 random stocks as sources for visualiza\\x02tion. The highlighted part is scattered instead of exhibit\\x02ing neat strips, implying that the correlation is momentary\\r\\nrather than long-standing. Also, the inter-stock correlation\\r\\nis sparse, with only a few stocks having strong correlations\\r\\ntoward the target stocks. Figure 6 displays the correlation\\r\\nbetween stock pairs to show how the correlation resides in\\r\\ntime. From source stock v to target stock u, we compute\\r\\nIu←v[i, j] = S1\\r\\nv\\r\\n[i, j]S2\\r\\ni\\r\\n[u, v] as the τ × τ correlation map,\\r\\nwhile S\\r\\n1\\r\\nand S\\r\\n2\\r\\nare the intra-stock and inter-stock atten\\x02tion map. First, the highlighted blocks are not centered on\\r\\nthe diagonal, because the stock correlation is usually cross\\x02time rather than temporally aligned. Second, the left two\\r\\nfigures are totally different, illustrating that correlation is\\r\\n1 2 3 4 5 6 7 8\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n7\\r\\n8\\r\\nCNPC CATL, 19th\\r\\n1 2 3 4 5 6 7 8\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n7\\r\\n8\\r\\nCATL CNPC, 19th\\r\\n1 2 3 4 5 6 7 8\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n7\\r\\n8\\r\\nCNPC ICBC, 19th\\r\\n1 2 3 4 5 6 7 8\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n5\\r\\n6\\r\\n7\\r\\n8\\r\\nCNPC ICBC, 25th\\r\\nAvg.\\r\\n0.001\\r\\n0.002\\r\\nFigure 6: Cross-time correlation of stock pairs on Aug 19th\\r\\nand 25th, 2022. The x-axis is the source time steps and the\\r\\ny-axis is the target time steps.\\r\\nhighly asymmetric between u ← v and v ← u. Third, the\\r\\nimportance of mined correlation changes slowly when the\\r\\nlookback window slides to forecast on different dates. For\\r\\nexample, blocked regions in the right two figures correspond\\r\\nto the same absolute time scope of different prediction dates,\\r\\nwhose patterns are to a certain degree similar.\\r\\nConclusion\\r\\nWe introduce a novel method MASTER for stock price\\r\\nforecasting, which models the realistic stock correlation\\r\\nand guide feature selection with market information. MAS\\x02TER consists of five steps, market-guided gating, intra-stock\\r\\naggregation, inter-stock aggregation, temporal aggregation,\\r\\nand prediction. Experiments on the Chinese market with 2\\r\\nstock universe shows that MASTER achieves averagely 13%\\r\\nimprovements on ranking metrics and 47% on portfolio\\x02based metrics compared with all baselines. Visualization of\\r\\nattention maps reveals the de-facto momentary and cross\\x02time stock correlation. In conclusion, we provide a more\\r\\ngranular perspective for studying stock correlation, while\\r\\nalso indicating an effective application of market informa\\x02tion. Future work can explore to mine stock correlations of\\r\\nhigher quality and study other uses of market information.\\nAcknowledgements\\r\\nThe authors would like to thank the anonymous review\\x02ers for their insightful reviews. This work is supported\\r\\nby the National Key Research and Development Program\\r\\nof China (2022YFE0200500), Shanghai Municipal Sci\\x02ence and Technology Major Project (2021SHZDZX0102),\\r\\nand SJTU Global Strategic Partnership Fund (2021SJTU\\x02HKUST).\\r\\nReferences\\r\\nBai, S.; Kolter, J. Z.; and Koltun, V. 2018. An empirical\\r\\nevaluation of generic convolutional and recurrent networks\\r\\nfor sequence modeling. arXiv preprint arXiv:1803.01271.\\r\\nBennett, S.; Cucuringu, M.; and Reinert, G. 2022. Lead–\\r\\nlag detection and network clustering for multivariate time\\r\\nseries with an application to the US equity market. Machine\\r\\nLearning, 111(12): 4497–4538.\\r\\nBulat, A.; Perez Rua, J. M.; Sudhakaran, S.; Martinez, B.;\\r\\nand Tzimiropoulos, G. 2021. Space-time mixing attention\\r\\nfor video transformer. Advances in neural information pro\\x02cessing systems, 34: 19594–19607.\\r\\nChen, T.; and Guestrin, C. 2016. Xgboost: A scalable tree\\r\\nboosting system. In Proceedings of the 22nd acm sigkdd\\r\\ninternational conference on knowledge discovery and data\\r\\nmining, 785–794.\\r\\nCho, K.; Van Merrienboer, B.; Gulcehre, C.; Bahdanau, ¨\\r\\nD.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014.\\r\\nLearning phrase representations using RNN encoder\\x02decoder for statistical machine translation. arXiv preprint\\r\\narXiv:1406.1078.\\r\\nCong, Y.; Liao, W.; Ackermann, H.; Rosenhahn, B.; and\\r\\nYang, M. Y. 2021. Spatial-temporal transformer for dynamic\\r\\nscene graph generation. In Proceedings of the IEEE/CVF in\\x02ternational conference on computer vision, 16372–16382.\\r\\nDing, Q.; Wu, S.; Sun, H.; Guo, J.; and Guo, J. 2020. Hier\\x02archical Multi-Scale Gaussian Transformer for Stock Move\\x02ment Prediction. In IJCAI, 4640–4646.\\r\\nFeng, F.; Chen, H.; He, X.; Ding, J.; Sun, M.; and Chua,\\r\\nT.-S. 2018. Enhancing stock movement prediction with ad\\x02versarial training. arXiv preprint arXiv:1810.09936.\\r\\nFeng, F.; He, X.; Wang, X.; Luo, C.; Liu, Y.; and Chua, T.-\\r\\nS. 2019. Temporal relational ranking for stock prediction.\\r\\nACM Transactions on Information Systems (TOIS), 37(2):\\r\\n1–30.\\r\\nGraves, A.; and Graves, A. 2012. Long short-term mem\\x02ory. Supervised sequence labelling with recurrent neural\\r\\nnetworks, 37–45.\\r\\nHuynh, T. T.; Nguyen, M. H.; Nguyen, T. T.; Nguyen, P. L.;\\r\\nWeidlich, M.; Nguyen, Q. V. H.; and Aberer, K. 2023. Ef\\x02ficient integration of multi-order dynamics and internal dy\\x02namics in stock movement prediction. In Proceedings of the\\r\\nSixteenth ACM International Conference on Web Search and\\r\\nData Mining, 850–858.\\r\\nKamble, R. A. 2017. Short and long term stock trend predic\\x02tion using decision tree. In 2017 International Conference\\r\\non Intelligent Computing and Control Systems (ICICCS),\\r\\n1371–1375. IEEE.\\r\\nKitaev, N.; Kaiser, Ł.; and Levskaya, A. 2020. Reformer:\\r\\nThe efficient transformer. arXiv preprint arXiv:2001.04451.\\r\\nLi, L.; Duan, L.; Wang, J.; He, C.; Chen, Z.; Xie, G.;\\r\\nDeng, S.; and Luo, Z. 2023. Memory-Enhanced Trans\\x02former for Representation Learning on Temporal Heteroge\\x02neous Graphs. Data Science and Engineering, 8(2): 98–111.\\r\\nLi, Q.; Jiang, L.; Li, P.; and Chen, H. 2015. Tensor-based\\r\\nlearning for predicting stock movements. In Proceedings of\\r\\nthe AAAI Conference on Artificial Intelligence, volume 29.\\r\\nLiu, J.; Lin, H.; Liu, X.; Xu, B.; Ren, Y.; Diao, Y.; and\\r\\nYang, L. 2019. Transformer-based capsule network for stock\\r\\nmovement prediction. In Proceedings of the first workshop\\r\\non financial technology and natural language processing,\\r\\n66–73.\\r\\nNie, Y.; Nguyen, N. H.; Sinthong, P.; and Kalagnanam, J.\\r\\n2022. A Time Series is Worth 64 Words: Long-term Fore\\x02casting with Transformers. In The Eleventh International\\r\\nConference on Learning Representations.\\r\\nNugroho, F. S. D.; Adji, T. B.; and Fauziati, S. 2014. Deci\\x02sion support system for stock trading using multiple indica\\x02tors decision tree. In 2014 The 1st International Conference\\r\\non Information Technology, Computer, and Electrical Engi\\x02neering, 291–296. IEEE.\\r\\nPiccolo, D. 1990. A distance measure for classifying\\r\\nARIMA models. Journal of time series analysis, 11(2): 153–\\r\\n164.\\r\\nSawhney, R.; Agarwal, S.; Wadhwa, A.; Derr, T.; and Shah,\\r\\nR. R. 2021. Stock selection via spatiotemporal hypergraph\\r\\nattention network: A learning to rank approach. In Proceed\\x02ings of the AAAI Conference on Artificial Intelligence, vol\\x02ume 35, 497–504.\\r\\nSawhney, R.; Agarwal, S.; Wadhwa, A.; and Shah, R. R.\\r\\n2020. Spatiotemporal hypergraph convolution network for\\r\\nstock movement forecasting. In 2020 IEEE International\\r\\nConference on Data Mining (ICDM), 482–491. IEEE.\\r\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\r\\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At\\x02tention is all you need. Advances in neural information pro\\x02cessing systems, 30.\\r\\nVelickovi ˇ c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, ´\\r\\nP.; and Bengio, Y. 2017. Graph attention networks. arXiv\\r\\npreprint arXiv:1710.10903.\\r\\nWang, H.; Li, S.; Wang, T.; and Zheng, J. 2021. Hierarchi\\x02cal Adaptive Temporal-Relational Modeling for Stock Trend\\r\\nPrediction. In IJCAI, 3691–3698.\\r\\nWang, H.; Wang, T.; Li, S.; Zheng, J.; Guan, S.; and Chen,\\r\\nW. 2022. Adaptive long-short pattern transformer for stock\\r\\ninvestment selection. In Proceedings of the Thirty-First\\r\\nInternational Joint Conference on Artificial Intelligence,\\r\\n3970–3977.\\r\\nWang, Y.; Qu, Y.; and Chen, Z. 2022. Review of graph con\\x02struction and graph learning in stock price prediction. Pro\\x02cedia Computer Science, 214: 771–778.\\r\\nXiang, S.; Cheng, D.; Shang, C.; Zhang, Y.; and Liang, Y.\\r\\n2022. Temporal and Heterogeneous Graph Neural Network\\r\\nfor Financial Time Series Prediction. In Proceedings of\\nthe 31st ACM International Conference on Information &\\r\\nKnowledge Management, 3584–3593.\\r\\nXie, B.; Passonneau, R.; Wu, L.; and Creamer, G. G. 2013.\\r\\nSemantic frames to predict stock price movement. In Pro\\x02ceedings of the 51st annual meeting of the association for\\r\\ncomputational linguistics, 873–883.\\r\\nXu, M.; Dai, W.; Liu, C.; Gao, X.; Lin, W.; Qi, G.-J.; and\\r\\nXiong, H. 2020. Spatial-temporal transformer networks for\\r\\ntraffic flow forecasting. arXiv preprint arXiv:2001.02908.\\r\\nXu, W.; Liu, W.; Wang, L.; Xia, Y.; Bian, J.; Yin, J.; and Liu,\\r\\nT.-Y. 2021. Hist: A graph-based framework for stock trend\\r\\nforecasting via mining concept-oriented shared information.\\r\\narXiv preprint arXiv:2110.13716.\\r\\nYang, X.; Liu, W.; Zhou, D.; Bian, J.; and Liu, T.-Y. 2020.\\r\\nQlib: An ai-oriented quantitative investment platform. arXiv\\r\\npreprint arXiv:2009.11189.\\r\\nYoo, J.; Soun, Y.; Park, Y.-c.; and Kang, U. 2021. Accu\\x02rate multivariate stock movement prediction via data-axis\\r\\ntransformer with multi-level contexts. In Proceedings of the\\r\\n27th ACM SIGKDD Conference on Knowledge Discovery &\\r\\nData Mining, 2037–2045.\\r\\nZhang, Y.; and Yan, J. 2022. Crossformer: Transformer uti\\x02lizing cross-dimension dependency for multivariate time se\\x02ries forecasting. In The Eleventh International Conference\\r\\non Learning Representations.'},\n",
       " {'name': '2405.10584v1.pdf',\n",
       "  'content': '1 \\r\\nA Hybrid Deep Learning Framework for Stock Price Prediction Considering the \\r\\nInvestor Sentiment of Online Forum Enhanced by Popularity\\r\\nHuiyu Lia\\r\\n, Junhua Hua,* \\r\\na: School of Business, Central South University, Changsha 410083, China\\r\\nEmail address: huiyu_li@csu.edu.cn (H.L.); hujunhua@csu.edu.cn (J.H.);\\r\\nCorresponding author*: Junhua Hu (Email: hujunhua@csu.edu.cn) \\r\\nAbstract: Stock price prediction has always been a difficult task for forecasters. Using \\r\\ncutting-edge deep learning techniques, stock price prediction based on investor \\r\\nsentiment extracted from online forums has become feasible. We propose a novel \\r\\nhybrid deep learning framework for predicting stock prices. The framework leverages \\r\\nthe XLNET model to analyze the sentiment conveyed in user posts on online forums, \\r\\ncombines these sentiments with the post popularity factor to compute daily group \\r\\nsentiments, and integrates this information with stock technical indicators into an \\r\\nimproved BiLSTM-highway model for stock price prediction. Through a series of \\r\\ncomparative experiments involving four stocks on the Chinese stock market, it is \\r\\ndemonstrated that the hybrid framework effectively predicts stock prices. This study \\r\\nreveals the necessity of analyzing investors\\' textual views for stock price prediction.\\r\\nKeywords: stock price prediction; investor sentiment; post text analysis; XLNET \\r\\nmodel; BiLSTM model \\r\\n1 Introduction\\r\\nThe stock market is an active marketplace for exchanging financial instruments \\r\\nand is vital to the world economy. At the heart of this market is the continuous \\r\\nfluctuation of stock prices, which is impacted by several factors, including corporate \\r\\nperformance, financial indicators, and investor sentiment (Hamburger & Kochin, \\r\\n1972; Shiller et al., 1984). Therefore, accurately forecasting stock prices is paramount \\r\\nfor shareholders, financial experts, and policymakers. It enables stakeholders to make \\r\\ninformed decisions, manage financial risks effectively, and optimize investment \\r\\nstrategies (Kanwal et al., 2022). \\r\\nVarious factors such as politics, economics, markets, technology and investor \\r\\nbehavior may cause stock prices to fluctuate, which means that the process of stock \\r\\nprice formation is very complex (Jin et al., 2020). Because of the turbulent and erratic \\r\\ncharacter of the stock market, predicting stock market movements is one of the most \\n2\\r\\ncomplex tasks in time series forecasting (Picasso et al., 2019). The efficient market \\r\\nhypothesis (Fama, 1991) suggests that stock price fluctuations are only affected by \\r\\nnew information. Given the unpredictability of news, stock prices should theoretically \\r\\nfollow a random wandering pattern, which implies that they are unpredictable.\\r\\nHowever, many studies contradict this viewpoint. These studies achieve stock price \\r\\nforecasting by drawing on historical experience and data. (Hafezi et al., 2015; Junqué \\r\\nde Fortuny et al., 2014; Lu et al., 2021; Rezaei et al., 2021; Selvin et al., 2017; Sharaf \\r\\net al., 2021).\\r\\nNevertheless, accurate forecasting of stock price trends remains unresolved.\\r\\nAlthough traditional time-series methods provide valuable historical context, they \\r\\ncannot cover the complex elements of stock markets and the tremendous amount of \\r\\ninformation available. Therefore, they may not precisely capture the dynamics of \\r\\nstock price changes (Dimic et al., 2016). As a result, several researchers have \\r\\nattempted to improve stock price forecasting by explaining the relationship between \\r\\npast performance and current pricing (Guo et al., 2022; Kumar et al., 2021; Yun et al., \\r\\n2021). \\r\\nIn recent years, the proliferation of online forum platforms such as Seeking \\r\\nAlpha, StockTwits, and Reddit has provided a medium for retail investors and experts \\r\\nto express their views on stock trends through posts and blogs. According to \\r\\ninformation cascade theory in behavioral finance (Anderson & Holt, 1997; \\r\\nBikhchandani et al., 1992), individual investors tend to be influenced by their \\r\\npredecessors, adjusting their preferences and following their predecessors\\' choices.\\r\\nTherefore, many stock market-related opinions on online forums can shape investors\\'\\r\\ndecision-making behavior and influence stock market activity. Previous studies have\\r\\nshown that extracting information from textual sources such as news, investor forums, \\r\\nand social networks can help predict stock market movements (Costola et al., 2023; \\r\\nHuang et al., 2023; Pedersen, 2022). That is, these texts are rich vehicles for \\r\\nexpressing investor sentiment and opinions. Compared to numerical indicators such as \\r\\nsurvey indices and proxies, they can reflect stock market dynamics quickly and \\r\\ndirectly (Eachempati et al., 2022). Thus, a significant research question arises: how \\r\\ncan the valuable insights from these texts be effectively utilized to predict stock prices \\r\\nand support investment decisions?\\r\\nTo this end, this study proposes a hybrid framework for stock price prediction\\r\\nthat integrates text analysis, sentiment processing and prediction modules. The \\n3\\r\\nXLNET model is first fine-tuned to analyze the text of online forum posts to obtain \\r\\nthe sentiment information of each post. Subsequently, a daily sentiment index is \\r\\nformed by considering the post sentiment information and the popularity factor.\\r\\nFinally, the sentiment index and technical indicators are merged and fed through an \\r\\nimproved BiLSTM to predict stock prices. Our contributions can be summarized as \\r\\nfollows:\\r\\n(1) We propose a novel framework for stock price forecasting, the process of \\r\\nwhich is described above.\\r\\n(2) We explore whether the introduction of the popularity factor effectively \\r\\nenhances investor sentiment responsiveness to stock prices.\\r\\n(3) Extensive experiments on four stocks confirm the framework\\'s efficacy and \\r\\nits components and improvements, such as incorporating investor sentiment for stock \\r\\nprice prediction and utilizing the highway mechanism for capturing time-series \\r\\ninformation.\\r\\nIn short, this study offers new insights into stock market prediction through \\r\\ntextual sentiment analysis, provides a comprehensive and practical approach to stock \\r\\nmarket forecasting and investment decisions, and underscores the effectiveness of \\r\\nretail investors\\' sentiment for predicting stock prices. The rest of the paper is \\r\\nstructured as follows: Section 2 offers an overview of prior research relevant to this \\r\\npaper; Section 3 outlines the proposed hybrid framework for stock price prediction \\r\\nand provides a detailed explanation of the functioning of each module; Section 4\\r\\ndescribes the experimental procedure; Section 5 presents the experimental results; and \\r\\nfinally, Section 6 summarizes the findings and conclusions of this paper.\\r\\n2 Related work\\r\\nIn this section, we will primarily review the research related to our paper, which \\r\\nmainly includes how to analyze texts on social media, time series prediction of the \\r\\nstock market, and the relationship between investor sentiment and the stock market.\\r\\n2.1 Text analysis on social media\\r\\nHuman language is a typical form of unstructured data (Gharehchopogh & \\r\\nKhalifelu, 2011), and its feature extraction, analysis, and understanding are hot topics \\r\\nin research. The development of relevant technologies has undergone four stages: \\r\\nrule-based, statistical, deep learning, and large-scale pretrained language model \\r\\n(LPLM) (Dong, 2023).\\n4\\r\\nWith the widespread use of LPLM and the improvement of computer \\r\\ncomputational power, analyzing a significant amount of textual data on the internet \\r\\nhas become possible. A typical research scenario is text sentiment analysis in the \\r\\ninvestment market. Specifically, it has been demonstrated that news affects stock price \\r\\nreturns (Li et al., 2014), and the text analysis techniques used for this purpose have \\r\\ngradually shifted from dictionary-based models to LPLM, such as Transformers \\r\\n(Mishev et al., 2020).\\r\\nMost text sentiment analysis studies focus on financial news headlines because \\r\\nthey provide concise and critical information (Johnman et al., 2018). With the \\r\\nenhanced ability of the LPLM to nonlinearly map text features, some studies have \\r\\nstarted to explore the use of multisource text data, including individual investor \\r\\ncomments, for stock price prediction (Wu et al., 2022). However, compared to news \\r\\nprofessionalism, individual investors possess typical irrational characteristics (Daniel \\r\\n& Titman, 1999). It is still under exploration whether the textual information they post \\r\\non social media can reflect market fluctuations. Furthermore, the interpretability of \\r\\nsocial media text for the stock market is still an ongoing research topic (Gite et al.,\\r\\n2021).\\r\\n2.2 Time series prediction for the stock market\\r\\nForecasting the stock market relies primarily on time series techniques and \\r\\nfundamental analysis. This process involves forecasting various indicators, including \\r\\nmarket fluctuations, volatility, and yield. The pertinent techniques can be classified \\r\\ninto three main categories: regression, machine learning, and deep learning.\\r\\nEarlier researchers were predominantly constrained by computer performance \\r\\nlimitations, making it challenging for them to deal with massive amounts of data for \\r\\nforecasting. Consequently, they primarily relied on traditional regression models for \\r\\nstock market prediction. One commonly used tool is the autoregressive integrated \\r\\nmoving average (ARIMA). For instance, Ariyo et al. (2014) employed data from two\\r\\nexchanges to determine the parameters of the ARIMA model, which in turn \\r\\nconstructed a stock price prediction model. Durairaj and Krishna (2021) explored the \\r\\nprediction of chosen stock market values in the Philippines by employing a blend of \\r\\nARIMA models, explicitly emphasizing the influence of COVID-19 on share prices.\\r\\nThe findings show that the ARIMA model exhibits great potential for short-term \\r\\nforecasting, but its applicability is limited to time-series data that are either smooth or \\n5\\r\\ndifferentially smooth.\\r\\nMoreover, multiple regression (MR) and logistic regression (LR) models are \\r\\nfrequently employed in stock market prediction. Asghar et al. (2019) constructed an \\r\\nMR model that performs well on multiple datasets through enhanced selection of \\r\\npredictor variables. By optimizing the parameters of the LR model, Gong and Sun \\r\\n(2009) obtained better prediction results for the Shenzhen Development stock A stock \\r\\nthan did the baseline model, including the neural network. Nonetheless, these methods \\r\\nheavily rely on feature selection and data quality.\\r\\nUnfortunately, traditional time-series regression models exhibit two significant \\r\\nlimitations in the stock market. Some models do not apply to datasets that deviate \\r\\nfrom the statistical assumptions, and most models perform poorly in forecasting when \\r\\nthe input data are noisy.\\r\\nTo overcome the limitations of time series regression models, complex time \\r\\nseries features have been identified using machine learning techniques to improve the \\r\\naccuracy of stock price forecasting. Sapankevych and Sankar (2009) introduced the \\r\\nsupport vector machine (SVM) model for stock market forecasting. Based on this, \\r\\nShanthini et al. (2023) introduced a hybrid reptile search remora-based SVM \\r\\napproach to accurately determine stock market movements, demonstrating the \\r\\nimportance of hybrid methods in stock market forecasting. Park et al. (2022) proposed \\r\\na multitask framework that combines random forest (RF) for feature interpretation \\r\\nand long short-term memory (LSTM) models to predict stock market returns. Such a \\r\\nhybrid framework has increasingly appeared in recent studies. For example, Wang and \\r\\nGuo (2020) proposed combining the discrete wavelet transform with ARIMA and \\r\\nimproving XGBoost to predict stock prices. Yun et al. (2021) constructed a hybrid \\r\\nframework for closing price prediction using a genetic algorithm and XGBoost. These \\r\\nhybrid models better address the challenges posed by high-dimensional data and \\r\\nimprove the generalizability of machine learning for stock market time-series \\r\\nprediction tasks.\\r\\nAs an advanced machine learning method, applying deep learning techniques in \\r\\ntime series forecasting of the stock market has recently attracted broad interest. The \\r\\nrecurrent neural network (RNN) is a classical deep learning architecture that performs \\r\\nwell in dealing with serial data and is widely used in stock price forecasting (Lawi et \\r\\nal., 2022). Moreover, the Transformer model has been gradually applied to stock price \\r\\nprediction (Chen et al., 2023). Furthermore, Zhang et al. (2023) found that the \\n6\\r\\nattention mechanism helps neural networks more accurately capture the long-term \\r\\ndependencies among stock market data.\\r\\n2.3 Investor sentiment and the stock market\\r\\nWhen applying the above techniques to stock market forecasting, previous \\r\\nstudies have focused on technical indicators for forecasting purposes, neglecting to \\r\\nconsider the impact of investor sentiment on market volatility. However, it is worth \\r\\nnoting that many researchers have concluded that investor sentiment significantly \\r\\naffects stock market dynamics. Xu et al. (2022) found that the attitudes and emotions \\r\\ndisplayed by corporate managers in disseminating news can effectively predict the \\r\\nstock market, especially during periods of high emotions. Similarly, Gong et al. (2022)\\r\\nobserved that investor sentiment generated during market volatility exhibited \\r\\npredictive solid power. Researchers have also made several insightful observations. \\r\\nFor example, the emotion represented in Twitter financial tweets can considerably \\r\\nimpact global financial indices (Valle-Cruz et al., 2022), and there is a link between \\r\\nmusic consumption activities and stock market returns (Edmans et al., 2022).\\r\\nGenerally, it is customary to measure investor sentiment by aggregating \\r\\nnumerical metrics such as volatility, turnover rate, and trading volume (Gao & Martin, \\r\\n2021; Lin et al., 2024; Reis & Pinho, 2021). In recent years, due to advances in the \\r\\ndata processing capabilities of neural networks, it has become possible to introduce \\r\\ntext and images to assess investor sentiment (Liu et al., 2023; Obaid and \\r\\nPukthuanthong, 2022).\\r\\n3 Proposed framework\\r\\n3.1 Overview\\r\\nThis study introduces XSI-BiLSTM, a hybrid framework for predicting stock \\r\\nprices. Fig. 1 depicts the framework\\'s three key components: the post sentiment \\r\\nanalysis module, the sentiment index construction module, and the prediction module. \\r\\nFirst, the post-sentiment analysis module analyzes online forum post titles and bodies.\\r\\nNext, the sentiment index construction module generates a daily investor sentiment \\r\\nindex, which considers the results of the above post analysis and the post popularity. \\r\\nThis index is then combined with the stock\\'s technical indicators and fed into the \\r\\nstock price prediction module to obtain predictions. The methods used in each module \\r\\nare described in detail below.\\n7 \\r\\nFig. 1 The structure of XSI-BiLSTM. \\r\\n3.2 Post sentiment analysis based on the XLNET model\\r\\nXLNET is a powerful autoregressive LPLM that achieves bidirectional context \\r\\nlearning by randomly disrupting the order of sentences (Yang et al., 2019). It \\r\\nrandomly adopts one of the orders, masks the end words, and then predicts the \\r\\nmasked words autoregressively. Its objective can be formulated as follows:\\r\\n~ ( )\\r\\n1\\r\\nmax log | T t t\\r\\nT\\r\\nz Z z z\\r\\nt\\r\\nE px x θ θ <\\r\\n=\\r\\n\\uf8ee \\uf8f9\\r\\n\\uf8ef \\uf8fa \\uf8f0 \\uf8fb ∑ (1) \\r\\nFor a text sequence x of length T , there are T ! permutations, where z is one of \\r\\nall permutations ZT . In addition, t represents the t-th element, and < t represents the \\r\\nfirst t-1 elements. For example, consider the input text \"[This stock has negative \\r\\nnews]\" and its random reordering \"[has, stock, negative, This, news]\". The likelihood \\r\\nof optimization can be easily understood using Eq. (2): \\r\\n( ) ( ) ( ) ( )\\r\\n( ) ( )\\r\\n,\\r\\n, , ,, ,\\r\\np z p has p stock has p negative has stock\\r\\np This has stock negative p news has stock negative This\\r\\n=× ×\\r\\n× ×\\r\\n (2) \\r\\nXLNET can learn to extract information about each alignment in a text sequence \\r\\nby sharing model parameters. However, it is impractical to compute all possible \\r\\nalignments due to computational constraints. Therefore, during pretraining, only one \\r\\nalignment is randomly selected for each input sequence. Instead of destroying \\r\\nsequences, XLNET uses a mask matrix to implement the alignments. This approach \\r\\nmaintains input format consistency between the fine-tuning and downstream tasks, \\n8 \\r\\nthus avoiding discrepancies between pretraining and fine-tuning. To compensate for \\r\\nthe corruption caused by the mask matrix, XLNET merges the position information \\r\\ninto the softmax function likelihood as follows: \\r\\n( ) ( ( ) ( ))\\r\\n( ( ) ( ))\\r\\n,\\r\\n, | t\\r\\nzt t\\r\\nz zt\\r\\nzt t x\\r\\nexp e x x z\\r\\np\\r\\np\\r\\ng\\r\\ng\\r\\nX xx\\r\\nex e x x z\\r\\nθ\\r\\nθ\\r\\nθ\\r\\nΤ\\r\\nΤ\\r\\n′\\r\\n<\\r\\n<\\r\\n<\\r\\n= =\\r\\n∑ ′ (3) \\r\\nwhere g ( ). θ is the hidden state with t z location information added. Additionally, \\r\\nXLNET employs a two-stream self-attention mechanism to train the model effectively. \\r\\nThis mechanism involves two streams: the Query stream, which operates with only \\r\\nthe current location information, and the Content stream, which has access to the \\r\\ncurrent content information:\\r\\n1 1\\r\\n1 1\\r\\n: ( , ;)\\r\\n: ( , ;)\\r\\nt t\\r\\nt t\\r\\nm mm\\r\\nz z z t\\r\\nm mm\\r\\nz z z t\\r\\nQuery g Attn Q g KV h\\r\\nContent h Attn Q h KV h\\r\\nθ\\r\\nθ\\r\\n− −\\r\\n<\\r\\n− −\\r\\n≤\\r\\n\\uf8f1\\r\\n\\uf8f4\\r\\n\\uf8f2\\r\\n\\uf8f4\\r\\n←= =\\r\\n← = \\uf8f3 = (4) \\r\\nIn the above attention operation, Q , K , and V represent the query, key, and \\r\\nvalue, respectively. The content stream is set to the corresponding word embedding, \\r\\ni.e., (0) ( ) i i h ex = . There are a total of M m = … { } 1, 2, , attention layers.\\r\\nFor objective (1) above, only the last token in the permutation sequence is \\r\\npredicted during training to avoid slow convergence. Specifically, z is split into a \\r\\nnontarget subsequence z c ≤ and a target subsequence z c > by setting c as the cutoff \\r\\npoint of the sequence. The objective is adjusted to:\\r\\n| |\\r\\nmax log ( | ) log ( | ) . T cc T t c\\r\\nz\\r\\nz Z z z zZ z z\\r\\nt c\\r\\nE px x E px x θ θ ∼ ∼ θ > ≤ ≤\\r\\n=\\r\\n\\uf8ee \\uf8f9 \\uf8ee \\uf8f9 = \\uf8f0 \\uf8fb \\uf8ef \\uf8fa \\uf8f0 \\uf8fb ∑ (5) \\r\\nAfter training and fine-tuning the XLNET model, the sentiment tendency scores \\r\\nof the post were computed using the linear FC layer. The entire process of text \\r\\nprediction using XLNET is shown in Fig. 2. \\n9 \\r\\nFig. 2 XLNET-based post sentiment analysis module architecture\\r\\nBecause XLNET uses a more extensive Transformer-XL architecture, it can be \\r\\nadapted for the processing and analysis of long text sequences (Dai et al., 2019). In \\r\\nsome natural language processing applications, such as sentiment analysis, XLNET \\r\\nshows promising performance, as demonstrated by our experiments. Therefore, we \\r\\napply it to the text processing and analysis of posts. The specific process is outlined as \\r\\nfollows: (1) Fine-tune XLNET using a relevant Chinese financial corpus to make it \\r\\nsuitable for sentiment analysis of Chinese posts; (2) capture posts in web forums \\r\\ncorresponding to the target stocks; and (3) use the fine-tuned XLNET to generate \\r\\nsentiment scores for the title and body of each post.\\r\\n3.3 Construction of the sentiment index with the popularity factor\\r\\nConsidering the vast number of daily forum posts, feeding them directly into the \\r\\npredictive model would generate much noise. Therefore, it is necessary to aggregate \\r\\nthe sentiment scores of the posts into a daily group sentiment index. Regarding the \\r\\nmethod of analyzing group sentiment, we first draw on the treatment proposed by \\r\\nAntweiler and Frank (2004) to compute the daily aggregated group sentiment index: \\r\\n1 ln\\r\\n1\\r\\nbullish\\r\\nbearish\\r\\nt\\r\\nt\\r\\nt\\r\\nM BI\\r\\nM\\r\\n\\uf8eb \\uf8f6 + = \\uf8ec \\uf8f7 \\uf8ed \\uf8f8 +\\r\\n(6) \\r\\nwhere BIt represents the sentiment index for day t , bullish M is the bullish weight, and \\r\\nbearish M is the bearish weight. The original calculation method derives the weights by \\n10 \\r\\nsumming the related messages. However, this method is overly simplistic. Even when \\r\\ntwo posts share the same opinion about market fluctuations, they often differ in \\r\\nsentiment intensity. Therefore, it is necessary to calculate the weights in conjunction \\r\\nwith the post sentiment tendency level. Given that the trained XLNET can provide the \\r\\nsentiment tendency score of the post, Eq.(6) is optimized as follows:\\r\\n\\' 1\\r\\n1\\r\\n1\\r\\nln\\r\\n1\\r\\nm t\\r\\nm\\r\\nt n t\\r\\nm\\r\\nSBull\\r\\nBI\\r\\nSBear\\r\\n\\uf8eb \\uf8f6\\r\\n+ \\uf8ec \\uf8f7\\r\\n= \\uf8ec \\uf8f7\\r\\n\\uf8ec \\uf8f7 − \\uf8ed \\uf8f8\\r\\n∑\\r\\n∑\\r\\n(7) \\r\\nwhere SBull represents the propensity score of a post to be bullish for the stock \\r\\nmarket, with a value greater than 0, and SBear represents the propensity score of a \\r\\npost to be bearish for the stock market, with a value less than 0. Eq.(7) replaces the \\r\\nprevious daily group sentiment calculation method by summing the propensity scores \\r\\nof bearish or bullish posts. \\r\\nMoreover, since user behaviors, such as reading, commenting, sharing, and \\r\\nliking, provide insights into users\\' perceptions and emotional inclination toward the \\r\\npost (Kim & Yang, 2017), we regard them as augmentations of the post\\'s sentiment \\r\\nimpact. Hence, we compute the daily group sentiment using Eq. (8), taking into \\r\\naccount the amplification effect of the post\\'s popularity factor on its sentiment: \\r\\n( )\\r\\n( )\\r\\n\\' 1\\r\\n1\\r\\n1\\r\\nln\\r\\n1\\r\\nm t std std std\\r\\nmm m m\\r\\nt n t std std std\\r\\nnn n n\\r\\nSBull R C L\\r\\nBI\\r\\nSBear R C L\\r\\n\\uf8eb \\uf8f6\\r\\n+ ⋅ ++ \\uf8ec \\uf8f7\\r\\n=\\r\\n− ⋅ ++\\r\\n\\uf8ed \\uf8f8\\r\\n∑\\r\\n∑\\r\\n(8) \\r\\nwhere R , C , and L correspond to the number of reads, comments, and likes on that \\r\\npost, respectively. Additionally, to reduce the impact of extreme values and accelerate \\r\\nthe training speed, standardization is used on the popularity factor with Eq. (9): \\r\\nstd\\r\\nx x\\r\\nx\\r\\nσ\\r\\n− = (9) \\r\\nwhere x is the mean of the original data and σ is the standard deviation.\\r\\nTo explore whether constructed sentiment indices can reflect stock price trends \\r\\nmore accurately, we apply the Granger causality test (GCT) to examine their causal \\r\\nrelationships. The GCT is used to determine whether one set of time series can be \\r\\nconsidered the cause of another (Granger, 1969). For time series X and Y, X is \\r\\nconsidered to be a Granger cause of Y if the addition of X\\'s historical data in addition \\n11\\r\\nto Y enhances the prediction of Y, i.e., there is a Granger causality between X and Y, \\r\\nindicating that X helps to explain the changes in Y. The Granger causality regression \\r\\nequation for the effect of X on Y is shown in Eq. (10):\\r\\nt 0 i ti i t j\\r\\ni j\\r\\no o\\r\\ny yx αβ γ ε =+ + + ∑ ∑ − − (10)\\r\\nwhere ε represents the temporal noise. In Eq. (10), if X is not a Granger cause of Y, \\r\\nthe null hypothesis can be stated as:\\r\\nH :01 2 3 ... 0 i γγ γ γ = = = = = (11)\\r\\nIn this paper, we utilize the F test to test the validity of the original hypothesis at \\r\\ndifferent lag orders (denoted by o in Eq. (10)), where the original hypothesis states \\r\\nthat sentiment indices are not the cause of stock price changes. Since the post\\'s title is \\r\\nexposed in the forum list and the body content is visible only after the visitors visit the \\r\\npost, we calculate their sentiment scores separately and name them the post title \\r\\nsentiment index (PTSI) and post body sentiment index (PBSI).\\r\\n3.4 Stock price prediction based on BiLSTM with a highway mechanism\\r\\nLSTM is a type of classical RNN that effectively tackles the issues of gradient \\r\\nexplosion and vanishing gradient within the training process (Ergen & Kozat, 2018). \\r\\nThe LSTM model comprises a memory cell ( t c ), an input gate ( t\\r\\ni ), a forgetting gate \\r\\n( t f ), and an output gate ( t o ). Based on the hidden state at the moment t −1 , the \\r\\nhidden state at the moment t is computed. The steps involved in this computation can \\r\\nbe summarized as follows:\\r\\n(1) The hidden state at moment t −1 with the input at moment t is fed to the \\r\\ninput gate, the forgetting gate, and the output gate, respectively:\\r\\n1\\r\\n1\\r\\n1\\r\\n( )\\r\\n( )\\r\\n( )\\r\\nt i it i\\r\\nt f ft f t\\r\\nto t t o o\\r\\nt i U x Wh b\\r\\nf U x Wh b\\r\\no U x Wh b\\r\\nσ\\r\\nσ\\r\\nσ\\r\\n−\\r\\n−\\r\\n−\\r\\n\\uf8f1 = ++ \\uf8f4\\r\\n\\uf8f2 = ++\\r\\n\\uf8f4\\r\\n\\uf8f3 = ++\\r\\n(12)\\r\\nwhere σ is the sigmoid activation function. U and W are the weights to be trained, \\r\\nand b denotes bias.\\r\\n(2) Subsequently, the memory cell state was calculated:\\r\\n1 1 tanh( ) t t t t c ct c t c f c i U x Wh b = + ++ \\uf065 \\uf065 − − (13)\\r\\nwhere \\uf065 denotes the dot product operation and tanh is the activation function, for \\r\\nwhich Eq.(14) gives:\\n12 \\r\\n2\\r\\n1 tanh( ) 1\\r\\n1 x e\\r\\nx − − + = (14) \\r\\n(3) Finally, the hidden state at moment t is obtained: \\r\\ntanh( ) tt t ho c = \\uf065 (15) \\r\\nTo further enhance the capability of time-series modeling of stock prices, we \\r\\nemploy a bidirectional LSTM (BiLSTM) to integrate the contextual information. By \\r\\nusing two LSTM structures, BiLSTM processes both sequential and inverse order \\r\\nhidden states ( t h\\r\\n→\\r\\n and t h\\r\\n←\\r\\n) and eventually represents them in series as a long vector for \\r\\nfurther computation. \\r\\nTemporal data may suffer from gradient dispersion when entering a multilayer \\r\\nBiLSTM network, which ultimately hinders the network\\'s learning efficiency. We \\r\\nintroduced a highway mechanism to allow a seamless flow of information between \\r\\nthe front and back layers (Zilly et al., 2017). To achieve this goal, we introduce a \\r\\ncontrol mechanism of carry gates in the BiLSTM network to regulate the amount of \\r\\ninformation transferred from lower memory cell states to the next level. This is \\r\\nrealized explicitly by improving Eq.(13) as follows:\\r\\n1 ( ) l l\\r\\nt d d\\r\\nl\\r\\nt\\r\\nl\\r\\nd t d Ux W c b σ − = + + ⊗ (16) \\r\\n1\\r\\n1 1 tanh( ) l l l l l l ll ll\\r\\nttt t t t c t c c t c d c f c i Ux b hW − = + +⊗ + + \\uf065 \\uf065 − − (17) \\r\\nIn the prediction module, we employ the improved BiLSTM-highway model \\r\\ndescribed above, which effectively facilitates the flow of time series information from \\r\\nlayer to layer. Fig. 3 depicts the actual workflow of the module. \\n13\\r\\nFig. 3 The BiLSTM-highway module structure for stock closing price prediction\\r\\n4 Experiment\\r\\n4.1 Data sources, description and processing\\r\\n4.1.1 Textual data\\r\\nFor our proposal, two types of text data need to be collected. The first type is \\r\\nChinese investor stock comments with labels, which are used to fine-tune the XLNET \\r\\nmodel. The second type is forum posts, which are used to predict the closing price of \\r\\na stock in conjunction with stock technical indicators.\\r\\nWe collected four relevant datasets from the GitHub community1 to form the \\r\\nfirst text data type. The researchers labeled them as positive, neutral, or negative \\r\\nsentiments. After summarizing the data and removing short texts, 33,329 texts were \\r\\nobtained. A statistical description of the data is shown in Table 1.\\r\\nTable 1 Statistical description of the training data\\r\\nCategory Text \\r\\nquantity Example Translation Label\\r\\n1. Data sources: github.com/algosenses/Stock_Market_Sentiment_Analysis, github.com/HongWooo/Sentimen\\r\\nt, github.com/zyiyy/TextSentiment, github.com/zhy0313/double-degree.\\n14\\r\\nPositive 9554 没事解套很快的，耐\\r\\n心持有不用担心\\r\\nIt is okay. You will escape the encumbrance \\r\\nquickly; hold on patiently and do not worry. 1\\r\\nNeutral 12975 建议周一再观望一下 I suggest waiting and seeing again on Monday. 0\\r\\nNegative 10800 此股不建议，处于下\\r\\n降通道\\r\\nDo not recommend this stock. It is in a downward \\r\\nchannel. -1\\r\\nTotal 33329 - - -\\r\\nWe collected the second type of textual data from Eastmoney Guba2, the largest \\r\\nfinancial online forum in China. The forum contains posts from retail and professional \\r\\ninvestors, including information such as title and body text, likes, comments and visits. \\r\\nWe selected the following two component indices and stocks: the Growth Enterprise \\r\\nIndex (GEI, SZ399006), the SZSE Composite Index (SZCI, SZ399001), Kweichow \\r\\nMoutai (KM, SH600519), and Contemporary Amperex Technology (CAT, SZ300750). \\r\\nThese stocks are also the ones for which closing price forecasts are conducted in the \\r\\nexperiment.\\r\\nAfter the initial post data collection, we removed redundant emoticons, hashtags, \\r\\nand spaces from the post text, yielding approximately 340,000 post data points from \\r\\nMay 31, 2022, to May 31, 2023. A statistical description of the data is given in Table 2, \\r\\nand an example of the post data is given in Fig. 4.\\r\\nTable 2 Statistical description of the post data\\r\\nIndicator SZ399006 SZ399001 SH600519 SZ300750 Total\\r\\nPost quantity 95392 55334 90144 96631 337501\\r\\nTitle average length 16.37 19.22 19.81 18.78 18.45\\r\\nBody text average length 155.42 202.10 323.38 250.73 235.22\\r\\nThe average quantity of likes 5.17 7.03 2.73 3.24 4.27\\r\\nThe average quantity of comments 2.50 3.71 3.10 2.88 2.97\\r\\nThe average quantity of views 527.74 641.00 740.89 796.20 680.10\\r\\nFig. 4 Post example\\r\\n4.1.2 Technical indicators\\r\\nFor the stocks introduced in Section 4.1.1, we selected nine technical indicators \\r\\nas their historical data for forecasting, as shown in Fig. 3. The above historical data \\r\\n2. Official website: guba.eastmoney.com\\n15 \\r\\nwere gathered from the RESSET database3, and the date range is consistent with that \\r\\nof the textual data, which are from May 30, 2022, to June 1, 2023. Due to the stock \\r\\nmarket being closed on weekends and holidays, approximately 247 days of actual \\r\\ntrading history data were collected. Similarly, the technical indicators are standardized \\r\\naccording to Eq. (9). \\r\\n4.2 Metrics\\r\\n4.2.1 Classification evaluation metrics of the sentiment analysis model\\r\\nWe evaluate the performance of the models on the sentiment classification task \\r\\nusing accuracy, precision, recall, and F1-score as evaluation metrics. The following \\r\\nequations define these metrics:\\r\\nTP TN\\r\\naccuracy TP FN FP TN\\r\\n+ = + ++\\r\\n(18) \\r\\nTP precision TP FP = +\\r\\n(19) \\r\\nTP recall\\r\\nTP FN = +\\r\\n(20) \\r\\n2 2 F1 1 1precision recall\\r\\nprecision recall\\r\\npresicion recall\\r\\n⋅ ⋅ = = + +\\r\\n(21) \\r\\nwhere accuracy is used to measure the ability of the model to classify samples \\r\\ncorrectly, precision represents the proportion of samples classified as positive cases \\r\\nthat are positive cases, recall denotes how many of the samples predicted positively \\r\\nby the model were predicted correctly, and F1 is the harmonic mean of accuracy and \\r\\nrecall, which may be a better metric when the categories are unbalanced.\\r\\nIn Eq.(18) - (21), \"true positive\" and \"true negative\" (TP and TN, respectively) \\r\\nrelate to the ability to forecast the total number of positive or negative samples \\r\\naccurately. \"False positive\" and \"false negative\" (FP and FN) indicate the number of \\r\\nincorrectly predicted positive or negative samples, respectively.\\r\\n4.2.2 Evaluation metrics for stock price forecasting\\r\\nWe employ standard regression model evaluation metrics, such as the root mean \\r\\nsquare error (RMSE), mean absolute percentage error (MAPE), and R2 score, to \\r\\nevaluate stock price forecasting tasks. Their expressions are shown as follows:\\r\\n3. Official website: www.resset.com\\n16 \\r\\n2\\r\\n1\\r\\n1 RE ( ) MS ˆ n\\r\\ni i\\r\\ni\\r\\ny y\\r\\nn =\\r\\n= − ∑ (22) \\r\\n1\\r\\n100 ( ) MAPE | | ˆ n\\r\\ni i\\r\\ni i\\r\\ny y\\r\\nn y =\\r\\n− = ∑ (23) \\r\\n2\\r\\n1\\r\\n2\\r\\n1\\r\\n( )\\r\\nR 1 2\\r\\n)\\r\\nˆ\\r\\n( ˆ\\r\\ni i\\r\\ni\\r\\ni\\r\\nn\\r\\ni\\r\\ni\\r\\nn\\r\\ny y\\r\\ny y\\r\\n=\\r\\n=\\r\\n−\\r\\n= −\\r\\n−\\r\\n∑\\r\\n∑\\r\\n(24) \\r\\nIn Eq.(22) - (24), n is the total number of samples, i y and ˆi y , respectively \\r\\nindicate the actual and forecasted stock price, and i y indicates the stock\\'s average \\r\\nprice in the testing set.\\r\\n4.3 Experiment settings\\r\\nTo ensure the superiority of the model we proposed, support vector regression \\r\\n(SVR), BiLSTM, LSTM-Att, GRU-Att, and Transformer are selected as benchmark \\r\\nmodels (Smola and Schölkopf 2004; Althelaya et al. 2018; Zhang et al. 2019; Niu and \\r\\nXu 2020; Vaswani et al. 2017). In addition, to ensure that XLNET can precisely \\r\\nanalyze investor sentiment from text, we use SVM, naive Bayes classification (NB), \\r\\nTextCNN, and BERT as comparison models (Joachims, 1998; Flach & Lachiche, \\r\\n2004; Yuan et al., 2021; Devlin et al., 2018). The hyperparameter settings of XSI\\x02BiLSTM are provided in Table 3. \\r\\nTable 3 Hyperparameter settings of XSI-BiLSTM\\r\\nModule Hyperparameter Parameter\\r\\nPost-sentiment analysis module\\r\\n(Based XLNET)\\r\\nBatch size 16\\r\\nMax length 128\\r\\nEpochs 100 with earlystop\\r\\nWarm up ratio 0.1\\r\\nLearning rate 2e-5\\r\\nDropout 0.2\\r\\nPrediction module\\r\\n(Based BiLSTM)\\r\\nBatch size 32\\r\\nHidden layer size 128\\r\\nLearning rate 1e-3\\r\\nWeight decay 1e-2\\r\\nEpochs 1000 with earlystop\\r\\n5 Results \\r\\n5.1 Training and comparison of sentiment analysis models\\r\\nWe divide the first type of text data into a training set, a testing set and a \\r\\nvalidation set at a ratio of 8:1:1 to compare the performance of each model \\n17\\r\\nhorizontally. Cross-entropy is used as the loss function during the training process, as \\r\\nshown in Eq. (25). Table 4 shows the results of each model.\\r\\n1\\r\\n( )log( ( ))\\r\\nm\\r\\ni i\\r\\ni\\r\\nL px qx == −∑ (25)\\r\\nwhere ( )i p x denotes the actual label with a value of 1 or 0. ( )i q x is the probability of \\r\\nbelonging to the category as output by the model after softmax computation, and m is \\r\\nthe number of samples.\\r\\nTable 4 Performance results of five sentiment analysis models. The best results are highlighted in bold,\\r\\nfollowed by the same.\\r\\nMetrics XLNET SVM NB Bert TextCNN\\r\\nAccuracy 81.64 44.97 74.20 81.34 76.12\\r\\nPrecision 82.73 45.07 77.20 82.33 77.06\\r\\nRecall 81.49 44.98 72.79 81.24 75.47\\r\\nF1-Score 81.97 44.52 73.43 81.68 75.99\\r\\nAs shown in Table 4, XLNET performed the best in the sentiment analysis task. \\r\\nThe F1 score of the XLNET, which represents comprehensive performance, is 81.97. \\r\\nThe performance of BERT is similar to that of XLNET, with an F1 score of 81.68. \\r\\nGenerally, the LPLM is superior to ordinary neural network models or traditional \\r\\nmachine learning techniques (Han et al., 2021), which is also reflected in our \\r\\ncomparison results. In addition, the LPLM has a breakneck convergence speed in \\r\\ntransfer learning. Fig. 5 shows the changes in training and testing loss of XLNET with \\r\\nthe number of epochs. A good fit was achieved after only a dozen epochs.\\r\\nFig. 5 Changes in loss during the training process of the XLNET model over time\\n18\\r\\nThe comparison results show that applying XLNET to analyze the hidden \\r\\nsentiment in textual data is effective, and its performance is the same as that of BERT. \\r\\nConsidering that the structure of the XLNET is more suitable for processing long text \\r\\n(Sweidan et al., 2021), it is reasonable to believe that the XLNET can better analyze \\r\\nthe sentiment of a post, which often contains hundreds of words in the body of the \\r\\npost, compared to other models.\\r\\nThen, we use the fine-tuned XLNET to analyze the post sentiment and output \\r\\nscores through the final FC linear layer, with a range of [−1,1]. When the score is \\r\\ncloser to -1, 0, or 1, the post\\'s sentiment toward stock prices tends to be bearish, \\r\\nneutral, or bullish, respectively.\\r\\n5.2 Sentiment validity verification\\r\\nIn this section, we present the results of the GCT to showcase the effectiveness \\r\\nof utilizing the sentiment index in predicting stock closing prices. Additionally, we \\r\\nanalyze whether sentiment indices better capture stock market fluctuations when \\r\\nconsidering the popularity factor. It is essential to confirm the stability of the time \\r\\nseries before performing GCT. To accomplish this, we apply the augmented Dickey\\x02Fuller (ADF) test to verify the time series, and the results are displayed in Table 5. \\r\\nGenerally, the stock closing price exhibits a nonsmooth series. Hence, we employ the \\r\\nprice rate of change (ROC) as the test variable.\\r\\nTable 5 ADF test results\\r\\nVariables SZ399006 SZ399001 SH600519 SZ300750\\r\\nROC -10.146*** -10.17*** -6.319*** -10.265***\\r\\nPTSI -3.065** -6.532*** -6.125*** -4.616***\\r\\nPBSI -2.903** -6.203*** -6.129*** -4.779***\\r\\nPTSI\\' -4.314** -10.558*** -5.93*** -8.681***\\r\\nPBSI\\' -2.807* -8.302*** -7.086*** -5.48***\\r\\nNote: ***, **, and * represent significance at the 1%, 5%, and 10% levels respectively, followed by the \\r\\nsame.\\r\\nAccording to Table 5, the ROC, PTSI, PBSI, PTSI\\', and PBSI\\' of the four stocks \\r\\nexhibit ADF significance, making them suitable for GCTs. Among these indices, the \\r\\nPTSI and PBSI are sentiment indices that do not consider the popularity factor, \\r\\nwhereas the PTSI\\' and PBSI\\' are considered. The GCTs with different lag durations \\r\\nare shown in Table 6.\\r\\nTable 6 Results of GCTs with lags of 1, 2, and 3 days\\r\\nStock Variables Sentiment index does not ROC does not Granger cause \\n19\\r\\ncode Granger cause ROC sentiment index\\r\\nlags(1) lags(2) lags(3) lags(1) lags(2) lags(3)\\r\\nSZ399006\\r\\nPTSI 0.79 1.13 0.48 127.64*** 54.79*** 25.51***\\r\\nPBSI 0.48 2.56* 0.94 180.17*** 79.59*** 31.91***\\r\\nPTSI\\' 6.01** 3.38*** 0.62 112.25*** 48.24*** 22.61***\\r\\nPBSI\\' 0.82 2.58* 0.75 172.73*** 80.69*** 34.56***\\r\\nSZ399001\\r\\nPTSI 0.33 0.33 0.04 132.69*** 56.97*** 23.66***\\r\\nPBSI 0.19 0.06 0.08 172.41*** 67.17*** 25.52***\\r\\nPTSI\\' 2.77* 2.85* 1.22 96.32*** 79.81*** 46.77***\\r\\nPBSI\\' 1.56 0.43 0.54 122.75*** 87.27*** 48.87***\\r\\nSZ600519\\r\\nPTSI 0.09 1.85 0.17 103.41*** 39.20*** 15.89***\\r\\nPBSI 0.65 1.98 0.92 134.45*** 57.27*** 24.06***\\r\\nPTSI\\' 3.67* 1.16 1.62 81.83*** 62.57*** 30.93***\\r\\nPBSI\\' 0.08 0.60 0.81 99.15*** 91.85*** 41.83***\\r\\nSZ300750\\r\\nPTSI 1.38 0.17 0.82 222.91*** 108.06*** 47.50***\\r\\nPBSI 1.42 0.48 1.67 324.65*** 157.37*** 79.07***\\r\\nPTSI\\' 2.14 1.61 2.13* 87.73*** 87.25*** 24.52***\\r\\nPBSI\\' 3.04* 0.69 0.60 227.66*** 228.34*** 56.50***\\r\\nAccording to Table 6, stock price fluctuation is a significant factor causing a \\r\\nchange in users\\' posting sentiment in online forums, and all the indices are significant \\r\\nat the 1% level at different lag orders. In other words, the sentiment index hidden \\r\\nunder the post text can effectively reflect users\\' reactions to stock prices. Interestingly, \\r\\nthe PBSI shows more robust Granger causality at one lag, while the PBSI\\' is more \\r\\nsignificant at 2 and 3 lags. This may imply that the former is more effective in \\r\\nreflecting users\\' perceptions of stock prices during short periods of stock price \\r\\nchanges. However, in the long run, the essential posts that gain more recognition are \\r\\nmore representative of popular tendencies.\\r\\nIn contrast, the PTSI and PBSI have fewer Granger causality effects on stock \\r\\nprice changes. Only in the lagged 2-period instance of SZ399006 does the PBSI show \\r\\nsignificance at the 10% level. However, after considering the popularity factor, both \\r\\nPTSI\\' and PBSI\\' are significant at specific lag periods for the four stocks. One \\r\\npossible explanation is that the diversity of information in a forum may interfere with \\r\\njudgment if it is not selectively received. When the popularity factor is incorporated, \\r\\nit allows posts that receive more attention and recognition to carry more weight in \\r\\nsentiment index calculations, reflecting more realistic investor sentiment. These \\r\\nsentiments are further reflected in the public\\'s subsequent decision-making behavior \\r\\nin the stock market, which impacts stock prices. This finding is supported by the fact \\r\\nthat PTSI\\' has a more significant impact than does PBSI\\'. In online forums, post titles \\r\\nappear in a list. Generally, users click and participate in post discussions only if they \\r\\nare interested in the ideas in the title, thus increasing the popularity of the post. That is, \\r\\npost titles will be more influenced by popularity. The significance of the coefficients \\n20\\r\\nsuggests that PTSI\\' is more reflective of most investors\\' inclination toward future \\r\\nstock prices than is PBSI\\', which verifies this view.\\r\\nIn conclusion, investor sentiment effectively reflects investor perceptions of \\r\\nchanges in closing prices. After incorporating the popularity factor, bidirectional \\r\\nGranger causality with stock prices is established. Therefore, its application in \\r\\npredicting stock closing prices is justified.\\r\\n5.3 Prediction results and comparison\\r\\nWe divide the stock data into training and testing sets at a ratio of 8:2 and use the \\r\\nhistorical data from 7, 15, and 30 days before the target prediction date as the inputs \\r\\nto the model. The predicted prices of the models are compared with the actual stock \\r\\nprices using the introduced metrics to evaluate the effectiveness of the XSI-BiLSTM \\r\\nframework proposed in this paper.\\r\\nTable 7 Detailed metrics comparison of each stock under six models\\r\\nInput windows 7 15 30\\r\\nStock Model RMSE MAPE R2 RMSE MAPE R2 RMSE MAPE R2\\r\\nSZ399006\\r\\nSVR 35.91 1.27% 0.7776 48.83 1.67% 0.5889 60.93 2.13% 0.3600\\r\\nGRU-Att 55.84 1.96% 0.4624 43.13 1.40% 0.6792 57.01 2.08% 0.4397\\r\\nBiLSTM 47.20 1.68% 0.6158 43.55 1.53% 0.6730 51.98 1.83% 0.5341\\r\\nLSTM-Att 56.84 2.02% 0.4430 49.27 1.71% 0.5815 50.11 1.77% 0.5670\\r\\nTransformer 66.21 2.48% 0.2443 64.75 2.49% 0.2772 74.25 2.72% 0.0496\\r\\nXSI-BiLSTM 35.06 1.23% 0.7773 36.65 1.27% 0.7566 40.32 1.52% 0.7055\\r\\nSZ399001\\r\\nSVR 198.91 1.39% 0.6914 242.67 1.71% 0.5406 270.79 2.10% 0.4280\\r\\nGRU-Att 187.39 1.42% 0.7261 191.02 1.44% 0.7154 136.16 0.99% 0.8554\\r\\nBiLSTM 124.67 0.83% 0.8788 123.85 0.84% 0.8804 112.83 0.76% 0.9007\\r\\nLSTM-Att 174.42 1.31% 0.7627 121.79 0.85% 0.8843 216.34 1.51% 0.6349\\r\\nTransformer 187.65 1.35% 0.7253 160.97 1.17% 0.7979 159.55 1.15% 0.8014\\r\\nXSI-BiLSTM 100.60 0.67% 0.9173 99.92 0.71% 0.9184 99.84 0.68% 0.9186\\r\\nSH600519\\r\\nSVR 23.42 1.09% 0.6966 28.04 1.25% 0.5653 31.16 1.41% 0.4630\\r\\nGRU-Att 29.30 1.45% 0.5254 24.93 1.11% 0.6563 30.15 1.42% 0.4972\\r\\nBiLSTM 23.36 1.10% 0.6981 23.97 1.03% 0.6822 23.12 1.09% 0.7043\\r\\nLSTM-Att 24.13 1.06% 0.6779 30.59 1.43% 0.4825 34.82 1.67% 0.3293\\r\\nTransformer 27.76 1.22% 0.5738 37.55 1.88% 0.2201 33.71 1.55% 0.3714\\r\\nXSI-BiLSTM 20.76 0.91% 0.7297 20.87 0.94% 0.7269 20.29 0.89% 0.7418\\r\\nSZ300750\\r\\nSVR 6.43 2.33% 0.1835 6.56 2.33% 0.1507 6.24 2.29% 0.2301\\r\\nGRU-Att 4.41 1.58% 0.6166 4.71 1.74% 0.5615 5.51 1.96% 0.3994\\r\\nBiLSTM 4.57 1.66% 0.5868 4.62 1.67% 0.5782 4.44 1.62% 0.6110\\r\\nLSTM-Att 4.65 1.61% 0.5722 4.02 1.49% 0.6804 5.01 1.73% 0.5039\\r\\nTransformer 5.88 2.00% 0.3167 6.01 2.24% 0.2871 4.47 1.57% 0.6055\\r\\nXSI-BiLSTM 3.94 1.43% 0.7172 4.18 1.52% 0.6809 4.40 1.58% 0.6467\\r\\nTable 7 gives a quantitative assessment of the performance of the different \\r\\nmodels. Among them, XSI-BiLSTM achieves the best performance in most cases. For \\r\\nexample, for the MAPE, which can most intuitively reflect the difference between \\r\\nforecasted and actual stock prices, the XSI-BiLSTM outperforms the other five \\r\\nbaseline models by 67.03%, 46.06%, 17.73%, 44.08%, and 67.24%, respectively. This \\r\\npreliminary result confirms the effectiveness of our proposed framework. Notice that \\r\\nXSI-BiLSTM does not have a significant performance lead in SZ300750, which may \\r\\nbe because CAT is characterized by a more pronounced cyclicality than the other \\n21\\r\\nstocks (Fig. 6d illustrates this cyclical variation). This cyclical volatility is likely to be \\r\\nsuccessfully captured by the attention mechanism. Therefore, LSTM-Att and \\r\\nTransformer achieve good prediction performance.\\r\\nInterestingly, as the input window time increases, the predictive performance of \\r\\nthe models does not significantly improve, and SVR even shows a decrease in \\r\\nperformance. This may be because stock prices are nonstationary, and the SVR model \\r\\nis not complex enough to accommodate a more extended time series. However, XSI\\x02BiLSTM exhibits superior predictive performance under either short-term or long\\x02term input windows, which is attributed to the timely feedback and adjustment of \\r\\ninvestor sentiment.\\r\\nThe prediction results of each model on the testing set (March 21, 2023 to May \\r\\n31, 2023) are shown in Fig. 6. The results show that XSI-BiLSTM maintains good \\r\\ntracking performance in predicting the closing prices of the four stocks without \\r\\nsignificant bias. SVR performs well in the early period but shows serious bias later. \\r\\nThis phenomenon may be because the features learned by SVR during the training \\r\\nprocess are only applicable to stock price movements in the time frame covered by the \\r\\ntraining set. Consequently, they cannot accurately capture the changes in stock price \\r\\ndynamics on the date the testing set is located. This leads to a bias between the \\r\\nprediction results and the actual stock prices. The GRU-Att and LSTM-Att models \\r\\nexhibit strong performance with respect to specific stocks. However, they demonstrate \\r\\nsignificant bias in others, likely stemming from a misallocation of weights within the \\r\\nattention mechanism. Transformer performs well in most cases but does not control \\r\\nthe magnitude of the change when there is a change in the stock price trend. This may \\r\\nbe caused by its self-attention mechanism focusing too much on short-term \\r\\nfluctuations. BiLSTM\\'s tracking ability is superior. Nevertheless, it captures changes \\r\\nin conditions more slowly than XSI-BiLSTM, possibly due to its lack of feedback on \\r\\nmarket sentiment, resulting in slower adjustment.\\n22\\r\\nFig. 6 Actual closing price and predicted price of four stocks under different models.\\r\\n(a): SZ399006 (b): SZ399001 (c): SH600519 (d): SZ300750\\r\\nFig. 7 - 10 show each model\\'s relative percentage error (RPE) and average one\\x02sided error (AOSE) between the predicted and actual closing prices. Comparatively, \\r\\nBiLSTM, Transformer, and XSI-BiLSTM exhibit relatively small and stable overall \\r\\nbiases. However, the RPE of the Transformer is more severe on certain trading days, \\r\\ne.g., April 25, 2023 for SZ399006; April 13, 2023 for SZ600519; and May 11, 2023 \\r\\nfor SZ300750. In some cases, the GRU-Att and LSTM-Att models are too \\r\\n\"optimistic\" and predict stock prices much higher than their actual values. Upon \\r\\nreversal of the stock price, the RPE of most models becomes more significant.\\r\\nIn contrast, the XSI-BiLSTM model, which integrates investor sentiment, \\r\\nexhibits superior performance. This enhancement may be because people\\'s judgment \\r\\nof future stock prices is reflected in textual sentiment, and accurate bias correction can \\r\\nbe achieved by using textual sentiment, which allows the XSI-BiLSTM to achieve a \\r\\nmore balanced AOSE. In short, the stock price prediction task is highly complex, and \\r\\npredicting the price based on the time-series characteristics of the stock\\'s technical \\r\\nindicators alone may not be accurate enough.\\n23 \\r\\nFig. 7 RPE of each model for SZ399006\\r\\nFig. 8 RPE of each model for SZ399001\\n24\\r\\nFig. 9 RPE of each model for SZ600519\\r\\nFig. 10 RPE of each model for SZ300750\\r\\n5.4 Ablation experiment\\r\\nIn the prediction module of XSI-BiLSTM, we incorporate an investor sentiment \\r\\nindex and introduce a highway mechanism to enhance the information-capturing \\r\\nability of the BiLSTM model. Therefore, it is necessary to verify its effectiveness \\r\\nthrough ablation experiments. Table 8 shows the performance comparison results \\r\\nbetween BiLSTM, BiLSTM using sentiment index (SI), BiLSTM using the highway \\r\\nmechanism, and both (i.e., the modules we use). The results are presented in \\r\\nmultiplicative form using BiLSTM performance as a benchmark.\\r\\nTable 8 Performance comparison results of the ablation experiment\\r\\nInput windows 7 15 30\\r\\nStock Model RMSE MAPE R2 RMSE MAPE R2 RMSE MAPE R2\\r\\nSZ399006\\r\\nBiLSTM 1x 1x 1x 1x 1x 1x 1x 1x 1x\\r\\nBiLSTM-SI 1.18x 1.14x 1.16x 1.23x 1.20x 1.15x 1.22x 1.14x 1.26x\\r\\nBiLSTM-highway 1.29x 1.23x 1.25x 1.04x 1.01x 1.04x 1.25x 1.21x 1.31x\\r\\nOur 1.35x 1.37x 1.26x 1.19x 1.21x 1.12x 1.29x 1.20x 1.32x\\r\\nSZ399001\\r\\nBiLSTM 1x 1x 1x 1x 1x 1x 1x 1x 1x\\r\\nBiLSTM-SI 1.16x 1.14x 1.03x 1.20x 1.22x 1.04x 1.16x 1.15x 1.03x\\r\\nBiLSTM-highway 1.17x 1.19x 1.04x 1.23x 1.16x 1.05x 1.11x 1.08x 1.02x\\r\\nOur 1.24x 1.23x 1.04x 1.24x 1.19x 1.04x 1.13x 1.11x 1.02x\\r\\nSH600519\\r\\nBiLSTM 1x 1x 1x 1x 1x 1x 1x 1x 1x\\r\\nBiLSTM-SI 1.07x 1.14x 1.01x 1.15x 1.08x 1.06x 1.07x 1.13x 1.00x\\r\\nBiLSTM-highway 1.09x 1.12x 1.07x 1.13x 1.07x 1.10x 1.02x 1.08x 1.02x\\r\\nOur 1.13x 1.21x 1.05x 1.15x 1.10x 1.07x 1.14x 1.23x 1.05x\\r\\nSZ300750\\r\\nBiLSTM 1x 1x 1x 1x 1x 1x 1x 1x 1x\\r\\nBiLSTM-SI 1.12x 1.14x 1.18x 1.10x 1.11x 1.17x 0.98x 0.99x 1.02x\\r\\nBiLSTM-highway 1.12x 1.09x 1.14x 1.05x 1.03x 1.06x 1.02x 1.02x 1.03x\\r\\nOur 1.16x 1.16x 1.22x 1.11x 1.10x 1.18x 1.01x 1.02x 1.06x\\r\\nAs shown in Table 8, our module achieves optimal prediction performance in \\r\\nmost scenarios, especially when the input sequence length is short (input window = 7). \\r\\nThe model\\'s ability to mine the data is enhanced by adding investor sentiment and \\r\\nhighway methods, which help extract time-series features. Furthermore, as the input \\n25\\r\\nwindow expands, the degree of performance improvement from both diminishes, \\r\\nshowing that long-term time series may compensate for the lack of external \\r\\ninformation and help the neural network anticipate stock values.\\r\\nFig. 11 shows the average percentage improvement of each model compared to \\r\\nBiLSTM for different input windows and metrics. For shorter input windows, the \\r\\nBiLSTM-highway improves better than the BiLSTM-SI; for medium-length input \\r\\nwindows, the BiLSTM-SI improves better than the BiLSTM-highway; and for longer \\r\\ninput windows, the improvements are similar. These situations may be because the \\r\\nhighway mechanism is more effective in dealing with short-term dependencies, as it \\r\\nallows information to be passed more directly between layers, reducing the risk of \\r\\ninformation loss. Moreover, as the input window length increases, the introduction of \\r\\ninvestor sentiment can help the model capture long-term market sentiment. In \\r\\nsummary, the results for each indicator reveal that the combined use of the two always \\r\\nyields better predictions than their individual use.\\r\\nFig. 11 Performance improvement effect of each model\\r\\nIn addition, Fig. 12 shows a box plot of the absolute error of prediction for each \\r\\nmodel. The errors always show a skewed distribution, i.e., the difference between the \\r\\nmodel\\'s predicted price and the actual price is negligible in most cases. The \\n26\\r\\nintroduction of investor sentiment effectively reduces the median prediction error, \\r\\nwhile introducing the highway mechanism reduces the extreme values and avoids \\r\\nlarge errors. After combining the two approaches, the forecast error decreases overall, \\r\\nas reflected by the fact that its average error (i.e., the black square in the figure) is the \\r\\nlowest among all four stocks.\\r\\nFig. 12 Box-distribution plots of each model\\'s absolute error\\r\\n6 Conclusion\\r\\nIn this paper, we propose a novel hybrid framework for stock price prediction \\r\\ncalled XSI-BiLSTM, which improves the computation of daily investor sentiment by \\r\\nanalyzing investor sentiment in online forums and taking into account the popularity \\r\\nof posts. Finally, we predict the next day\\'s closing price of a stock using the improved \\r\\nBiLSTM. The effectiveness of XSI-BiLSTM is demonstrated through extensive \\r\\nvalidation experiments conducted on two constituent indices and two stocks in the \\r\\nChinese stock market.\\r\\nSpecifically, the experiments show that the XLNET model used by the \\r\\nframework can effectively extract textual sentiment from posts, and the improved \\n27\\r\\nsentiment index forms a bidirectional Granger causality with stock price changes to a \\r\\ncertain extent, which can better reflect and predict stock price trends. The prediction \\r\\nresults of XSI-BiLSTM outperform those of the other prediction models in most of \\r\\nthe performance metrics, and the actual stock closing prices differ less from the \\r\\npredicted stock closing prices. In addition, ablation experiments evaluate the \\r\\neffectiveness of the highway mechanism in the improved prediction module and the \\r\\nnecessity of introducing investor sentiment.\\r\\nIn this study, we found considerable information noise in stock forums, which \\r\\ndoes not contribute to stock price prediction. However, views that receive more \\r\\nattention and recognition reflect stock price movements more accurately. Future \\r\\nresearch will explore in more depth the impact of investor sentiment and its external \\r\\nattributes on stock prices and analyze how to better analyze stock market-related \\r\\ntextual information that can help improve model prediction performance.\\r\\nCRediT authorship contribution statement\\r\\nHuiyu Li: Conceptualization, Methodology, Data curation, Software, Writing -\\r\\noriginal draft. Junhua Hu: Funding acquisition, Project administration, Writing –\\r\\nreview & editing.\\r\\nData availability\\r\\nData will be available on request.\\r\\nAcknowledgments\\r\\nThis work received support by the Fundamental Research Funds for the Central \\r\\nUniversities of Central South University (Grant number: 1053320220429).\\r\\nReferences\\r\\nAlthelaya, K. A., El-Alfy, E.-S. M., & Mohammed, S. (2018). Evaluation of bidirectional LSTM \\r\\nfor short-and long-term stock market prediction. 2018 9th International Conference on \\r\\nInformation and Communication Systems (ICICS), 151–156.\\r\\nAnderson, L. R., & Holt, C. A. (1997). Information Cascades in the Laboratory. The American \\r\\nEconomic Review, 87(5), 847–862.\\r\\nAntweiler, W., & Frank, M. Z. (2004). Is All That Talk Just Noise? The Information Content of \\r\\nInternet Stock Message Boards. The Journal of Finance, 59(3), 1259–1294. \\r\\nhttps://doi.org/10.1111/j.1540-6261.2004.00662.x\\n28\\r\\nAriyo, A. A., Adewumi, A. O., & Ayo, C. K. (2014). Stock Price Prediction Using the ARIMA \\r\\nModel. 2014 UKSim-AMSS 16th International Conference on Computer Modelling and \\r\\nSimulation, 106–112. https://doi.org/10.1109/UKSim.2014.67\\r\\nAsghar, M. Z., Rahman, F., Kundi, F. M., & Ahmad, S. (2019). Development of stock market \\r\\ntrend prediction system using multiple regression. Computational and Mathematical \\r\\nOrganization Theory, 25(3), 271–301. https://doi.org/10.1007/s10588-019-09292-7\\r\\nBikhchandani, S., Hirshleifer, D., & Welch, I. (1992). A Theory of Fads, Fashion, Custom, and \\r\\nCultural Change as Informational Cascades. Journal of Political Economy, 100(5), 992–1026. \\r\\nhttps://doi.org/10.1086/261849\\r\\nChen, Z., Ma, M., Li, T., Wang, H., & Li, C. (2023). Long sequence time-series forecasting with \\r\\ndeep learning: A survey. Information Fusion, 97, 101819. \\r\\nhttps://doi.org/10.1016/j.inffus.2023.101819\\r\\nCostola, M., Hinz, O., Nofer, M., & Pelizzon, L. (2023). Machine learning sentiment analysis, \\r\\nCOVID-19 news and stock market reactions. Research in International Business and Finance, \\r\\n64, 101881. https://doi.org/10.1016/j.ribaf.2023.101881\\r\\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-XL: \\r\\nAttentive Language Models Beyond a Fixed-Length Context (arXiv:1901.02860). arXiv. \\r\\nhttps://doi.org/10.48550/arXiv.1901.02860\\r\\nDaniel, K., & Titman, S. (1999). Market Efficiency in an Irrational World. Financial Analysts \\r\\nJournal, 55(6), 28–40. https://doi.org/10.2469/faj.v55.n6.2312\\r\\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep \\r\\nbidirectional transformers for language understanding. arXiv Preprint arXiv:1810.04805.\\r\\nDimic, N., Kiviaho, J., Piljak, V., & Äijö, J. (2016). Impact of financial market uncertainty and \\r\\nmacroeconomic factors on stock–bond correlation in emerging markets. Research in \\r\\nInternational Business and Finance, 36, 41–51. https://doi.org/10.1016/j.ribaf.2015.09.001\\r\\nDong, J. (2023). Natural Language Processing Pretraining Language Model for Computer \\r\\nIntelligent Recognition Technology. ACM Transactions on Asian and Low-Resource \\r\\nLanguage Information Processing. https://doi.org/10.1145/3605210\\r\\nDurairaj, M., & Krishna, M. B. H. (2021). Statistical Evaluation and Prediction of Financial Time \\n29\\r\\nSeries Using Hybrid Regression Prediction Models. International Journal of Intelligent \\r\\nSystems and Applications in Engineering, 9(4), Article 4. \\r\\nhttps://doi.org/10.18201/ijisae.2021473645\\r\\nEachempati, P., Srivastava, P. R., Kumar, A., Muñoz de Prat, J., & Delen, D. (2022). Can customer \\r\\nsentiment impact firm value? An integrated text mining approach. Technological Forecasting \\r\\nand Social Change, 174, 121265. https://doi.org/10.1016/j.techfore.2021.121265\\r\\nErgen, T., & Kozat, S. S. (2018). Efficient Online Learning Algorithms Based on LSTM Neural \\r\\nNetworks. IEEE Transactions on Neural Networks and Learning Systems, 29(8), 3772–3783. \\r\\nhttps://doi.org/10.1109/TNNLS.2017.2741598\\r\\nFama, E. F. (1991). Efficient Capital Markets: II. The Journal of Finance, 46(5), 1575–1617. \\r\\nhttps://doi.org/10.1111/j.1540-6261.1991.tb04636.x\\r\\nFlach, P. A., & Lachiche, N. (2004). Naive Bayesian classification of structured data. Machine \\r\\nLearning, 57, 233–269.\\r\\nGao, C., & Martin, I. W. R. (2021). Volatility, Valuation Ratios, and Bubbles: An Empirical \\r\\nMeasure of Market Sentiment. The Journal of Finance, 76(6), 3211–3254. \\r\\nhttps://doi.org/10.1111/jofi.13068\\r\\nGharehchopogh, F. S., & Khalifelu, Z. A. (2011). Analysis and evaluation of unstructured data: \\r\\nText mining versus natural language processing. 2011 5th International Conference on \\r\\nApplication of Information and Communication Technologies (AICT), 1–4. \\r\\nhttps://doi.org/10.1109/ICAICT.2011.6111017\\r\\nGite, S., Khatavkar, H., Kotecha, K., Srivastava, S., Maheshwari, P., & Pandey, N. (2021). \\r\\nExplainable stock prices prediction from financial news articles using sentiment analysis. \\r\\nPeerJ Computer Science, 7, e340. https://doi.org/10.7717/peerj-cs.340\\r\\nGong, J., & Sun, S. (2009). A New Approach of Stock Price Prediction Based on Logistic \\r\\nRegression Model. 2009 International Conference on New Trends in Information and Service \\r\\nScience, 1366–1371. https://doi.org/10.1109/NISS.2009.267\\r\\nGong, X., Zhang, W., Wang, J., & Wang, C. (2022). Investor sentiment and stock volatility: New \\r\\nevidence. International Review of Financial Analysis, 80, 102028. \\r\\nhttps://doi.org/10.1016/j.irfa.2022.102028\\n30\\r\\nGranger, C. W. J. (1969). Investigating Causal Relations by Econometric Models and Cross\\x02spectral Methods. Econometrica, 37(3), 424–438. https://doi.org/10.2307/1912791\\r\\nGuo, Y., Guo, J., Sun, B., Bai, J., & Chen, Y. (2022). A new decomposition ensemble model for \\r\\nstock price forecasting based on system clustering and particle swarm optimization. Applied \\r\\nSoft Computing, 130, 109726. https://doi.org/10.1016/j.asoc.2022.109726\\r\\nHafezi, R., Shahrabi, J., & Hadavandi, E. (2015). A bat-neural network multi-agent system \\r\\n(BNNMAS) for stock price prediction: Case study of DAX stock price. Applied Soft \\r\\nComputing, 29, 196–210. https://doi.org/10.1016/j.asoc.2014.12.028\\r\\nHamburger, M. J., & Kochin, L. A. (1972). Money and Stock Prices: The Channels of Influences. \\r\\nThe Journal of Finance, 27(2), 231–249. https://doi.org/10.2307/2978472\\r\\nHan, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y., Qiu, J., Yao, Y., Zhang, A., Zhang, L., Han, \\r\\nW., Huang, M., Jin, Q., Lan, Y., Liu, Y., Liu, Z., Lu, Z., Qiu, X., Song, R., … Zhu, J. (2021). \\r\\nPre-trained models: Past, present and future. AI Open, 2, 225–250. \\r\\nhttps://doi.org/10.1016/j.aiopen.2021.08.002\\r\\nHuang, C., Cao, Y., Lu, M., Shan, Y., & Zhang, Y. (2023). Messages in online stock forums and \\r\\nstock price synchronicity: Evidence from China. Accounting & Finance, 63(3), 3011–3041. \\r\\nhttps://doi.org/10.1111/acfi.13005\\r\\nJin, Z., Yang, Y., & Liu, Y. (2020). Stock closing price prediction based on sentiment analysis and \\r\\nLSTM. Neural Computing and Applications, 32(13), 9713–9729. \\r\\nhttps://doi.org/10.1007/s00521-019-04504-2\\r\\nJoachims, T. (1998). Text categorization with support vector machines: Learning with many \\r\\nrelevant features. European Conference on Machine Learning, 137–142.\\r\\nJohnman, M., Vanstone, B. J., & Gepp, A. (2018). Predicting FTSE 100 returns and volatility \\r\\nusing sentiment analysis. Accounting & Finance, 58(S1), 253–274. \\r\\nhttps://doi.org/10.1111/acfi.12373\\r\\nJunqué de Fortuny, E., De Smedt, T., Martens, D., & Daelemans, W. (2014). Evaluating and \\r\\nunderstanding text-based stock price prediction models. Information Processing & \\r\\nManagement, 50(2), 426–441. https://doi.org/10.1016/j.ipm.2013.12.002\\r\\nKanwal, A., Lau, M. F., Ng, S. P. H., Sim, K. Y., & Chandrasekaran, S. (2022). BiCuDNNLSTM-\\n31\\r\\n1dCNN — A hybrid deep learning-based predictive model for stock price prediction. Expert \\r\\nSystems with Applications, 202, 117123. https://doi.org/10.1016/j.eswa.2022.117123\\r\\nKim, C., & Yang, S.-U. (2017). Like, comment, and share on Facebook: How each behavior \\r\\ndiffers from the other. Public Relations Review, 43(2), 441–449. \\r\\nhttps://doi.org/10.1016/j.pubrev.2017.02.006\\r\\nKumar, G., Jain, S., & Singh, U. P. (2021). Stock Market Forecasting Using Computational \\r\\nIntelligence: A Survey. Archives of Computational Methods in Engineering, 28(3), 1069–\\r\\n1101. https://doi.org/10.1007/s11831-020-09413-5\\r\\nLawi, A., Mesra, H., & Amir, S. (2022). Implementation of Long Short-Term Memory and Gated \\r\\nRecurrent Units on grouped time-series data to predict stock prices accurately. Journal of Big \\r\\nData, 9(1), 89. https://doi.org/10.1186/s40537-022-00597-0\\r\\nLi, X., Xie, H., Chen, L., Wang, J., & Deng, X. (2014). News impact on stock price return via \\r\\nsentiment analysis. Knowledge-Based Systems, 69, 14–23. \\r\\nhttps://doi.org/10.1016/j.knosys.2014.04.022\\r\\nLin, P., Ma, S., & Fildes, R. (2024). The extra value of online investor sentiment measures on \\r\\nforecasting stock return volatility: A large-scale longitudinal evaluation based on Chinese \\r\\nstock market. Expert Systems with Applications, 238, 121927. \\r\\nhttps://doi.org/10.1016/j.eswa.2023.121927\\r\\nLiu, Q., Lee, W.-S., Huang, M., & Wu, Q. (2023). Synergy between stock prices and investor \\r\\nsentiment in social media. Borsa Istanbul Review, 23(1), 76–92. \\r\\nhttps://doi.org/10.1016/j.bir.2022.09.006\\r\\nLu, W., Li, J., Wang, J., & Qin, L. (2021). A CNN-BiLSTM-AM method for stock price prediction. \\r\\nNeural Computing and Applications, 33(10), 4741–4753. https://doi.org/10.1007/s00521-\\r\\n020-05532-z\\r\\nMishev, K., Gjorgjevikj, A., Vodenska, I., Chitkushev, L. T., & Trajanov, D. (2020). Evaluation of \\r\\nSentiment Analysis in Finance: From Lexicons to Transformers. IEEE Access, 8, 131662–\\r\\n131682. https://doi.org/10.1109/ACCESS.2020.3009626\\r\\nNiu, H., & Xu, K. (2020). A hybrid model combining variational mode decomposition and an \\r\\nattention-GRU network for stock price index forecasting. Mathematical Biosciences and \\n32\\r\\nEngineering, 17(6), 7151–7166.\\r\\nObaid, K., & Pukthuanthong, K. (2022). A picture is worth a thousand words: Measuring investor \\r\\nsentiment by combining machine learning and photos from news. Journal of Financial \\r\\nEconomics, 144(1), 273–297. https://doi.org/10.1016/j.jfineco.2021.06.002\\r\\nPark, H. J., Kim, Y., & Kim, H. Y. (2022). Stock market forecasting using a multi-task approach \\r\\nintegrating long short-term memory and the random forest framework. Applied Soft \\r\\nComputing, 114, 108106. https://doi.org/10.1016/j.asoc.2021.108106\\r\\nPedersen, L. H. (2022). Game on: Social networks and markets. Journal of Financial Economics, \\r\\n146(3), 1097–1119. https://doi.org/10.1016/j.jfineco.2022.05.002\\r\\nPicasso, A., Merello, S., Ma, Y., Oneto, L., & Cambria, E. (2019). Technical analysis and \\r\\nsentiment embeddings for market trend prediction. Expert Systems with Applications, 135, \\r\\n60–70. https://doi.org/10.1016/j.eswa.2019.06.014\\r\\nReis, P. M. N., & Pinho, C. (2021). A Reappraisal of the Causal Relationship between Sentiment \\r\\nProxies and Stock Returns. Journal of Behavioral Finance, 22(4), 420–442. \\r\\nhttps://doi.org/10.1080/15427560.2020.1792910\\r\\nRezaei, H., Faaljou, H., & Mansourfar, G. (2021). Stock price prediction using deep learning and \\r\\nfrequency decomposition. Expert Systems with Applications, 169, 114332. \\r\\nhttps://doi.org/10.1016/j.eswa.2020.114332\\r\\nSapankevych, N. I., & Sankar, R. (2009). Time Series Prediction Using Support Vector Machines: \\r\\nA Survey. IEEE Computational Intelligence Magazine, 4(2), 24–38. \\r\\nhttps://doi.org/10.1109/MCI.2009.932254\\r\\nSelvin, S., Vinayakumar, R., Gopalakrishnan, E. A., Menon, V. K., & Soman, K. P. (2017). Stock \\r\\nprice prediction using LSTM, RNN and CNN-sliding window model. 2017 International \\r\\nConference on Advances in Computing, Communications and Informatics (ICACCI), 1643–\\r\\n1647. https://doi.org/10.1109/ICACCI.2017.8126078\\r\\nShanthini, P. M., Parthasarathy, S., Venkatesan, P., & Nandhini, S. (2023). HRSR-SVM: Hybrid \\r\\nReptile Search Remora-based Support Vector Machine for forecasting stock price movement. \\r\\nInternational Journal of Information Technology, 15(6), 3127–3134. \\r\\nhttps://doi.org/10.1007/s41870-023-01331-6\\n33\\r\\nSharaf, M., Hemdan, E. E.-D., El-Sayed, A., & El-Bahnasawy, N. A. (2021). StockPred: A \\r\\nframework for stock Price prediction. Multimedia Tools and Applications, 80(12), 17923–\\r\\n17954. https://doi.org/10.1007/s11042-021-10579-8\\r\\nShiller, R. J., Fischer, S., & Friedman, B. M. (1984). Stock Prices and Social Dynamics. Brookings \\r\\nPapers on Economic Activity, 1984(2), 457–510. https://doi.org/10.2307/2534436\\r\\nSmola, A. J., & Schölkopf, B. (2004). A tutorial on support vector regression. Statistics and \\r\\nComputing, 14, 199–222.\\r\\nSweidan, A. H., El-Bendary, N., & Al-Feel, H. (2021). Sentence-Level Aspect-Based Sentiment \\r\\nAnalysis for Classifying Adverse Drug Reactions (ADRs) Using Hybrid Ontology-XLNet \\r\\nTransfer Learning. IEEE Access, 9, 90828–90846. \\r\\nhttps://doi.org/10.1109/ACCESS.2021.3091394\\r\\nValle-Cruz, D., Fernandez-Cortez, V., López-Chau, A., & Sandoval-Almazán, R. (2022). Does \\r\\nTwitter Affect Stock Market Decisions? Financial Sentiment Analysis During Pandemics: A \\r\\nComparative Study of the H1N1 and the COVID-19 Periods. Cognitive Computation, 14(1), \\r\\n372–387. https://doi.org/10.1007/s12559-021-09819-8\\r\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & \\r\\nPolosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing \\r\\nSystems, 30.\\r\\nWang, Y., & Guo, Y. (2020). Forecasting method of stock market volatility in time series data \\r\\nbased on mixed model of ARIMA and XGBoost. China Communications, 17(3), 205–221. \\r\\nhttps://doi.org/10.23919/JCC.2020.03.017\\r\\nWu, S., Liu, Y., Zou, Z., & Weng, T.-H. (2022). S_I_LSTM: Stock price prediction based on \\r\\nmultiple data sources and sentiment analysis. Connection Science, 34(1), 44–62. \\r\\nhttps://doi.org/10.1080/09540091.2021.1940101\\r\\nXu, Y., Liang, C., Li, Y., & Huynh, T. L. D. (2022). News sentiment and stock return: Evidence \\r\\nfrom managers’ news coverages. Finance Research Letters, 48, 102959. \\r\\nhttps://doi.org/10.1016/j.frl.2022.102959\\r\\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). XLNet: \\r\\nGeneralized Autoregressive Pretraining for Language Understanding. Advances in Neural \\n34\\r\\nInformation Processing Systems, 32. \\r\\nhttps://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-\\r\\nAbstract.html\\r\\nYuan, X., Li, Y., Xue, Z., & Kou, F. (2021). Financial sentiment analysis based on pre-training and \\r\\ntextcnn. Proceedings of 2020 Chinese Intelligent Systems Conference: Volume II, 48–56.\\r\\nYun, K. K., Yoon, S. W., & Won, D. (2021). Prediction of stock price direction using a hybrid GA\\x02XGBoost algorithm with a three-stage feature engineering process. Expert Systems with \\r\\nApplications, 186, 115716. https://doi.org/10.1016/j.eswa.2021.115716\\r\\nZhang, Q., Zhang, Y., Yao, X., Li, S., Zhang, C., & Liu, P. (2023). A Dynamic Attributes-driven \\r\\nGraph Attention Network Modeling on Behavioral Finance for Stock Prediction. ACM \\r\\nTransactions on Knowledge Discovery from Data, 18(1), 16:1-16:29. \\r\\nhttps://doi.org/10.1145/3611311\\r\\nZhang, X., Liang, X., Zhiyuli, A., Zhang, S., Xu, R., & Wu, B. (2019). At-lstm: An attention-based \\r\\nlstm model for financial time series prediction. IOP Conference Series: Materials Science \\r\\nand Engineering, 569(5), 052037.\\r\\nZilly, J. G., Srivastava, R. K., Koutnı́k, J., & Schmidhuber, J. (2017). Recurrent Highway \\r\\nNetworks. Proceedings of the 34th International Conference on Machine Learning, 4189–\\r\\n4198. https://proceedings.mlr.press/v70/zilly17a.html'},\n",
       " {'name': '2405.13959v1.pdf',\n",
       "  'content': '1\\r\\nDecision Trees for Intuitive Intraday \\r\\nTrading Strategies\\r\\nNaga Prajwal\\r\\nUG Student Department of CSE\\r\\nThe National Institute of \\r\\nEngineering \\r\\nMysuru, India \\r\\nnagaprajwalnpb@gmail.com\\r\\nDinesh Balivada \\r\\nUG Student Department of CSE\\r\\nThe National Institute of \\r\\nEngineering \\r\\nMysuru, India \\r\\ndineshbalivada8@gmail.com\\r\\nSharath Chandra Nirmala\\r\\nUG Student Department of CSE\\r\\nThe National Institute of \\r\\nEngineering \\r\\nMysuru, India \\r\\nsharathnimala@gmail.com\\r\\nTiruveedi Poornoday\\r\\nUG Student Department of CSE\\r\\nThe National Institute of \\r\\nEngineering \\r\\nMysuru, India \\r\\ntpoornoday01@gmail.com\\r\\nAbstract—This research paper aims to investigate the \\r\\nefficacy of decision trees in constructing intraday trading \\r\\nstrategies using existing technical indicators for \\r\\nindividual equities in the NIFTY50 index. Unlike \\r\\nconventional methods that rely on a fixed set of rules \\r\\nbased on combinations of technical indicators developed \\r\\nby a human trader through their analysis, the proposed \\r\\napproach leverages decision trees to create unique \\r\\ntrading rules for each stock, potentially enhancing \\r\\ntrading performance and saving time. By extensively \\r\\nbacktesting the strategy for each stock, a trader can \\r\\ndetermine whether to employ the rules generated by the \\r\\ndecision tree for that specific stock. While this method \\r\\ndoes not guarantee success for every stock, decision tree\\x02based strategies outperform the simple buy-and-hold \\r\\nstrategy for many stocks. The results highlight the \\r\\nproficiency of decision trees as a valuable tool for \\r\\nenhancing intraday trading performance on a stock-by\\x02stock basis and could be of interest to traders seeking to \\r\\nimprove their trading strategies.\\r\\nKeywords— Intraday Trading, Decision Trees, Machine \\r\\nLearning, Equities, Technical Indicators\\r\\nI. INTRODUCTION\\r\\nIntraday trading involves buying and selling stocks within \\r\\nthe same day to benefit from small price movements in the \\r\\nmarket, yielding small profits that add up over the trading \\r\\nperiod. Technical analysis is a long-established method in \\r\\nintraday trading that utilizes past market data to create \\r\\nindicators, identify patterns, and make trading decisions \\r\\nbased on the observed patterns. However, traditional \\r\\ntechnical analysis methods rely on a fixed set of rules based \\r\\non combinations of technical indicators, which tend to be \\r\\ntediously generated and may not perform well for all stocks. \\r\\nFurthermore, these methods may not consider individual \\r\\nstock characteristics, which can result in suboptimal trading \\r\\ndecisions.\\r\\nDecision Trees can serve as an alternative approach to the \\r\\nmanual construction of trading rules in a trading strategy. \\r\\nThey create unique and interpretable trading rules for each \\r\\nstock, potentially enhancing trading performance and saving \\r\\ntime. Additionally, decision trees can perform classification \\r\\nand regression tasks [1].\\r\\nThis paper demonstrates the efficacy of decision trees as a \\r\\nvaluable tool for enhancing intraday trading performance on \\r\\na stock-by-stock basis. Despite using the same indicators for \\r\\neach stock, a decision tree-based classifier model discovers \\r\\ndifferent rules depending on the stocks\\' characteristics. \\r\\nFurthermore, decision trees can create interpretable and \\r\\nunderstandable models, helping traders better understand and \\r\\ninterpret their trading strategies.\\r\\nII. DATASET COLLECTION\\r\\nThe dataset collection methodology in this paper adopts a \\r\\nhybrid approach, which efficiently and economically \\r\\nacquires comprehensive intraday trading data. The approach \\r\\nutilizes two vendors, namely ICICI Breeze and Yahoo \\r\\nFinance. The ICICI Breeze API was the primary source for \\r\\ndata, providing one-minute time interval data from 2022 to \\r\\n2024. However, ICICI Breeze data lacks adjusted close \\r\\nprices, which is essential for backtesting. The Yahoo Finance \\r\\nAPI is used to surpass this limitation. Yahoo Finance\\'s daily \\r\\ninterval data, which includes adjusted close prices, is used to \\r\\ncompute adjustment factors for each trading day. These \\r\\nadjustment factors are resampled to a one-minute frequency, \\r\\naligning with the one-minute interval of the ICICI Breeze \\r\\ndata. This procedure ensures the synchronization of adjusted \\r\\nclose prices with the ICICI Breeze data, facilitating a \\r\\nthorough analysis of intraday price movements. The hybrid \\r\\napproach used in this paper enables the acquisition of \\r\\nadjusted one-minute data over the past two years at no cost, \\r\\nmaking it a quick, cost-effective method for retail traders. By \\r\\nutilizing this approach, traders can access comprehensive \\r\\nhistorical intraday trading data, which aids in making \\r\\ninformed trading decisions.\\r\\nIII. STRATEGY BUILDING\\r\\nA. Indicators Used\\r\\nThe strategy-building process involved four traditional \\r\\nTechnical Indicators, and five statistical relationships \\r\\nbetween the prices, resulting in nine inputs for the Decision \\r\\nTree model. These indicators include the returns of the close \\r\\nprice series, the 15-period return of the close price series, the \\r\\n14-period Relative Strength Index (RSI) Indicator, the 14-\\r\\nperiod Average Directional Index (ADX), the ratio between \\r\\nthe 14-period Simple Moving Average (SMA) and the close \\r\\nprice series, the correlation coefficient between the SMA and \\r\\nthe close prices, the 14-period rolling volatility, the 210-\\r\\nperiod rolling volatility (which is essentially the 14-period \\r\\nvolatility of the 15-period rolling returns), and the ratio \\r\\nbetween the 14 period rolling Volume Weighted Average \\r\\nPrice (VWAP) and the Close Price Series. \\n2\\r\\nThe indicators were chosen as they provide meaningful \\r\\ninformation about the stocks\\' current and probable future \\r\\nperformance. The returns of the close price series and the 15-\\r\\nperiod return of the close price series were included to \\r\\ncapture the price changes over short and medium-term \\r\\nhorizons. The 14-period RSI and ADX were used to assess \\r\\nthe momentum and direction of the trend, respectively. The \\r\\nratio between the 14-period SMA and the close price series \\r\\nand the correlation coefficient between the SMA and the \\r\\nclose prices were included to capture the long-term price \\r\\ntrend. The 14-period rolling volatility and the 210-period \\r\\nrolling volatility were used to capture the market volatility \\r\\nover different periods. Finally, the ratio between the 14-\\r\\nperiod rolling VWAP and the close price series was included \\r\\nto assess the volume-weighted average price and its \\r\\nrelationship with the price change.\\r\\nOverall, the chosen indicators provide a comprehensive \\r\\nand diverse collection of inputs for the Decision tree-based \\r\\nclassifier model, enabling the creation of unique and \\r\\ninterpretable trading rules for each stock.\\r\\nB. Decision Trees\\r\\nDecision trees, a widely utilized supervised machine \\r\\nlearning model, find extensive application across diverse \\r\\ndomains such as finance, medicine, and marketing. This \\r\\nmodel operates by recursively partitioning the dataset into \\r\\nsubsets based on the input variables. Within the tree \\r\\nstructure, nodes represent the input variables, branches \\r\\ndepict the potential values of these input variables, and \\r\\nleaves correspond to the output variables [2].\\r\\nClassification trees are a specific type of decision tree, \\r\\nfrequently employed in financial models to generate discrete \\r\\nbuy and sell signals. This model produces a binary output, \\r\\nwhere a value of 1 indicates a buy signal and a value of 0 \\r\\nindicates a sell signal. To create the output for the model, \\r\\nhistorical data of the stock\\'s closing price returns, shifted by \\r\\n-1, are utilized. In this context, future negative returns map \\r\\nto 0 and future positive returns to 1 [3].\\r\\n1) Criteria for Building Decision Trees\\r\\nCriteria for building decision trees can be entropy, \\r\\nGini coefficient, or classification error. These criteria \\r\\nmeasure node impurity, which indicates the extent to \\r\\nwhich output variables are mixed within a node. The \\r\\ncriterion used to build the decision tree determines the \\r\\nsplitting rule used to partition the data at each node.\\r\\n2) Gini Coefficient as the Preferred Criterion\\r\\nThis paper employs the Gini coefficient for \\r\\nconstructing decision trees. The Gini coefficient \\r\\nassesses the probability of misclassifying a randomly \\r\\nselected sample from a node, based on the distribution \\r\\nof the output variables within that node. It was chosen \\r\\nover other criteria due to its lower sensitivity to the \\r\\ndistribution of output variables and its capacity to \\r\\nmanage imbalanced datasets effectively. Moreover, \\r\\nthe Gini coefficient typically generates smaller trees, \\r\\nthereby mitigating the risk of overfitting the model.\\r\\nOptimizing the depth of the decision tree is essential, as \\r\\nit is a critical hyperparameter that affects both the model\\'s \\r\\ncomplexity and performance. A tree depth that is too shallow \\r\\ncan render the model overly simplistic, preventing it from \\r\\ncapturing the underlying patterns in the data. This can lead\\r\\nto underfitting, where the model performs inadequately on \\r\\nboth training and testing data. In such cases, the decision \\r\\nboundaries may be overly simplified, diminishing the \\r\\nmodel\\'s predictive accuracy. Conversely, an excessively \\r\\ndeep tree can make the model overly complex and \\r\\nsusceptible to overfitting. Overfitting occurs when the model \\r\\nmemorizes the training data instead of identifying the \\r\\nunderlying relationships, resulting in excellent performance \\r\\non training data but poor generalization to new data.\\r\\nFor our paper, we conducted experiments with decision \\r\\ntree depths ranging from three to six. After thorough \\r\\nanalysis, we determined that a depth of four provides the \\r\\noptimal balance for model complexity and performance. \\r\\nCharts of performance metrics concerning tree depth are \\r\\nprovided below from Fig. 1 to Fig. 3. A tree depth of four \\r\\nenables the model to capture sufficient complexity for \\r\\naccurate predictions while minimizing the risk of overfitting.\\r\\nFig. 1 Sharpe and Profit Factor of an average portfolio w.r.t tree depth\\r\\n(testing dataset).\\r\\nFig. 2 Number of stocks where strategy outperformed benchmark\\r\\n(testing dataset).\\n3\\r\\nFig. 3 Additional KPIs w.r.t tree depth (testing dataset)\\r\\nIV. STRATEGY IMPLEMENTATION\\r\\nA. Dataset Preprocessing\\r\\nThe adjusted dataset constructed from various sources, as \\r\\ndiscussed in the \"Dataset Collection\", forms the basis for the \\r\\nintraday trading strategy. However, before proceeding with \\r\\nmodel training, it is necessary to preprocess the dataset to \\r\\nensure consistency and continuity across all stocks. \\r\\nAll stocks are aligned to have an identical number of rows \\r\\nand indexes with matching values. Due to varying market \\r\\nconditions and brokers\\' data quality, certain stocks may \\r\\nexhibit missing values for a few timestamps. A combined \\r\\nindex for all stocks was created by applying the set union \\r\\noperator to all stocks\\' indexes. The set union operator creates \\r\\nnew indexes with empty values in some stocks, resulting in \\r\\nNaN (Not a Number) values. We fill these NaN values using \\r\\nlinear interpolation to ensure data continuity for backtesting. \\r\\nThis method is relatively safe as it maintains data integrity \\r\\nwhile filling relatively small gaps with interpolated values.\\r\\nAfter aligning the data and interpolating the missing \\r\\nvalues, we apply the aforementioned technical indicators to \\r\\nthe dataset. These indicators serve as inputs to the decision \\r\\ntree model, offering valuable insights into market trends and \\r\\nprice movements. Additionally, the necessary output value \\r\\nfor the model, indicating the trading signal (1 for buy and 0 \\r\\nfor sell), is incorporated into the dataset. Since decision trees \\r\\nare not sensitive to monotonic transformations [6], there is \\r\\nno requirement to scale the features or transform the data. \\r\\nConsequently, the raw features are utilized directly in the \\r\\nmodel without preprocessing. Subsequently, the \\r\\npreprocessed data is divided into training and testing sets, \\r\\nwith the training set spanning from 2022 to 2023, and the \\r\\ntesting set spanning from 2023 to 2024. The training set is \\r\\nutilized to train the decision tree model, while the testing set \\r\\nis employed to evaluate its predictive performance on unseen \\r\\ndata.\\r\\nB. Model Implementation\\r\\nThe decision tree-based intraday trading model is \\r\\ndeveloped using the DecisionTreeClassifier module from the \\r\\nScikit Learn library. This module provides an efficient and \\r\\nversatile implementation of decision trees for classification \\r\\ntasks. Through experimentation, a maximum depth of four is \\r\\nchosen for the decision tree, striking a balance between \\r\\ncapturing adequate complexity for accurate predictions and \\r\\navoiding overfitting. The Gini coefficient serves as the \\r\\ncriterion for constructing the decision tree model, effectively \\r\\nmeasuring the impurity of a node\\'s class distribution, which \\r\\nis particularly suitable for binary classification tasks. Despite \\r\\nproviding nine inputs to the decision tree model, it is \\r\\nobserved that, on average, most models utilize approximately \\r\\nfive to seven indicators to make actual decisions. Moreover, \\r\\nthese inputs exhibit variability across different stocks, \\r\\nindicating the model\\'s adaptability in selecting relevant \\r\\nfeatures for each stock. Fig 4 and Fig 5 depict the decision \\r\\ntrees for the two stocks. An immediate distinction is the \\r\\nabsence of the ADX as a deciding parameter in Fig. 4.\\r\\nFig. 4 Decision Tree Model for Nestle India’s stock.\\r\\nFig. 5 Decision Tree Model for Reliance’s Stock\\n4\\r\\nC. Backtesting\\r\\nBacktesting played a crucial role in evaluating the \\r\\nperformance and robustness of the decision tree-based \\r\\nintraday trading strategy. The backtesting process and results \\r\\nwere implemented using Python\\'s vectorbt library for \\r\\nvectorized backtesting. The backtesting process involved \\r\\nsimulating the execution of trading signals generated by the \\r\\ndecision tree model on historical data. Python\\'s vectorbt \\r\\nlibrary provided efficient tools for vectorized backtesting, \\r\\nenabling fast and convenient evaluation of trading strategies. \\r\\nIt\\'s important to note that commissions and slippage were not \\r\\nexplicitly incorporated into the backtesting process. \\r\\nHowever, the buying and selling signals were adjusted \\r\\nforward by one step to accommodate the trading delay, \\r\\nthereby simulating a realistic trading scenario. Returns were \\r\\ncomputed using close prices. To assess its relative \\r\\nperformance, the decision tree-based intraday trading \\r\\nstrategy was compared to a buy-and-hold benchmark. The \\r\\noutcomes of the backtesting offered insights into the \\r\\nprofitability, risk-adjusted return, and consistency of the \\r\\ndecision tree-based intraday trading strategy. Through the \\r\\nanalysis of performance metrics, traders could make well\\x02informed decisions regarding the potential implementation \\r\\nof the strategy in real-world trading scenarios. Overall, the \\r\\nbacktesting process played a pivotal role in evaluating the \\r\\neffectiveness and efficacy of the decision tree-based intraday \\r\\ntrading strategy, furnishing valuable insights for potential \\r\\ndeployment in live trading environments.\\r\\nV. RESULTS\\r\\nKey performance indicators (KPIs) assume a pivotal role \\r\\nin the evaluation of trading strategy efficacy and \\r\\nperformance. Serving as invaluable metrics, these KPIs \\r\\nfacilitate the assessment of performance and risk \\r\\ncharacteristics inherent in the decision tree-based intraday \\r\\ntrading strategy. The Python open-source libraries \"vectorbt\" \\r\\nand \"quantstats\" furnish these KPIs for individual stocks. It \\r\\nis pertinent to mention that the annualizing of KPIs utilizes \\r\\n252 × 375 = 94500 periods per year [4], with 252 \\r\\nrepresenting trading days per year and 375 denoting trading \\r\\nintervals within a trading day. The risk-free rate is assumed \\r\\nto average 7.2% [5]. The ensuing section delineates the KPIs \\r\\nemployed in the analysis:\\r\\n1) Sharpe Ratio\\r\\nMeasures the risk-adjusted return of an investment, \\r\\nindicating how much return an investment generates \\r\\nper unit of risk.\\r\\n2) Total Return\\r\\nRepresents the overall return generated by an \\r\\ninvestment over a specified period, including capital \\r\\nappreciation and income.\\r\\n3) CAGR (Compound Annual Growth Rate)\\r\\nRepresents the geometric average annual rate of \\r\\nreturn over a specified period, providing a smooth \\r\\nannualized growth rate.\\r\\n4) Maximum Drawdown\\r\\nMeasures the maximum loss from peak to trough \\r\\nexperienced by an investment or trading strategy over \\r\\na specific period.\\r\\n5) Win Rate\\r\\nRepresents the total profitable trades relative to the \\r\\ntotal trades executed, indicating the consistency of a \\r\\ntrading strategy\\'s performance.\\r\\n6) Profit Factor\\r\\nMeasures the relationship between profits and \\r\\nlosses, calculated by dividing the total profit from \\r\\nwinning trades by the total loss from losing trades.\\r\\n7) Volatility\\r\\nMeasures the degree of variation or dispersion in the \\r\\nreturns of an investment or trading strategy over time, \\r\\nindicating the riskiness or stability of returns.\\r\\n8) PSBBR\\r\\nThe percentage of Stocks beating their benchmark \\r\\n(buy-and-hold) in total returns metric.\\r\\n9) PSBBS\\r\\nThe percentage of Stocks beating their benchmark \\r\\n(buy-and-hold) in Sharpe Ratio metric.\\r\\nThe strategy demonstrated promising results, \\r\\noutperforming the benchmark in terms of Sharpe ratio, \\r\\nvolatility, and maximum drawdown. However, it\\'s \\r\\nnoteworthy that the total returns of the strategy were lower \\r\\nthan the benchmark in the testing dataset. Despite this, when \\r\\nadjusted for volatility, the strategy outperformed the \\r\\nbenchmark, delivering a better reward-to-risk ratio and lower \\r\\nmaximum drawdown.\\r\\nThese findings illustrate that despite the strategy\\'s total \\r\\nreturns being marginally lower in the testing dataset, its risk\\x02adjusted performance is superior to the benchmark. This \\r\\nindicates that the decision tree-based intraday trading \\r\\nstrategy is more efficient in terms of managing risk and \\r\\ndelivering consistent returns. This is further highlighted by \\r\\nPSBBR and PSBBS values drop from the training to the \\r\\ntesting dataset, but the drop is far more significant for \\r\\nPSBBR at a 42.50% drop compared to a drop of 28.57% for \\r\\nPSBBS. The results for both the training and testing data \\r\\nhave been tabulated below in Table 1. Additionally, Fig. 6 to \\r\\nFig. 11 are charts to aid visualization of the returns.\\n5\\r\\nTABLE 1 Final Results\\r\\nKPIs Table Training Dataset Testing Dataset\\r\\nBenchmark Strategy Under/Overperforms Benchmark Strategy Under/Overperforms\\r\\nSharpe Ratio 0.1 3.37 OVER 2.36 5.81 OVER\\r\\nTotal Return [%] 7.17 26.9 OVER 34.23 28.62 UNDER\\r\\nCAGR [%] 4.96 18.15 OVER 22.95 19.33 UNDER\\r\\nMax Drawdown [%] -14.31 -2.33 OVER -7.02 -2.63 OVER\\r\\nWin Rate [%] NA 48.20 NA NA 48.15 NA\\r\\nProfit Factor 1.08 1.07 UNDER 1.63 1.11 UNDER\\r\\nVolatility [%] 18.27 5.21 OVER 10.23 3.32 OVER\\r\\nPSBBR [%] NA 80 NA NA 46 NA\\r\\nPSBBS [%] NA 84 NA NA 60 NA\\r\\nFig. 6 Returns of Strategy and. Benchmark (Training Dataset)\\r\\nFig. 7 Volatility adjusted returns of Strategy and Benchmark (Training \\r\\nDataset)\\r\\nFig. 8 Monthly Returns of Strategy (Training Dataset)\\r\\nFig. 9 Returns of Strategy and. Benchmark (Testing Dataset)\\r\\nFig. 10 Volatility adjusted returns of Strategy and Benchmark (Testing \\r\\nDataset)\\r\\nFig. 11 Monthly Returns of Strategy (Testing Dataset)\\n6\\r\\nVI. CONCLUSION\\r\\nThis paper has demonstrated the effectiveness of decision \\r\\ntrees in creating intraday trading strategies with technical \\r\\nindicators. The study found that decision trees offer a \\r\\npowerful and efficient method for generating trading rules \\r\\nbased on a combination of technical indicators, providing \\r\\ntraders with valuable insights and actionable signals.\\r\\nThe research provides evidence that decision trees can \\r\\nefficiently select relevant features from a pool of technical \\r\\nindicators, making them adaptable to different stocks and \\r\\nmarket conditions. Moreover, decision trees-based classifier\\r\\noutperformed the buy-and-hold benchmark in terms of \\r\\nefficiency and adaptability. By leveraging decision trees and \\r\\ntechnical indicators, traders can make better trading \\r\\ndecisions in dynamic market environments. \\r\\nFuture enhancements to the strategy could focus on \\r\\nportfolio-level measures to boost overall performance. For \\r\\ninstance, pruning stocks from the universe that don\\'t \\r\\noutperform their benchmark could enhance overall portfolio \\r\\nperformance. In our case, removing five underperforming \\r\\nstocks improved the strategy\\'s performance. Such pruning \\r\\nwould be particularly essential for longer periods and larger \\r\\nuniverses of stocks.\\r\\nFurthermore, while this paper tested only a few \\r\\nindicators, there are hundreds of indicators available to create \\r\\nnew trading strategies. Future research could explore the \\r\\neffectiveness of different combinations of indicators and \\r\\nrefine the decision tree-based approach for optimal \\r\\nperformance.\\r\\nACKNOWLEDGEMENTS\\r\\nWe would like to express our sincere gratitude to the \\r\\nfollowing mentors and professors for their invaluable \\r\\nguidance and support throughout this research:\\r\\n• Amruthasree V M, Assistant Professor, Department of \\r\\nCSE, The National Institute of Engineering\\r\\n• Mahe Mubeen Akhtar D, Assistant Professor, \\r\\nDepartment of CSE, The National Institute of \\r\\nEngineering\\r\\n• Ramesh G, Assistant Professor, Department of CSE, \\r\\nThe National Institute of Engineering\\r\\nTheir expertise and encouragement have been \\r\\ninstrumental in shaping this research work. We are grateful \\r\\nfor their mentorship and unwavering support.\\r\\nREFERENCES\\r\\n[1]. “Y. Lu, T. Ye and J. Zheng, \"Decision Tree Algorithm in Machine \\r\\nLearning,\" 2022 IEEE International Conference on Advances in \\r\\nElectrical Engineering and Computer Applications (AEECA), Dalian, \\r\\nChina, 2022, pp. 1014-1017, doi: \\r\\n10.1109/AEECA55500.2022.9918857.\\r\\n[2]. Daniya, T., et al. “Classification and regression trees with Gini Index.” \\r\\nAdvances in Mathematics: Scientific Journal, vol. 9, no. 10, 23 Sept. \\r\\n2020, pp. 8237–8247, https://doi.org/10.37418/amsj.9.10.53.\\r\\n[3]. Shah, Ishan, and Rekhit Pachanekar. “Chap-ter 12 - Decision Trees.” \\r\\nMachine Learning in Trading, QuantInsti Quantitative Learning Pvt. \\r\\nLtd., Mumbai, Maharastra, 2023, pp. 143–155.\\r\\n[4]. Samaha, Lee. “How to Calculate Annualized Volatility.” The Motley \\r\\nFool, www.fool.com/investing/how-to-calculate/annualized\\x02volatility/#:~:text=Annualized%20volatility%20formula,-\\r\\nBut%20first%2C%20here%27s&text=For%20example%2C%20you\\r\\n%20might%20calculate,to%20get%20the%20annualized%20volatilit\\r\\ny. Accessed 13 May 2024.\\r\\n[5]. “India 10-Year Bond Yield Historical Data.” Investing.Com, \\r\\nin.investing.com/rates-bonds/india-10-year-bond-yield-historical\\x02data. Accessed 12 May 2024.\\r\\n[6]. Filho, Mario. “Do Decision Trees Need Feature Scaling or \\r\\nNormalization?” Forecastegy, 24 Mar. 2023, \\r\\nforecastegy.com/posts/do-decision-trees-need-feature-scaling-or\\x02normalization/#:~:text=In%20general%2C%20no.,as%20we%27ll%\\r\\n20see%20later.'},\n",
       " {'name': '1904.06619v1.pdf',\n",
       "  'content': 'Solving Differential Equation with Constrained\\r\\nMultilayer Feedforward Network\\r\\nZeyu Liu\\r\\n1\\r\\n, Yantao Yang1,2, and Qing-Dong Cai1,2,3 1College of Engineering\\r\\n2State Key Laboratory for Turbulence and Complex System 3Center for Applied Physics and Technology {zeyuliu,yantao.yang,caiqd}@pku.edu.cn\\r\\nPeking University\\r\\nApril 16, 2019\\r\\nAbstract\\r\\nIn this paper, we present a novel framework to solve differential equa\\x02tions based on multilayer feedforward network. Previous works indicate\\r\\nthat solvers based on neural network have low accuracy due to that the\\r\\nboundary conditions are not satisfied accurately. The boundary condition\\r\\nis now inserted directly into the model as boundary term, and the model\\r\\nis a combination of a boundary term and a multilayer feedforward network\\r\\nwith its weight function. As the boundary condition becomes predefined\\r\\nconstraintion in the model itself, the neural network is trained as an un\\x02constrained optimization problem. This leads to both ease of training\\r\\nand high accuracy. Due to universal convergency of multilayer feedfor\\x02ward networks, the new method is a general approach in solving different\\r\\ntypes of differential equations. Numerical examples solving ODEs and\\r\\nPDEs with Dirichlet boundary condition are presented and discussed.\\r\\n1 Introduction\\r\\nDifferential equations, including ordinary differential equations (ODEs) and\\r\\npartial differential equations (PDEs), are key mathematical models for vari\\x02ous physics and engineering applications. In most situations, it is impractical\\r\\nto find analytical solutions, and numerical solutions become increasingly pop\\x02ular for these problems. When solving ODE/PDEs, one seeks for a function\\r\\nsatisfying both (1) the differential equations within the domain, and (2) all\\r\\ninitial/boundary conditions. Common numerical methods for ODEs are Runge\\x02Kutta methods, linear multistep methods, and predictor-corrector methods [1].\\r\\nAs for PDEs, tremendous methods for discretizing the physical space or spec\\x02tral space are developed, and the most common choices are finite difference 1\\r\\narXiv:1904.06619v1 [math.NA] 14 Apr 2019\\nmethod (FDM), finite volume method (FVM), finite element method (FEM),\\r\\nand spectral method. These methods are special cases of weighted residual\\r\\nmethod. Galerkin method is another numerical method based on weighted\\r\\nresidual method for converting a continuous operator to a discrete form. It\\r\\napplies the method of variation of parameters to a function space and converts\\r\\nthe original equation to a weak formulation.\\r\\nIn the present study, we take advantage of the fast developing machine learn\\x02ing technique and propose a framework of solving ODE/PDE by applying the\\r\\nvariation of parameters of a neural network [2, 3]. Neural network (NN) is in\\x02spired by the complex biological neural network, and is now a computing system\\r\\nwildly applied in machine learning [4, 5, 6]. Feedforward network with full con\\x02nection between neighboring layers is one of the first models introduced [7, 8],\\r\\nand the algorithms evaluating and training them have been studied since then\\r\\n[8, 9, 10, 11]. Besides applications in image recognition [5], natural language\\r\\nprocessing [6], cognitive science [12], and genomics [13], neural network is also\\r\\na powerful tool for function approximation [14, 15, 16, 17, 18]. It is proved\\r\\nthat functions in the form of multilayer feedforward network (MFN) is dense\\r\\nin function spaces such as C(I) and L\\r\\n2\\r\\n(I) (I is unit cube1). It is also easily\\r\\nshown that increasing the layers of MFN will enormously increase its func\\x02tion approximation capability. However, deep neural networks are difficult to\\r\\ntrain with gradient methods such as backpropagation due to gradient vanishing\\r\\n[19, 20]. Here four-layer feedforward networks are chosen to avoid using special\\r\\ntechniques such as ResNet [21].\\r\\nDue to the ability of NN in function approximation, lots of efforts have been\\r\\nmade to construct ODE/PDE solvers based on NN [22, 23, 2]. One of the major\\r\\ndifficulties in such solvers is how to train a particular NN to satisfy the boundary\\r\\ncondition accurately, since that the original form of NN as a trial function\\r\\ndoes not match boundary condition like trial functions in Galerkin methods.\\r\\nOne strategy is the penalty method [2, 18, 24]. The penalty method has been\\r\\napplied successfully in Burgers equation [2], Laplace equation [18], and diffusion\\r\\nequations [24], but only limited accuracy can be achieved. Another issue is how\\r\\nto evaluate the derivatives in equations, which need to be compatible with the\\r\\nNN-based solver. One option is the so-called automatic differentiation (AD)\\r\\n[2]. AD evaluates the derivative with respect to input variable of any function\\r\\ndefined by a computer program, and it is done by performing a non-standard\\r\\ninterpretation of the program: the interpretation replaces the domain of the\\r\\nvariables and redefines the semantic of the operators [25].\\r\\nIn our framework, we define a trial function which consists of a bulk term\\r\\nand a boundary term. The boundary term matches initial/boundary conditions,\\r\\nand the bulk term satisfies a reduced problem with relaxed boundary constrains,\\r\\nrespectively. The boundary term can be construct explicitly. We then define\\r\\nfor the bulk term a loss function, which is actually the residual of the reduced\\r\\nproblem. Such loss function does not involve any boundary conditions since the\\r\\nboundary conditions are relaxed in the reduced problem. Finally the bulk term,\\r\\n1The unit cube on Rn is defined as [0, 1]n.\\r\\n2\\nand therefore the trial function, is determined by minimizing the loss function.\\r\\nMachine learning technique is used for this minimization of the loss function.\\r\\nWe refer to this new strategy as the constrained multilayer feedforward network\\r\\n(CMFN) method. With such novel strategy we will show that much higher\\r\\naccuracy can be achieve. It should also be pointed out that any method can be\\r\\nused to minimizing the loss function.\\r\\nBefore proceeding, we would like to clarify some terminology. In the language\\r\\nof the machine learning community, the trial function is usually called model.\\r\\nThe minimization of the loss function is actually a learning process, during which\\r\\nthe trial function learns the correct data distribution of analytical solution.\\r\\nThe minimization process is also a standard optimization problem, and it is\\r\\nequivalent to training in machine learning. Thus, the terminologies “training”,\\r\\n“optimization”, and “minimization” will be used interchangeably throughout\\r\\nthis paper.\\r\\nThe paper is organized as follows. In section 2 we describe the framework in\\r\\ndetail. Section 3 presents some numerical examples. Finally section 4 concludes\\r\\nthe paper.\\r\\n2 Numerical Method\\r\\nTo solve ODEs/PDEs numerically, one finds a function which satisfies the dif\\x02ferential equations inside the domain and all initial/boundary conditions at\\r\\n(temporal/spatial) boundaries. That is, two parts of information need to be\\r\\ntransferred into the numerical solver. For instance, in FVM the former part of\\r\\ninformation is transferred by flux reconstruction and the later part by opera\\x02tions on the boundary cells, respectively. In CMFN method, the former part\\r\\nof information is transferred by directly applying the differential operators with\\r\\nAD technique. The initial/boundary conditions are dealt with the boundary\\r\\nterm in the trial function.\\r\\nThe CMFN method is based on the concept of MFN. MFN with n layers\\r\\ncan be defined as a computing algorithm as follows. The input layer as a vector\\r\\nis denoted by y\\r\\n(1), the output layer is denoted by y(n)\\r\\n, and the hidden layers\\r\\nby y\\r\\n(i)\\r\\n. The output layer y\\r\\n(n)\\r\\nis computed by hidden layer y\\r\\n(n−1):\\r\\ny\\r\\n(n)\\r\\nk =\\r\\nX\\r\\nj\\r\\nθ\\r\\n(n−1)\\r\\nkj y\\r\\n(n−1)\\r\\nj + β\\r\\n(n−1)\\r\\nk\\r\\n,\\r\\nand the hidden layers are computed recursively by:\\r\\n(\\r\\nz\\r\\n(i+1)\\r\\nk =\\r\\nP\\r\\nj\\r\\nθ\\r\\n(i)\\r\\nkj y\\r\\n(i)\\r\\nj + β\\r\\n(i)\\r\\nk\\r\\ny\\r\\n(i+1)\\r\\nk = φ(z\\r\\n(i+1)\\r\\nk\\r\\n)\\r\\ni = 1, 2, . . . , n − 2.\\r\\nExplicitly, a three-layer feedforward network N(x; θ, β) is defined with superpo\\x02sition of activation function φ over linear transformation (x = {xi}n×1 as input\\r\\n3\\nlayer and N(x; θ, β) = {Nk}m×1 as output layer):\\r\\nNk =\\r\\nX\\r\\nj\\r\\nθ\\r\\n(2)\\r\\nkj φ(\\r\\nX\\r\\ni\\r\\nθ\\r\\n(1)\\r\\nji xi + β\\r\\n(1)\\r\\nj\\r\\n) + β\\r\\n(2)\\r\\nk\\r\\n, (1)\\r\\nand a four-layer MFN is\\r\\nNl =\\r\\nX\\r\\nk\\r\\nθ\\r\\n(3)\\r\\nlk φ(\\r\\nX\\r\\nj\\r\\nθ\\r\\n(2)\\r\\nkj φ(\\r\\nX\\r\\ni\\r\\nθ\\r\\n(1)\\r\\nji xi + β\\r\\n(1)\\r\\nj\\r\\n) + β\\r\\n(2)\\r\\nk\\r\\n) + β\\r\\n(3)\\r\\nl\\r\\n. (2)\\r\\nThe parameters of the MFN are its weights θ = {θ\\r\\n(k)\\r\\nij } and biases β = {β\\r\\n(k)\\r\\nj\\r\\n}. It\\r\\nhas been proved that the MFN eq. (2) with proper activation φ is dense in C(I),\\r\\nnamely the set of all continuous functions defined on unit cube [15]. Therefore,\\r\\nfor any continuous function y(x) defined on a finite domain, one set of param\\x02eters (θ\\r\\n∗\\r\\n, β∗) can be found such that the corresponding network N(x; θ\\r\\n∗\\r\\n, β∗) is\\r\\nclose enough to y(x), i.e. the norm ky(x) − N(x; θ\\r\\n∗\\r\\n, β∗)k could be sufficiently\\r\\nsmall. Similar conclusion holds for y ∈ L\\r\\n2\\r\\n(I), which is R\\r\\nI\\r\\n|y(x)|\\r\\n2 dx < ∞.\\r\\nSuch properties guarantee that an optimal set of N(x; θ\\r\\n∗\\r\\n, β∗) exists with the\\r\\ncorresponding MFN being a good numerical approximation of the solution.\\r\\nA well-posed ODE/PDE with Dirichlet boundary condition can be written\\r\\nas\\r\\n(\\r\\nLu = f in Ω,\\r\\nBu = g in ∂Ω.\\r\\n(3)\\r\\nIn CMFN we define a model function:\\r\\nyˆ(x; θ, β) = G(x) + N˜(x; θ, β) ≡ G(x) + w(x) · N(x; θ, β). (4)\\r\\nAs the boundary operator B is linear and algebraic, we choose the two terms\\r\\nG(x) and w(x) in eq. (4) such that\\r\\n1. BG = g when x ∈ ∂Ω,\\r\\n2. BN˜ → 0 when x → ∂Ω.\\r\\nG(x) is a boundary term which is a pre-defined function, N˜(x) is bulk term,\\r\\nand N(x) is the unknown part which is approximated by a neural network. The\\r\\noriginal problem with respect to u(x) in eq. (4) is reduced to solving a new\\r\\ndifferential equation with respect to N˜(x). The the new equation is defined as\\r\\nthe reduced equation, and the unknown part N(x) separated from bulk term is\\r\\ncalled reduced solution.\\r\\nIn eq. (4), as long as a pre-defined weight w(x) is a bounded continuous\\r\\nfunction, the bulk term N˜(x) is continuous and bounded according to eq. (2).\\r\\nThe bulk term is further written as N˜(x) = w(x) · N(x), where the pre-defined\\r\\nweight w(x) satisfies:\\r\\n1. w(x) → 0 as x → ∂Ω (vanishing on domain boundary),\\r\\n2. for all x\\r\\n∗ ∈ Ω, w(x∗\\r\\n) 6= 0 (non-vanishing within domain).\\r\\n4\\nNow the boundary conditions are automatically satisfied, or we say, the bound\\x02ary conditions are relaxed in the reduced equation. Once the reduced equation\\r\\nis determined, the loss function can be constructed without considering the\\r\\noriginal boundary conditions.\\r\\nBy substituting the trial function ˆy(x) = G(x) + w(x) · N(x) into eq. (3),\\r\\nthe original problem is converted to its reduced equation:\\r\\nL[G + w · N] = L˜[N] = f N ∈ C(R\\r\\nn\\r\\n). (5)\\r\\nOne may think that there could be multiple solutions to reduced equation since\\r\\nit has no boundary conditions. However, while the original problem eq. (3) has\\r\\nunique solution y\\r\\n∗\\r\\n, the reduced one eq. (5) should also have a unique solution\\r\\n(y\\r\\n∗ − G)/w. The paradox indicates that, among all solutions to eq. (5), there\\r\\nexists a unique solution satisfies: BN˜ → 0 while x → ∂Ω. We do not have a\\r\\nrigorous proof for this statement yet, but as supported by the examples shown\\r\\nlater, a unique solution can always be obtained.\\r\\nAfter construction of trial function ˆy, the loss function towards which opti\\x02mization is done is defined by residual RN = L˜N − f of eq. (3):\\r\\nL =\\r\\nZ\\r\\nΩ\\r\\nh(RN)(x),(RN)(x)i dx =\\r\\nX\\r\\nx∗∈T(Ω)\\r\\nk(RN)(x\\r\\n∗\\r\\n)k\\r\\n2\\r\\n, (6)\\r\\nwhere T(Ω) is the training set containing points selected from domain Ω. The\\r\\noperator R is defined by AD instead of manually working it out. This not only\\r\\nsaves the researches from laborious job [23], but also produces robust and reliable\\r\\ncode [26, 25]. There are successful AD implements on nearly all programming\\r\\nplatforms [26]. In this work, reverse mode AD application programming inter\\x02face on TensorFlow [27] is called. Since AD solves the problem of much too\\r\\ncomplicated differential problem, high order differential operator L are solved\\r\\nin this work without extra efforts.\\r\\nThe final stage of the framework solving ODE/PDE is optimization during\\r\\nwhich loss function L defined by eq. (6) is minimized with respect to its free\\r\\nparameters. In cases where MFN N(x; θ, β) is the reduced solution, its weights\\r\\nand biases (θ, β) are trained to minimize L = L(θ, β). In this work, the op\\x02timization is done by second order method L-BFGS [28], instead of SGD, the\\r\\nmost popular choice in building machine learning models [6]. The second order\\r\\nmethod is not always robust in general machine learning problems, but it serves\\r\\nwell in ODE/PDE solver according to our observation. All numerical examples\\r\\npresented in later section in this paper is trained by second order method L\\x02BFGS which greatly improves training efficiency. The training process requires\\r\\nlarge amount of computational resource which used to be an obstacle in de\\x02velopment of machine learning [10]. Parallelism and heterogeneous computing\\r\\nthrow lights on the problem, and the model in this work is defined and trained\\r\\non TensorFlow [27].\\r\\n5\\n3 Examples and Discussion\\r\\nThe first example on ODE solving is a definite integral problem as illustration:\\r\\n(\\r\\ny\\r\\n0\\r\\n(x) = cos x\\r\\ny(x = 0) = y0 = 1\\r\\n. (7)\\r\\nThe analytical solution is simply integration of R.H.S. of eq. (7): y(x) = 1+sin x.\\r\\nIn order to find a numerical solution in domain [0, 10]. The trial function is\\r\\ndefined as\\r\\nyˆ(x; θ, β) = y0e\\r\\n−x + (1 − e−x\\r\\n)N(x; θ, β). (8)\\r\\nIt is easily verified that requirements for G(x) and N˜(x) are all satisfied. The\\r\\nnetwork N(x; θ, β) is set as a four-layer network with 20 neurons in each hidden\\r\\nlayer. Loss function is defined as\\r\\nL =\\r\\nZ 10\\r\\n0\\r\\nkyˆ\\r\\n0 − cos xk2 dx =\\r\\n1000\\r\\nX\\r\\ni=1\\r\\n|yˆ\\r\\n0\\r\\n(xi) − cos xi|\\r\\n2\\r\\n, (9)\\r\\nwith {xi}i=1,2,...,1000 are uniformly selected in interval [0, 10]. The loss function\\r\\nis minimized by L-BFGS method, the result is illustrated in fig. 1.\\r\\nThe boundary condition is well maintained in this example. Figures 1a, 1c\\r\\nand 1e shows that the trained MFN provides a correct reduced solution, as well\\r\\nas a good match in bulk term and the whole model. However, error distributions\\r\\nshown in figs. 1b, 1d and 1f reveal that the reduced solution actually has the\\r\\nlowest accuracy, especially on domain boundary, the pre-defined constrains in\\r\\nG(x) and w(x) reduce error in model, as well as turn the training process into\\r\\na standard non-constrained optimization problem.\\r\\nAnother important property shown in fig. 1b is that the solution learned by\\r\\nCMFN deviates from analytical solution randomly, where traditional numerical\\r\\nmethods usually have increasing errors along iterations. This property is natural\\r\\nfor CMFN method since all data points are equal to the learning process, while in\\r\\niteration methods error grows accumulatively. A more accurate measure of error\\r\\nin solution provided by eq. (8) is calculating the L\\r\\n2\\r\\n-norm of error distribution:\\r\\nError = s\\r\\n1\\r\\n10 Z 10\\r\\n0\\r\\n|y(x) − yˆ(x)|\\r\\n2 dx . (10)\\r\\nThe previous example has an average error of 1.96×10−4. The authors also tried\\r\\nother choices on defining boundary term G and weight function w: G∗(x) ≡\\r\\n1 and w\\r\\n∗\\r\\n(x) = x. Numerical test shows that definitions in eq. (8) is more\\r\\noptimized than their alternatives. G∗(x) instead of original G(x) in eq. (8)\\r\\nroughly doubles the average error, and w\\r\\n∗\\r\\n(x) instead the original weight even\\r\\nreduces the accuracy for one order of magnitude.\\r\\nAs is mentioned above, the reduced equation with relaxed boundary condi\\x02tion should have unique solution instead of multiple solution under the premise\\r\\n6\\n0 2 4 6 8 10\\r\\n0.00\\r\\n0.25\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\n1.25\\r\\n1.50\\r\\n1.75\\r\\n2.00\\r\\nAccurate\\r\\nTrained\\r\\n(a) Model ˆy(x)\\r\\n0 2 4 6 8 10\\r\\n0.0003\\r\\n0.0002\\r\\n0.0001\\r\\n0.0000\\r\\n0.0001\\r\\n0.0002\\r\\n0.0003\\r\\n0.0004\\r\\n0.0005 Deviation\\r\\n(b) Error distribution of Model\\r\\n0 2 4 6 8 10\\r\\n0.00\\r\\n0.25\\r\\n0.50\\r\\n0.75\\r\\n1.00\\r\\n1.25\\r\\n1.50\\r\\n1.75\\r\\n2.00\\r\\nAccurate\\r\\nTrained\\r\\n(c) Bulk term w(x) · N(x)\\r\\n0 2 4 6 8 10\\r\\n0.0003\\r\\n0.0002\\r\\n0.0001\\r\\n0.0000\\r\\n0.0001\\r\\n0.0002\\r\\n0.0003\\r\\n0.0004\\r\\n0.0005 Deviation\\r\\n(d) Error distribution of bulk term\\r\\n0 2 4 6 8 10\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\nAccurate\\r\\nTrained\\r\\n(e) Reduced solution N(x)\\r\\n0 2 4 6 8 10\\r\\n0.0020\\r\\n0.0015\\r\\n0.0010\\r\\n0.0005\\r\\n0.0000\\r\\n0.0005\\r\\n0.0010 Deviation\\r\\n(f) Error distribution of reduced solution\\r\\nFigure 1: Results of definite integral problem in eq. (7) with Error = 1.96×10−4\\r\\n7\\nthat the solution is bounded. The reduced equation for eq. (7) is\\r\\n(1 − e\\r\\n−x\\r\\n)N\\r\\n0 + e−xN − y0e−x − cos x = 0, (11)\\r\\nand the general solution to eq. (11) is\\r\\nN(x) = C − y0e\\r\\n−x\\r\\n1 − e−x\\r\\n+\\r\\nsin x\\r\\n1 − e−x\\r\\nC ∈ R. (12)\\r\\nAt x = 1, the first part in R.H.S. of eq. (12) is unbounded if C 6= y0; if N(x) is\\r\\nassumed to be a bounded function on [0, 1], then in eq. (12) there exists unique\\r\\nsolution which is a proper reduced solution to problem in eq. (7).\\r\\nThe CMFN treats the problems defined by eq. (3) equally, so any initial\\r\\ncondition problem is solved with similar process and accuracy. The following\\r\\npresents solution of boundary value problem (BVP) of a second order ODE. It\\r\\nis boundary layer problem [29] reduced to 1D:\\r\\nuu0 = νu00 u(0) = 1 u(1) = 0. (13)\\r\\nThe problem has analytical solution:\\r\\nu(x) = 2C\\r\\n1 + exp \\r\\nx−1\\r\\nν\\r\\n· C\\r\\n\\x01 − C C > 1, (14)\\r\\nwith C as a constant determined by algebraic equation:\\r\\n1 −\\r\\n2\\r\\n1 + C\\r\\n= exp \\x12−\\r\\nC\\r\\nν\\r\\n\\x13\\r\\n.\\r\\nWe have C ≈ 1.2 when ν = 0.5, and C tends to 1 rapidly as ν decreases to\\r\\nν = 0.\\r\\nThe boundary term of model is constructed according to boundary condi\\x02tions in eq. (13) where linear function f(x) = 1 − x is sufficient in matching\\r\\nvalues of the two boundary points. The weight function is constructed by poly\\x02nomial such that w(0) = 0 and w(1) = 0. Finally, the trial function is defined\\r\\nas\\r\\nuˆ(x; θ, β) = (1 − x) + x(1 − x) · N(x; θ, β), (15)\\r\\nand loss function is defined similar to eq. (8). The model is trained with 100\\r\\ndata points uniformly distributed in [0, 1].\\r\\nFigure 2 shows that BVP is solved with high numerical accuracy, and in\\r\\nfigs. 3a to 3h differential property of the learned solution is studied. CMFN\\r\\nsolution to the problem is not only an accurate numerical approximation to the\\r\\nanalytical solution, but its first to eighth order derivatives are also all accurate\\r\\nnumerical approximations to their analytical counterpart. This is very hard to\\r\\nachieve with commonly used numerical methods enumerated in section 1.\\r\\nAnother important observation is that once weight is defined by polynomials,\\r\\nthere is a sole proper choice based on initial/boundary condition: for 1D Dirich\\x02let boundary contion at x = a, there would be a factor (x − a) in the weight\\r\\n8\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n6\\r\\n4\\r\\n2\\r\\n0\\r\\n2\\r\\n4\\r\\n6\\r\\n1e 7\\r\\nDeviation\\r\\n(a) Model ˆy(x)\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.000006\\r\\n0.000004\\r\\n0.000002\\r\\n0.000000\\r\\n0.000002\\r\\n0.000004\\r\\n0.000006\\r\\nDeviation\\r\\n(b) Reduced solution N(x)\\r\\nFigure 2: Error distributions of BVP with average error < 10−5\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n5\\r\\n4\\r\\n3\\r\\n2\\r\\n1\\r\\n0 Accurate\\r\\nTrained\\r\\n(a) 1st order derivative\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n20.0\\r\\n17.5\\r\\n15.0\\r\\n12.5\\r\\n10.0\\r\\n7.5\\r\\n5.0\\r\\n2.5\\r\\n0.0\\r\\nAccurate\\r\\nTrained\\r\\n(b) 2nd order derivative\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n100\\r\\n50\\r\\n0\\r\\n50\\r\\n100\\r\\n150\\r\\n200\\r\\n250 Accurate\\r\\nTrained\\r\\n(c) 3rd order derivative\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n500\\r\\n0\\r\\n500\\r\\n1000\\r\\n1500\\r\\n2000\\r\\n2500 Accurate\\r\\nTrained\\r\\n(d) 4th order derivative\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n50000\\r\\n40000\\r\\n30000\\r\\n20000\\r\\n10000\\r\\n0\\r\\n10000\\r\\n20000 Accurate\\r\\nTrained\\r\\n(e) 5th order derivative\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n800000\\r\\n600000\\r\\n400000\\r\\n200000\\r\\n0\\r\\n200000 Accurate\\r\\nTrained\\r\\n(f) 6th order derivative\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n1.0\\r\\n0.5\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n2.0\\r\\n1e7\\r\\nAccurate\\r\\nTrained\\r\\n(g) 7th order derivative\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n2\\r\\n1\\r\\n0\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n1e8\\r\\nAccurate\\r\\nTrained\\r\\n(h) 8th order derivative\\r\\nFigure 3: Differentiation of learned solution of eq. (13)\\r\\n9\\nfunction. For example, weight in eq. (15) should be defined as w(x) = x(1 − x),\\r\\nwhich is Hermite interpolation based on boundary condition: w(1) = w(0) = 0;\\r\\nif weight is defined as w\\r\\n∗\\r\\n(x) = x\\r\\n2\\r\\n(1 − x) instead, there will be a huge reduction\\r\\nin accuracy. Since the term x\\r\\n2\\r\\nin w\\r\\n∗\\r\\n(x) not only vanishes itself at x = 0, but\\r\\nalso has zero first order derivative there; as a result, the boundary term G(x)\\r\\nunexpectedly dominates both function value and first order derivative at x = 0.\\r\\nTwo dimensional problem is tested with heat conduction problem (Laplace\\r\\nequation):\\r\\n∇2T(x, y) = 0 x ∈ [0, 1] y ∈ [0, 1] (16)\\r\\nThe boundary condition of Dirichlet problem is:\\r\\nT(0, y) = T(1, y) = T(x, 0) = 0 T(x, 1) = sin πx. (17)\\r\\nThe problem has analytical solution:\\r\\nT(x, y) = sin πx sinh πy\\r\\nsinh π\\r\\n, (18)\\r\\nso the error of numerical solution Tˆ(x, y) is evaluated as:\\r\\nError = s\\r\\n1\\r\\nSΩ\\r\\nZ\\r\\nΩ\\r\\n|T(x, y) − Tˆ(x, y)|\\r\\n2 dΩ, (19)\\r\\nwith SΩ being area of domain (SΩ = 1).\\r\\nThe model for Dirichlet problem is\\r\\nTˆ(x; θ, β) = y sin πx + x(1 − x)y(1 − y) · N(x; θ, β). (20)\\r\\nIt is easily verified that the requirements for G and N˜ are satisfied. The weight\\r\\nfunction is actually constructed by the principle discussed above: it consists of\\r\\nfactors from all four boundary conditions.\\r\\nThe loss function is constructed similar to eq. (9), and the simulation is\\r\\ndone by a MFN with 2 hidden layers and 40 neurons in each hidden layer. The\\r\\ndata points training the network is a set of 900 items which are vertices of a\\r\\n30×30 uniform mesh on two dimensional unit cube. The results of simulation is\\r\\ndemonstrated in fig. 4. Figure 4a is contour of analytical solution to eq. (18), and\\r\\nthe simulated solution in fig. 4b matches the analytical solution quite exactly.\\r\\nFigure 4c illustrates the pointwise deviation from analytical solution of bulk\\r\\nterm. The average error in fig. 4b is 2.8 × 10−6. The similar case calculated by\\r\\npenalty method reported in [2, 18] is only of order 10−3.\\r\\nThe two properties distinguish CMFN from other methods are its generality\\r\\nand accuracy. As for the generality side, this framework is not sensitive to\\r\\nthe property of differential equation; it works similarly on elliptic, hyperbolic,\\r\\nand parabolic problems. One interesting verification is that turning the original\\r\\nproblem into a convection-diffusion problem:\\r\\nu\\r\\n∂T\\r\\n∂x + v\\r\\n∂T\\r\\n∂y = ∇2T + f, (21)\\r\\n10\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n0.200\\r\\n0.400\\r\\n0.600\\r\\n0.800\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n(a) Analytical solution\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n0.000\\r\\n0.200\\r\\n0.400\\r\\n0.600\\r\\n0.800\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n(b) Numerical Solution\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n4\\r\\n2\\r\\n0\\r\\n2\\r\\n4\\r\\n1e 7\\r\\n(c) Deviation of bulk term\\r\\nFigure 4: The results for simulating Laplace equation\\r\\nthe boundary condition is set the same as in eq. (17), as a Dirichlet problem,\\r\\nand all other configurations such as network topology and training set are all\\r\\nthe same as previous. The convection velocities (u, v) and source term f are\\r\\nassigned artificially as:\\r\\nu(x, y) = y\\r\\n2\\r\\ncos x,\\r\\nv(x, y) = 1\\r\\n3\\r\\ny\\r\\n3\\r\\nsin x,\\r\\nf(x, y) = y\\r\\n2\\r\\ncos x\\r\\nπ cos πx sinh πy\\r\\nsinh π\\r\\n+\\r\\n1\\r\\n3\\r\\ny\\r\\n3\\r\\nsin x\\r\\nπ sin πx cosh πy\\r\\nsinh π\\r\\n+\\r\\nα\\r\\n\\r\\n2π\\r\\n2\\r\\nsin 2πx cos 2πy − 4π\\r\\n2\\r\\nsin 2πx sin2 πy\\x01−\\r\\nα\\r\\n\\x10\\r\\n2πy2cos x cos 2πx sin2 πy +\\r\\nπ\\r\\n3\\r\\ny\\r\\n3\\r\\nsin x sin 2πx sin 2πy\\x11\\r\\nα = 0.1.\\r\\nThis setup ensures that the analytical solution is:\\r\\nT(x, y) = sin πx sinh πy\\r\\nsinh π\\r\\n− α sin 2πx sin2 πy (22)\\r\\nThe simulated solution is shown in fig. 5, its average error is 8.57 × 10−4, while\\r\\nthe elliptic counterpart of the problem has average error of 2.6 × 10−5.\\r\\n11\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n0.000\\r\\n0.200\\r\\n0.400\\r\\n0.800 0.600\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n(a) Numerical solution\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n0.000\\r\\n0.200\\r\\n0.400\\r\\n0.800 0.600\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n(b) Analytical solution\\r\\nFigure 5: Convection-diffusion problem with average error of 8.57 × 10−4\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.200\\r\\n0.400\\r\\n0.600\\r\\n0.800\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n(a) Numerical solution\\r\\n0.0 0.2 0.4 0.6 0.8 1.0\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n0.8\\r\\n1.0\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.000\\r\\n0.006\\r\\n0.004\\r\\n0.002\\r\\n0.000\\r\\n0.002\\r\\n0.004\\r\\n0.006\\r\\n(b) Error distribution\\r\\nFigure 6: Numerical solution by PINN method with average error of 1.6 × 10−3\\r\\nAnother important property of CMFN is its accuracy. While solving Laplace\\r\\nequation, penalty method such as PINN [18] is only able to achieve an average\\r\\nerror of 1.6 × 10−3, and it is easily observed that boundary conditions are not\\r\\naccurately satisfied (especially in the two lower corners of fig. 6), but CMFN\\r\\nkeeps the boundary conditions being satisfied accurately in an intrinsic way.\\r\\n4 Conclusion and Future Work\\r\\nIn this paper, we present a novel framework of constructing ODE/PDE solver\\r\\nbased on CMFN method. The numerical method and its application are dis\\x02cussed with regard to ODEs and PDEs with Dirichlet boundary conditions.\\r\\nCMFN method stands out for its generality and accuracy. Traditional neural\\r\\nnetwork methods based on RBF [30] or penalty methods [2] have very limited\\r\\naccuracy. By constructing the trial function with a weighted reduced solution\\r\\nas bulk term and a pre-defined boundary term, the model satisfies the bound\\x02ary condition automatically, and as a result, the training based on residuals of\\r\\ndifferential equations could be more effective. Moreover, the CMFN method\\r\\n12\\ntrains the neural network with all input data simultaneously, so it is intrinsi\\x02cally able to remain accuracy in numerically solving the differential equation\\r\\non large domain and large time span. The iteration methods such as FVM and\\r\\nFDM accumulate truncation error in each step, so the scheme has to be designed\\r\\ncarefully to be applied to larger domain and larger time span, while methods\\r\\nbased on neural network does not has the obsession.\\r\\nThe generality of CMFN framework is in several aspects. The iteration\\r\\nmethods such as FDM, FVM, and FEM are sensitive to property of PDE, since\\r\\nthe growth of numerical error differs in hyperbolic, parabolic, and elliptic prob\\x02lems. CMFN instead provides a unified method. Compared with traditional\\r\\nneural network methods based on RBF [30], CMFN can be applied similarly on\\r\\nboth linear and nonlinear problems while the later usually only works on linear\\r\\nproblem. In this work, very simple network topology (four-layer feedforward\\r\\nnetwork with twenty neurons) and very small data (less than 103 points) are\\r\\nused, but heat transfer equation and convection-diffusion equation with Dirich\\x02let boundary condition on unit cube are solved successfully on the same model.\\r\\nAnother property of CMFN method being worth mentioning is the indeter\\x02minacy in numerical result. In the training stage of our new framework, there is\\r\\nan initial guess on MFN. Since the network has tremendous parameters, it has\\r\\nto be randomly initialized; after training, the loss function would be reduced\\r\\nto a small number, but usually not zero, so the optimal solution is not usually\\r\\nobtained. The above two factors lead to the result that each specific parameter\\r\\nof MFN has rather random behavior. However the overall behavior of the com\\x02putational machine is controllable, because as long as the object function has\\r\\nenough continuity, the error would reduce along with reduction of loss function.\\r\\nThis work is a new starting point in the field of constructing PDE solver for\\r\\nthe authors. There are several works could be considered in the future:\\r\\n1. finding a general method to construct proper form of bulk term for Neu\\x02mann boundary condition;\\r\\n2. finding a systematical method of constructing weight and boundary term,\\r\\nespecially for complex geometry;\\r\\n3. building larger and deeper networks for more complex problems such as\\r\\nNavier-Stokes equation; and\\r\\n4. giving out a more mathematically rigorous proof on existence and unique\\x02ness of reduced solution.\\r\\nReferences\\r\\n[1] Richard L Burden and J Douglas Faires. Numerical analysis. 2001.\\r\\nBrooks/Cole, USA, 2001.\\r\\n[2] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics in\\x02formed deep learning (part i): Data-driven solutions of nonlinear partial\\r\\ndifferential equations. arXiv preprint arXiv:1711.10561, 2017.\\r\\n13\\n[3] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neu\\x02ral networks for solving ordinary and partial differential equations. IEEE\\r\\ntransactions on neural networks, 9(5):987–1000, 1998.\\r\\n[4] Simon S Haykin. Neural networks and learning machines, third edition.\\r\\nPearson Education, Inc., 2009.\\r\\n[5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet clas\\x02sification with deep convolutional neural networks. In Advances in neural\\r\\ninformation processing systems, pages 1097–1105, 2012.\\r\\n[6] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,\\r\\n521(7553):436, 2015.\\r\\n[7] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas\\r\\nimmanent in nervous activity. The bulletin of mathematical biophysics,\\r\\n5(4):115–133, 1943.\\r\\n[8] Frank Rosenblatt. The perceptron: a probabilistic model for information\\r\\nstorage and organization in the brain. Psychological review, 65(6):386, 1958.\\r\\n[9] Frank Rosenblatt. Principles of neurodynamics. Spartan Book, 1962.\\r\\n[10] Marvin Minsky and Seymour A Papert. Perceptrons: an introduction to\\r\\ncomputational geometry. MIT press, 1969.\\r\\n[11] Paul Werbos. Beyond regression: new tools for prediction and analysis in\\r\\nthe behavioral sciences. PhD thesis, Harvard University, 1974.\\r\\n[12] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum.\\r\\nHuman-level concept learning through probabilistic program induction.\\r\\nScience, 350(6266):1332–1338, 2015.\\r\\n[13] Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J\\r\\nFrey. Predicting the sequence specificities of dna-and rna-binding proteins\\r\\nby deep learning. Nature biotechnology, 33(8):831, 2015.\\r\\n[14] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedfor\\x02ward networks are universal approximators. Neural networks, 2(5):359–366,\\r\\n1989.\\r\\n[15] George Cybenko. Approximation by superpositions of a sigmoidal function.\\r\\nMathematics of control, signals and systems, 2(4):303–314, 1989.\\r\\n[16] Lee K Jones. Constructive approximations for neural networks by sigmoidal\\r\\nfunctions. Proceedings of the IEEE, 78(10):1586–1589, 1990.\\r\\n[17] S Carroll and B Dickinson. Construction of neural networks using the\\r\\nradon transform. In IEEE International Conference on Neural Networks,\\r\\nvolume 1, pages 607–611. IEEE, 1989.\\r\\n14\\n[18] Zeyu Liu, Yantao Yang, and Qingdong Cai. Neural network as a function\\r\\napproximator and its application in solving differential equations. Applied\\r\\nMathematics and Mechanics, 40(2):237–248, 2019.\\r\\n[19] S Hochreiter. Untersuchungen zu dynamischen neuronalen netzen [in ger\\x02man]. Diploma thesis, TU M¨unich, 1991.\\r\\n[20] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J¨urgen Schmidhuber,\\r\\net al. Gradient flow in recurrent nets: the difficulty of learning long-term\\r\\ndependencies, 2001.\\r\\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\\r\\nlearning for image recognition. In Proceedings of the IEEE conference on\\r\\ncomputer vision and pattern recognition, pages 770–778, 2016.\\r\\n[22] Susmita Mall and Snehashish Chakraverty. Chebyshev neural network\\r\\nbased model for solving lane–emden type equations. Applied Mathemat\\x02ics and Computation, 247:100–114, 2014.\\r\\n[23] Jens Berg and Kaj Nystr¨om. A unified deep artificial neural network\\r\\napproach to partial differential equations in complex geometries. arXiv\\r\\npreprint arXiv:1711.06464, 2017.\\r\\n[24] Qianshi Wei, Ying Jiang, and Jeff ZY Chen. Machine-learning solver for\\r\\nmodified diffusion equations. Physical Review E, 98(5):053304, 2018.\\r\\n[25] Louis B Rall. Automatic differentiation: Techniques and applications.\\r\\nSpringer, 1981.\\r\\n[26] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul,\\r\\nand Jeffrey Mark Siskind. Automatic differentiation in machine learning:\\r\\na survey. arXiv preprint arXiv:1502.05767, 2015.\\r\\n[27] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng\\r\\nChen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu\\r\\nDevin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving,\\r\\nMichael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\\r\\nKudlur, Josh Levenberg, Dandelion Man´e, Rajat Monga, Sherry Moore,\\r\\nDerek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner,\\r\\nIlya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Va\\x02sudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten\\x02berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large\\x02scale machine learning on heterogeneous systems, 2015. Software available\\r\\nfrom tensorflow.org.\\r\\n[28] Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for\\r\\nlarge scale optimization. Mathematical Programming, 45(1):503–528, Aug\\r\\n1989.\\r\\n15\\n[29] L Prandtl. Zur berechnung der grenzschichten. Zamm-zeitschrift Fur Ange\\x02wandte Mathematik Und Mechanik, 18(1):77–82, 1938.\\r\\n[30] Nam Mai-Duy and Thanh Tran-Cong. Numerical solution of differential\\r\\nequations using multiquadric radial basis function networks. Neural net\\x02works, 14(2):185–199, 2001.\\r\\n16'},\n",
       " {'name': '1711.01667v4.pdf',\n",
       "  'content': 'Multivariate Bayesian Predictive Synthesis\\r\\nin Macroeconomic Forecasting∗\\r\\nKenichiro McAlinn1,2† Knut Are Aastveit3,4, Jouchi Nakajima5, & Mike West2\\r\\n1Booth School of Business, University of Chicago\\r\\n2Department of Statistical Science, Duke University\\r\\n3Norges Bank\\r\\n4BI Norwegian Business School\\r\\n5Bank for International Settlements\\r\\nAugust 14, 2018\\r\\nAbstract\\r\\nWe develop the methodology and a detailed case study in use of a class of Bayesian predictive\\r\\nsynthesis (BPS) models for multivariate time series forecasting. This extends the recently intro\\x02duced foundational framework of BPS to the multivariate setting, with detailed application in\\r\\nthe topical and challenging context of multi-step macroeconomic forecasting in a monetary pol\\x02icy setting. BPS evaluates– sequentially and adaptively over time– varying forecast biases and\\r\\nfacets of miscalibration of individual forecast densities, and– critically– of time-varying inter\\x02dependencies among them over multiple series. We develop new BPS methodology for a specific\\r\\nsubclass of the dynamic multivariate latent factor models implied by BPS theory. Structured dy\\x02namic latent factor BPS is here motivated by the application context– sequential forecasting of\\r\\nmultiple US macroeconomic time series with forecasts generated from several traditional econo\\x02metric time series models. The case study highlights the potential of BPS to improve of forecasts\\r\\nof multiple series at multiple forecast horizons, and its use in learning dynamic relationships\\r\\namong forecasting models or agents.\\r\\nJEL Classification: C11; C15; C53; E37\\r\\nKeywords: Agent opinion analysis, Bayesian forecasting, Multivariate density forecast combina\\x02tion, Dynamic latent factors models, Macroeconomic forecasting\\r\\n∗The views expressed are those of the authors and do not necessarily reflect those of the Norges Bank or of the Bank\\r\\nfor International Settlements.\\r\\n†Corresponding author.\\r\\nE-mail: kenichiro.mcalinn@chicagobooth.edu, knut-are.aastveit@norges-bank.no, jouchi.nakajima@bis.org\\r\\nand mike.west@duke.edu\\r\\narXiv:1711.01667v4 [stat.ME] 13 Aug 2018\\n1 Introduction\\r\\nThe societal importance of sequential decision making in many areas has promoted developments\\r\\nin time series modeling and forecasting to feed into decisions. In complex dynamic systems with\\r\\nmultiple, inter-related time series, the dependencies across series can critically impact on decisions,\\r\\npolicies, and their outcomes. In economic policy making, dependencies among macroeconomic\\r\\ntime series provide fundamental insights into the state of economy. The interest is to use such\\r\\nrelationships to improve forecasts over multiple horizons, to help to guide the policy decisions\\r\\nand understand their impact. Central banks set national target interest rates based on (implicit or\\r\\nexplicit) utility/loss considerations that weigh future outcomes of inflation and other measures of\\r\\nthe real economy. Understanding the (time-varying) dependencies of these measures– especially\\r\\nthe dynamics over multiple horizons– is simply critical. Driven by this, multivariate models, ranging\\r\\nfrom vector autoregressive models (VAR) to dynamic stochastic generalized equilibrium models\\r\\n(DSGE), have been developed and used by researchers and policy makers. A huge literature reflects\\r\\nthe critical nature of the field, from the early works of, for example, Sims (1993), Stock and Watson\\r\\n(1996), and Sims and Zha (1998), to more recent advances in dynamic Bayesian models in Cogley\\r\\nand Sargent (2005), Primiceri (2005), Benati and Surico (2008), Koop et al. (2009), Koop et al.\\r\\n(2010), Nakajima (2011), Nakajima and West (2013a,b), and Zhou et al. (2014), among others.\\r\\nConcerned with accurate and useful forecasts, policy makers routinely rely on multiple sources,\\r\\nemploying multiple models, forecasters, and economists, to produce forecasts. To ensure appro\\x02priate normative decision making as well as reflecting increased uncertainty into the future, it has\\r\\nbecome popular, particularly for central banks, to provide probabilistic (density) forecasts. For\\r\\nexample, forecasts reported in the monetary policy reports of the Bank of England, Norges Bank,\\r\\nSwedish Riksbank, and recently also for the Federal Reserve Bank, have reflected this change. To re\\x02spond to this increased usage of density forecasts, there has been a recent resurgence in interest in\\r\\nforecast comparison, calibration, and combination of density forecasts in macroeconomics, econo\\x02metrics, and statistics. These new developments range from combining predictive densities using\\r\\nweighted linear combinations of prediction models, evaluated using various scoring rules (e.g.\\r\\nHall and Mitchell 2007; Amisano and Giacomini 2007; Jore et al. 2010; Hoogerheide et al. 2010;\\r\\nKascha and Ravazzolo 2010; Geweke and Amisano 2011, 2012; Aastveit et al. 2014), to more\\r\\ncomplex combination approaches that allows for time-varying weights with possibly both learning\\r\\nand model set incompleteness (e.g. Billio et al. 2013; Casarin et al. 2015; Aastveit et al. 2017b;\\r\\nPettenuzzo and Ravazzolo 2016; Del Negro et al. 2016).\\r\\nThe extensive literature on forecast combination has, for the most part, focused on forecasting\\r\\na single series. This is true from the seminal paper by Bates and Granger (1969) to applications\\r\\nin business, economics, technology, meteorology, management science, military intelligence, seis\\x02mic risk, and environmental risk, among other areas (e.g. Clemen 1989; Clemen and Winkler\\r\\n1999; Timmermann 2004; Clemen and Winkler 2007), as it is to the recent developments reported\\r\\nabove. In contrast, the literature on multivariate forecasting is dominated by traditional statistical\\r\\nmodel comparison and variable selection (e.g. Chan et al. 2012; Korobilis 2013; Nakajima and West\\r\\n2013a). Little attention has yet been given to forecast comparison, calibration, and combination\\r\\n1\\nin the context of forecasting multiple series. A few exceptions (e.g. Andersson and Karlsson 2008;\\r\\nAmendola and Storti 2015; Amisano and Geweke 2017) recognize this, but restrict attention to di\\x02rect extensions of univariate methods, with models combined linearly using one metric for overall\\r\\nperformance. This is potentially limiting in several ways: in ignoring inter-dependencies among se\\x02ries that can be detrimental in informing decisions, in ignoring the reality that some models might\\r\\nbe good at forecasting one series but poor in another, and for the fact that some or all models maybe\\r\\nbe poor overall. Partly reflecting the lack of formal statistical frameworks for holistic multivariate\\r\\nforecast model assessment and combination, economic policy makers use “ad hoc” strategies, which\\r\\neither rely on the policy maker’s “favorite” model, or ignore inter-dependencies all together. The\\r\\nneed for coherent methodology that gives policy makers flexibility in incorporating multivariate\\r\\ndensity forecasts from multiple sources cannot be understated.\\r\\nThe developments of the current paper address the above issues, challenges, and needs. The\\r\\nnew methodology and case study presented builds on theory and methods of dynamic Bayesian Pre\\x02dictive Synthesis (BPS) recently introduced in a univariate forecasting setting (McAlinn and West\\r\\n2017). BPS is a coherent Bayesian framework for evaluation, calibration, comparison, and context\\x02and data-informed combination of multiple forecast densities. The approach applies whether fore\\x02cast densities arise from sets of models, forecasters, agencies, or institutions. As detailed in McAlinn\\r\\nand West (2017) the framework includes, as special cases, a range of existing univariate density\\r\\nforecasts combination methods. Our multivariate extensions here naturally allow modeling and\\r\\nestimation of varying forecast biases and facets of miscalibration of individual forecast densities,\\r\\ntime-varying inter-dependencies among models or forecasters over multiple series, and addresses\\r\\nthe above noted problems in multivariate settings.\\r\\nSection 2 summarizes the BPS framework and implications in terms of the broad class of im\\x02plied theoretical models for dynamic multivariate problems. Methodological details are developed\\r\\nfor one specific subclass of models– flexible dynamic latent factor models with seemingly-unrelated\\r\\nregression structure (DFSUR models). In this setting, each individual model generating multivari\\x02ate forecast densities is linked to a set of multivariate dynamic latent factor processes– the relation\\x02ships across each set of latent factors are then a key focus in understanding and leading to forecast\\r\\ncombination that addresses interdependencies. In Section 3, analyses of U.S macroeconomic time\\r\\nseries illustrates and highlights the benefits of the new framework using the class of DFSUR models\\r\\nin the BPS context. Additional comments in Section 4 conclude the main paper. An appendix of\\r\\ndetailed supplementary material summarizes Bayesian computational methods (MCMC) for fitting\\r\\nand using DFSUR models in the BPS context, and contains more extensive graphical and tabular\\r\\nsummaries of results of the multiple BPS forecasting analyses from the case study.\\r\\nSome notation: We use lower case bold font for vectors and upper case bold font for matrices.\\r\\nVectors are columns by default. Distributional notation y ∼ N(f, v), x ∼ N(a, A) and k ∼ G(a, b)\\r\\nare for the univariate normal, multivariate normal and gamma distributions, respectively. We use,\\r\\nfor example, N(y|f, v) to denote the actual density function of y when y ∼ N(f, v). Index sets s:t\\r\\nstand for s, s + 1, . . . , t when s < t, such as in y1:t = {y1, . . . , yt}.\\r\\n2\\n2 Dynamic Multivariate BPS\\r\\nThe new developments forming the methodological core of this paper adapt and extend the basic\\r\\nBPS framework of McAlinn and West (2017) to multivariate density forecast synthesis with prac\\x02tical decision goals in mind. McAlinn and West (2017) defined formal, coherent methodology for\\r\\nintegrating density forecasts from multiple, potentially competing statistical model– or forecasters,\\r\\nor institutions– in a univariate time series setting. The dynamic BPS approach there has a founda\\x02tion in coherent Bayesian reasoning with predictive and decision analytic goals, based on historical\\r\\ndevelopments in assessing and combining subjective probabilities (Lindley et al. 1979; West 1984;\\r\\nGenest and Schervish 1985; West 1988; West and Crosse 1992; West 1992; Dawid et al. 1995;\\r\\nFrench 2011). Drawing on key theoretical results from that Bayesian “agent/expert opinion anal\\x02ysis” literature, McAlinn and West (2017) define a class of time-varying parameter, latent factor\\r\\nmodels in which each of the univariate latent factors relates to one of the set of models or forecast\\x02ers generating predictions. The models are developed methodologically and shown to have promise\\r\\nin understanding relationships among forecasting models, their biases and inter-dependencies over\\r\\ntime, and can improve short and medium term forecasting for univariate time series.\\r\\nWe now develop the new, multivariate extension of dynamic BPS, beginning with a brief sum\\x02mary of the key background theory free from the time series context.\\r\\n2.1 BPS Background\\r\\nConsider forecasting a q×1− dimensional vector of outcomes y. Outcomes are typically real-valued,\\r\\nas is the case in our applications below, though the foundational theory is general. A Bayesian\\r\\ndecision maker D is to receive forecast distributions for y from each of J agents; in our application,\\r\\nthe agents are different Bayesian time series models, while in other contexts they may include\\r\\nprofessional forecasters, or forecasting agencies, etc., labelled Aj , (j = 1:J). Then D aims to\\r\\nincorporate the information provided by the set of agent forecast distributions in her thinking\\r\\nand forecasting y and any resulting decisions. Agent Aj provides a probability density function\\r\\nhj (y). These forecast densities represent the individual inferences from the agents, and define the\\r\\ninformation set H = {h1(·), . . . , hJ (·)} now available to D. Formal subjective Bayesian analysis\\r\\ndictates that, D will then use the information set H to predict y using the implied posterior p(y|H)\\r\\nfrom a full Bayesian prior-to-posterior analysis.\\r\\nTo obtain a full Bayesian prior-to-posterior analysis, West (1992) extended prior theory (Genest\\r\\nand Schervish 1985; West and Crosse 1992) to show that there is a subset of all Bayesian models\\r\\nin which D’s posterior has the mathematical form\\r\\np(y|H) = Z\\r\\nX\\r\\nα(y|X)\\r\\nY\\r\\nj=1:J\\r\\nhj (xj )dxj (1)\\r\\nwhere each xj is a latent q×1−dimensional vector, X = [x1, . . . , xJ ]\\r\\n0\\r\\ncollects these latent vectors\\r\\nin a J×q−dimensional matrix, and α(y|X) is a conditional p.d.f. for y given X. The interpretation\\r\\nis as follows. First, in the subjective view of D there must exist latent factors xj potentially related\\r\\n3\\nto y and such that agent Aj ’s forecast density is that of xj . Second, conditional on learning H, the\\r\\nD regards the latent factors as conditionally independent with xj ∼ hj (xj ). Note that this does not\\r\\nimply that D regards the forecasts as independent, since under her prior the hj (·) are uncertain\\r\\nand likely highly inter-dependent, and the key element α(y|X) is how D expresses her views of\\r\\ndependencies. Third, this key element α(y|X) is D’s regression model relating the xj as a collective\\r\\nto the outcome y. Refer to α(y|X) as the BPS synthesis function, and to the xj as latent agent states.\\r\\nThe key eqn. (1) does define the functional form of α(y|X). McAlinn and West (2017) show\\r\\nthat, for scalar outcomes y = y, many forecast and model combination methods (e.g. Geweke\\r\\nand Amisano 2011; Kapetanios et al. 2015; Pettenuzzo and Ravazzolo 2016; Aastveit et al. 2017b,\\r\\namong others) can be considered as special cases of eqn. (1), realized via different choices of the\\r\\nform of the BPS synthesis function α(·|·). For vector outcomes y, eqn. (1) similarly allows flexibility\\r\\nfor D to specify α(y|X) to reflect decision goals and incorporate views and historical information\\r\\nabout, for example, agent-specific biases, patterns of miscalibration, inter-dependencies among\\r\\nagents, and their relative expertise and expected forecast accuracy. Any specific BPS model will be\\r\\ncreated by assuming a specific model form for the synthesis p.d.f.\\r\\n2.2 Dynamic Sequential Setting\\r\\nFor a q−vector time series yt, t = 1, 2, . . . , decision maker D receives forecast densities from\\r\\neach agent sequentially over time. At time t − 1, D receives current forecast densities Ht =\\r\\n{ht1(yt), . . . , htJ (yt)} from the set of agents and aims to forecast yt. The full information set\\r\\nused by D at time t is thus {y1:t−1, H1:t}. As D observes more information, her views of the agent\\r\\nbiases and calibration characteristics, as well as of inter-dependencies among agents are repeat\\x02edly updated. A formal, parametrized Bayesian dynamic model is the vehicle for structuring this\\r\\nsequential learning in a general state-space context. This defines the dynamic BPS framework.\\r\\nThe time series extension of eqn. (1) implies that D has a time t − 1 distribution for yt as\\r\\np(yt|Φt, y1:t−1, H1:t) ≡ p(yt|Φt, Ht) = Zαt(yt|Xt, Φt)\\r\\nY\\r\\nj=1:J\\r\\nhtj (xtj )dxtj (2)\\r\\nwhere Xt = [xt1, . . . , xtJ ]\\r\\n0\\r\\nis a J×q−dimensional matrix of latent agent states at time t, the con\\x02ditional p.d.f. αt(yt\\r\\n|Xt, Φt) is D’s synthesis p.d.f. for yt given Xt, and involves time-varying\\r\\nparameters Φt for which D has current beliefs represented in terms of her (time t − 1) posterior\\r\\np(Φt|y1:t−1, H1:t−1).\\r\\nThis general framework defines the xtj as realizations of inherent dynamic latent factors– the\\r\\nlatent agent states at time t– and synthesis is achieved by relating these latent factor processes to\\r\\nthe time series yt via models of the time-varying synthesis function αt(yt|Xt, Φt). The foundational\\r\\ntheory does not specify this p.d.f., and methodology is based on specific chosen forms. For the mul\\x02tivariate extension of McAlinn and West (2017), we look to a specific class of models that extends\\r\\nthe traditional seemingly unrelated regression model (SUR; Zellner 1962) to a dynamic Bayesian\\r\\nframework, as a first approach to defining a computationally accessible yet flexible framework for\\r\\ndynamic multivariate BPS.\\r\\n4\\n2.3 Multivariate Latent Factor Dynamic Models\\r\\nConsider a dynamic multivariate BPS synthesis function\\r\\nαt(yt|Xt, Φt) = N(yt|Ftθt,V t) (3)\\r\\nwith\\r\\nFt =\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\r\\n1 f\\r\\n0\\r\\nt1 0 0\\r\\n0\\r\\n· · · · · · 0 0\\r\\n0\\r\\n0 0\\r\\n0 1 f\\r\\n0\\r\\nt2\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n0 0\\r\\n0\\r\\n· · · · · · · · · · · · 1 f\\r\\n0\\r\\ntq\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\r\\nand θt =\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\r\\nθt1\\r\\nθt2\\r\\n.\\r\\n.\\r\\n.\\r\\nθtq\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\r\\n(4)\\r\\nwhere for each series r = 1:q, the J×1−vector ftr = (xtr1, xtr2, ..., xtrJ )\\r\\n0\\r\\nis a realization of the\\r\\nset of J latent agents states for series r, and the (J + 1)×1−vector θtr = (1, θtr1, θtr2, ..., , θtrJ )\\r\\n0\\r\\ncontains an intercept and coefficients representing time-varying bias/calibration weights of the J\\r\\nlatent agent states for series r. Note that Ft has q rows and (J + 1)q columns, and θtis a column\\r\\n(J + 1)q−vector. Observation noise is reflected in the– likely volatile– residual q×q variance matrix\\r\\nV t, and the general time-varying parameter of eqn. (2) is set as Φt = {θt,V t}.\\r\\nThis defines the first component of a conditionally linear, conditionally normal dynamic mul\\x02tivariate model– a subclass of multivariate dynamic linear models but with latent factors as pre\\x02dictors. Modeling time evolution of the parameter processes Φt = (θt\\r\\n,V t) is needed to complete\\r\\nmodel specification. We do this using the first step into dynamic models, with traditional random\\r\\nwalk models to allow for– but not anticipate direction in– stochastic changes over time in both\\r\\nregressions θt and matrix volatilities V t, as is traditional in Bayesian time series literatures; see,\\r\\nfor example, West and Harrison (1997) (chap. 16) and Prado and West (2010) (chap. 10). Thus\\r\\nwe take\\r\\nyt = Ftθt + νt, νt ∼ N(0,V t), (5a)\\r\\nθt = θt−1 + ωt, ωt ∼ N(0,Wt) (5b)\\r\\nwhere θt evolves in time according to a linear/normal random walk with innovations variance\\r\\nmatrix Wt at time t, and V tis the residual variance in predicting yt based on past information and\\r\\nthe set of agent forecast distributions.\\r\\nModel specification is completed using standard discount methods. As with the univariate\\r\\nDLM, the time-varying intercept and agent coefficients θt follow the random walk evolution of\\r\\neqn. (5b) where Wtis defined via a standard, single discount factor specification (Prado and West\\r\\n2010, Chap 10). The residual variance matrix V t follows a standard inverse Wishart random walk\\r\\nvolatility model, also based on discounting with a second discount factor.\\r\\nWe now have a class of dynamic, multivariate latent factor models in which latent factors are\\r\\nrealized as draws from the set of agent densities htj (·), becoming available to D at t−1 for forecast\\x02ing yt\\r\\n. Thus, coupled with eqns. (5a,5b), we have the time t prior for the latent states– conditional\\r\\n5\\non H1:t\\r\\n, as\\r\\np(Xt|Φt,Y 1:t−1, H1:t) ≡ p(Xt|Ht) = Y\\r\\nj=1:J\\r\\nhtj (xtj ) (6)\\r\\nwith Xt, Xs conditionally independent for all t 6= s. Again it is important to stress that the con\\x02ditional independence of the xtj given the htj (·) must not be confused with the D’s modeling and\\r\\nestimation of the dependencies among agents. This dependence is central and integral, and is\\r\\nreflected through the effective dynamic parameters Φt = (θt,V t).\\r\\n2.4 Bayesian Analysis and Computation\\r\\nAt any current time t, D has historical information {y1:t, H1:t} and the history of the BPS analysis\\r\\nup until that point. This will have defined inferences on past latent agent states X∗ and the\\r\\ndynamic BPS model parameters Φ∗ = (θ∗,V ∗). The former, importantly, provides insight into\\r\\nthe dependencies, biases, and other characteristics pertaining to y1:t, among agents and individual\\r\\nagents. Posterior summaries for Xt over time inform on this– a key feature of BPS. This inference\\r\\nis topical, as issues of herding (overlap and redundancies) among groups of agents (either models\\r\\nor individuals) is of practical importance, and understanding how these characteristics change over\\r\\ntime and across series is key.\\r\\nPosterior analysis is enabled by Markov chain Monte Carlo (MCMC) methods, followed by fore\\x02casting from time t onward utilizing theoretical and simulation-based extrapolation of the model.\\r\\nD is interested in the inference on the full set of past latent agent states and dynamic parameters\\r\\n{X1:t, Φ1:t}, as well as forward filtering to update posteriors for current values {Xt, Φt}. Posterior\\r\\nMCMC-based computation uses nowadays traditional methods, and extends the MCMC method\\r\\nused in McAlinn and West (2017) for the univariate case with several modifications.\\r\\nPosterior Computations via MCMC. At a given current time t, the multivariate dynamic latent\\r\\nfactor model structure of eqns. (5a,5b,6) leads easily to a three-component block Gibbs sampler\\r\\nfor the latent agent states, dynamic coefficient parameters, and dynamic volatility parameters. The\\r\\ncomponents are iteratively resampled from the three conditional posteriors noted below, initialized\\r\\ngiven agent states drawn independently from priors h∗(∗).\\r\\nFirst, conditional on the agent states and residual volatility, the MCMC step draws new dynamic\\r\\ncoefficient parameters from p(θ1:t|X1:t,V 1:t, y1:t). This is the full (normal) posterior for the se\\x02quence of states in the implied conditional multivariate DLM, and is efficiently sampled using an\\r\\nextension of the traditional forward filtering, backward sampling (FFBS) algorithm (e.g. Prado and\\r\\nWest 2010, chap 10).\\r\\nSecond, the MCMC step draws new dynamic volatility matrices V t from the full joint con\\x02ditional posterior p(V 1:t\\r\\n|X1:t, θ1:t, y1:t)– conditional on the agent states and dynamic coefficient\\r\\nparameters. This employs the standard FFBS algorithm for inverse Wishart discount volatility mod\\x02els (Prado and West 2010, chap. 10)\\r\\nThird, conditional on values of dynamic parameters Φ1:t = (θ1:t,V 1:t), the MCMC draws new\\r\\nagent states from p(X1:t|Φ1:t, y1:t, H1:t). As with the univariate case, the Xt are conditionally in\\x026\\ndependent over time t in this conditional distribution, with time t conditionals p(Xt\\r\\n|Φt, yt, Ht) ∝\\r\\nN(yt|Ftθt,V t)\\r\\nQ\\r\\nj=1:J\\r\\nhtj (xtj ). In cases when all of the agents’ forecasts are multivariate normal,\\r\\nthe posterior is a multivariate normal that is trivially sampled using the properties of conditional\\r\\nnormal. For a more central and practically important case of forecasts being multivariate T distri\\x02bution, each htj (·) can be represented as a scale mixture of normals, and augmenting the posterior\\r\\nMCMC to include the implicit underlying latent scale factors generates conditional multivariate\\r\\nnormals for each Xt coupled with conditional inverse gammas for those scales. In other cases,\\r\\naugmenting the MCMC utilizing Metropolis-Hastings simulator or an augmentation can be used.\\r\\nMore discussion of these algorithmic details is given in Appendix A.\\r\\nForecasting 1-Step Ahead. At time t we forecast 1-step ahead by generating “synthetic futures”\\r\\nfrom the BPS model, as follows. First, for each sampled Φt from the posterior MCMC above, draw\\r\\nV t+1 from its discount volatility evolution model, and then θt+1 conditional on θt,V t+1 from the\\r\\nevolution model eqn. (5b)– this gives a draw Φt+1 = {θt+1,V t+1} from p(Φt+1|y1:t, H1:t). Second,\\r\\ndraw Xt+1 via independent sampling of the ht+1,j (xt+1,j ), (j = 1:J). Third, bring these samples\\r\\ntogether and draw a synthetic 1-step outcome yt+1 from the conditional normal of eqn. (5a) given\\r\\nthese sampled parameters and agent states. Repeating this generates a random Monte Carlo sample\\r\\nfrom the 1-step ahead forecast distribution for time t + 1.\\r\\n2.5 Multi-Step Ahead Forecasting\\r\\nIn many applications involving multivariate analysis, long term forecasting and analysis is often\\r\\nof equal or greater importance than the basic 1-step ahead horizon. For example, in terms of\\r\\neconomic policy and macroeconomic time series– based on traditional monthly or quarterly data–\\r\\nthe most important horizons of interest are 1-3 years ahead. This is especially true when dealing\\r\\nwith monthly data, as knowing a month ahead has very little utility compared to understanding\\r\\nthe long term dynamics and structure over multiple years. Thus, economic policy makers advise\\r\\npolicy decisions based on inputs from their own forecast models, judgemental inputs, views of other\\r\\neconomists, and forecast over the next year or 2-3 years. However, forecast difficulty increases as\\r\\nthe horizon increases, especially when models are calibrated on the short-term basis. Traditional\\r\\nstatistical evaluation of time series models is inherently based on 1-step ahead forecasts, raising\\r\\ninterest in developing BPS to address longer-term forecasting goals.\\r\\nBPS provides two methods for multi-step ahead forecasting, as laid out in McAlinn and West\\r\\n(2017). The first method is direct sequential projection of Φt+1, Φt+2, . . . , Φt+k, updating the pa\\x02rameters over time until it reaches t + k (k being the horizon of interest) and plugging in the\\r\\nrelevant forecasts, Xt+k, sampled from ht,1:J (xt+k). The second is denoted by BPS(k), referring\\r\\nto applying the BPS model to synthesise k-step ahead forecasts directly, as expanded upon below.\\r\\nBPS(k) often outperforms direct projection in terms of forecast accuracy. This is sensible, as we can\\r\\nexpect some agents to perform differently (relative to other agents) for different forecast horizons.\\r\\nIn the context of macroeconomic forecasts, we might observe that economists, who rely on quali\\x02tative information and policy experience, to outperform purely quantitative models on a long term\\r\\n7\\nbasis and thus calibrating the forecasts on their short term predictive ability– for which quantitative\\r\\nmodels often are superior– can be problematic.\\r\\nBPS(k) for customized multi-step forecasting. BPS provides D a flexible strategy to focus on\\r\\nthe horizon k of interest as it is customizable to the forecasting goals. This involves a trivial mod\\x02ification of methodology in Section 2 in which the model at time t − 1 for predicting yt\\r\\nchanges\\r\\nas follows. For a specific forecast horizon k > 1, replace ht−k,j (xtj ) with htj (xtj ) so BPS is cali\\x02brated using the forecasts from t − k. Doing this results in dynamic model parameters {θt\\r\\n,V t}\\r\\nto be explicitly geared to the k-step horizon, calibrating and “tuning” to the horizon k of interest.\\r\\nForecasting, then, simply follows the model extrapolation via simulation as in Section 2.4.\\r\\n3 Case Study in US Macroeconomic Forecasting\\r\\n3.1 Data, Forecasting Models and Implementation\\r\\nTime Series Data We analyze monthly US macroeconomic data, focusing on forecasting six\\r\\nmacroeconomic time series with 1-, 12-, and 24-month ahead interests. The study involves the\\r\\nfollowing monthly macro series: annual inflation rate (p), wage (w), unemployment rate (u), con\\x02sumption (c), investment (i), and short-term nominal interest rate (r) in the US economy from\\r\\n1986/1 to 2015/12, a context of topical interest (Cogley and Sargent 2005; Primiceri 2005; Koop\\r\\net al. 2009; Nakajima and West 2013a). The inflation rate is the consumer price index for all urban\\r\\nconsumers: all items less food and energy, not seasonally adjusted, wage is the average hourly\\r\\nearnings of production and nonsupervisory employees: total private, not seasonally adjusted, the\\r\\nunemployment rate is the civilian unemployment rate, seasonally adjusted, consumption is the\\r\\npersonal consumption expenditures, seasonally adjusted annual rate, investment is the ISM manu\\x02facturing: new orders index, and the interest rate is the effective federal funds rate, not seasonally\\r\\nadjusted; the first four being annual changes, investment being monthly changes, and monthly\\r\\ninterest rates. Fig. 1 shows the data for the six series over the time span considered. We focus\\r\\non forecasting the six series, with an emphasis on inflation, using past values of the six series as\\r\\ncandidate predictors underlying a set of five time series models– the J = 5 agents– to be evaluated,\\r\\ncalibrated, and synthesized.\\r\\nDuring the period of analysis, the sub-prime mortgage crisis and great recession of the late\\r\\n2000s warrant special attention. This period involved a series of significant, unique shocks to the\\r\\nUS economy, so any analysis is challenged in terms of predictive ability in short and longer terms.\\r\\nFor any combination strategy to be effective and useful, its predictive performance must be robust\\r\\nunder these conditions. Additionally, due to the structural changes in the overall economy (e.g.\\r\\nAastveit et al. 2017a), there is also interest in understanding changes in the inter-dependencies\\r\\namong series over the crisis periods. On this goal, multivariate analysis offers opportunity for\\r\\nimproved understanding that simple univariate analyses just cannot.\\r\\nIn our BPS(k) analyses, for k = 12 we take investment as the cumulative value of the previous\\r\\nyear of monthly differences, since investment measures monthly difference and forecasting the\\r\\n8\\nchange from 11th to 12th month is of little relevance to the policy maker. Similarly, for k = 24,\\r\\ninvestment is the cumulative value of the 24 months of monthly differences. Additionally, inflation,\\r\\nwage, unemployment, and consumption are defined as changes from the current time. Agents\\r\\nwill produce forecasts according to the target value by either summing the forecasts to the target\\r\\nhorizon, or summing certain periods within the horizon. In this way, the target forecast is directly\\r\\nin line with what policy makers are interested in and focus on for decision making.\\r\\nAgent Models and BPS Specification. For the J = 5 agents we use time varying parameter vec\\x02tor autoregressive (TVP-VAR) models that cover multiple dynamic structures utilized in the litera\\x02ture (Cogley and Sargent 2005; Primiceri 2005; Koop et al. 2009; Nakajima and West 2013a) and\\r\\nin practice. Labelled as M*, the agent models are: M1- VAR(1); M2- VAR(12); M3- VAR(3); M4-\\r\\nVAR(1:3:9); M5- VAR(1:6:12). The numbers in parentheses are the lags and the number between\\r\\ncolons represent intervals (e.g. 1:3:9 uses lags of 1, 3, 6, and 9). Each M* is a standard TVP-VAR\\r\\nmodel (or exchangeable time series; Prado and West 2010, Chap 10) with the residual volatility\\r\\nfollowing a matrix-beta/Wishart random walk so that model fitting and generation of forecasts is\\r\\nroutine. Though more recent variants of these core models– such as, for example, Bayesian latent\\r\\nthreshold TVP-VARs with stochastic volatility as in Nakajima and West (2013a)– might be consid\\x02ered, one benefit and appeal of forecast synthesis is making improvements over a set of relatively\\r\\nsimple models. That is, we explore BPS applied to rather standard, and currently accepted variants\\r\\nof models that are traditional and whose basic model forms (up to assumptions about lags and\\r\\nvariables to include) are accepted in the applied macroeconomic forecasting community.\\r\\nIn the dynamic BPS models for forecast horizons k = 1, 12, 24, we take initial priors using inde\\x02pendence across series r and with θtr ∼ N(a0, R0) with a0 = (0, 1\\r\\n0/J)0 and where R0 is diagonal\\r\\nwith diagonal elements of 1 apart from: (a) elements of 0.001 for the intercept coefficients; and (b)\\r\\nelements of 0.1 for coefficients on investment. Coupled with this, we take V 0 ∼ IW(7, 7 ∗ 0.01I).\\r\\nDiscount factors in the BPS(1) model are set at (β, δ) = (0.99, 0.99), and variants for BPS(k) for\\r\\nk = 12, 24-month ahead forecasting are as discussed in Section 2.5, though now we increase the\\r\\nprior variance on the intercept from 0.001 to 0.01 and 0.1, for k = 12, 24, respectively, to reflect the\\r\\nincreased uncertainty about the relevance of the agent forecasts at longer horizons.\\r\\nWe have explored analyses across ranges of choices of initial priors and discount factors, and\\r\\nchosen these values as they lead to good agent-specific and BPS forecasting accuracy; conclusions\\r\\nabout the main questions– how BPS can improve forecasts while generating insights into agent\\r\\ncharacteristics and dependencies over time– to not change materially with different values close to\\r\\nthose chosen for the summary examples.\\r\\nData Analysis and Forecasting. The 5 agent models are analyzed and synthesized as follows.\\r\\nFirst, the agent models are analyzed in parallel over 1986/1-1993/6 as a training period to calibrate\\r\\nthe VARs. This continues over 1993/7-2001/12 while at each month t during this period, the\\r\\nMCMC-based BPS analysis is run in parallel using data from 1993/7 up to time t in an “expanding\\r\\nwindow” fashion, adding data as we move forward in time. We do this for the traditional 1-step\\r\\nfocused BPS model, and– separately and in parallel– for the k = 12, 24-step ahead focused BPS(k)\\r\\n9\\nmodel as discussed in Section 2.5. This continues over the third period to the end of the series,\\r\\n2001/1-2015/12, generating forecasts (for the agents and BPS) for each t until the end of the\\r\\ntesting period. This testing period spans over a decade and a half and includes 180 data points,\\r\\nproviding a good measure on how the agents and BPS perform under different economic situations;\\r\\nmost notably before, during, and after the sub-prime mortgage crisis. Out-of-sample forecasting is\\r\\nthus conducted and evaluated in a way that mirrors the realities facing decision and policy makers.\\r\\nForecast Accuracy and Comparisons. Following the recent literature on macroeconomic fore\\x02casting, we compare both point and density forecasts to give a broader understanding of the pre\\x02dictive abilities of the agents and BPS. For the point forecasts, we compute and compare mean\\r\\nsquared forecast error (MSFE) over the forecast horizons of interest and for each series. For density\\r\\nforecasts with BPS, we evaluate log predictive density ratios (LPDR); at horizon k and across time\\r\\nindices t for the joint set of series, this is\\r\\nLPDR1:t(k) = X\\r\\ns=1:t\\r\\nlog{pj (ys+k|y1:s)/pBPS(ys+k|y1:s)}\\r\\nwhere pj (ys+k|y1:s) is the predictive density under each agent indexed by j, at each time s over\\r\\nthe next k time points. The LPDR measures are, at each t, baselined against the corresponding BPS\\r\\nforecasts over each horizon k. LPDR provides a direct statistical assessment of the distributional ac\\x02curacy of a forecast relative to, in this case, BPS for multiple horizons, extending the 1-step focused\\r\\nBayes’ factors. They compare the location and dispersion of the forecasts, giving an assessment of\\r\\nrisk, elaborating on MSFE measure, and have been increasingly used in broader model comparison\\r\\nand forecast accuracy studies (e.g. Nakajima and West 2013a; Aastveit et al. 2017b).\\r\\n3.2 Dynamic BPS and Forecasting\\r\\n1-step ahead forecasting. Table 1 summarizes the predictive measures compared for the 1-step\\r\\nahead forecasts. Looking at point forecasts, BPS exhibits clear improvements over agent forecasts 5\\r\\nout of the 6 series; the 5 are inflation, wage, consumption, investment, and interest rate. Even for\\r\\nthe series for which BPS does not show substantial improvement over the models, the difference\\r\\nbetween the best model is within 1%. On the series BPS on which makes an improvement, the\\r\\ngains are at least 2%, except compared to one model for interest rate. It is also notable that the\\r\\nbest model differs for each series. VAR(1:3:9) is best for inflation while it is the worst for wage,\\r\\nfor example. Under traditional model combination strategies, such as Bayesian model averaging\\r\\n(BMA) in which each model is assessed only on 1-step ahead density forecast accuracy and for the\\r\\nfull multivariate forecast, accuracy is inherently aggregated over series. BPS, due to its flexible\\r\\nsynthesis function, is able to synthesize forecasts on each series, while retaining the inter-series\\r\\ndependencies. This leads to BPS improving on multiple series without trading-off one over another.\\r\\nIn fact, the results in Table 1 shows that BPS exhibits improvements for all 6 series relative to BMA.\\r\\nBPS demonstrates an ability to substantially improves characterization of forecast uncertainties\\r\\nas well as adaptation in forecast locations, reflected in the LPDR measures. Note that the best\\r\\n10\\nmodel, in terms of LPDR, is only best for wage in terms of MSFE and performs average for the\\r\\nother series. This indicates how LPDR measures for multiple series favor overall performance over\\r\\nmodels that are good for some but bad for others. Model combination schemes that are dependent\\r\\non likelihood measures– such as BMA– heavily favor the average performing model (VAR(3) in the\\r\\ncase of 1-step ahead forecasts). In contrast, BPS dynamically synthesizes forecasts for each series,\\r\\nwhile improving uncertainty assessment (per series and dependence between series), to improve\\r\\nin terms of overall distribution forecasts as well as point forecasts. This feature of BPS is critical, as\\r\\nD typically has priorities among the series being forecasted.\\r\\nWe next review summary graphs showing aspects of analysis evolving over time during the\\r\\ntesting period, a period that includes challenging economic times that impede good predictive\\r\\nperformance. Figs. 2–14 summarize sequential analysis for 1-step forecasting.\\r\\nFig. 2 shows the 1-step ahead measures MSFE1:t(1) over time t in forecasting inflation. The\\r\\nother series are omitted for the sake of brevity, but the patterns in inflation are consistent (see\\r\\nsupplementary appendix material). Additionally, forecasting inflation is one of the most important\\r\\ntasks for an economic policy maker, and therefore focusing on inflation is appropriate for this\\r\\nexample. While BPS does not outperform the other models over the whole testing period, we see\\r\\nthat it is on par with the best models considered. BPS ends up improving on the other models\\r\\nbased on its performance during and after the sub-prime mortgage crisis, demonstrating how BPS\\r\\ndynamically adapts over time to produce robust forecasts over crisis periods and changing regimes.\\r\\nFig. 3 confirms that BPS performs uniformly better than the other models based on LPDR mea\\x02sures. The gradual decline in LPDR and a more drastic decline after the crisis is indicative of how\\r\\nBPS dynamically adapts its location and uncertainty to improve its distribution forecasts.\\r\\nOne crucial aspect of the BPS model is that it can adapt coefficients specific to each series.\\r\\nFigs. 4-9 are the on-line posterior means of BPS model coefficients for 1-step ahead forecasts for\\r\\neach series. We first note how the coefficients for each series evolve and are different across series,\\r\\nreflecting how different models are better at forecasting different series and how the relative accu\\x02racy differs in time. Additionally, we note that the dynamic coefficients can appear to be somewhat\\r\\nerratic, which is reflective of the level of uncertainty in the latent agent states.\\r\\nThe somewhat erratic coefficient trajectories for the 1-step ahead forecasts–particularly com\\x02pared to those from the 12- and 24-step analyses (Figs. C16-C27)– arises due to a number of\\r\\nfactors. It is partly a result of the constraining initial prior on the intercept, heavily favoring small\\r\\nvalues, so the on-line posteriors for the BPS coefficients adapt more dramatically than were the\\r\\nintercept to be “freer” to move around. This is coupled with the generally strong positive inter\\x02dependencies among agent forecasts that lead to high collinearity. Coefficients trajectories are\\r\\nmost erratic during the stable pre-crisis period, where agent forecasts are particularly similar, and\\r\\nless erratic when agent forecasts diverge more substantially post-crisis. For 12- and 24-step ahead\\r\\nforecasts, while the agent forecasts are generally poorer, their inter-dependencies are much weaker\\r\\nyielding more stable coefficient trajectories based on sustained differences in relative forecasting\\r\\naccuracy across agents even in the context of high uncertainties.\\r\\nFor inflation (Fig. 4), the coefficients clearly exhibit a structural change after the sub-prime\\r\\nmortgage crisis. VAR(1) and VAR(3), which are relatively simple models with short lags, have the\\r\\n11\\nhighest coefficients up until the crisis, but quickly drop off, replaced by VAR(1:3:9) and VAR(1:6:12),\\r\\nwhich are more complex models with longer lags. This can be viewed as a structural change where\\r\\nsimpler dynamics are being replaced by longer, more complex, dynamics after the crisis.\\r\\nCoefficients of wage (Fig. 5), on the other hand, are relatively stable over time. The VAR(3)\\r\\nmodel, using a quarters worth of lags, has the highest coefficient and that stays the highest through\\x02out. In comparison to inflation, VAR(1), the simplest model has limited impact, while the most\\r\\ncomplex VAR(12) model has a persistent negative coefficient which, to some degree, balances the\\r\\nimpact of the simpler, short-lag models.\\r\\nIn Fig. 6, the estimated trajectory of coefficients for unemployment exhibits an increase for the\\r\\nVAR(1) model and a gradual decrease for the more complex VAR(3) model after the sub-prime\\r\\nmortgage crisis, as well as overall small effects from the other more complex models. Due to\\r\\nunemployment being heavily impacted by the crisis, this characteristic is understandable. Long\\r\\nterm unemployment trends become irrelevant in light of the recent shock to the economy, and the\\r\\ncoefficients reflect that shift.\\r\\nFor consumption Fig. 7, we see a clear grouping of agents: that of VAR(1)-VAR(1:6:12) and that\\r\\nof VAR(3)-VAR(12). Before the crisis, we see that these two groups converging to almost equal\\r\\nweight (except for VAR(1:3:9), which is almost always negative), then quickly re-separating again\\r\\nafter the crisis. This suggests that consumption is mainly driven by biannual lags.\\r\\nThe coefficients for investment (Fig. 8) uses a more restrictive initial prior due to experience in\\r\\nthe training data period of extremely high uncertainty in the agent forecasts linked to the highly\\r\\nvolatile nature of this series. This results in relatively more stable coefficients trajectories, with all\\r\\nbeing above zero and around equal weight. However, there are still clear patterns that emerge;\\r\\nnotably, we see an upward spike in VAR(1:6:12) at around 2003 and some fluctuations during the\\r\\nsubprime mortgage crisis.\\r\\nFor interest rate (Fig. 9) the coefficients favor more complex models with longer lags. Inter\\x02estingly, we see a gradual decrease in coefficients on the VAR(1) model up until the sub-prime\\r\\nmortgage crisis, at which point it stays level. Long term dynamics, we can infer, were taking over\\r\\nshort term dynamics leading up to the crisis, bringing up interesting questions about lending and\\r\\ncredit characteristics pre-crisis. We also note that the introduction of zero interest rates after the\\r\\ncrisis does not seem to effect the coefficients at all.\\r\\nFigs. 10-13 exhibit selected aspects of inferences on the trends in uncertainty and dependence\\r\\nbetween and within agents over time. The posterior latent agent variances (the diagonal elements\\r\\nof the covariance matrix) displays how the uncertainty measures of the forecasts change over time;\\r\\nsee Fig. 10. Complex models for multiple series– that require estimation methods that are also\\r\\ncomplex– often produce large forecast standard deviations coming from the model, data, estima\\x02tion method, or all of the above. Large VAR models are popular in practice, due to modeling flexi\\x02bility and interpretability, but naturally lead to inflated uncertainty measures due to large numbers\\r\\nof parameters, colinearities and resulting estimation uncertainties. BPS, on the other hand, has\\r\\nsmaller uncertainty in synthesized forecasts, resulting in decreased forecast uncertainty relative to\\r\\neach of the agents. This is a critical benefit of BPS. Though underestimating real risk is as danger\\x02ous as overestimating it, the LPDR results indicate that the BPS uncertainty estimates are valid–\\r\\n12\\npoint forecasts are generally improved and lower predictive uncertainties couple with that to lead\\r\\nto a win-win analysis.\\r\\nWe move attention to posterior distributions on the posterior latent agent factors over time to\\r\\nexplore inter-dependencies among agent forecasts and their temporal evolutions. One set of nu\\x02merical summary of dependencies among agent forecasts is given by the retrospective posterior\\r\\ncorrelation of Xt. The posterior dependencies among agents are measured through the off block\\r\\ndiagonal elements of the correlation, which is zero when the forecasters provide their forecasts.\\r\\nFor instance, high positive dependencies among agent forecasts will generate high negative corre\\x02lations among the corresponding dynamic regression coefficients on the agent latent factors, and\\r\\nvice-versa; discovering the underlying temporal dependencies. We look at these measures of depen\\x02dence at 3 specific time points that represent different regimes in the testing period; pre-sub-prime\\r\\nmortgage crisis (2003/12), immediately after the crisis (2008/12), and post-crisis (2014/12). Cor\\x02relations are arranged in order agents. See Figs. 11-13.\\r\\nAt the beginning of the testing period, where the series are relatively stable (Fig. 11), we note an\\r\\noverall strong negative and positive dependence within agents, with some notable positive depen\\x02dence between 12-month (long term) investment and interest rate and strong negative dependence\\r\\nfor unemployment and consumption across all agents. The negative correlation within series is ex\\x02pected, as these are modeled by the agents and updated through BPS. Across agents, we note some\\r\\ndiagonal patterns that are present, albeit weak. For example, looking at the off diagonal block\\r\\nbetween VAR(1) and VAR(12), we can see negative dependencies between all series except for in\\x02terest rates. This indicates some long terms dependencies, although we do expect them to exist\\r\\nsince VAR(1) is nested within VAR(12).\\r\\nImmediately after the crisis (Fig. 12), we note a drastic change in correlations across and within\\r\\nagents appearing, as seen in the positive correlations (yellow/red) radiating from and within series\\r\\nand negative correlations across agents. Focusing on VAR(1:3:9), there are strong negative corre\\x02lations between VAR(3), but comparatively very little between the other agents. Other patterns,\\r\\nnotably block patterns appear between VAR(1), VAR(12), and VAR(3) that were not present pre\\x02crisis. These emergent patterns suggest a large shift in dependence during the crisis that would\\r\\nhave been overlooked without BPS. Post crisis (Fig. 13), we see a new dependence structure ap\\x02pearing, with almost no dependence across– and even within– series, except for mild dependencies\\r\\nwithin series and some diagonal structure seen in Fig. 11 reappearing. In particular, the correlation\\r\\nbetween VAR(1:3:9) and VAR(12) and VAR(3) has seem to have completely vanished.\\r\\nThe patterns of changes in dependencies among agents and across series over time provide\\r\\ninsights into how the economy changes with respect to economic shocks defining different regimes.\\r\\nFigs. 11 and 13 are both snapshots of relatively stable periods, yet the characteristics exhibited\\r\\nthrough the estimated dependencies are different. These differences in economic structure are not\\r\\nunexpected, though graphically visualizing the differences through the lens of agents and BPS\\x02defined inferences on inter-dependencies provides new insight and perceptions into the overall\\r\\nchange in economy.\\r\\nFinally, Fig. 14 measures the Kullback-Leibler divergence from the prior (forecast densities from\\r\\nthe agents with block diagonal covariance structure) to the posterior (learned through BPS) agent\\r\\n13\\nforecasts, measured at each time t. The Kullback-Leibler divergence for each time t is defined as\\r\\nfollows:\\r\\nDKL(t) = E\\r\\np(Xt)\\r\\n[log(p(Xt)/h(Xt))]\\r\\nwhere h(Xt) is the prior agent forecast density and p(Xt) = p(Xt|Φt, yt, Ht) is the posterior\\r\\nagent forecast density. Note that, while the Kullback-Leibler divergence is analytically estimable for\\r\\ntwo multivariate Gaussian densities, for other distributions, or samples from posterior densities,\\r\\nthe Kullback-Leibler divergence must be approximated by a Gaussian density (if suitable) or by\\r\\ncomputing the following Monte Carlo approximation\\r\\nD\\r\\napprox\\r\\nKL (t) = 1\\r\\nn\\r\\nX\\r\\ni=1:n\\r\\n[log(p(Xti)/h(Xti))].\\r\\nSince the Kullback-Leibler divergence measures the information gained from h(Xt) to p(Xt), it is\\r\\na direct measurement/summary of the amount of biases and inter-dependencies that are learned\\r\\nthrough BPS using the data and past agent forecast information. For example, if an agent produces\\r\\nsignificantly biased forecasts during economic distress, BPS will learn and calibrate that bias, and\\r\\nthe difference will be captured in the Kullback-Leibler divergence. Likewise, we expect and observe\\r\\ninter-dependencies across agents to increase during crises and shocks, as seen in Fig. 12. Under\\r\\nthese conditions, the Kullback-Leibler divergence will naturally increase, since the prior agent inter\\x02dependencies are zero. Looking at Fig. 14, we see three major spikes that are notable; the first\\r\\nduring the dot-com bubble, and two before and after the great financial crisis. The first and third\\r\\nspike is understandable, as the results in Fig. 12 indicate a large amount of inter-dependencies\\r\\nacross agents to appear during a crisis. The second spike is perhaps more interesting, as early\\r\\n2008 was when– in hindsight– early signs of the crisis were emerging. Because the subtle shifts in\\r\\nagent forecasts (increased biases and inter-dependencies not reported in the prior agent forecasts)\\r\\nare captured using BPS, the Kullback-Leibler divergence is able to pick up the early signals of a\\r\\nrecession. Although the results presented here are retrospective (i.e. the posterior is in regard to\\r\\nt = 1:T), it is indicative of the strengths of BPS in capturing crises and shocks measured through\\r\\nhow much information is gained via Bayesian learning.\\r\\nk-step ahead forecasting. Long term forecasting for economic policy is far more important than\\r\\n1-step ahead forecasting. For this study, we forecast 12- (one year) and 24- (two years) step ahead\\r\\nto demonstrate the effectiveness of BPS over the set of agents at practically important horizons.\\r\\nTables 2 and 3 summarizes the predictive measures compared for the two forecast horizons.\\r\\nFor point forecasts, BPS(k) outperforms all other models for all series with the exception of wage\\r\\ngrowth. The improvement hold for all k-step ahead forecasts considered and the improvements of\\r\\nBPS(k) significantly increase with k. The improvements come from BPS(k) directly synthesizing the\\r\\nk-step ahead forecasts from the agents, calibrating, adapting, and learning the latent dependencies\\r\\nand biases over the k-step ahead quantity of interest. For 24-step ahead forecasts of inflation, one\\r\\nof the most important series for a central bank when setting their key policy rate, BPS(k) greatly\\r\\n14\\nimproves on the agent models, with massive gains over the best agent model. As with 1-step ahead\\r\\nforecasting, it is also notable that agent model performances vary significantly across series. In\\r\\ncontrast to BPS, traditional model combination schemes, such as BMA, fail to improve over all\\r\\nseries by sacrificing improved accuracy for one series over others; in fact, for both 12- and 24-\\r\\nstep ahead forecasts, BMA-based analysis degenerates to the VAR(1) model. In addition, BPS(k)\\r\\nsignificantly improves in quantifying uncertainty in forecasts, as evident in the comparison of LPDR.\\r\\nCreating long-term forecasts for multiple time series is a very difficult problem due to the nature of\\r\\nthese models being built and trained on 1-step ahead forecasting metrics (likelihood) and failing to\\r\\npropagate forward accurately. In contrast, BPS(k) synthesizes the k-step ahead forecasts directly,\\r\\nadjusting and calibrating uncertainty according to the actual quantity of interest. Thus, no matter\\r\\nhow the agent uncertainty forecasts are over- or under-estimating, BPS(k) can re-adjust accordingly\\r\\nby learning how the agents over- or under-estimate. The consistency of the LPDR improvements\\r\\nover multiple k-steps demonstrate this key feature of BPS(k).\\r\\nAs with the sequential MSFE results for 1-step ahead forecasts, we focus now on multi-step\\r\\nMSFE results for inflation. The characteristics of the results for inflation are similar to those of the\\r\\nother series and are omitted for the sake of brevity. Figs. 15 and 17 exhibit MSFE comparisons for\\r\\ninflation over the testing period for k = 12, 24-step ahead forecasts. Although the scale is different\\r\\nfor each k, there are notable common characteristics that characterize the BPS(k) results. For\\r\\nexample, agent models experience several large shocks in precision over the testing period; this\\r\\noccurs, in particular, around the time of the advent of the sub-prime mortgage crisis in the late\\r\\n2000s. These shocks particularly effect the precision of the agent forecasts, especially the 24-step\\r\\nahead forecasts. In comparison, BPS(k) stays relatively robust throughout multiple shocks and\\r\\nstructural breaks.\\r\\nLooking at LPDR evolutions over time (Figs. 16 and 18), BPS(k) improves over the agent mod\\x02els over all of the time period considered, except for slight increases in favoring the simpler VAR(1)\\r\\nmodel immediately post crisis. BPS(k) is able to adapt to maintain improved forecasting perfor\\x02mance both in terms of location and uncertainty assessment, a key positive feature for decision\\r\\nmakers tasked with forecasting risk and quantiles for long horizons under possible shocks and\\r\\nregime change.\\r\\nFigs. C22-C27 exhibit the on-line posterior means of BPS model coefficients for the 24-step\\r\\nahead forecasts. The coefficients for 6- and 12-step ahead forecasts are omitted– similar conclusions\\r\\narise in those analyses. Overall, the BPS(k) coefficients are relatively stable compared to the 1-step\\r\\nahead results, due somewhat to the lack of signal from the agent forecasts. The agent forecasts’\\r\\nability for 24-steps are considerably worse than from their 1-step ahead counterparts, leading to\\r\\nless useful information to be synthesized by BPS(k). The lack of signal from all of the agent models\\r\\nleads to less movement in the coefficients, and in turn, an increase in adaptability in the intercept.\\r\\n4 Additional Comments\\r\\nOur extensions and development of multivariate BPS define a theoretically and conceptually sound\\r\\nframework to compare and synthesize multivariate density forecasts in a dynamic context. The\\r\\n15\\napproach will enable decision makers to dynamically calibrate, learn, and update predictions based\\r\\non ranges of forecasts from sets of models, as well as from more subjective sources such as in\\x02dividual forecasters or agencies. While it will be of interest to develop future studies in which\\r\\nagents are represented by sets of more elaborate macroeconomic models– such as dynamic thresh\\x02old models and dynamic stochastic general equilibrium (DSGE) models– and to integrate forecasts\\r\\ncoming from professional forecasters and economists, the current case study already demonstrates\\r\\nthe real practical potential. In our sequential BPS analysis of multiple US macroeconomic series,\\r\\nwe have highlighted questions of forecast synthesis methodology with respect to forecasting goals:\\r\\ninterest in 12 or 24 month-ahead forecasting demands– from a formal Bayesian perspective– anal\\x02ysis customized to the horizon, and the results bear out the practical relevance of that perspective.\\r\\nThe studies show that the flexible and interpretable DFSUR models can (i) adapt to time-varying\\r\\nbiases and miscalibration of multiple models or forecasters, (ii) adaptively and practically account\\r\\nfor– while generating useful insights into– patterns of time-varying relationships and dependen\\x02cies among sets of models or forecasters, and (iii) improving forecast accuracy– in some cases,\\r\\nmost substantially– for each of several multiple macroeconomic series together, at multiple hori\\x02zons. The predictive performance of BPS is robust in times of severe economic distress, which\\r\\nis important for practical applications. Additionally, inference on the inter-dependencies among\\r\\nforecasting models– linked to the BPS foundational latent factor structure and aspects of inference\\r\\non time-varying parameters characterizing that structure– provides both illumination of the inter\\x02dependencies, and how they may vary across subsets of the multivariate series. This also provides\\r\\nthe decision maker with the opportunity to respond and change or intervene in the BPS modeling\\r\\nfor continued forecast synthesis into the future.\\r\\nMultivariate BPS has further potential in applications to other fields and data where inter\\x02dependencies between series have impact on the decision making, and where multiple forecasts,\\r\\nwhether from forecasters or models, are available. Such applications include financial data, such\\r\\nas stocks, indexes, and bonds with portfolio decisions in mind. Further methodological extensions\\r\\nthat warrant investigation include non-normal forecasts and discrete data, and missing or incom\\x02plete/partial forecasts. A second, major area for extension arises from the observation that our BPS\\r\\nsetting is not, in fact, as general as it could be in terms of building on the historical Bayesian agent\\r\\nanalysis literature. Referring back to Section 2.1, each agent Aj could provides a forecast density\\r\\nhj (zj ) for a set of outcomes zj that is not, in fact, exactly the outcome y of interest to D. For ex\\x02ample, zj may be a subset of the macroeconomic variables in y, but not all of them; and/or it may\\r\\ninclude additional, relevant variables not included in y. The same foundational theory applies, and\\r\\nthis provides potential to explore BPS when different models or forecasters have differing areas of\\r\\nexpertise as well as different strategies in forecasting collections of related series.\\r\\nQuestions about the computational aspects of fitting and forecasting with BPS are also relevant.\\r\\nThe current analysis, as developed and exemplified in this paper, relies on repeat reanalysis using\\r\\nMCMC for each time t. This is a common strategy in the application of Bayesian dynamic latent\\r\\nfactor models of other forms in a sequential forecasting context, and state-of-the-art as an approach\\r\\nif full and accurate numerical Bayesian analysis is to be achieved. In the case study developed, the\\r\\ncomputational burdens are not at all detrimental, and the potential improvements in forecasting\\r\\n16\\naccuracy and insights that our example illustrates outweighs the computational cost. In other con\\x02texts requiring fast data processing in sequential analyses, it may be that some form of sequential\\r\\nMonte Carlo (SMC, e.g. Lopes and Tsay 2011) may prove useful, especially if enabled by decoupling\\r\\nof multivariate series into small or univariate but linked subsets, within each of which customized\\r\\nSMC methods might be more effective. This idea would aim to exploit and extend to a BPS frame\\x02work the concept of decouple/recouple in modeling increasingly high-dimensional time series in\\r\\nother contexts (e.g. Gruber and West 2016; Chen et al. 2017; Gruber and West 2017). Further,\\r\\nbeyond our development of DFSUR models in the current study, some of these referenced mod\\x02eling approaches may be of interest in themselves as candidates for defining relationships among\\r\\noutcome time series and the inherent latent factors in dynamic multivariate BPS models.\\r\\nReferences\\r\\nAastveit, K. A., Carriero, A., Clark, T. E., and Marcellino, M. (2017a), “Have standard VARs re\\x02mained stable since the crisis?” Journal of Applied Econometrics, 32, 931–951.\\r\\nAastveit, K. A., Gerdrup, K. R., Jore, A. S., and Thorsrud, L. A. (2014), “Nowcasting GDP in real\\r\\ntime: A density combination approach,” Journal of Business & Economic Statistics, 32, 48–68.\\r\\nAastveit, K. A., Ravazzolo, F., and van Dijk, H. K. (2017b), “Combined density Nowcasting in an\\r\\nuncertain economic environment,” Journal of Business & Economic Statistics, in press- published\\r\\non line April 27, 2017.\\r\\nAmendola, A. and Storti, G. (2015), “Model uncertainty and forecast combination in high\\x02dimensional multivariate volatility prediction,” Journal of Forecasting, 34, 83–91.\\r\\nAmisano, G. and Geweke, J. (2017), “Prediction using several macroeconomic models,” Review of\\r\\nEconomics and Statistics, 99, 912–925.\\r\\nAmisano, G. G. and Giacomini, R. (2007), “Comparing density forecasts via weighted likelihood\\r\\nratio tests,” Journal of Business & Economic Statistics, 25, 177–190.\\r\\nAndersson, M. K. and Karlsson, S. (2008), “Bayesian forecast combination for VAR models,” in\\r\\nBayesian Econometrics (Advances in Econometrics, Volume 23), eds. S. Chib, W., Griffiths, Koop,\\r\\nG., and Terrell, D., Emerald Group Publishing Limited, pp. 501–524.\\r\\nBates, J. M. and Granger, C. W. J. (1969), “The combination of forecasts,” Operational Research\\r\\nQuarterly, 20, 451–468.\\r\\nBenati, L. and Surico, P. (2008), “Evolving US monetary policy and the decline of inflation pre\\x02dictability,” Journal of the European Economic Association, 6, 634–646.\\r\\nBillio, M., Casarin, R., Ravazzolo, F., and van Dijk, H. K. (2013), “Time-varying combinations of\\r\\npredictive densities using nonlinear filtering,” Journal of Econometrics, 177, 213–232.\\r\\n17\\nCasarin, R., Grassi, S., Ravazzolo, F., and van Dijk, H. K. (2015), “Parallel sequential Monte Carlo\\r\\nfor efficient density combination: The DeCo MATLAB Toolbox,” Journal of Statistical Software,\\r\\nArticles, 68, 1–30.\\r\\nChan, J. C., Koop, G., Leon-Gonzalez, R., and Strachan, R. W. (2012), “Time varying dimension\\r\\nmodels,” Journal of Business & Economic Statistics, 30, 358–367.\\r\\nChen, X., Irie, K., Banks, D., Haslinger, R., Thomas, J., and West, M. (2017), “Scalable Bayesian\\r\\nmodeling, monitoring and analysis of dynamic network flow data,” Journal of the American Sta\\x02tistical Association, in press- published online July 10, 2016, arXiv:1607.02655.\\r\\nClemen, R. T. (1989), “Combining forecasts: A review and annotated bibliography,” International\\r\\nJournal of Forecasting, 5, 559–583.\\r\\nClemen, R. T. and Winkler, R. L. (1999), “Combining probability distributions from experts in risk\\r\\nanalysis,” Risk Analysis, 19, 187–203.\\r\\n— (2007), “Aggregating probability distributions,” in Advances in Decision Analysis: From Foun\\x02dations to Applications, eds. W. Edwards, R. M. and von Winterfeldt, D., Cambridge University\\r\\nPress, chap. 9, pp. 154–176.\\r\\nCogley, T. and Sargent, T. J. (2005), “Drifts and volatilities: Monetary policies and outcomes in the\\r\\npost WWII U.S.” Review of Economic Dynamics, 8, 262–302.\\r\\nDawid, A. P., DeGroot, M. H., Mortera, J., Cooke, R., French, S., Genest, C., Schervish, M. J.,\\r\\nLindley, D. V., McConway, K. J., and Winkler, R. L. (1995), “Coherent combination of experts’\\r\\nopinions,” Test, 4, 263–313.\\r\\nDel Negro, M., Hasegawa, R. B., and Schorfheide, F. (2016), “Dynamic prediction pools: An investi\\x02gation of financial frictions and forecasting performance,” Journal of Econometrics, 192, 391–405.\\r\\nFrench, S. (2011), “Aggregating expert judgement,” Revista de la Real Academia de Ciencias Exactas,\\r\\nFisicas y Naturales (Serie A: Matematicas), 105, 181–206.\\r\\nFruhwirth-Schnatter, S. (1994), “Data augmentation and dynamic linear models,” ¨ Journal of Time\\r\\nSeries Analysis, 15, 183–202.\\r\\nGenest, C. and Schervish, M. J. (1985), “Modelling expert judgements for Bayesian updating,”\\r\\nAnnals of Statistics, 13, 1198–1212.\\r\\nGeweke, J. and Amisano, G. G. (2012), “Prediction with misspecified models,” The American Eco\\x02nomic Review, 102, 482–486.\\r\\nGeweke, J. F. and Amisano, G. G. (2011), “Optimal prediction pools,” Journal of Econometrics, 164,\\r\\n130–141.\\r\\nGruber, L. F. and West, M. (2016), “GPU-accelerated Bayesian learning in simultaneous graphical\\r\\ndynamic linear models,” Bayesian Analysis, 11, 125–149.\\r\\n18\\n— (2017), “Bayesian forecasting and scalable multivariate volatility analysis using simultaneous\\r\\ngraphical dynamic linear models,” Econometrics and Statistics, 3, 3–22, arXiv:1606.08291.\\r\\nHall, S. G. and Mitchell, J. (2007), “Combining density forecasts,” International Journal of Forecast\\x02ing, 23, 1–13.\\r\\nHoogerheide, L., Kleijn, R., Ravazzolo, F., Van Dijk, H. K., and Verbeek, M. (2010), “Forecast accu\\x02racy and economic gains from Bayesian model averaging using time-varying weights,” Journal of\\r\\nForecasting, 29, 251–269.\\r\\nJore, A. S., Mitchell, J., and Vahey, S. P. (2010), “Combining forecast densities from VARs with\\r\\nuncertain instabilities,” Journal of Applied Econometrics, 25, 621–634.\\r\\nKapetanios, G., Mitchell, J., Price, S., and Fawcett, N. (2015), “Generalised density forecast combi\\x02nations,” Journal of Econometrics, 188, 150–165.\\r\\nKascha, C. and Ravazzolo, F. (2010), “Combining inflation density forecasts,” Journal of Forecasting,\\r\\n29, 231–250.\\r\\nKoop, G., Korobilis, D., et al. (2010), “Bayesian multivariate time series methods for empirical\\r\\nmacroeconomics,” Foundations and Trends in Econometrics, 3, 267–358.\\r\\nKoop, G., Leon-Gonzalez, R., and Strachan, R. W. (2009), “On the evolution of the monetary policy\\r\\ntransmission mechanism,” Journal of Economic Dynamics and Control, 33, 997–1017.\\r\\nKorobilis, D. (2013), “VAR forecasting using Bayesian variable selection,” Journal of Applied Econo\\x02metrics, 28, 204–230.\\r\\nLindley, D. V., Tversky, A., and Brown, R. V. (1979), “On the reconciliation of probability assess\\x02ments,” Journal of the Royal Statistical Society (Series A: General), 142, 146–180.\\r\\nLopes, H. F. and Tsay, R. S. (2011), “Particle filters and Bayesian inference in financial economet\\x02rics,” Journal of Forecasting, 30, 168–209.\\r\\nMcAlinn, K. and West, M. (2017), “Dynamic Bayesian predictive synthesis in time series forecast\\x02ing,” Journal of Econometrics, forthcoming, arXiv:1601.07463.\\r\\nNakajima, J. (2011), “Time-varying parameter VAR model with stochastic volatility: An overview\\r\\nof methodology and empirical applications,” Monetary and Economic Studies, 29, 107–142.\\r\\nNakajima, J. and West, M. (2013a), “Bayesian analysis of latent threshold dynamic models,” Journal\\r\\nof Business & Economic Statistics, 31, 151–164.\\r\\n— (2013b), “Bayesian dynamic factor models: Latent threshold approach,” Journal of Financial\\r\\nEconometrics, 11, 116–153.\\r\\nPettenuzzo, D. and Ravazzolo, F. (2016), “Optimal portfolio choice under decision-based model\\r\\ncombinations,” Journal of Applied Econometrics, 31, 1312–1332.\\r\\n19\\nPrado, R. and West, M. (2010), Time Series: Modelling, Computation & Inference, Chapman &\\r\\nHall/CRC Press.\\r\\nPrimiceri, G. E. (2005), “Time varying structural vector autoregressions and monetary policy,” Re\\x02view of Economic Studies, 72, 821–852.\\r\\nSims, C. A. (1993), “A nine-variable probabilistic macroeconomic forecasting model,” in Business\\r\\nCycles, Indicators and Forecasting, eds. Stock, J. H. and Watson, M. W., University of Chicago\\r\\nPress, pp. 179–212.\\r\\nSims, C. A. and Zha, T. (1998), “Bayesian methods for dynamic multivariate models,” International\\r\\nEconomic Review, 96, 949–968.\\r\\nStock, J. H. and Watson, M. W. (1996), “Evidence on structural instability in macroeconomic time\\r\\nseries relations,” Journal of Business & Economic Statistics, 14, 11–30.\\r\\nTimmermann, A. (2004), “Forecast combinations,” in Handbook of Economic Forecasting, eds. El\\x02liott, G., Granger, C. W. J., and Timmermann, A., North Holland, vol. 1, chap. 4, pp. 135–196.\\r\\nWest, M. (1984), “Bayesian aggregation,” Journal of the Royal Statistical Society (Series A: General),\\r\\n147, 600–607.\\r\\n— (1988), “Modelling expert opinion (with discussion),” in Bayesian Statistics 3, eds. Bernardo,\\r\\nJ. M., DeGroot, M. H., Lindley, D. V., and Smith, A. F. M., Oxford University Press, pp. 493–508.\\r\\n— (1992), “Modelling agent forecast distributions,” Journal of the Royal Statistical Society (Series\\r\\nB: Methodological), 54, 553–567.\\r\\nWest, M. and Crosse, J. (1992), “Modelling of probabilistic agent opinion,” Journal of the Royal\\r\\nStatistical Society (Series B: Methodological), 54, 285–299.\\r\\nWest, M. and Harrison, P. J. (1997), Bayesian Forecasting & Dynamic Models, Springer Verlag, 2nd\\r\\ned.\\r\\nZellner, A. (1962), “An efficient method of estimating seemingly unrelated regressions and tests for\\r\\naggregation bias,” Journal of the American Statistical Association, 57, 348–368.\\r\\nZhou, X., Nakajima, J., and West, M. (2014), “Bayesian forecasting and portfolio decisions using\\r\\ndynamic dependent sparse factor models,” International Journal of Forecasting, 30, 963–980.\\r\\n20\\nMultivariate Bayesian Predictive Synthesis\\r\\nin Macroeconomic Forecasting\\r\\nKenichiro McAlinn, Knut Are Aastveit, Jouchi Nakajima & Mike West\\r\\nTables and Figures\\r\\nTable 1: US macroeconomic forecasting 2001/1-2015/12: 1-step ahead forecast evaluations for\\r\\nmonthly US macroeconomic series over the 15 years 2001/1-2015/12, comparing mean squared\\r\\nforecast errors and log predictive density ratios for this T = 180 months. The column % denotes\\r\\nimprovements over the standard BPS model. Note: LPDR1:T is relative to the standard BPS model.\\r\\nMSFE1:T\\r\\n1-step Infl % Wage % Unemp %\\r\\nVAR(1) 0.0141 −8.22 0.1444 −35.91 0.0206 0.74\\r\\nVAR(12) 0.0160 −22.93 0.1110 −4.44 0.0230 −10.73\\r\\nVAR(3) 0.0147 −13.24 0.1105 −3.96 0.0219 −5.67\\r\\nVAR(1:3:9) 0.0135 −3.76 0.1198 −12.77 0.0222 −7.18\\r\\nVAR(1:6:12) 0.0137 −5.14 0.1449 −36.40 0.0215 −3.70\\r\\nBMA 0.0146 −12.20 0.1111 −4.53 0.0218 −5.26\\r\\nBPS 0.0130 − 0.1063 − 0.0207 −\\r\\nMSFE1:T\\r\\n1-step Cons % Invest % Interest %\\r\\nVAR(1) 0.3908 −3.50 13.2183 −2.99 0.0275 −35.07\\r\\nVAR(12) 0.4697 −24.41 15.3571 −19.65 0.0246 −20.92\\r\\nVAR(3) 0.3982 −5.48 13.3210 −3.79 0.0211 −3.74\\r\\nVAR(1:3:9) 0.4049 −7.25 13.8918 −8.24 0.0204 −0.55\\r\\nVAR(1:6:12) 0.3889 −3.02 13.4301 −4.64 0.0228 −12.02\\r\\nBMA 0.3971 −5.18 13.2145 −2.96 0.0215 −5.80\\r\\nBPS 0.3775 − 12.8346 − 0.0203 −\\r\\n1-step LPDR1:T\\r\\nVAR(1) −77.25\\r\\nVAR(12) −103.82\\r\\nVAR(3) −31.00\\r\\nVAR(1:3:9) −34.22\\r\\nVAR(1:6:12) −52.69\\r\\nBMA −32.48\\r\\nBPS −\\r\\n21\\nTable 2: US macroeconomic forecasting 2001/1-2015/12: 12-step ahead forecast evaluations for\\r\\nmonthly US macroeconomic series over the 15 years 2001/1-2015/12, comparing mean squared\\r\\nforecast errors and log predictive density ratios for this T = 180 months. The column % denotes\\r\\nimprovements over BPS(12). Note: LPDR1:T is relative to BPS(12).\\r\\nMSFE1:T\\r\\n12-step Infl % Wage % Unemp %\\r\\nVAR(1) 0.5317 −143.15 0.4453 19.50 1.2028 −10.66\\r\\nVAR(12) 0.4272 −95.35 0.7750 −40.12 1.6918 −55.65\\r\\nVAR(3) 0.5789 −164.74 0.5215 5.72 1.1788 −8.45\\r\\nVAR(1:3:9) 0.4541 −107.69 1.1207 −102.62 1.6353 −50.46\\r\\nVAR(1:6:12) 0.5342 −144.30 0.8934 −61.52 1.3585 −24.99\\r\\nBPS(12) 0.2187 − 0.5531 − 1.0869 −\\r\\nMSFE1:T\\r\\n12-step Cons % Invest % Interest %\\r\\nVAR(1) 7.2471 −23.21 7067.67 −65.55 5.5916 −68.74\\r\\nVAR(12) 18.4145 −213.07 8824.02 −106.68 6.1707 −86.22\\r\\nVAR(3) 7.3142 −24.35 6378.42 −49.40 4.8222 −45.52\\r\\nVAR(1:3:9) 10.3823 −76.51 9111.99 −113.43 4.6622 −40.69\\r\\nVAR(1:6:12) 10.1116 −71.91 10013.45 −134.54 7.4612 −125.16\\r\\nBPS(12) 5.8818 − 4269.33 − 3.3137 −\\r\\n12-step LPDR1:T\\r\\nVAR(1) −119.05\\r\\nVAR(12) −535.09\\r\\nVAR(3) −366.85\\r\\nVAR(1:3:9) −463.46\\r\\nVAR(1:6:12) −361.20\\r\\nBPS(12) −\\r\\n22\\nTable 3: US macroeconomic forecasting 2001/1-2015/12: 24-step ahead forecast evaluations for\\r\\nmonthly US macroeconomic series over the 15 years 2001/1-2015/12, comparing mean squared\\r\\nforecast errors and log predictive density ratios for this T = 180 months. The column % denotes\\r\\nimprovements over BPS(24). Note: LPDR1:T is relative to BPS(24).\\r\\nMSFE1:T\\r\\n24-step Infl % Wage % Unemp %\\r\\nVAR(1) 3.9536 −331.10 2.4117 7.71 16.46 −55.68\\r\\nVAR(12) 2.7373 −198.47 4.5054 −72.41 18.32 −73.28\\r\\nVAR(3) 3.8504 −319.85 3.1877 −21.98 13.78 −30.35\\r\\nVAR(1:3:9) 4.8627 −430.23 8.8723 −239.52 21.06 −99.17\\r\\nVAR(1:6:12) 4.4141 −381.32 8.4162 −222.06 16.99 −60.65\\r\\nBPS(24) 0.9171 − 2.6132 − 10.58 −\\r\\nMSFE1:T\\r\\n24-step Cons % Invest % Interest %\\r\\nVAR(1) 56.27 −104.54 51937 −776.23 31.68 −480.56\\r\\nVAR(12) 118.09 −329.23 38151 −543.65 25.89 −374.58\\r\\nVAR(3) 46.80 −70.09 39671 −569.30 21.84 −300.31\\r\\nVAR(1:3:9) 78.73 −186.15 80278 −1254.37 25.41 −365.71\\r\\nVAR(1:6:12) 72.54 −163.67 86671 −1362.23 62.16 −1039.32\\r\\nBPS(24) 27.51 − 5927 − 5.46 −\\r\\n24-step LPDR1:T\\r\\nVAR(1) −445.81\\r\\nVAR(12) −489.98\\r\\nVAR(3) −462.48\\r\\nVAR(1:3:9) −808.31\\r\\nVAR(1:6:12) −804.49\\r\\nBPS(24) −\\r\\n23\\nFigure 1: US macroeconomic data 1986/1-2015/12: US macroeconomic time series (indices ×100\\r\\nfor % basis): annual inflation rate (p), wage (w), unemployment rate (u), consumption (c), invest\\x02ment (i), and short-term nominal interest rate (r).\\r\\n24\\nFigure 2: US macroeconomic forecasting 2001/1-2015/12: Mean squared 1-step ahead forecast\\r\\nerrors MSFE1:t(1) of inflation (p) sequentially revised at each of the t = 1:180 months.\\r\\nFigure 3: US macroeconomic forecasting 2001/1-2015/12: 1-step ahead log predictive density\\r\\nratios LPDR1:t(1) sequentially revised at each of the t = 1:180 months. The baseline at 0 over all t\\r\\ncorresponds to the standard BPS model.\\r\\n25\\nFigure 4: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS model\\r\\ncoefficients for inflation (p) sequentially computed at each of the t = 1:180 months.\\r\\nFigure 5: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS model\\r\\ncoefficients for wage (w)sequentially computed at each of the t = 1:180 months.\\r\\n26\\nFigure 6: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS model\\r\\ncoefficients for unemployment (u) sequentially computed at each of the t = 1:180 months.\\r\\nFigure 7: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS model\\r\\ncoefficients for consumption (c) sequentially computed at each of the t = 1:180 months.\\r\\n27\\nFigure 8: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS model\\r\\ncoefficients for investment (i) sequentially computed at each of the t = 1:180 months.\\r\\nFigure 9: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS model\\r\\ncoefficients for interest rate (r) sequentially computed at each of the t = 1:180 months.\\r\\n28\\nFigure 10: US macroeconomic forecasting 2001/1-2015/12: 1-step ahead forecast standard devia\\x02tions sequentially computed at each of the t = 1:180 months.\\r\\n29\\nFigure 11: US macroeconomic forecasting 2001/1-2015/12: Retrospective posterior correlations\\r\\nof latent agent factors at 2003/12.\\r\\nFigure 12: US macroeconomic forecasting 2001/1-2015/12: Retrospective posterior correlations\\r\\nof latent agent factors at 2008/12.\\r\\n30\\nFigure 13: US macroeconomic forecasting 2001/1-2015/12: Retrospective posterior correlations\\r\\nof latent agent factors at 2013/12.\\r\\nFigure 14: US macroeconomic forecasting 2001/1-2015/12: Retrospective Kullback-Leibler diver\\x02gence between prior agent forecasts and posterior agent forecasts computed at each of the t = 1:180\\r\\nmonths.\\r\\n31\\nFigure 15: US macroeconomic forecasting 2001/1-2015/12: Mean squared 12-step ahead forecast\\r\\nerrors MSFE1:t(12) of inflation (p) sequentially revised at each of the t = 1:180 months.\\r\\nFigure 16: US macroeconomic forecasting 2001/1-2015/12: 12-step ahead log predictive density\\r\\nratios LPDR1:t(12) sequentially revised at each of the t = 1:180 months. The baseline at 0 over all t\\r\\ncorresponds to BPS(12).\\r\\n32\\nFigure 17: US macroeconomic forecasting 2001/1-2015/12: Mean squared 24-step ahead forecast\\r\\nerrors MSFE1:t(24) of inflation (p) sequentially revised at each of the t = 1:180 months.\\r\\nFigure 18: US macroeconomic forecasting 2001/1-2015/12: 24-step ahead log predictive density\\r\\nratios LPDR1:t(24) sequentially revised at each of the t = 1:180 months. The baseline at 0 over all t\\r\\ncorresponds to BPS(24).\\r\\n33\\nFigure 19: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for inflation (p) sequentially computed at each of the t = 1:180 months.\\r\\nFigure 20: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for wage (w)sequentially computed at each of the t = 1:180 months.\\r\\n34\\nFigure 21: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for unemployment (u) sequentially computed at each of the t = 1:180 months.\\r\\nFigure 22: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for consumption (c) sequentially computed at each of the t = 1:180 months.\\r\\n35\\nFigure 23: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for investment (i) sequentially computed at each of the t = 1:180 months.\\r\\nFigure 24: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for interest rate (r) sequentially computed at each of the t = 1:180 months.\\r\\n36\\nMultivariate Bayesian Predictive Synthesis\\r\\nin Macroeconomic Forecasting\\r\\nKenichiro McAlinn, Knut Are Aastveit, Jouchi Nakajima, & Mike West\\r\\nSupplementary Material\\r\\nA Appendix: Summary of MCMC for Dynamic BPS\\r\\nA.1 Overview and Initialization\\r\\nThis appendix summarizes algorithmic details of implementation of the MCMC computations for\\r\\ndynamic BPS model fitting of Section 2.4. This involves a standard set of steps in a customized\\r\\nthree-component block Gibbs sampler: the first component samples the latent agent states, the\\r\\nsecond, the second samples the dynamic BPS model states/parameters, and the third component\\r\\nsamples the observation variance. The latter two involves a modified FFBS algorithm central to\\r\\nMCMC in all conditionally normal DLMs (Fruhwirth-Schnatter ¨ 1994; West and Harrison 1997, Sect\\r\\n15.2; Prado and West 2010, Sect 4.5).\\r\\nIn our sequential learning and forecasting context, the full MCMC analysis is performed anew\\r\\nat each time point as time evolves and new data are observed. We detail MCMC steps for analysis\\r\\nbased on data over times t = 1:T for any chosen T.\\r\\nStanding at time t = 0, the decision maker has initial information summarized via in terms\\r\\nof θ0 ∼ N(m0, C0) and V 0 ∼ IW(n0, D0), independently. Here the q × q variance matrix V 0\\r\\nhas the inverse Wishart distribution with n > 0 degrees of freedom and prior “sum-of-squares”\\r\\nmatrix D0. Equivalently, the precision matrix V\\r\\n−1\\r\\n0 ∼ W(h0, D−10\\r\\n), the Wishart distribution with\\r\\nh0 = n0 + q − 1 and mean h0D−1\\r\\n0\\r\\nso that the initial estimate D0/h0 is the prior harmonic mean of\\r\\nV 0. Model specification is completed with two chosen discount factors: β, defining the extent of\\r\\ntime variation in the evolution of the states θt, and δ defining levels of variation in the evolution of\\r\\nthe volatility matrices V t.\\r\\nAt time T, the decision maker has accrued information {y1:T , H1:T }. The MCMC analysis is then\\r\\nrun iteratively as follows.\\r\\nInitialization: First, initialize by setting\\r\\nFt =\\r\\n\\uf8eb\\r\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\r\\n1 f\\r\\n0\\r\\nt1 0 0\\r\\n0\\r\\n· · · · · · 0 0\\r\\n0\\r\\n0 0\\r\\n0 1 f\\r\\n0\\r\\nt2\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n0 · · · · · · · · · · · · · · · 1 f\\r\\n0\\r\\ntq\\r\\n\\uf8f6\\r\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\r\\n, (7)\\r\\nfor each t = 1:T, with elements set at some chosen initial values of the latent agent states. Initial\\r\\nvalues can be chosen arbitrarily. One obvious and appropriate choice– our recommended default\\r\\n1\\nchoice– is to simply generate agent states from their priors, i.e., from the agent forecast distribu\\x02tions, xtj ∼ htj (xtj ) independently for all t = 1:T and j = 1:J. This is easily implemented in cases\\r\\nwhen the agent forecasts are T or normal distributions, or can be otherwise directly sampled; we\\r\\nuse this in our analyses reported in the paper, and recommend it as standard. An obvious alterna\\x02tive initialization is to simply set xtj = yt\\r\\nfor each t, j, though we prefer to initialize with some\\r\\ninherent dispersion in starting values. Ultimately, since the MCMC is rapidly convergent, choice of\\r\\ninitial values is not critical. Given initial values of agent factor vector xtj = (xt1j , xt2j , ..., xtqj )\\r\\n0\\r\\nfor\\r\\neach agent j = 1:J and each time t, the Ft matrices are initialized with series-specific row entries\\r\\nfrom ftr = (xtr1, xtr2, ..., xtrJ )\\r\\n0\\r\\nfor each r = 1:q.\\r\\nA.2 Three Sampling Steps in Each MCMC Iterate\\r\\nFollowing initialization, the MCMC iterates repeatedly to resample three sets of conditional poste\\x02riors to generate the MCMC samples from the target posterior p(X1:T , θ1:T ,V 1:T |y1:T\\r\\n, H1:T ). These\\r\\nconditional posteriors and algorithmic details of their simulation are as follows.\\r\\nA.2.1 Per MCMC Iterate Step 1: Sampling BPS DLM parameters θ1:T\\r\\nConditional on any values of the latent agent states and observation error, we are in the setting of\\r\\na conditionally normal, multivariate DLM with the agent states as known predictors based on their\\r\\nspecific values. The BPS DLM form,\\r\\nyt = Ftθt + νt, νt ∼ N(0,V t),\\r\\nθt = θt−1 + ωt, ωt ∼ N(0,Wt),\\r\\nhas known elements Ft,Wt and specified initial prior at t = 0. The implied conditional posterior\\r\\nfor θ1:T then does not depend on H1:T , reducing to p(θ1:T |X1:T ,V 1:T , y1:T). This is simulated using\\r\\nthe efficient and standard FFBS algorithm (e.g. Fruhwirth-Schnatter ¨ 1994; West and Harrison 1997,\\r\\nSect 15.2; Prado and West 2010, Sect 4.5). In detail, this proceeds as follows.\\r\\nForward filtering: For each t = 1:T in sequence, perform the standard one-step filtering\\r\\nupdates to compute and save the sequence of sufficient statistics for the on-line posteriors\\r\\np(θt|X1:t,V 1:t, y1:t) at each t. The summary technical details are as follows:\\r\\n1. Time t − 1 posterior:\\r\\nθt−1|X1:t−1,V 1:t−1, y1:t−1 ∼ N(mt−1, Ct−1),\\r\\nwith point estimate mt−1 of θt−1.\\r\\n2. Update to time t prior:\\r\\nθt|X1:t−1,V 1:t−1, y1:t−1 ∼ N(mt−1, Rt) with Rt = Ct−1/δ,\\r\\n2\\nwith (unchanged) point estimates mt−1 of θt\\r\\n, but with increased uncertainty relative to\\r\\nthe time t−1 posteriors, the level of increased uncertainty being defined by the discount\\r\\nfactors.\\r\\n3. 1-step predictive distribution: yt|X1:t,V 1:t, y1:t−1 ∼ Tβnt−1(ft, Qt) where\\r\\nft = Ftmt−1 and Qt = FtRtF\\r\\n0\\r\\nt + V t\\r\\n.\\r\\n4. Filtering update to time t posterior:\\r\\nθt|V 1:t, X1:t, y1:t ∼ N(mt, Ct),\\r\\nwith defining parameters mt = mt−1 + Atet and Ct = Rt − AtQtA0\\r\\nt\\r\\n, based on 1-step\\r\\nforecast error et = yt − ft and the state adaptive coefficient vector (a.k.a. “Kalman\\r\\ngain”) At = RtF\\r\\n0\\r\\ntQ−1\\r\\nt\\r\\n.\\r\\nBackward sampling: Having run the forward filtering analysis up to time T, the backward\\r\\nsampling proceeds as follows.\\r\\na. At time T: Simulate θT from the final multivariate normal posterior\\r\\np(θT |X1:T ,V 1:T , y1:T) = N(mT , CT ).\\r\\nb. Recurse back over times t = T − 1, T − 2, . . . , 0 : At each time t, simulate the state\\r\\nθt from the conditional posterior p(θt|θt+1, X1:t,V 1:t, y1:t); this is multivariate normal\\r\\nwith mean vector mt + δ(θt+1 − mt) and variance matrix Ct(1 − δ).\\r\\nA.2.2 Per MCMC Iterate Step 2: Sampling BPS DLM parameters V 1:T\\r\\nConditional on the sampled values of the BPS DLM parameters θ1:T and latent agent states X1:T ,\\r\\nthe next step in the MCMC iterate samples the full conditional posterior of the sequence of volatility\\r\\nmatrices, generating a draw from p(V 1:T |X1:T , θ1:T , y1:T).\\r\\nForward filtering: For each t = 1:T in sequence, update and save the forward filtering\\r\\nsummaries (nt, Dt) of on-line posteriors\\r\\nV t|X1:t, θ1:t, y1:t ∼ IW(nt, Dt),\\r\\ngiven by nt = ht − q + 1 where ht = βht−1 + 1, and Dt = βDt−1 + (yt − F\\r\\n0\\r\\ntθt)(yt − F\\r\\n0\\r\\ntθt)\\r\\n0\\r\\n.\\r\\nBackwards sampling: Having run the forward filtering analysis up to time T, the backward\\r\\nsampling proceeds as follows.\\r\\na. At time T: Simulate V T from the final inverse Wishart posterior IW(nT , DT ).\\r\\n3\\nb. Recurse back over times t = T −1, T −2, . . . , 0 : At time t, sample V t from the conditional\\r\\nposterior p(V t|V t+1, X1:t, θ1:t, y1:t). Algorithmically, this is achieved via\\r\\nV\\r\\n−1\\r\\nt = βV\\r\\n−1\\r\\nt+1 + Υt where Υt ∼ W((1 − β)ht\\r\\n, D−1\\r\\nt\\r\\n),\\r\\nand where the Υt are independent over t.\\r\\nA.2.3 Per MCMC Iterate Step 3: Sampling the latent agent states X1:T\\r\\nConditional on most recently sampled values of the BPS DLM parameters Φ1:T , the MCMC it\\x02erate completes with resampling of the latent agent states from their full conditional posterior\\r\\np(X1:t|Φ1:t, y1:t, H1:t). It is immediate that the Xt are conditionally independent over time t in this\\r\\nconditional distribution, with time t conditionals\\r\\np(Xt|Φt, yt, Ht) ∝ N(yt|Ftθt,V t)\\r\\nY\\r\\nj=1:J\\r\\nhtj (xtj ). (8)\\r\\nSeveral comments are relevant to studies with different forms of the agent forecast densities.\\r\\n1. Multivariate normal agent forecast densities: In cases when each of the agent forecast densities\\r\\nis normal, the posterior in eqn. (8) yields a multivariate normal distribution for vectorized\\r\\nXt. Computation of its defining parameters and then drawing a new sample vector Xt are\\r\\ntrivial.\\r\\n2. In some cases, as in our study in this paper, the agent forecast densities will be those of\\r\\nStudent T distributions. In our case study the five agents represent conjugate exchangeable\\r\\ndynamic linear models in which all forecast densities are multivariate T, with parameters\\r\\nvarying over time and with step-ahead forecast horizon. In such cases, standard Bayesian aug\\x02mentation methods apply to enable simulation. Each multivariate T distribution is expressed\\r\\nas a scale mixture of multivariate normals, with the mixing scale parameters introduced as\\r\\ninherent latent variables with inverse gamma distributions. This expansion of the parameter\\r\\nspace makes the multivariate T distributions conditional multivariate normals, and the mix\\x02ing scales are resampled (from implied conditional posterior inverse gamma distributions)\\r\\neach MCMC iterate along with the agent states. This is again a standard MCMC approach\\r\\nand much used in Bayesian time series, as in other areas (e.g. Fruhwirth-Schnatter ¨ 1994;\\r\\nWest and Harrison 1997, Chap 15). Then, conditional on the current values of these latent\\r\\nscales, sampling the Xt reduces technically to that conditional normals above.\\r\\nSpecifically, suppose that htj (xtj ) is density of the normal Tntj (htj , Htj ); the notation means\\r\\nthat (xtj − htj )/\\r\\np\\r\\nHtj has a standard multivariate Student T distribution with ntj degrees\\r\\nof freedom. Then latent scale factors φtj exist such that: (i) conditional on φtj , latent\\r\\nagent factor xtj has a conditional multivariate normal density xtj |φtj ∼ N(htj , Htj/φtj )\\r\\nindependently over t, j; (ii) the φtj are independent over t, j with gamma distributions,\\r\\nφtj ∼ G(ntj/2, ntj/2). Then, at each MCMC step, the above normal update for latent agent\\r\\n4\\nstates is replaced by normal simulations conditional on the φtj . Following this, we resample\\r\\nvalues of the φtj from their trivially implied conditional gamma posteriors.\\r\\n3. In some cases, agent densities may be more elaborate mixtures of normals, such as (dis\\x02crete or continuous) location and/or scale mixtures that represent asymmetric distributions.\\r\\nThe same augmentation strategy can be applied in such cases, with augmented parameters\\r\\nincluding location shifts in place of, or in addition to, scale shifts.\\r\\n4. In other cases, we may be able to directly simulate the agent forecast distributions and eval\\x02uate forecast density functions at any point, but do not have access to analytic forms. One\\r\\nclass of examples is when the agents are simulation models, e.g., DSGE models. Another\\r\\ninvolves forecasts in terms of histograms. In such cases, MCMC will proceed using some form\\r\\nof Metropolis-Hastings algorithm, or accept/reject methods, or importance sampling for the\\r\\nlatent agent states.\\r\\nFor example, suppose we only have access to simulations from the agent forecast distribu\\x02tions, in terms of I independent draws from each collated in the simulated matrix X\\r\\n(i)\\r\\nt\\r\\nfor\\r\\ni = 1:I. We can apply importance sampling as follows: (a) compute the marginal likelihood\\r\\nvalues p(yt|Φt, X\\r\\n(i)\\r\\nt\\r\\n, Ht) for each i = 1:I; (b) compute and normalize the implied importance\\r\\nsampling weights wti ∝ N(yt|Φt, X\\r\\n(i)\\r\\nt\\r\\n, Ht), and then (c) resample latent agent states for this\\r\\nMCMC stage according to the probabilities these weights define.\\r\\n5\\nB Appendix: Additional Graphical Summaries from Macroeconomic\\r\\nAnalysis\\r\\nThis appendix lays out additional graphical summaries of results from the macroeconomic forecast\\x02ing analysis in the paper, providing material supplementary to that discussed in Section 3.\\r\\nFigure C1: US macroeconomic forecasting 2001/1-2015/12: Mean squared 1-step ahead forecast\\r\\nerrors MSFE1:t(1) of wage (w) sequentially revised at each of the t = 1:180 months.\\r\\nFigure C2: US macroeconomic forecasting 2001/1-2015/12: Mean squared 1-step ahead forecast\\r\\nerrors MSFE1:t(1) of unemployment rate (u) sequentially revised at each of the t = 1:180 months.\\r\\n6\\nFigure C3: US macroeconomic forecasting 2001/1-2015/12: Mean squared 1-step ahead forecast\\r\\nerrors MSFE1:t(1) of consumption (c) sequentially revised at each of the t = 1:180 months.\\r\\nFigure C4: US macroeconomic forecasting 2001/1-2015/12: Mean squared 1-step ahead forecast\\r\\nerrors MSFE1:t(1) of investment (i) sequentially revised at each of the t = 1:180 months.\\r\\n7\\nFigure C5: US macroeconomic forecasting 2001/1-2015/12: Mean squared 1-step ahead forecast\\r\\nerrors MSFE1:t(1) of interest rate (r) sequentially revised at each of the t = 1:180 months.\\r\\n8\\nFigure C6: US macroeconomic forecasting 2001/1-2015/12: Mean squared 12-step ahead forecast\\r\\nerrors MSFE1:t(12) of wage (w) sequentially revised at each of the t = 1:180 months.\\r\\nFigure C7: US macroeconomic forecasting 2001/1-2015/12: Mean squared 12-step ahead forecast\\r\\nerrors MSFE1:t(12) of unemployment rate (u) sequentially revised at each of the t = 1:180 months.\\r\\n9\\nFigure C8: US macroeconomic forecasting 2001/1-2015/12: Mean squared 12-step ahead forecast\\r\\nerrors MSFE1:t(12) of consumption (c) sequentially revised at each of the t = 1:180 months.\\r\\nFigure C9: US macroeconomic forecasting 2001/1-2015/12: Mean squared 12-step ahead forecast\\r\\nerrors MSFE1:t(12) of investment (i) sequentially revised at each of the t = 1:180 months.\\r\\n10\\nFigure C10: US macroeconomic forecasting 2001/1-2015/12: Mean squared 12-step ahead fore\\x02cast errors MSFE1:t(12) of interest rate (r) sequentially revised at each of the t = 1:180 months.\\r\\n11\\nFigure C11: US macroeconomic forecasting 2001/1-2015/12: Mean squared 24-step ahead fore\\x02cast errors MSFE1:t(24) of wage (w) sequentially revised at each of the t = 1:180 months.\\r\\nFigure C12: US macroeconomic forecasting 2001/1-2015/12: Mean squared 24-step ahead fore\\x02cast errors MSFE1:t(24) of unemployment rate (u) sequentially revised at each of the t = 1:180\\r\\nmonths.\\r\\n12\\nFigure C13: US macroeconomic forecasting 2001/1-2015/12: Mean squared 24-step ahead fore\\x02cast errors MSFE1:t(24) of consumption (c) sequentially revised at each of the t = 1:180 months.\\r\\nFigure C14: US macroeconomic forecasting 2001/1-2015/12: Mean squared 24-step ahead fore\\x02cast errors MSFE1:t(24) of investment (i) sequentially revised at each of the t = 1:180 months.\\r\\n13\\nFigure C15: US macroeconomic forecasting 2001/1-2015/12: Mean squared 24-step ahead fore\\x02cast errors MSFE1:t(24) of interest rate (r) sequentially revised at each of the t = 1:180 months.\\r\\n14\\nFigure C16: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(12)\\r\\nmodel coefficients for inflation (p) sequentially computed at each of the t = 1:180 months.\\r\\nFigure C17: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(12)\\r\\nmodel coefficients for wage (w)sequentially computed at each of the t = 1:180 months.\\r\\n15\\nFigure C18: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(12)\\r\\nmodel coefficients for unemployment (u) sequentially computed at each of the t = 1:180 months.\\r\\nFigure C19: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(12)\\r\\nmodel coefficients for consumption (c) sequentially computed at each of the t = 1:180 months.\\r\\n16\\nFigure C20: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(12)\\r\\nmodel coefficients for investment (i) sequentially computed at each of the t = 1:180 months.\\r\\nFigure C21: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(12)\\r\\nmodel coefficients for interest rate (r) sequentially computed at each of the t = 1:180 months.\\r\\n17\\nFigure C22: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for inflation (p) sequentially computed at each of the t = 1:180 months.\\r\\nFigure C23: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for wage (w)sequentially computed at each of the t = 1:180 months.\\r\\n18\\nFigure C24: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for unemployment (u) sequentially computed at each of the t = 1:180 months.\\r\\nFigure C25: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for consumption (c) sequentially computed at each of the t = 1:180 months.\\r\\n19\\nFigure C26: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for investment (i) sequentially computed at each of the t = 1:180 months.\\r\\nFigure C27: US macroeconomic forecasting 2001/1-2015/12: On-line posterior means of BPS(24)\\r\\nmodel coefficients for interest rate (r) sequentially computed at each of the t = 1:180 months.\\r\\n20'},\n",
       " {'name': '2205.01593v2.pdf',\n",
       "  'content': 'Causal Regularization\\r\\nOn the trade-off between in-sample risk and out-of-sample risk guarantees\\r\\nLucas Kania∗1and Ernst Wit†2\\r\\n1Carnegie Mellon University\\r\\n2Universit´a della Svizzera italiana\\r\\nAbstract\\r\\nIn recent decades, several data analytic ways of dealing with causality have been\\r\\nintroduced, such as propensity score matching, the PC algorithm and invariant causal\\r\\nprediction. Although originally hailed for their interpretational appeal, here we study\\r\\nthe identification of causal-like models from in-sample data that provide out-of-sample\\r\\nrisk guarantees when predicting a target variable from a set of covariates.\\r\\nWhereas ordinary least squares provides the best in-sample risk with limited out-of\\x02sample guarantees, causal models have the best out-of-sample guarantees by sacrificing\\r\\nin-sample risk performance. We introduce causal regularization, by defining a trade\\x02off between these properties. As the regularization increases, causal regularization\\r\\nprovides estimators whose risk is more stable at the cost of increasing their overall\\r\\nin-sample risk. The increased risk stability is shown to result in out-of-sample risk\\r\\nguarantees. We provide finite sample risk bounds for all models and prove the adequacy\\r\\nof cross-validation for attaining these bounds.\\r\\n∗\\r\\nlucaskania@cmu.edu\\r\\n†\\r\\nernst.jan.camiel.wit@usi.ch\\r\\n1\\r\\narXiv:2205.01593v2 [stat.ME] 14 Jul 2023\\n1 Introduction\\r\\nIn many settings, the goal is to obtain models from in-sample data that are highly predictive\\r\\nfor out-of-sample data. Heterogeneous in-sample data is oftentimes detrimental to discover\\x02ing models that do not overfit [Hernan and Robins, 2010]. However, when the heterogeneity\\r\\nin the sample can be attributed to a covariate shift that does not affect the functional rela\\x02tionship between the target and the covariates, the invariance of that structure implies that\\r\\nthe causal model provides predictions that are robust to other future covariate shifts. Hence,\\r\\nthe functional causal model is an interesting inferential goal also from a purely predictive\\r\\npoint of view.\\r\\nIf the source of the covariate shift is known, instrumental variables [Didelez et al., 2010,\\r\\nImbens, 2014] can be constructed. Alternatively, if the sample can be split into sub-samples\\r\\nthat isolate different instances of the covariate shift, the causal model provides invariant\\r\\npredictions regardless of the sub-sample. Under linearity and non-confounding assumptions,\\r\\nPeters et al. [2016] noted that regressing the target on its direct causes returns a model that\\r\\nis invariant to the covariate shift across the sub-samples. They proposed an algorithm that\\r\\nregresses the target and all possible subsets of covariates in each sub-sample and tests which\\r\\nregression returns the same model regardless of the sub-sample. Under several heterogeneity\\r\\nsources, the authors proved that the algorithm identifies the causal model. Unfortunately,\\r\\ntheir search algorithm suffers from a combinatorial explosion and does not allow for con\\x02founding.\\r\\nArjovsky et al. [2019] resolves some of these issues by approximating the search algorithm,\\r\\nwhich results in more restricted guarantees regarding the identification of the causal model\\r\\nunder linearity [Rosenfeld et al., 2020]. Rothenh¨ausler et al. [2019] avoids the combinatorial\\r\\nsearch by limiting the source of heterogeneity. They model the covariate shift as a system of\\r\\nstructural equations (SEM) being shifted by an instrument. In that scenario, the covariance\\r\\nbetween the covariates and the residuals under the causal model remains invariant across\\r\\ndatasets. Given access to two datasets with different shifts, the difference between these\\r\\ninvariant covariances provides a moment condition that uniquely identifies the causal model\\r\\nas long as the covariate shifts affect enough variables in the SEM. Estimating the moment\\r\\ncondition gives rise to the causal Dantzig estimator. If the solution is not unique, the\\r\\nauthors proposed an L1 penalty to select a sparse approximation of the causal model. Under\\r\\nsome stringent assumptions on the regularization parameter, finite sample bounds can be\\r\\nobtained for the distance between the causal model and the regularized estimator, but no\\r\\nrisk guarantees were given for it.\\r\\nRojas-Carulla et al. [2018] showed that the prediction invariance of the causal model\\r\\nimplies that it minimizes the maximum risk over all possible covariate shifts. Rothenh¨ausler\\r\\net al. [2021] generalized that notion under the assumption that the covariate shift originates\\r\\nfrom an instrumental variable. Their key result is that the out-of-sample risk depends on\\r\\nthe correlation between the residuals and the instrument. The more uncorrelated they are,\\r\\n2\\nthe stronger the out-of-sample risk guarantees under unseen covariate shifts, whereby the\\r\\ncausal model provides the best guarantees. Assuming access to the instrumental variable\\r\\nthat generates the covariate shift, the authors propose to build a regularized estimator, called\\r\\nanchor regression, that regulates the amount of correlation between the residuals and the\\r\\ninstrumental variable. From that correlation, the out-of-sample guarantees and finite sample\\r\\nbounds of all regularized models follow. Ideally, a regularized model should be chosen based\\r\\non subject matter knowledge about the maximum future shift of the data. If it is not known,\\r\\ncross-validation is recommended, albeit no proof was provided regarding its adequacy for the\\r\\nproposed loss. Follow-up work extended the method to noisy instrumental variables [Oberst\\r\\net al., 2021] and discrete and censored outcomes [Kook et al., 2021].\\r\\nIn this work, we consider the multiple datasets setting used in Rothenh¨ausler et al.\\r\\n[2019], but one in which there is no access to instrumental variables. We propose causal\\r\\nregularization for obtaining estimators that progressively decrease the risk bound on shifted\\r\\nout-of-sample datasets, akin to anchor regression. Unlike it, the instrument can be unknown,\\r\\nand the estimators are identifiable even when the unknown instrument does not shift all\\r\\ncovariates. We provide finite sample risk bounds for all regularized models and prove the\\r\\nadequacy of cross-validation and data splitting for attaining these bounds.\\r\\nIt is worth remarking that the search for models that provide out-of-sample guarantees as\\r\\na consequence of in-sample invariance parallels the idea of algorithmic stability [Villa et al.,\\r\\n2013]. The main difference is that the latter studies out-of-sample risk bounds under the\\r\\nassumption that the training and test data come from the same distribution. Algorithmic\\r\\nstability requires model stability [Devroye and Wagner, 1979]. This means that under sample\\r\\nperturbation (usually by leaving one observation out) the algorithm returns models that are\\r\\nsimilar, in the sense that their predictions do not differ much on average. Kearns and\\r\\nRon [1999] and Bousquet and Elisseeff [2002] relaxed the stringent requirement of model\\r\\nstability to the requirement of risk stability, i.e., an algorithm returns models that have\\r\\nsimilar risk given perturbed samples. The work of Peters et al. [2016] follows the model\\r\\nstability approach, where they look for a model that has similar predictions under perturbed\\r\\nsub-samples, while Rothenh¨ausler et al. [2021], when using a categorical instrument that\\r\\nindicates the sub-samples, looks for a model that provides similar risk across sub-samples.\\r\\nAlthough the connection with risk stability is not immediate from their results, our work\\r\\nmakes the requirement explicit and helps to elucidate the relationship between the two sub\\x02fields.\\r\\n2 Structural equation model with shifts\\r\\nIn this paper, we assume that the data generating process is given by a system of linear\\r\\nstructural equations (SEM) and that the covariate shift across different datasets is produced\\r\\nby an exogenous random variable.\\r\\n3\\nDefinition 1 (Structural equation model). A SEM(B, A) is defined as the stochastic\\r\\nsolution (X, Y ) of\\r\\n[\\r\\nY\\r\\nX\\r\\n] = B\\r\\nÍÑÏ\\r\\nconstant\\r\\nstructure\\r\\n⋅ [\\r\\nY\\r\\nX\\r\\n] + ϵ\\r\\nÍÑÏ\\r\\nnoise\\r\\n+ [\\r\\n0\\r\\nA\\r\\n]\\r\\nÍ ÒÒÑ ÒÒÏ\\r\\ncovariate\\r\\nshift\\r\\ns.t. B .\\r\\n.= [\\r\\n0 βPA\\r\\nT\\r\\nβCH BX\\r\\n] and ϵ\\r\\n.\\r\\n.= [\\r\\nϵY\\r\\nϵX\\r\\n]\\r\\nwhere X, ϵX, A ∈ R\\r\\np\\r\\nand Y, ϵY ∈ R are random vectors and variables, respectively. The matrix\\r\\nBX ∈ R\\r\\np×p\\r\\nconsists of the interactions among the covariates X, the vector βPA ∈ R\\r\\np\\r\\ndescribes\\r\\nthe causal effects of X on the target Y , and the vector βCH ∈ R\\r\\np\\r\\nare the downstream effects\\r\\nof Y on X.\\r\\nThe noise is identically distributed irrespective of the shift, i.e., for all A we have ϵ ∼\\r\\nL(0, Σ), some distribution with zero first moment and a finite second moment. Additionally,\\r\\nthe shift A is assumed to be uncorrelated with the noise, i.e., C[A, ϵ] = 0. Finally, I − B is\\r\\nrequired to be non-singular in order for SEM(B, A) to be uniquely defined.\\r\\nLet A be the set of all distributions that have a finite second moment, then SEM(B)\\r\\n.\\r\\n.=\\r\\n∪A˜∈A SEM(B, A˜ ) is the set of all the possible realizations of the SEM for a constant structure B\\r\\nwith noise distributed as L(0, Σ). As our focus is on providing risk guarantees when predicting\\r\\nY from X, the SEM definition does not allow for a target shift. This is known as the\\r\\nexclusion restriction in the potential outcomes framework [Rubin, 1974]. This assumption\\r\\ncannot be easily relaxed since a direct target shift affects the identifiability of the SEM.\\r\\nConversely, all proved results in this work can be generalized to the case where the shift and\\r\\nnoise variables are correlated and have a constant covariance over all possible shifts, i.e.,\\r\\nC[A, ϵ] = C[A˜ , ϵ] ∀ A, A˜ ∈ A.\\r\\nSince the target is never directly shifted and the noise is identically distributed regard\\x02less of the shifted distribution, the residuals under the causal model βPA are identically\\r\\ndistributed for any shift distribution, i.e., ∀ A ∈ A\\r\\nY\\r\\nA\\r\\n− βPA\\r\\nT XA\\r\\n= ϵY ∼ LY (0, σ\\r\\n2\\r\\n) independent of A . (1)\\r\\nDenote the risk of the using the model β for (X\\r\\nA\\r\\n, Y\\r\\nA\\r\\n)\\r\\n.\\r\\n.= SEM(B, A) as\\r\\nRA(β) = E[(Y\\r\\nA\\r\\n−β\\r\\nT XA\\r\\n)\\r\\n2\\r\\n]\\r\\nthen due to (1) the risk using the causal parameters RA(βPA) is invariant over A. In other\\r\\nwords, the risk of the causal model is invariant to all shifts A ∈ A. Other models do not have\\r\\nthis property since for β =/ βPA, there exists always a sequence of shifts such that the risk\\r\\nunder β is arbitrarily bad, i.e., ∃{Ak}\\r\\n∞\\r\\nk=1\\r\\ns.t. limk→∞ RAk\\r\\n(β) = ∞. Hence, if the causal model\\r\\nwere able to be estimated from in-sample data, then under that model the out-of-sample risk\\r\\nwould be constant, which would be the best possible guarantee for the out-of-sample risk.\\r\\n4\\n3 Causal regularization\\r\\nStudies with data from well-designed randomized interventions are able to extract causal\\r\\nparameters [Fısher, 1935]. In this paper, we assume we have much more unstructured\\r\\ndata. We consider the setting, in which data from two distributions are available, for\\r\\nexample from two separate studies. In particular, we assume that data is available from\\r\\nthe observational distribution, i.e. (X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n) = SEM(B, 0), and a shifted distribution, i.e.\\r\\n(X\\r\\nA\\r\\n, Y\\r\\nA\\r\\n) = SEM(B, A) s.t. A ≡/ 0. There is no direct access to the instrumental variable,\\r\\ni.e., the intervention, that generated the covariate shift A.\\r\\n3.1 Causal Dantzig\\r\\nRothenh¨ausler et al. [2019] noted that as the noise and shift are uncorrelated, the correlation\\r\\nbetween the predictors and the residual distribution under the causal model is invariant, i.e.,\\r\\nthe covariance C[X, Y − βPA\\r\\nT X] is constant for all (X, Y ) ∈ SEM(B). Consequently, the\\r\\ndifference in this quantity for two arbitrary distributions in SEM(B) is zero. This insight\\r\\ncharacterizes the causal parameter βPA, i.e.,\\r\\nG∆ βPA − Z∆ = 0 (2)\\r\\nwhere G∆\\r\\n.\\r\\n.= E[X\\r\\nA XAT\\r\\n]−E[X\\r\\n0 X0t\\r\\n] captures the shift in the covariates, and Z∆\\r\\n.\\r\\n.= E[X\\r\\nA YA\\r\\n]−\\r\\nE[X\\r\\n0 Y0\\r\\n] captures the shift of the target due to the covariate shift. The causal Dantzig\\r\\nestimator [Rothenh¨ausler et al., 2019] is defined as the solution of\\r\\nβCD\\r\\n.\\r\\n.= arg min\\r\\nβ∈R\\r\\np\\r\\n∥G∆ β − Z∆∥2(3)\\r\\nIt is identifiable and unique, if and only if G∆ is full-rank, which happens if and only if\\r\\nC[A] is full-rank (see proposition 3 in appendix A). In this case βCD = G∆\\r\\n−1\\r\\nZ∆ = βPA. The\\r\\nauthors use the infinity norm instead of the L2 norm but the solutions are equivalent insofar\\r\\nG∆ is not singular.\\r\\nIf random samples (X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n) ∈ R\\r\\nn0 ×(p+1)\\r\\nand (X\\r\\nA\\r\\n, Y\\r\\nA\\r\\n) ∈ R\\r\\nnA ×(p+1)\\r\\nfrom the observational and\\r\\nshifted distributions are available, then the plug-in causal Dantzig estimator can be defined\\r\\nas\\r\\nβˆ\\r\\nCD\\r\\n.\\r\\n.= arg min\\r\\nβ∈R\\r\\np\\r\\n∥G∆ β − Z∆∥2\\r\\nwhere G∆ = X\\r\\nAT\\r\\nX\\r\\nA\\r\\n/ nA − X\\r\\n0\\r\\nX\\r\\n0\\r\\n/ n0 and Z∆ = X\\r\\nAT\\r\\nY\\r\\nA\\r\\n/ nA − X\\r\\n0T\\r\\nY\\r\\n0\\r\\n/ n0 are the plug-in esti\\x02mators of G∆ and Z∆. If G∆ or G∆ are rank deficient, the authors add an L1 penalty, but\\r\\nno out-of-sample risk guarantees are given for the regularized estimators.\\r\\n5\\n3.2 Causal regularization with risk guarantees\\r\\nThe aim of this work is to define a regularized version of the causal Dantzig estimator that (1)\\r\\ndoes not require the existence of a shift variable with a full-rank second moment and (2) has\\r\\nexplicit risk guarantees over certain subsets of out-of-sample distributions. In other words,\\r\\ngiven data from SEM(B, 0) and SEM(B, A), we want explicit guarantees for our performance\\r\\non SEM(B, A˜ ), where A is ˜ larger than A, in some precise sense. Hence, we consider the\\r\\nout-of-sample risk over the set Cγ of shifts A that are at most ˜ γ-times stronger than A,\\r\\nintroduced in Rothenh¨ausler et al. [2021].\\r\\nDefinition 2 (Set of γ-times stronger shifts). For a fixed shift-distribution A ∈ A, let\\r\\nCγ be the set of shifts that are γ-times stronger than A, i.e.,\\r\\nCγ\\r\\n.\\r\\n.= {A˜ ∈ A\\r\\n»\\r\\n»\\r\\n»\\r\\n»\\r\\n»\\r\\n»\\r\\n»\\r\\n»\\r\\n»\\r\\n»\\r\\n{\\r\\nE[A˜ A˜\\r\\nT\\r\\n] ⪯ γ E[A AT] if γ ∈ [0, ∞)\\r\\nsupp(A˜ ) = supp(A) if γ = ∞\\r\\n}\\r\\nwhere M ⪯ N if M − N is positive semi-definite.\\r\\nIt is possible to decompose the worst out-of-sample risk over shifts in C1+τinto a weighted\\r\\nsum of the pooled risk and the risk difference without requiring the second moment of the\\r\\nshift variables to be full-rank. This is formalized in the following lemma.\\r\\nLemma 1 (Worst risk decomposition).\\r\\n∀β ∈ R\\r\\np\\r\\nsup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β) =\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nR∆(β),\\r\\nwhere R+(β)\\r\\n.\\r\\n.= RA(β) + R0\\r\\n(β) is the pooled risk\\r\\nR∆(β)\\r\\n.\\r\\n.= RA(β) − R0\\r\\n(β) is the risk difference\\r\\nThe risk difference directly measures the risk stability of the model across distributions, and it\\r\\nis related to the covariance invariance exploited by the causal Dantzig in the following sense:\\r\\nthe smaller the risk difference, the more uncorrelated the residuals are of the model with\\r\\nthe covariate shift generated by A. This worst risk decomposition motivates the definition\\r\\nof causal regularization.\\r\\nDefinition 3 (causal regularization). For λ ∈ [0, ∞), the causal regularizer βλ is defined\\r\\nas the minimizer of the worst risk over C(1+λ)/2,\\r\\nβλ\\r\\n.\\r\\n.= arg min\\r\\nβ∈R\\r\\np\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\nλ\\r\\n2\\r\\nR∆(β)\\r\\n6\\n0.00\\r\\n0.01\\r\\n0.02\\r\\n0.03\\r\\n0.04\\r\\nβ\\r\\n^\\r\\nCD β\\r\\n^\\r\\nCR\\r\\n0.25 0.5 0.75 β\\r\\n^\\r\\nOLS\\r\\nRegularization path β\\r\\n^\\r\\nγ\\r\\nAbsolute risk difference\\r\\nAverage\\r\\nFold\\r\\nSelected\\r\\nCross−validation in training set\\r\\n−1\\r\\n0\\r\\n1\\r\\nβ\\r\\n^\\r\\nCD β\\r\\n^\\r\\nCR\\r\\n0.25 0.5 0.75 β\\r\\n^\\r\\nOLS\\r\\nRegularization path β\\r\\n^\\r\\nγ\\r\\nCoefficient\\r\\nYBL081W\\r\\nYCR052W\\r\\nYDR318W\\r\\nYDR154C\\r\\nYDR250C\\r\\nYDL087C\\r\\nYDL119C\\r\\nYDL174C\\r\\nYDR435C\\r\\nYDL187C\\r\\nYCL004W\\r\\nYDL006W\\r\\nCoefficient path in training set\\r\\n0\\r\\n5\\r\\n10\\r\\n15\\r\\n20\\r\\n0.0 0.2 0.4\\r\\nRisk(β\\r\\n^\\r\\nCD) − Risk(β\\r\\n^\\r\\nCR)\\r\\nApproximate density\\r\\nCross−validated risk difference in test set\\r\\nFigure 1: Causal regularization on gene knock-out experiments. (Left) βˆ\\r\\nCR denotes the\\r\\nmodel selected by 5-fold cross-validation. (Upper right) Coefficient path generated by causal\\r\\nregularization. (Lower right) Cross-validated risk difference on test set between the causal\\r\\nDantzig and causal regularization. Observe that in all cases causal regularization achieves a\\r\\nsmaller risk as evidence by Risk(βˆ\\r\\nCD) − Risk(βˆCR) being always positive.\\r\\nBy increasing λ, causal regularization increases the pooled risk and reduces its risk difference\\r\\nacross distributions, thereby increasing its risk stability. It is easy to check with the help\\r\\nof lemma 1 that, by definition, β1+2τ optimizes the worst risk over C1+τ where τ ∈ [0, ∞).\\r\\nLetting the set of shifts C1+τincrease in magnitude towards the limit set C∞, we recover\\r\\nthat the normalized worst risk is equal to the risk difference,\\r\\n∀β ∈ R\\r\\np\\r\\nlimτ→∞\\r\\nsupA˜∈C1+τ RA˜ (β)\\r\\nτ\\r\\n= R∆(β) (4)\\r\\nwhich means that the pooled risk under βλ increases and the normalized worst out-of-sample\\r\\nrisk decreases as λ increases. In other words, the set of shifts, C1+τ, for which βλ provides\\r\\nrisk guarantees increases as λ increases.\\r\\nEquation (4) also asserts that the regularizer R∆(β) is non-negative for β ∈ R\\r\\np\\r\\n. This fact\\r\\n7\\nis elucidated by noting that it can be rewritten as a quadratic form centred at βPA, i.e.,\\r\\nR∆(β) = (β − βPA)\\r\\nT G∆(β − βPA)\\r\\nas shown in proposition 4 in the appendix, which together with the positive semi-definiteness\\r\\nof G∆ imply the non-negativity of R∆(β).\\r\\nBased on the quadratic form, we can rewrite the regularizer in a way that clarifies the\\r\\nconnection between causal Dantzig and causal regularization. The regularizer corresponds\\r\\nto the causal Dantzig objective multiplied by a pre-conditioner. Hence, by increasing the\\r\\nregularization, causal regularization approaches the causal Dantzig.\\r\\nProposition 1 (Convex regularizer).\\r\\nR∆(β) = R∣∣⋅∣∣(β) s.t. R∣∣⋅∣∣(β)\\r\\n.\\r\\n.=\\r\\nÂÂÂÂÂ\\r\\n(G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nT\\r\\n(G∆ β − Z∆)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\nwhere G\\r\\ng/2\\r\\n∆\\r\\nis the Moore–Penrose pseudoinverse of the square root of G∆, i.e., the matrix\\r\\nthat satisfies G∆ = (G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nTG\\r\\ng/2\\r\\n∆\\r\\n.\\r\\nIt follows that causal regularization interpolates between the causal Dantzig and ordinary\\r\\nleast squares applied to both the observational and shifted distributions. The following\\r\\ncorollary formalizes this notion.\\r\\nCorollary 1 (Interpolation by causal regularization). Let λ ∈ [0, ∞) and G+\\r\\n.\\r\\n.= E(X\\r\\nA XAT\\r\\n) +\\r\\nE(X\\r\\n0 X0T\\r\\n), if G+ is non-singular, then\\r\\nβλ = arg min\\r\\nβ∈R\\r\\np\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\nλ\\r\\n2\\r\\nR∆(β) = arg min\\r\\nβ∈R\\r\\np\\r\\n∥Gλ β − Zλ∥\\r\\n2\\r\\n2 = G\\r\\n-1\\r\\nλ Zλ\\r\\nwhere Gλ\\r\\n.\\r\\n.= G+ +λ G∆ and Zλ\\r\\n.\\r\\n.= Z+ +λ Z∆. In particular, β0 = G\\r\\n-1\\r\\n+ Z+ is the population\\r\\nordinary least squares. Additionally, if G∆ is non-singular, then β∞\\r\\n.\\r\\n.= limλ→∞ βλ = G\\r\\n-1\\r\\n∆ Z∆\\r\\nis the causal Dantzig.\\r\\n3.3 Empirical causal regularization\\r\\nConsider the plugin estimates Z+\\r\\n.\\r\\n.= X\\r\\nAT\\r\\nY\\r\\nA\\r\\n/ nA + X\\r\\n0T\\r\\nY\\r\\n0\\r\\n/ n0, G+\\r\\n.\\r\\n.= X\\r\\nAT\\r\\nX\\r\\nA\\r\\n/ nA + X\\r\\n0T\\r\\nX\\r\\n0\\r\\n/ n0,\\r\\nGλ\\r\\n.\\r\\n.= G+ +λ G∆ and Zλ\\r\\n.\\r\\n.= Z+ +λ Z∆. Furthermore, let the empirical risk of β on (X\\r\\nA\\r\\n, Y\\r\\nA\\r\\n) be\\r\\nRˆ\\r\\nA(β) = ∥Y\\r\\nA\\r\\n− X\\r\\nA\\r\\nβ∥\\r\\n2\\r\\n2\\r\\n/ nA, and define the empirical pooled risk and risk difference as\\r\\nRˆ\\r\\n+(β)\\r\\n.\\r\\n.= Rˆ\\r\\nA(β) + Rˆ0\\r\\n(β) and Rˆ ∆(β)\\r\\n.\\r\\n.= Rˆ\\r\\nA(β) − Rˆ0\\r\\n(β)\\r\\nEvery minimizer of 1\\r\\n2\\r\\nRˆ\\r\\n+(β) +\\r\\nλ\\r\\n2\\r\\nRˆ ∆(β) satisfies the optimality condition Gλ β = Zλ; hence, we\\r\\ndefine the causal regularization estimator βˆ\\r\\nλ as the minimum norm least squares solution of\\r\\nthe optimality condition, i.e., βˆ\\r\\nλ = G\\r\\ng\\r\\nλ\\r\\nZλ where G\\r\\ng\\r\\nλ\\r\\ndenote the pseudoinverse of Gλ.\\r\\n8\\nDefinition 4 (Empirical causal regularization). Given data from an observational and\\r\\nshifted environment, the empirical causal regularizer for λ ∈ [0, ∞) is defined as\\r\\nβˆ\\r\\nλ\\r\\n.\\r\\n.= arg min\\r\\nβ∈M\\r\\n∥β∥2s.t. M .\\r\\n.= arg min\\r\\nβ∈R\\r\\np\\r\\n1\\r\\n2\\r\\nRˆ\\r\\n+(β) +\\r\\nλ\\r\\n2\\r\\nRˆ ∆(β) (5)\\r\\nor equivalently βˆ\\r\\nλ = G\\r\\ng\\r\\nλ\\r\\nZλ.\\r\\nIts uniqueness is always guaranteed by the minimum norm condition [Planitz, 1979].\\r\\nFurthermore, if G∆ is full rank, the empirical causal regularization interpolates between the\\r\\nOLS estimator on all the data and the causal Dantzig estimator. Its consistency depends\\r\\nonly on whether the population matrix G+ = E(X\\r\\nA XAT\\r\\n) + E(X\\r\\n0 X0T\\r\\n) is full rank, while the\\r\\nconsistency of βˆ∞, the causal Dantzig estimator, depends on the non-singularity of G∆.\\r\\nProposition 2. If G+ is positive definite, then βˆ\\r\\nλ\\r\\np\\r\\n→ βλ for λ ∈ [0, ∞)\\r\\n4 Finite sample bound for out-of-sample risk\\r\\nWe derive a finite sample bound for the out-of-sample risk that can be used for any β ∈ R\\r\\np\\r\\n,\\r\\nand in particular for βˆ\\r\\nλ. The crux of the proof is to decompose the out-of-sample risk\\r\\nusing lemma 1, and exploit the concentration of Rˆ\\r\\nA(β) and Rˆ0\\r\\n(β) around RA(β) and R0(β),\\r\\nrespectively (see proposition 5 in the appendix). The lemma assumes that covariates are\\r\\nnormally distributed to avoid boundedness assumptions. The condition can be relaxed to\\r\\nsub-gaussian variables, and a distribution-free bound can be recovered by assuming bounded\\r\\nvariables.\\r\\nLemma 2 (Finite sample bound for worst population risk). If X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n, X\\r\\nA\\r\\nand Y\\r\\nA\\r\\nare\\r\\nmultivariate centred Gaussian variables\\r\\n∀β ∈ R\\r\\np\\r\\n∶ sup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β) ≤\\r\\n1\\r\\n2\\r\\nRˆ\\r\\n+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nRˆ ∆(β) + η(n) (6)\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0, where\\r\\nη(n)\\r\\n.\\r\\n.= (1 + τ )(∥β∥\\r\\n2\\r\\n1 + 1) φ+\\r\\n(p, n)\\r\\nφ+(p, n)\\r\\n.\\r\\n.= φ(p, nA) + φ(p, n0\\r\\n) and\\r\\nφ(p, nU)\\r\\n.\\r\\n.= V[Y\\r\\nU\\r\\n](max\\r\\n1≤k≤p\\r\\nV[X\\r\\nU\\r\\nk\\r\\n])\\r\\n⎛\\r\\n⎜⎜\\r\\n⎝\\r\\n√\\r\\n4q + 8 log(p)\\r\\nnU\\r\\n+\\r\\n4q + 8 log(p)\\r\\nnU\\r\\n⎞\\r\\n⎟⎟\\r\\n⎠\\r\\nHence η(n) = op(1/\\r\\n√\\r\\nmin(nA, n0))\\r\\n9\\n10−0.5\\r\\n10−1\\r\\n10−1.5\\r\\n10−2\\r\\n10−2.5\\r\\n10−3\\r\\n101 102 103 104 105\\r\\nSample size = n (log scale)\\r\\nAbsolute excess risk (log scale)\\r\\nEmpirical average\\r\\nPredicted Op(1 n)\\r\\nFigure 2: Absolute excess risk of βPA as the sample size is increased. Note that both lines have\\r\\napproximately the same slope, indicating that the convergence rate matches the predicted\\r\\none.\\r\\nWe remark that due to the decomposition given in lemma 1, the inequality becomes an\\r\\nequality in the limit. Furthermore, βˆ\\r\\n1+2τ can be motivated as the model that minimizes the\\r\\nfirst two terms of the RHS in equation (6) while controlling the norm ÂÂÂÂÂ\\r\\nβˆ\\r\\n1+2τ\\r\\nÂÂÂÂÂ2\\r\\nwhich in turns\\r\\ncontrols η(n).\\r\\nGenerally, the maximum future shift τ for which one wants guarantees is unknown. Thus,\\r\\nlemma 2 is mostly useful for controlling the normalized excess risk, i.e.,\\r\\nφ¯(β)\\r\\n.\\r\\n.=\\r\\n(\\r\\n1\\r\\n2\\r\\nRˆ\\r\\n+(β) +\\r\\n1+2τ\\r\\n2\\r\\n∣ Rˆ ∆(β)∣) − (supA˜∈C1+τ RA˜ (β))\\r\\n(1 + τ )(∥β∥\\r\\n2\\r\\n1 + 1)\\r\\n≤ op(1/\\r\\n√\\r\\nmin(nA, n0)) (7)\\r\\nwhich is composed by the normalized excess pooled risk and the normalized excess risk\\r\\ndifference\\r\\nφ¯(β) =\\r\\n1\\r\\n2\\r\\n(\\r\\nRˆ\\r\\n+(β) − R+(β)\\r\\n(1 + τ )(∥β∥\\r\\n2\\r\\n1 + 1)\\r\\n) +\\r\\n1 + 2τ\\r\\n2\\r\\n(\\r\\n∣ Rˆ ∆(β)∣ − R∆(β)\\r\\n(1 + τ )(∥β∥\\r\\n2\\r\\n1 + 1)\\r\\n)\\r\\nFigure 2 displays the empirical and predicted normalized excess risk as the sample sizes\\r\\n10\\ngrow for the example shown in figure 4. The experiment details are deferred to section 7.1.\\r\\nFinally, the bound can be modified to provide guarantees for out-of-sample datasets.\\r\\nCorollary 2 (Finite sample bound for worst sample risk). If X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n, X\\r\\nA\\r\\n, Y\\r\\nA\\r\\n, X\\r\\nA˜\\r\\n, and Y\\r\\nA˜\\r\\nare multivariate centred Gaussian variables, and A˜ ∈ C1+τ\\r\\n∀β ∈ R\\r\\np\\r\\n∶ Rˆ\\r\\nA˜ (β) ≤\\r\\n1\\r\\n2\\r\\nRˆ\\r\\n+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\n∣ Rˆ ∆(β)∣ + η(n)\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0, where\\r\\nη(n)\\r\\n.\\r\\n.= (∥β∥\\r\\n2\\r\\n1 + 1) ((1 + τ ) φ+\\r\\n(p, nA, n0) + φ(p, nA˜ ))\\r\\nThe proof follows from upper-bounding Rˆ\\r\\nA˜ by RA˜ via a concentration argument. Then,\\r\\nsince RA˜ (β) ≤ supA˜∈C1+τ RA˜ (β), the corollary follows by applying lemma 2.\\r\\n5 Model selection\\r\\nSo far, we have only considered out-of-sample risk evaluations for a fixed value of the reg\\x02ularization parameter λ. In this section, we consider selecting the regularization parameter\\r\\nbased on resampling. The main idea is to design a model selection procedure that asymptot\\x02ically chooses the same model as an oracle would with access to the unknown distributions\\r\\n(X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n) and (X\\r\\nA\\r\\n, Y\\r\\nA\\r\\n).\\r\\nGiven a set of models {βˆ\\r\\nλ1\\r\\n, . . . , βˆ\\r\\nλK\\r\\n} fitted on the in-sample data, a selector λ is a choice\\r\\nof one of the models, i.e., λ ∈ Λ\\r\\n.\\r\\n.= {λ1\\r\\n, . . . , λK}. Two selectors λˆ and λ are asymptotically\\r\\nequivalent under some loss ℓ, if their difference vanishes in probability asymptotically,\\r\\nℓ(λˆ) − ℓ(λ)\\r\\np\\r\\n→ 0.\\r\\nDudoit and van der Laan [2005] showed the asymptotic equivalence of the sample and pop\\x02ulation cross-validation selectors (defined for our situation in equations (9) and (8) below)\\r\\nunder boundedness assumptions of both the target and covariates. Under further stringent\\r\\nconditions, they prove that the population cross-validation risk of the sample cross-validation\\r\\nselector is equivalent to the risk of the optimal selector, defined in equation (10) below. How\\x02ever, they did not prove that the optimal selector, which has access to models fitted with the\\r\\nfull sample rather than a sub-sample, and the sample cross-validation selector are asymp\\x02totically equivalent. Van der Vaart et al. [2006] relaxed the assumptions required for the\\r\\nasymptotic equivalence of the sample and population cross-validation selectors, replacing\\r\\nboundedness with pairs of Bernstein numbers. Nevertheless, they also did not prove their\\r\\nasymptotic equivalence with the optimal selector.\\r\\nIn this section, we prove, for a particular loss, that (i) the optimal selector, (ii) the\\r\\nsample cross-validation selector, and (iii) the population cross-validation selector are all\\r\\n11\\nasymptotically equivalent under normal covariates, i.e., without boundedness assumptions.\\r\\nThe results follow from the concentration inequality described in lemma 2 (see proposition\\r\\n5 in the appendix). The result can be generalized to sub-gaussian covariates. As before,\\r\\ndistribution-free results can be recovered by assuming bounded covariates.\\r\\nIf the maximum future shift is unknown, one may want to choose a model that will per\\x02form well under arbitrarily strong shifts. Corollary 2 states that for arbitrary out-of-sample\\r\\ndatasets, supA˜∈C1+τ Rˆ\\r\\nA˜ (β) concentrates around supA˜∈C1+τ RA˜ (β). Thus, the bound given in\\r\\nlemma 2 can be used to perform model selection. Equation (4) tells us that the normalized\\r\\nworst out-of-sample risk is controlled by the population risk difference corresponding to the\\r\\nin-sample datasets, i.e., R∆(β). Hence, minimizing R∆(β) provides out-of-sample guarantees.\\r\\nGiven that we only have access to random samples, performing sample-splitting or cross\\x02validation on the absolute in-sample risk difference ∣ Rˆ ∆(β)∣ provides a useful surrogate loss.\\r\\nIn the following, we show the validity of this idea.\\r\\nWe formally define data resampling in order to later recover the cross-validation and\\r\\nsample-splitting losses. Let S\\r\\nA˜\\r\\n= (S\\r\\nA˜\\r\\n1\\r\\n, . . . , SA˜\\r\\nnA˜\\r\\n) ∈ {0, 1}\\r\\nnA˜\\r\\nbe a random variable that indicates\\r\\nif an observation from the dataset (X\\r\\nA˜\\r\\n, Y\\r\\nA˜\\r\\n) is in the train or test set. That is, S\\r\\nA˜\\r\\ni = 0 means\\r\\nthat (x\\r\\nA˜\\r\\ni\\r\\n, y\\r\\nA˜\\r\\ni\\r\\n) is in the training set, while S\\r\\nA˜\\r\\ni = 1 indicates that it belongs to the test set. The\\r\\ndistribution of S\\r\\nA˜\\r\\ndetermines the methodology. For instance, if its distribution is a point\\r\\nmass at a unique binary string, i.e.,\\r\\n∃s ∈ {0, 1}\\r\\nnA˜\\r\\ns.t. P(S\\r\\nA˜\\r\\n= s) = 1\\r\\nwe recover sample-splitting. Alternatively, if there are V binary strings among which the\\r\\nprobability mass is homogeneously distributed, i.e.,\\r\\n∃s\\r\\n1\\r\\n, . . . , s\\r\\nV\\r\\n∈ {0, 1}\\r\\nnA˜\\r\\ns.t. P(S\\r\\nA˜\\r\\n= s\\r\\nj\\r\\n) = 1/V for j = 1, . . . , V\\r\\nwhere ∑\\r\\nV\\r\\nj=1\\r\\ns\\r\\nj\\r\\ni\\r\\n= 1 and ∑\\r\\nnA˜\\r\\ni=1\\r\\ns\\r\\nj\\r\\ni\\r\\n= nA˜ /V , we recover V -fold cross-validation. Note that S\\r\\nA˜\\r\\nneed not to be random but it must be stochastically independent from all observations,\\r\\nincluding those of other datasets. The notion can be formalized using extended conditional\\r\\nindependence across the set S of values that S\\r\\nA˜\\r\\ncan assume [Constantinou and Dawid, 2017],\\r\\ndefined as\\r\\n∀s, s˜ ∈ S (X\\r\\nU\\r\\n, Y\\r\\nU\\r\\n)∣S\\r\\nA˜\\r\\n= s ∼ (X\\r\\nU\\r\\n, Y\\r\\nU\\r\\n)∣S\\r\\nA˜\\r\\n= s˜ for A˜ , U ∈ {A, 0}\\r\\nThe sub-empirical distributions P\\r\\nA˜\\r\\nS,1 and P\\r\\nA˜\\r\\nS,0 are defined by restricting the empirical\\r\\ndistribution P\\r\\nA˜\\r\\nnA˜\\r\\nto the corresponding set,\\r\\nP\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,j\\r\\n.\\r\\n.=\\r\\n1\\r\\nnA˜ (S\\r\\nA˜\\r\\n, j)\\r\\nnA˜ (S\\r\\nA˜\\r\\n,j)\\r\\n∑\\r\\ni∶S\\r\\nA˜\\r\\ni\\r\\n=j\\r\\nδ\\r\\nA˜\\r\\ni where nA˜ (S\\r\\nA˜\\r\\n, j)\\r\\n.\\r\\n.= #{1 ≤ i ≤ nA˜ ∶ S\\r\\nA˜\\r\\ni = j} for j ∈ {0, 1} ,\\r\\n12\\nLet S = (S\\r\\nA˜\\r\\n, S0) be the pair of indicators for each sample, and (X\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,j\\r\\n, Y\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,j\\r\\n) ∈ R\\r\\nnA˜ (S\\r\\nA˜\\r\\n,j) ×(p+1)\\r\\nfor j ∈ 0, 1 denote the design matrix and target vector corresponding to the previous sub\\x02empirical distributions, then the in-sample risk on the test set is\\r\\nRˆ\\r\\nS,1\\r\\nA˜\\r\\n(β) = Rˆ\\r\\nA˜ (β, P\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,1\\r\\n) =\\r\\nÂÂÂÂÂÂ\\r\\nY\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,1\\r\\n− X\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,1\\r\\nβ\\r\\nÂÂÂÂÂÂ\\r\\n/ nA˜ (S\\r\\nA˜\\r\\n, j)\\r\\nand the estimator fitted in the in-sample training set is given by equation (5), where Gλ\\r\\nand Zλ are computed on the sub-sambples {(X\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,0\\r\\n, X\\r\\nA˜\\r\\nS\\r\\nA˜\\r\\n,0\\r\\n)}A˜∈{A,0}rather than the full samples\\r\\n{(X\\r\\nA˜\\r\\n, Y\\r\\nA˜\\r\\n)}A˜∈{A,0}\\r\\nβˆ\\r\\nS,0\\r\\nλ\\r\\n= βˆ\\r\\nλ(P\\r\\nA\\r\\nS\\r\\nA,0\\r\\n, P\\r\\n0\\r\\nS\\r\\n0\\r\\n,0\\r\\n)\\r\\nWe define the population and sample selectors as\\r\\nλ˜\\r\\nS\\r\\n.\\r\\n.= arg min\\r\\nλ∈Λ\\r\\n˜θS(λ) s.t. ˜θS(λ)\\r\\n.\\r\\n.= ES R∆(βˆ\\r\\nS,0\\r\\nλ\\r\\n) Population selector (8)\\r\\nλˆ\\r\\nS\\r\\n.\\r\\n.= arg min\\r\\nλ∈Λ\\r\\nˆθS(λ) s.t. ˆθS(λ)\\r\\n.\\r\\n.= ES ∣ Rˆ\\r\\nS,1\\r\\n∆\\r\\n(βˆ\\r\\nS,0\\r\\nλ\\r\\n)∣ Sample selector (9)\\r\\nwhere the expectation is with respect to the sub-sampling distribution S and where Rˆ\\r\\nS,1\\r\\n∆\\r\\n(β)\\r\\n.\\r\\n.=\\r\\nRˆ\\r\\nS,1\\r\\nA (β) − Rˆ\\r\\nS,1\\r\\n0\\r\\n(β) is in-sample risk difference on the test set. It holds that the sample selector\\r\\nis asymptotically equivalent to the population selector\\r\\nLemma 3 (Asymptotic equivalence of sample and population selectors). If X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n,\\r\\nX\\r\\nA\\r\\nand Y\\r\\nA\\r\\nare multivariate centred Gaussian variables\\r\\n∣\\r\\n˜θS(λˆ\\r\\nS) − ˜θS(λ˜S)∣ p\\r\\n→ 0\\r\\nThe result can be interpreted as follows. Define the optimal selector for a particular\\r\\nrealization of {S\\r\\nA˜\\r\\n}A˜∈{A,0}\\r\\n, called the S-optimal selector, as\\r\\nλ˜\\r\\nS,0 = arg min\\r\\nλ∈Λ\\r\\n˜θS,0\\r\\n(λ) s.t. ˜θS,0(λ)\\r\\n.\\r\\n.= R∆(βˆ\\r\\nS,0\\r\\nλ\\r\\n) S-optimal selector\\r\\nThe sample selector is asymptotically equivalent to the S-optimal selector for the average\\r\\ntrain-test split, as shown in the following corollary.\\r\\nCorollary 3 (Asymptotic equivalence of sample and S-optimal selectors). If X\\r\\n0\\r\\n,\\r\\nY\\r\\n0\\r\\n, X\\r\\nA\\r\\nand Y\\r\\nA\\r\\nare multivariate centred Gaussian variables\\r\\n∣ ES[\\r\\n˜θS,0\\r\\n(λˆ\\r\\nS) − ˜θS,0\\r\\n(λ˜\\r\\nS,0\\r\\n)]∣ p→ 0\\r\\n13\\nConsequently, if the model selection methodology is such that the sub-empirical distributions\\r\\n{P\\r\\nA˜\\r\\nS,0\\r\\n}A˜∈{A,0}converge in law to the empirical distributions {P\\r\\nA˜\\r\\nnA˜\\r\\n}A˜∈{A,0} as nA and n0 grow to\\r\\ninfinity, then βˆ\\r\\nS,0\\r\\nλ\\r\\nconverges to βˆ\\r\\nλ(P\\r\\nA\\r\\nnA\\r\\n, P\\r\\n0\\r\\nn0\\r\\n) due to the continuity of the estimator, and the\\r\\nsample selector is on average asymptotically equivalent to the optimal selector, defined as\\r\\nλ˜ .\\r\\n.= arg min\\r\\nλ∈Λ\\r\\n˜θ(λ) s.t. ˜θ(λ)\\r\\n.\\r\\n.= R∆(βλ(P\\r\\nA\\r\\nnA\\r\\n, P\\r\\n0\\r\\nn0\\r\\n)) Optimal (10)\\r\\nThis is the case for sample splitting, leave-one-out cross-validation, and V-fold cross-validation\\r\\ninsofar as the number of folds grows towards infinity as the sample sizes increase to infinity.\\r\\nIn other words, under a reasonable model selection strategy the selected estimator from\\r\\nthe causal regularization path will be asymptotically equivalent to the optimal selector.\\r\\n0.10\\r\\n0.15\\r\\n0.20\\r\\n0.25\\r\\n200 500 1000 1500 2000\\r\\nSample size\\r\\nAverage width\\r\\n0.900\\r\\n0.938\\r\\n0.950\\r\\n0.970\\r\\n1.000\\r\\n200 500 1000 1500 2000\\r\\nSample size\\r\\nEmpirical coverage Figure 3: Empirical average width and coverage of the 95% bootstrap confidence interval for\\r\\nthe normalized worst risk of βˆ\\r\\n0.3\\r\\n.\\r\\n6 Normalized worst risk confidence interval via Boot\\x02strap\\r\\nEquation (4) states that the normalized worst risk of a model limτ→∞ supA˜∈C1+τ RA˜ (βˆ\\r\\nλ)/τ is\\r\\nequal to its risk stability R∆(β). Thus, a confidence interval for the later is a confidence\\r\\n14\\ninterval for the former. In this section, we construct a bootstrap pivotal confidence interval\\r\\nfor R∆(β) [Wasserman, 2006].\\r\\nIn order to simplify the notation, let λˆ\\r\\n∗ denote the selected model and assume that\\r\\nwe have a fresh sample that is independent from the one used for model selection. Such\\r\\ncondition can always be satisfied via sample splitting. Henceforth, let (X\\r\\nA˜\\r\\n, Y\\r\\nA˜\\r\\n) for A˜ ∈ {A, 0}\\r\\ndenote the fresh sample, and (P\\r\\n0\\r\\n, P\\r\\nA\\r\\n) the corresponding empirical distributions.\\r\\nGiven a distribution P, let Pb denote a bootstrap resample of P for b ∈ {1, . . . , B}.\\r\\nThen, βˆ\\r\\nb\\r\\nλˆ\\r\\n∗\\r\\n.\\r\\n.= βˆ\\r\\nλˆ\\r\\n∗\\r\\n(P\\r\\nA\\r\\nb\\r\\n, P\\r\\n0\\r\\nb\\r\\n) is the selected model refitted in the b-bootstrap resample, and\\r\\nβˆ\\r\\nλˆ\\r\\n∗\\r\\n.\\r\\n.= βˆ\\r\\nλˆ\\r\\n∗\\r\\n(P\\r\\nA\\r\\n, P\\r\\n0\\r\\n) is the same model refitted in the whole sample. Let Rˆ(β, P) denote the\\r\\nsquared prediction error of the model β on the distribution P, we define the risk difference\\r\\nin the b-bootstrap resample and the whole sample as\\r\\n∆\\r\\nb .\\r\\n.= Rˆ(βˆ\\r\\nb\\r\\nλˆ\\r\\n∗\\r\\n, P\\r\\nA\\r\\nb\\r\\n) − Rˆ(βˆ\\r\\nb\\r\\nλˆ\\r\\n∗\\r\\n, P\\r\\n0\\r\\nb\\r\\n) ∆n\\r\\n.\\r\\n.= Rˆ(βˆ\\r\\nλˆ\\r\\n∗\\r\\n, P\\r\\nA\\r\\n) − Rˆ(βˆ\\r\\nλˆ\\r\\n∗\\r\\n, P\\r\\n0\\r\\n)\\r\\nAn asymptotic (1 − α) confidence interval for the normalized work risk is given by the 1 − α\\r\\nbootstrap pivotal confidence interval of ∆ restricted to the positive real line, that is\\r\\n[0, ∞) ∩ ( 2∆n − ∆\\r\\nB\\r\\n1−α/2\\r\\n, 2∆n − ∆\\r\\nB\\r\\nα/2\\r\\n)\\r\\nwhere ∆B\\r\\nα is the α-quantile of the set {∆\\r\\nb\\r\\n}\\r\\nB\\r\\nb=1\\r\\n. Figure 3 displays the empirical width and\\r\\ncoverage of the confidence interval as the sample size is increased. The details of this example\\r\\nare deferred to section 7.2.\\r\\n7 Simulation studies\\r\\nIn this section, we empirically study causal regularization, and compare it to the causal\\r\\nDantzig estimator. For all the simulations, the structural matrix B of the structural equation\\r\\nmodel is displayed in figure 4. In all cases the noise is sampled from a multivariate standard\\r\\nnormal. The shift random variable is specified in the following subsections. Moreover, when\\r\\nwe mention a sample size n, we mean that both the observational and shifted distribution\\r\\nare sampled n times. The code for reproducing the results presented in section 7 and 8 can\\r\\nbe found at github.com/lkania/causal-regularization\\r\\n7.1 Convergence of out-of-sample risk at Op(1/\\r\\n√\\r\\nn)\\r\\nFigure 2 displays the empirical and predicted normalized excess risk for βPA (see equation\\r\\n(7)) as the sample size is increased. For every sample size, 1000 experiments are realized\\r\\nand their resulting empirical normalized excess risk averaged. The shifted random variable\\r\\nis chosen as A ∼ N (0, I). The computation of the population worst case risk is detailed in\\r\\nappendix B. The fact that the slope of both theoretical and empirical excess risk are the\\r\\nsame means that the convergence rate of the empirical risk matches the predicted one.\\r\\n15\\nX2 X3\\r\\nX1\\r\\nY\\r\\nX4 X5\\r\\nX6\\r\\nB = [\\r\\n0 βPA\\r\\nT\\r\\nβCH BX\\r\\n] where βPA =\\r\\n⎡\\r\\n⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢\\r\\n⎣\\r\\n0\\r\\n1\\r\\n1\\r\\n0\\r\\n0\\r\\n0\\r\\n⎤\\r\\n⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥\\r\\n⎦\\r\\n, βCH =\\r\\n⎡\\r\\n⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢\\r\\n⎣\\r\\n0\\r\\n0\\r\\n0\\r\\n1\\r\\n1\\r\\n0\\r\\n⎤\\r\\n⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥\\r\\n⎦\\r\\nand BX =\\r\\n⎡\\r\\n⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢\\r\\n⎣\\r\\n0 0 0 0 0 0\\r\\n1 0 0 0 0 0\\r\\n1 1 0 0 0 0\\r\\n0 0 0 0 0 0\\r\\n0 0 0 0 0 0\\r\\n0 0 0 0 1 0\\r\\n⎤\\r\\n⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥\\r\\n⎦\\r\\nFigure 4: SEM(B, A) used for the simulation study.\\r\\n7.2 Bootstrap confidence interval\\r\\nFigure 3 presents the empirical width and coverage of the bootstrap confidence intervals for\\r\\nβˆ\\r\\n0.3\\r\\n, proposed in section 6, as the sample size is increased. The shift is taken to be standard\\r\\nnormal, A ∼ N (0, I), and the corresponding population worst risk computation is detailed\\r\\nin section B of the appendix. For every sample size, 500 repetitions are conducted in order\\r\\nto compute the empirical coverage, and for each experiment 1000 bootstrap resamples were\\r\\nused to compute the confidence interval. We observe that even for small sample sizes, the\\r\\nconfidence interval possesses, approximately, the desired coverage.\\r\\n7.3 Causal regularization vs causal Dantzig\\r\\nRecall that the causal Dantzig is an extreme point in the regularized path generated by\\r\\ncausal regularization. In section 5, we saw that causal regularization coupled with cross\\x02validation will be asymptotically equivalent to the optimal selector. Since by definition,\\r\\nthe causal Dantzig minimizes the population worst out-of-sample risk, cross-validation will\\r\\nconverge to the causal Dantzig for large sample sizes. Nevertheless, for smaller samples the\\r\\nvariance of the causal Dantzig might be so high that its actual out-of-sample risk is less than\\r\\noptimal, whereas other estimators in the causal regularization path perform much better.\\r\\nFigure 5 exemplifies such behaviour. It displays the median normalized out-of-sample risk\\r\\nfor out-of-sample datasets that are shifted 10, 100 and 1000 times more than the in-sample\\r\\ndatasets. The data is generated from the SEM structure shown in figure 4, and the in-sample\\r\\nshift is given by A ∼ N (0, I), while the out-of-sample shifts are chosen as A ∼ N (0, λI) such\\r\\nthat λ ∈ {10, 100, 1000}. The size of both in-sample datasets n are 250 and 1000 while the\\r\\n16\\nSample size = 250 Sample size = 1000\\r\\nβ\\r\\n^\\r\\nCD\\r\\n0.25 0.5 0.75 β\\r\\n^\\r\\nOLS β\\r\\n^\\r\\nCD\\r\\n0.25 0.5 0.75 β\\r\\n^\\r\\nOLS\\r\\n0.1\\r\\n0.2\\r\\n0.3\\r\\n0.4\\r\\n0.1\\r\\n0.2\\r\\n0.3\\r\\nRegularization path β\\r\\n^\\r\\nγ\\r\\nMedian normalized out−of−sample risk\\r\\nOut−of−sample shift\\r\\n10 100 1000\\r\\nFigure 5: Median normalized risk of the regularization path produced by causal regular\\x02ization as the out-of-sample datasets are 10, 100 and 1000 times more than the in-sample\\r\\ndatasets. Models on the left panel were trained on in-sample datasets consisting of 250\\r\\nobservations while on the right 1000 observation were used. The median was chosen rather\\r\\nthan the mean due to the high variance of the causal Dantzig estimator.\\r\\nout-of-sample datasets always have sample size 106. It is clear that for moderate sample\\r\\nsizes the causal Dantzig estimator can be dramatically improved by causal regularization.\\r\\nConsider the out-of-sample risk of the causal Dantzig and causal regularization as we\\r\\nincrease separation between the in-sample distributions, that is, as the magnitude of the\\r\\nin-sample shift is enlarged. Figure 6 compares the in-sample and out-of-sample risk for\\r\\nmodels obtained via causal regularization and causal Dantzig. All shifted in-sample datasets\\r\\nconsist of n examples where the shift was normally distributed i.e., A ∼ N (0, λI) such that\\r\\nλ ∈ {0.5, 1, 1.5}. Model selection for causal regularization was done by 3-fold cross-validation.\\r\\nAs expected, in all cases the out-of-sample risk corresponding to causal regularization is lower\\r\\nthan causal Dantzig.\\r\\n8 Applications\\r\\nBelow we describe two illustrations of causal regularization in more or less realistic settings.\\r\\nThis involves both appropriate transformations of variables, the definition of environments,\\r\\nas well as the definition of the system under consideration.\\r\\n17\\nSample size = 250 Sample size = 1000\\r\\n100 101 102 103 100 101 102 103\\r\\n10−1\\r\\n100\\r\\n101\\r\\n102\\r\\n10−1\\r\\n100\\r\\n101\\r\\n102\\r\\nNormalized in−sample risk (log scale)\\r\\nNormalized out−of−sample risk (log scale)\\r\\nInitial Shift 0.5 1 1.5 Algorithm causal Dantzig causal regularization\\r\\nFigure 6: Comparison between causal regularization and the causal Dantzig as the training\\r\\ndatasets are progressively shifted. Each faded point is the normalized in-sample and out-of\\x02sample risk achieved by the models. The solid points are medians. The median rather than\\r\\nthe mean was chosen due to the high variance of the causal Dantzig estimator.\\r\\n8.1 Fulton fish market\\r\\nWe apply our methodology to predict the demand in a fish market from the price [Graddy,\\r\\n1995, Imbens, 2014]. In equilibrium, it has been hypothesized that\\r\\nlog(quantity) = β0 + β1log(price) + ϵ\\r\\nwhere quantity is the daily total quantity of fish in pounds, and price is the average daily\\r\\nprice in cents.\\r\\nWe are interested in measuring the prediction quality of the causal regularization and\\r\\ncausal Dantzig across the week (Mondays, Tuesdays, Wednesdays and Thursdays). Hence,\\r\\nwe split the dataset in a training dataset consisting of Mondays, Tuesdays and Thursdays,\\r\\nand a test dataset composed by Wednesdays. We further divide the training dataset into two\\r\\ndatasets: one for stormy days and one for fair days. Stormy days are those when the wind\\r\\nspeed is greater than 18 knots and wave height is higher than 4.5 feats. Figure 7 visualizes\\r\\nthe covariate and response for each one of the datasets.\\r\\nWe fit causal regularization and the causal Dantzig on the training dataset. Model\\r\\nselection is done by cross-validation with 3 folds. Then, we proceed to compute their risk\\r\\non 1000 resamples of the test dataset. Figure 8 shows that in each each resample causal\\r\\n18\\nregularization obtains a smaller out-of-sample risk than the causal Dantzig.\\r\\n0.0\\r\\n0.5\\r\\n1.0\\r\\n1.5\\r\\n−0.8 −0.4 0.0 0.4\\r\\nlog(price)\\r\\nApproximate density\\r\\n0.0\\r\\n0.2\\r\\n0.4\\r\\n0.6\\r\\n7 8 9 10\\r\\nlog(quantity)\\r\\nApproximate density\\r\\nIn−Sample (Not Wednesday, Fair weather) (63 obs.)\\r\\nIn−Sample (Not Wednesday, Stormy weather) (27 obs.)\\r\\nOut−of−Sample (Wednesday) (21 obs.)\\r\\nFigure 7: Estimated densities for the amount of fish and its price for each dataset in the\\r\\nFulton fish market example. This shows that for the covariate, log(price), the observational,\\r\\nfair weather, distribution is indeed different from the shifted, stormy weather, distribution.\\r\\n8.2 Gene knockout experiments\\r\\nNext, we consider a dataset of gene expression in yeast under deletion of single genes [Kem\\x02meren et al., 2014, Meinshausen et al., 2016]. There are 262 non-interventional observations,\\r\\nwhich consists of no gene deletions, and 1479 observations where in each one a different gene\\r\\nis perturbed. In all cases, 6170 genes are measures. The goal is predict the gene expression\\r\\nof one of the genes based on the others. We take as the target the gene that was not directly\\r\\nintervened and whose mean was most shifted between the observation and interventional\\r\\ndatasets. Analogously, we choose 10 intervened genes whose mean was most shifted between\\r\\nthe interventional and observation datasets as the predictors.\\r\\nWe analyze the performance of causal regularization and the causal Dantzig by nested\\r\\ncross-validation. The interventional data was split in 25 folds; of those 24 were used as a\\r\\ntraining set, and the remaining as a test set. On the training set, model selection was done\\r\\nvia 5-fold cross-validation. Finally, the risk of the estimator was evaluated in the test set.\\r\\nFigure 1 displays the cross-validation selection and the corresponding point in the coefficient\\r\\npath for one of the iterations. The process was repeated leaving one of the 25 folds out at a\\r\\ntime. The right-lower pane of figure 1 shows that in all cases, causal regularization achieved\\r\\na smaller risk than the causal Dantzig.\\r\\n19\\n0.00\\r\\n0.05\\r\\n0.10\\r\\n0.15\\r\\n0.20\\r\\nβ\\r\\n^\\r\\nCD β\\r\\n^\\r\\nCR\\r\\n0.25 0.5 0.75 β\\r\\n^\\r\\nOLS\\r\\nRegularization path β\\r\\n^\\r\\nγ\\r\\nAbsolute risk difference\\r\\nAverage\\r\\nFold\\r\\nSelected\\r\\nCross−validation in training set\\r\\n0.0\\r\\n2.5\\r\\n5.0\\r\\n7.5\\r\\n10.0\\r\\n0.0 0.1 0.2 0.3\\r\\nRisk(β\\r\\n^\\r\\nCD) − Risk(β\\r\\n^\\r\\nCR)\\r\\nApproximate density\\r\\nRisk difference in test set\\r\\nFigure 8: Fulton fish market. (Left) 3-fold cross-validation for causal regularization. The\\r\\nred dashed line indicates the chosen model. (Right) Out-of-sample risk difference between\\r\\nthe causal Dantzig and causal regularization. In all cases, the risk achieved by causal regu\\x02larization is smaller as evidenced by Risk(βˆ\\r\\nCD) − Risk(βˆCR) always being positive.\\r\\n9 Conclusion\\r\\nIn this paper, we have introduced causal regularization, a technique for trading off in-sample\\r\\nand out-of-sample risk guarantees by exploiting the heterogeneity across in-sample datasets.\\r\\nThe method provides out-of-sample guarantees, for any regularization parameter, against\\r\\nspecific covariates shifts in the same sense as the causal Dantzig estimator while being\\r\\nidentifiable in a broader set of circumstances. Furthermore, we showed theoretically and\\r\\nempirically that cross-validation and sample-splitting can be used to do model selection for\\r\\ncausal regularization, and obtain estimators with better out-of-sample performance than the\\r\\ncausal Dantzig estimator.\\r\\nThe ideas developed in this work can be extended to some generalized linear models by\\r\\nappropriately defining the risk so that we have risk stability under the causal model. For\\r\\ninstance, consider the Poisson regression analogue of definition 1: YA∣ X\\r\\nA\\r\\n∼ Poisson(m(β))\\r\\nwhere m(β)\\r\\n.\\r\\n.= exp(β\\r\\nT XA\\r\\n), and consider the Pearson χ\\r\\n2\\r\\n-risk: RA(β)\\r\\n.\\r\\n.= E [(Y\\r\\nA\\r\\n−m(β))2/m(β)].\\r\\nIt follows that RA(βPA) is constant w.r.t. A, thus we can use causal regularization, as in\\r\\nequation (3), to trade-off overall in-sample fit for risk stability across sub-samples. The\\r\\nout-of-sample risk guarantees are, however, not trivial to derive due to the non-linearity and\\r\\nrequire further assumptions.\\r\\n20\\nCausal regularization is effective against additive covariate shifts, as specified in definition\\r\\n1. If the shift between datasets does not follow this pattern, then causal regularization only\\r\\nprotects against the projection of the shift to the set of additive shifts.\\r\\nAcknowledgement\\r\\nThe authors acknowledge funding from the Swiss National Science Foundation (SNSF 188534).\\r\\nReferences\\r\\nMartin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk\\r\\nminimization. arXiv:1907.02893, 2019.\\r\\nOlivier Bousquet and Andr´e Elisseeff. Stability and generalization. The Journal of Machine\\r\\nLearning Research, 2:499–526, 2002.\\r\\nPanayiota Constantinou and A. Philip Dawid. Extended conditional independence and ap\\x02plications in causal inference. The Annals of Statistics, 45(6):2618 – 2653, 2017. doi:\\r\\n10.1214/16-AOS1537.\\r\\nL. Devroye and T. Wagner. Distribution-free performance bounds for potential function\\r\\nrules. IEEE Transactions on Information Theory, 25(5):601–604, 1979. doi: 10.1109/TIT.\\r\\n1979.1056087.\\r\\nVanessa Didelez, Sha Meng, and Nuala A. Sheehan. Assumptions of iv methods for obser\\x02vational epidemiology. Statist. Sci., 25(1):22–40, 02 2010. doi: 10.1214/09-STS316.\\r\\nSandrine Dudoit and Mark J. van der Laan. Asymptotics of cross-validated risk estimation\\r\\nin estimator selection and performance assessment. Statistical Methodology, 2(2):131–154,\\r\\n2005. ISSN 1572-3127. doi: 10.1016/j.stamet.2005.02.003.\\r\\nRA Fısher. The design of experiments. Oliver and Boyd, Edinburgh, 1935.\\r\\nKathryn Graddy. Testing for imperfect competition at the fulton fish market. The RAND\\r\\nJournal of Economics, 26(1):75–92, 1995. ISSN 07416261. URL http://www.jstor.org/\\r\\nstable/2556036.\\r\\nMiguel Hernan and James Robins. Causal inference. CRC Taylor & Francis distributor,\\r\\nBoca Raton, Fla. London, 2010. ISBN 1420076167.\\r\\nGuido W. Imbens. Instrumental variables: An econometrician’s perspective. Statistical\\r\\nScience, 29(3):323–358, 2014. ISSN 08834237, 21688745.\\r\\n21\\nMichael Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave\\x02one-out cross-validation. Neural Computation, 11(6):1427–1453, 1999. doi: 10.1162/\\r\\n089976699300016304.\\r\\nPatrick Kemmeren, Katrin Sameith, Loes A. L. van de Pasch, Joris J. Benschop, Tineke\\r\\nL. Lenstra, Thanasis Margaritis, Eoghan O’Duibhir, Eva Apweiler, Sake van Wagenin\\x02gen, Cheuk W. Ko, Sebastiaan van Heesch, Mehdi M. Kashani, Giannis Ampatziadis\\x02Michailidis, Mariel O. Brok, Nathalie A. C. H. Brabers, Anthony J. Miles, Diane\\r\\nBouwmeester, Sander R. van Hooff, Harm van Bakel, Erik Sluiters, Linda V. Bakker,\\r\\nBerend Snel, Philip Lijnzaad, Dik van Leenen, Marian J. A. Groot Koerkamp, and Frank\\r\\nC. P. Holstege. Large-scale genetic perturbations reveal regulatory networks and an abun\\x02dance of gene-specific repressors. Cell, 157(3):740–752, April 2014. ISSN 0092-8674. doi:\\r\\n10.1016/j.cell.2014.02.054.\\r\\nLucas Kook, Beate Sick, and Peter B¨uhlmann. Distributional anchor regression, 2021. URL\\r\\nhttps://arxiv.org/abs/2101.08224.\\r\\nNicolai Meinshausen, Alain Hauser, Joris M. Mooij, Jonas Peters, Philip Versteeg, and\\r\\nPeter B¨uhlmann. Methods for causal inference from gene perturbation experiments and\\r\\nvalidation. Proceedings of the National Academy of Sciences, 113(27):7361–7368, 2016.\\r\\ndoi: 10.1073/pnas.1510493113. URL https://www.pnas.org/doi/abs/10.1073/pnas.\\r\\n1510493113.\\r\\nMichael Oberst, Nikolaj Thams, Jonas Peters, and David Sontag. Regularizing towards\\r\\ncausal invariance: Linear models with proxies. In Marina Meila and Tong Zhang, editors,\\r\\nProceedings of the 38th International Conference on Machine Learning, volume 139 of\\r\\nProceedings of Machine Learning Research, pages 8260–8270. PMLR, 18–24 Jul 2021.\\r\\nJonas Peters, Peter B¨uhlmann, and Nicolai Meinshausen. Causal inference by using invariant\\r\\nprediction: identification and confidence intervals. Journal of the Royal Statistical Society:\\r\\nSeries B (Statistical Methodology), 78(5):947–1012, 2016. doi: 10.1111/rssb.12167.\\r\\nM. Planitz. Inconsistent systems of linear equations. The Mathematical Gazette, 63(425):\\r\\n181–185, 1979. ISSN 00255572. URL http://www.jstor.org/stable/3617890.\\r\\nMateo Rojas-Carulla, Bernhard Sch¨olkopf, Richard Turner, and Jonas Peters. Invariant\\r\\nmodels for causal transfer learning. J. Mach. Learn. Res., 19(1):1309–1342, January 2018.\\r\\nISSN 1532-4435.\\r\\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk mini\\x02mization. CoRR, abs/2010.05761, 2020.\\r\\n22\\nDominik Rothenh¨ausler, Nicolai Meinshausen, Peter B¨uhlmann, and Jonas Peters. Anchor\\r\\nregression: Heterogeneous data meet causality. Journal of the Royal Statistical Society:\\r\\nSeries B (Statistical Methodology), 83(2):215–246, 2021.\\r\\nDominik Rothenh¨ausler, Peter B¨uhlmann, and Nicolai Meinshausen. Causal dantzig: Fast\\r\\ninference in linear structural equation models with hidden variables under additive inter\\x02ventions. Ann. Statist., 47(3):1688–1722, 06 2019. doi: 10.1214/18-AOS1732.\\r\\nDonald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized\\r\\nstudies. Journal of educational Psychology, 66(5):688, 1974.\\r\\nG. W. Stewart. On the perturbation of pseudo-inverses, projections and linear least squares\\r\\nproblems. SIAM Review, 19(4):634–662, 1977. ISSN 00361445. URL http://www.jstor.\\r\\norg/stable/2030248.\\r\\nSara A Van De Geer and Peter B¨uhlmann. On the conditions used to prove oracle results\\r\\nfor the lasso. Electronic Journal of Statistics, 3:1360–1392, 2009.\\r\\nAad W. van der Vaart, Sandrine Dudoit, and Mark J. van der Laan. Oracle inequalities\\r\\nfor multi-fold cross validation. Statistics & Decisions, 24(3):351–371, 2006. doi: doi:\\r\\n10.1524/stnd.2006.24.3.351.\\r\\nSilvia Villa, Lorenzo Rosasco, and Tomaso Poggio. On learnability, complexity and stability.\\r\\nIn Empirical Inference, pages 59–69. Springer, 2013.\\r\\nLarry Wasserman. All of Nonparametric Statistics (Springer Texts in Statistics). Springer\\x02Verlag, Berlin, Heidelberg, 2006. ISBN 0387251456.\\r\\n23\\nA Proofs\\r\\nA.1 Population statements\\r\\nLemma 1 (Worst risk decomposition).\\r\\n∀β ∈ R\\r\\np\\r\\nsup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β) =\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nR∆(β),\\r\\nwhere R+(β)\\r\\n.\\r\\n.= RA(β) + R0\\r\\n(β) is the pooled risk\\r\\nR∆(β)\\r\\n.\\r\\n.= RA(β) − R0\\r\\n(β) is the risk difference\\r\\nProof. Let A˜ ∈ C1+τ, by the SEM definition 1, it holds that\\r\\n[\\r\\nY\\r\\nA˜\\r\\nX\\r\\nA˜ ] = (I − B)\\r\\n−1\\r\\n(ϵ + [\\r\\n0\\r\\nA˜\\r\\n]) (11)\\r\\nLet PX ∈ R\\r\\np×(p+1)\\r\\nand PY ∈ R\\r\\n1×(p+1)\\r\\nbe the projections of the X and Y coordinates, i.e., PX =\\r\\n[0p×1 Ip×p] and PY = [1 01×p], the population residuals can be rewritten as a projection of\\r\\nequation (11)\\r\\nY\\r\\nA˜\\r\\n−β\\r\\nT XA˜\\r\\n= u (ϵ + [\\r\\n0\\r\\nA˜\\r\\n]) where u\\r\\n.\\r\\n.= (PY −β\\r\\nT\\r\\nPX)(I − B)\\r\\n−1\\r\\nIt follows that the residuals under the shift A are ˜\\r\\nRA˜ (β) = E[(Y\\r\\nA˜\\r\\n−β\\r\\nT XA˜\\r\\n)\\r\\n2\\r\\n] = u (C[ϵ] + [\\r\\n0 0\\r\\nT\\r\\n0 E[A˜ A˜\\r\\nT\\r\\n]\\r\\n]) u\\r\\nT\\r\\n(12)\\r\\nwhere we have exploited the fact that the shift variable and the noise are uncorrelated, i.e.,\\r\\nC[A˜ , ϵ] = 0. Taking the supremum over C1+τ, we obtain\\r\\nsup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β) = u (C[ϵ] + (1 + τ ) [\\r\\n0 0\\r\\nT\\r\\n0 E[A AT]\\r\\n]) u\\r\\nT\\r\\n(13)\\r\\nNoting that equation (12) implies that\\r\\nR∆(β) = u ([0 0\\r\\nT\\r\\n0 E[A AT]\\r\\n]) u\\r\\nT\\r\\nand replacing it back into equation (13), we get\\r\\nsup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β) = RA(β) + τ R∆(β) =\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nR∆(β)\\r\\n24\\nProposition 3. G∆ is positive semi-definite and rank G∆ = rank E[A AT\\r\\n]\\r\\nProof. By definition\\r\\nG∆ = E[X\\r\\nA XAT\\r\\n] − E[X\\r\\n0 X0T\\r\\n]\\r\\nLet PX ∈ R\\r\\np×(p+1)\\r\\nbe the projection of the X coordinates, i.e., PX = [0p×1 Ip×p], then for\\r\\nA˜ ∈ {A, 0}\\r\\nE[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n] = u (C[ϵ] + [\\r\\n0 0\\r\\nT\\r\\n0 E[A˜ A˜\\r\\nT\\r\\n]\\r\\n]) u\\r\\nT\\r\\nwhere u = PX(I − B)\\r\\n−1\\r\\n(14)\\r\\nsince C[A˜ , ϵ] = 0. Let (I − B)\\r\\n−1 .\\r\\n.= [\\r\\n1 v\\r\\nT\\r\\nw M], then u = PX(I − B)\\r\\n−1\\r\\n= [w M] and\\r\\nE[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n] = u C[ϵ]u\\r\\nT\\r\\n+ M E[A˜ A˜\\r\\nT\\r\\n] M\\r\\nT\\r\\nThus, G∆ = M E[A AT] M\\r\\nT\\r\\n. We can explicitly write the inverse of M by noting that\\r\\n(I − B)\\r\\n−1\\r\\n(I − B) = I(p+1)×(p+1) ⟹ {\\r\\n−w βPA\\r\\nT\\r\\n+M(I − BX) = Ip×p\\r\\nw − M βCH = 0\\r\\n(15)\\r\\n⟹ M ((I − BX) − βCH βPA\\r\\nT\\r\\n) = Ip×p (16)\\r\\nThus, M is square and has a right-inverse, hence M is invertible. In other words, M is a\\r\\nproduct of elementary matrices and doens’t modify the column-rank or the row-rank of any\\r\\nconformal matrix. Ergo,\\r\\nrank G∆ = rank M E[A AT] M\\r\\nT\\r\\n= rank E[A AT]\\r\\nFinally, note that under the assumption that E[A AT] is positive semi-definite, it follows\\r\\nthat G∆ is positive semi-definite since\\r\\n∀β ∈ R\\r\\np\\r\\nβ\\r\\nT G∆ β = (MT\\r\\nβ)\\r\\nT\\r\\nE[A AT](M\\r\\nT\\r\\nβ) ≥ 0\\r\\nProposition 4 (The risk difference is a centred quadratic form).\\r\\n∀β ∈ R\\r\\np\\r\\nR∆(β) = (β − βPA)\\r\\nT G∆(β − βPA)\\r\\n25\\nProof. For A˜ ∈ {A, 0}\\r\\nRA˜ (β) = E[(Y\\r\\nA˜\\r\\n−β\\r\\nT XA˜\\r\\n)\\r\\n2\\r\\n] = β\\r\\nT\\r\\nE[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n] β − 2β\\r\\nT\\r\\nE[X\\r\\nA˜\\r\\nY\\r\\nA˜\\r\\n] + E[(Y\\r\\nA˜\\r\\n)\\r\\n2\\r\\n]\\r\\nRecalling the definitions\\r\\nG∆ = E[X\\r\\nA XAT\\r\\n] − E[X\\r\\n0 X0T\\r\\n]\\r\\nZ∆ = E[X\\r\\nA YA\\r\\n] − E[X\\r\\n0 Y0\\r\\n] = G∆ βPA By equation (2)\\r\\nIt holds that\\r\\nR∆(β) = RA(β) − R0(β)\\r\\n= β\\r\\nT G∆ β − 2βT G∆ βPA + E[(YA\\r\\n)\\r\\n2\\r\\n] − E[(Y\\r\\n0\\r\\n)\\r\\n2\\r\\n] (17)\\r\\nNotice that for A˜ ∈ {A, 0}, E[(Y\\r\\nA˜\\r\\n)\\r\\n2\\r\\n] can be expanded as follows\\r\\nE[(Y\\r\\nA˜\\r\\n)\\r\\n2\\r\\n] = E[(β\\r\\nT\\r\\nPA X\\r\\nA˜\\r\\n+ϵY )\\r\\n2\\r\\n]\\r\\n= β\\r\\nT\\r\\nPA E[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n] βPA + E[ϵ\\r\\n2\\r\\nY ] + β\\r\\nT\\r\\nPA E[X\\r\\nA˜\\r\\nϵY ]\\r\\n= β\\r\\nT\\r\\nPA E[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n] βPA + E[ϵ\\r\\n2\\r\\nY ] + β\\r\\nT\\r\\nPA u E[(ϵ + A˜ )ϵY ]\\r\\n= β\\r\\nT\\r\\nPA E[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n] βPA + E[ϵ\\r\\n2\\r\\nY ] + β\\r\\nT\\r\\nPA u C[ϵ, ϵY ] since E[A˜ ϵY ] = 0\\r\\nhence\\r\\nE[(Y\\r\\nA\\r\\n)\\r\\n2\\r\\n] − E[(Y\\r\\n0\\r\\n)\\r\\n2\\r\\n] = β\\r\\nT\\r\\nPA G∆ βPA\\r\\nplugging the result back into equation (17), we get\\r\\nR∆(β) = β\\r\\nT G∆ β + βT G∆ βPA + βT\\r\\nPA G∆ βPA\\r\\n= (β − βPA)\\r\\nT G∆(β − βPA)\\r\\nTheorem 1 (Moore–Penrose inverse). Given a A ∈ R\\r\\np×p\\r\\n, the Moore–Penrose inverse A is\\r\\ndefined as the unique matrix A\\r\\ng\\r\\n∈ R\\r\\np×p\\r\\nthat satisfies the following properties\\r\\nA\\r\\ngAAg\\r\\n= A (1)\\r\\nAAgA = A (2)\\r\\n(AAg)\\r\\nT\\r\\n= AAg(3)\\r\\n(A\\r\\ngA)T\\r\\n= A\\r\\ngA (4)\\r\\n26\\nProposition 1 (Convex regularizer).\\r\\nR∆(β) = R∣∣⋅∣∣(β) s.t. R∣∣⋅∣∣(β)\\r\\n.\\r\\n.=\\r\\nÂÂÂÂÂ\\r\\n(G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nT\\r\\n(G∆ β − Z∆)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\nwhere G\\r\\ng/2\\r\\n∆\\r\\nis the Moore–Penrose pseudoinverse of the square root of G∆, i.e., the matrix\\r\\nthat satisfies G∆ = (G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nTG\\r\\ng/2\\r\\n∆\\r\\n.\\r\\nProof. Note that\\r\\nR∆(β) = (β − βPA)\\r\\nT G∆(β − βPA) =\\r\\nÂÂÂÂÂ\\r\\nG\\r\\n1/2\\r\\n∆\\r\\n(β − βPA)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\n(18)\\r\\nwhere the first equality holds by proposition 4, and in the last equality we applied the square\\r\\nroot definition. The existence of G1/2\\r\\n∆\\r\\npositive semi-definite is guaranteed by the positive\\r\\nsemi-definiteness of G∆, see proposition 3. Let Gg/2\\r\\n∆\\r\\nbe the pseudoinverse of G1/2\\r\\n∆\\r\\n, we rewrite\\r\\nthe regularizer as follows,\\r\\nG\\r\\n1/2\\r\\n∆\\r\\n= G\\r\\n1/2\\r\\n∆ G\\r\\ng/2\\r\\n∆ G\\r\\n1/2\\r\\n∆\\r\\nby theorem 1.2\\r\\n= (G\\r\\n1/2\\r\\n∆ G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nT G\\r\\n1/2\\r\\n∆\\r\\nby theorem 1.3\\r\\n= (G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nT\\r\\n(G\\r\\n1/2\\r\\n∆\\r\\n)\\r\\nT G\\r\\n1/2\\r\\n∆\\r\\n= (G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nT G∆ by square root def.\\r\\nplugging the result back into equation (18), and using equation (2) in the last equality, we\\r\\nget\\r\\nR∆(β) =\\r\\nÂÂÂÂÂ\\r\\n(G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nT\\r\\n(G∆ β − G∆ βPA)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\n=\\r\\nÂÂÂÂÂ\\r\\n(G\\r\\ng/2\\r\\n∆\\r\\n)\\r\\nT\\r\\n(G∆ β − Z∆)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\nCorollary 1 (Interpolation by causal regularization). Let λ ∈ [0, ∞) and G+\\r\\n.\\r\\n.= E(X\\r\\nA XAT\\r\\n) +\\r\\nE(X\\r\\n0 X0T\\r\\n), if G+ is non-singular, then\\r\\nβλ = arg min\\r\\nβ∈R\\r\\np\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\nλ\\r\\n2\\r\\nR∆(β) = arg min\\r\\nβ∈R\\r\\np\\r\\n∥Gλ β − Zλ∥\\r\\n2\\r\\n2 = G\\r\\n-1\\r\\nλ Zλ\\r\\nwhere Gλ\\r\\n.\\r\\n.= G+ +λ G∆ and Zλ\\r\\n.\\r\\n.= Z+ +λ Z∆. In particular, β0 = G\\r\\n-1\\r\\n+ Z+ is the population\\r\\nordinary least squares. Additionally, if G∆ is non-singular, then β∞\\r\\n.\\r\\n.= limλ→∞ βλ = G\\r\\n-1\\r\\n∆ Z∆\\r\\nis the causal Dantzig.\\r\\n27\\nProof. For λ ∈ [0, ∞)\\r\\nβλ\\r\\n.\\r\\n.= arg min\\r\\nβ∈R\\r\\np\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\nλ\\r\\n2\\r\\nR∆(β)\\r\\n= arg min\\r\\nβ∈R\\r\\np\\r\\n1\\r\\n2\\r\\nβ\\r\\nT Gλ β − βT\\r\\n(Z+ +λ G∆ βPA) by proposition 4\\r\\n= arg min\\r\\nβ∈R\\r\\np\\r\\n1\\r\\n2\\r\\nβ\\r\\nT Gλ β − βT\\r\\n(Z+ +λ Z∆) by equation 2\\r\\n= G\\r\\n-1\\r\\nλ Zλ\\r\\nNote that G-1\\r\\nλ always exists since Gλ is positive definite because G+ is assumed non-singular,\\r\\nand therefore positive definite, and G∆ is positive semi-definite by proposition 3. Given the\\r\\nuniqueness of the solution, we can express βλ as the least squares solution of Gλ β = Zλ, i.e.\\r\\nβλ\\r\\n.\\r\\n.= arg minβ∈R\\r\\np ∥Gλ β − Zλ∥2\\r\\n. Finally, if G∆ is full rank, β∞ = arg minβ∈R\\r\\np ∥G∆ β − Z∆∥2\\r\\n,\\r\\nthat is, causal regularization at γ = ∞ and the causal Dantzig have the same solution, see\\r\\nequation (3).\\r\\nA.2 Consistency\\r\\nDefinition 5. Let A ∈ R\\r\\np×p\\r\\nbe a symmetric matrix, its operator norm can be defined as\\r\\n∥A∥2\\r\\n.\\r\\n.= max\\r\\nx∈R\\r\\np s.t. ∥x∥2≤1\\r\\n∣x\\r\\nTAx∣\\r\\nand it holds that\\r\\n∥A∥2 = max\\r\\n1≤i≤p\\r\\n∣γi(A)∣\\r\\nwhere γi(A) is the i-th eigenvalue of A.\\r\\nTheorem 2 (Stewart [1977], Theorem 3.3). Let A ∈ R\\r\\np×p\\r\\nand B ∈ R\\r\\np×p\\r\\n, then\\r\\n∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\n∥2≤ C ⋅ max{∥A\\r\\ng\\r\\n∥\\r\\n2\\r\\n2\\r\\n, ∥B\\r\\ng\\r\\n∥\\r\\n2\\r\\n2\\r\\n} ⋅ ∥A − B∥\\r\\n2\\r\\n2\\r\\nwhere A, ∥⋅∥2 denotes the opetor norm, and C is a positive universal constant.\\r\\nLemma 4. Let A ∈ R\\r\\np×p\\r\\nbe a positive symmetric define matrix, and {Bn}\\r\\n∞\\r\\nn=1 ∈ R\\r\\np×p\\r\\na\\r\\nsequence of symmetric matrices such that ∥A − Bn∥2\\r\\np\\r\\n→ 0, then ∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2\\r\\np\\r\\n→ 0\\r\\n28\\nProof. By theorem 2, it holds that\\r\\n∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2\\r\\n≤ C ⋅ max{∥A\\r\\ng\\r\\n∥\\r\\n2\\r\\n2\\r\\n, ∥B\\r\\ng\\r\\nn∥\\r\\n2\\r\\n2\\r\\n} ⋅ ∥A − Bn∥\\r\\n2\\r\\n2\\r\\nSince A is a positive symmetric definite matrix, it has an inverse, and it follows that\\r\\n∥A\\r\\ng\\r\\n∥2=\\r\\nÂÂÂÂÂ\\r\\nA\\r\\n−1ÂÂÂÂÂ2\\r\\n= γmin(A)\\r\\n−1\\r\\nwhere γmin(A) is the smallest positive eigenvalue of A. Let UDV Tbe the SVD decomposition\\r\\nof Bn, it holds that\\r\\nB\\r\\ng\\r\\nn = UDg\\r\\nV\\r\\nT\\r\\nwhere γi(D\\r\\ng\\r\\n) = {\\r\\n0 if γi(Bn) = 0\\r\\n∣γi(Bn)∣−1otherwise\\r\\nand we rewrite the operator norm of Bn as\\r\\n∥B\\r\\ng\\r\\nn∥2\\r\\n=\\r\\nÂÂÂÂÂ\\r\\nUDgV\\r\\nT ÂÂÂÂÂ2\\r\\n= ∥D\\r\\ng\\r\\n∥2= {\\r\\n0 if γi(Bn) = 0 ∀1 ≤ i ≤ p\\r\\nmax{ ∣γi(Bn)∣−1s.t. ∣γi(Bn)∣ > 0 } otherwise\\r\\nwhere the second equality is due to the operator norm being unitarily invariant.\\r\\nWeyl’s inequality says that if Bn converges to A in operator norm, then their eigenvalues\\r\\nconverge uniformly,\\r\\n∣ γi(A) − ∣γi(Bn)∣ ∣ ≤ ∣γi(A) − γi(Bn)∣ ≤ ∥A − Bn∥2for 1 ≤ i ≤ p\\r\\nThus, a lowerbound for ∣γi(B)∣ is\\r\\nγmin(A) − ∥A − Bn∥2 ≤ γi(A) − ∥A − Bn∥2 ≤ ∣γi(Bn)∣ for 1 ≤ i ≤ p\\r\\nDefine the event En as\\r\\nEn\\r\\n.\\r\\n.= {γmin(A) − ∥A − Bn∥2 > 0}\\r\\nit follows that on the event En,\\r\\n∥B\\r\\ng\\r\\nn∥2\\r\\n≤\\r\\n1\\r\\nγmin(A) − ∥A − Bn∥2\\r\\nHence, if the event En happens, it holds that\\r\\n∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2\\r\\n≤ C ⋅ max{\\r\\n1\\r\\nγmin(A)\\r\\n2\\r\\n,\\r\\n1\\r\\n(γmin(A) − ∥A − Bn∥2)\\r\\n2\\r\\n} ⋅ ∥A − Bn∥\\r\\n2\\r\\n2\\r\\n≤ g(∥A − Bn∥2) where g(x)\\r\\n.\\r\\n.=\\r\\nx\\r\\n2\\r\\n(γmin(A) − x)\\r\\n2\\r\\n(19)\\r\\n29\\nPutting all together we get that for ϵ > 0 and δ > 0\\r\\nP(∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2\\r\\n≥ ϵ) = P( ∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2\\r\\n≥ ϵ ∧ En ) + P( ∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2\\r\\n≥ ϵ ∧ E\\r\\nc\\r\\nn )\\r\\n≤ P( g(∥A − Bn∥2) > ϵ ) + P( E\\r\\nc\\r\\nn )\\r\\n= P( g(∥A − Bn∥2) > ϵ ) + P( ∥A − Bn∥2 ≥ γmin(A) )\\r\\nWhere the inequality is due to equation (19). Note that g is continuous for x =/ γmin(A)\\r\\nand g(0) = 0. Since ∥A − Bn∥2\\r\\np\\r\\n→ 0 and γmin(A) > 0, by the continuous mapping theorem\\r\\ng(∥A − Bn∥2)\\r\\np\\r\\n→ 0. Hence, we can choose N .\\r\\n.= N(ϵ, γmin(A), δ) ∈ N such that P( g(∥A − Bn∥2\\r\\n) >\\r\\nϵ ) <\\r\\nδ\\r\\n2\\r\\nand P( ∥A − Bn∥2 ≥ γmin(A) ) <\\r\\nδ\\r\\n2\\r\\nfor all n ≥ N, which implies that P(∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2 ≥\\r\\nϵ) < δ for all n ≥ N and consequently ∥A\\r\\ng\\r\\n− B\\r\\ng\\r\\nn∥2\\r\\np\\r\\n→ 0\\r\\nProposition 2. If G+ is positive definite, then βˆ\\r\\nλ\\r\\np\\r\\n→ βλ for λ ∈ [0, ∞)\\r\\nProof. By the weak law of large numbers and continuity, G+, G∆, Z+, and Z∆ converge in\\r\\nprobability to G+, G∆, Z+, and Z∆ correspondingly. Hence, by continuity, Gλ\\r\\n.\\r\\n.= G+ +λ G∆\\r\\nand Zλ\\r\\n.\\r\\n.= Z+ +λ Z∆ converge in probability to Gλ and Zλ.\\r\\nSince p is finite, Gλ\\r\\np\\r\\n→ Gλ implies ∥Gλ − Gλ∥2\\r\\np\\r\\n→ 0. Then, theorem 2 implies that ∥G\\r\\ng\\r\\nλ\\r\\n− G\\r\\ng\\r\\nλ\\r\\n∥2\\r\\np\\r\\n→ 0,\\r\\nand consequently G\\r\\ng\\r\\nλ\\r\\np\\r\\n→ G\\r\\ng\\r\\nλ\\r\\n. It follows that\\r\\nβˆ\\r\\nλ\\r\\n.\\r\\n.= G\\r\\ng\\r\\nλ Zλ\\r\\np\\r\\n→ G\\r\\ng\\r\\nλ\\r\\nZλ = G\\r\\n-1\\r\\nλ Zλ = βλ\\r\\nwhere the convergence in probability is due to continuity, and the penultimate equality is\\r\\ndue to Gλ being positive definite since G+ is assumed positive definite, and G∆ is positive\\r\\nsemi-definite by proposition 3.\\r\\nA.3 Finite sample bounds\\r\\nProposition 5. For A˜ ∈ A,\\r\\n∣ RA˜ (β) − Rˆ\\r\\nA˜ (β)∣ ≤ (∥β∥\\r\\n2\\r\\n1 + 1) V[Y\\r\\nA˜\\r\\n](max\\r\\n1≤k≤p\\r\\nV[X\\r\\nA˜\\r\\nk\\r\\n])\\r\\n⎛\\r\\n⎜⎜\\r\\n⎝\\r\\n√\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n+\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n⎞\\r\\n⎟⎟\\r\\n⎠\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0, hence ∣ RA˜ (β) − Rˆ\\r\\nA˜ (β)∣ = op\\r\\n(\\r\\n1\\r\\n√\\r\\nnA˜\\r\\n)\\r\\n30\\nProof. Let VA˜\\r\\n.\\r\\n.= [\\r\\nY\\r\\nA˜\\r\\nX\\r\\nA˜ ], SA˜\\r\\n= E[V\\r\\nA˜\\r\\nV\\r\\nA˜ T\\r\\n], and υ\\r\\n.\\r\\n.= [\\r\\n−1\\r\\nβ\\r\\n] then\\r\\n∣ R+(β) − Rˆ\\r\\n+(β)∣ = ∣υ\\r\\nT\\r\\n(Sˆ\\r\\nA˜\\r\\n− S\\r\\nA˜\\r\\n)υ∣ ≤ ∥υ∥\\r\\n2\\r\\n1\\r\\nÂÂÂÂÂ\\r\\nSˆ\\r\\nA˜\\r\\n− S\\r\\nA˜ ÂÂÂÂÂ∞\\r\\n≤ (∥β∥\\r\\n2\\r\\n1 + 1)\\r\\nÂÂÂÂÂ\\r\\nSˆ\\r\\nA˜\\r\\n− S\\r\\nA˜ ÂÂÂÂÂ∞\\r\\nwhere ÂÂÂÂÂ\\r\\nSˆ\\r\\nA˜\\r\\n− S\\r\\nA˜ ÂÂÂÂÂ∞\\r\\n= maxi,j∈{1,...,p+1}∣Sˆ\\r\\nA˜\\r\\nij − S\\r\\nA˜\\r\\nij ∣. Note that due to their block structure, it\\r\\nfollows that\\r\\nwhere ÂÂÂÂÂ\\r\\nSˆ\\r\\nA˜\\r\\n− S\\r\\nA˜ ÂÂÂÂÂ∞\\r\\n= max(m1, m2, m3) s.t.\\r\\nm1\\r\\n.\\r\\n.=\\r\\nÂÂÂÂÂÂÂ\\r\\n1\\r\\nnA˜\\r\\nX\\r\\nA˜ T\\r\\nX\\r\\nA˜\\r\\n− E[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n]\\r\\nÂÂÂÂÂÂÂ∞\\r\\nm2\\r\\n.\\r\\n.=\\r\\nÂÂÂÂÂÂÂ\\r\\n1\\r\\nnA˜\\r\\nX\\r\\nA˜ T\\r\\nY\\r\\nA˜\\r\\n− E[X\\r\\nA˜\\r\\nY\\r\\nA˜\\r\\n]\\r\\nÂÂÂÂÂÂÂ∞\\r\\nm3\\r\\n.\\r\\n.=\\r\\nÂÂÂÂÂÂÂ\\r\\n1\\r\\nnA˜\\r\\nY\\r\\nA˜ T\\r\\nY\\r\\nA˜\\r\\n− E[(Y\\r\\nA˜\\r\\n)\\r\\n2\\r\\n]\\r\\nÂÂÂÂÂÂÂ∞\\r\\nIf XA˜is a multivariate centred Gaussian random variable, it holds that [Van De Geer and\\r\\nB¨uhlmann, 2009]\\r\\nm1 =\\r\\nÂÂÂÂÂÂÂ\\r\\n1\\r\\nnA˜\\r\\nX\\r\\nA˜ T\\r\\nX\\r\\nA˜\\r\\n− E[X\\r\\nA˜\\r\\nX\\r\\nA˜ T\\r\\n]\\r\\nÂÂÂÂÂÂÂ∞\\r\\n≤ max\\r\\nk\\r\\nV[X\\r\\nA˜\\r\\nk]\\r\\n⎛\\r\\n⎜⎜\\r\\n⎝\\r\\n√\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n+\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n⎞\\r\\n⎟⎟\\r\\n⎠\\r\\nwith probability exceeding 1 − 4e\\r\\n−q\\r\\nfor q > 0. Analogously, if YA˜is a multivariate centred\\r\\nGaussian random variable\\r\\nm3 =\\r\\nÂÂÂÂÂÂÂ\\r\\n1\\r\\nnA˜\\r\\nY\\r\\nA˜ T\\r\\nY\\r\\nA˜\\r\\n− E[(Y\\r\\nA˜\\r\\n)\\r\\n2\\r\\n]\\r\\nÂÂÂÂÂÂÂ∞\\r\\n≤ V[Y\\r\\nA˜\\r\\n] (\\r\\n√\\r\\n4q\\r\\nnA˜\\r\\n+\\r\\n4q\\r\\nnA˜\\r\\n)\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0, and\\r\\nm2 =\\r\\nÂÂÂÂÂÂÂ\\r\\n1\\r\\nnA˜\\r\\nX\\r\\nA˜ T\\r\\nY\\r\\nA˜\\r\\n− E[X\\r\\nA˜\\r\\nY\\r\\nA˜\\r\\n]\\r\\nÂÂÂÂÂÂÂ∞\\r\\n≤ V[Y\\r\\nA˜\\r\\n] max\\r\\nk\\r\\nV[X\\r\\nA˜\\r\\nk]\\r\\n⎛\\r\\n⎜⎜\\r\\n⎝\\r\\n√\\r\\n4q + 4 log(p)\\r\\nnA˜\\r\\n+\\r\\n4q + 4 log(p)\\r\\nnA˜\\r\\n⎞\\r\\n⎟⎟\\r\\n⎠\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0. Therefore,\\r\\nÂÂÂÂÂ\\r\\nSˆ\\r\\nA˜\\r\\n− S\\r\\nA˜ ÂÂÂÂÂ∞\\r\\n≤ V[Y\\r\\nA˜\\r\\n] max\\r\\nk\\r\\nV[X\\r\\nA˜\\r\\nk]\\r\\n⎛\\r\\n⎜⎜\\r\\n⎝\\r\\n√\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n+\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n⎞\\r\\n⎟⎟\\r\\n⎠\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0, and\\r\\nÂÂÂÂÂ\\r\\nSˆ\\r\\nA˜\\r\\n− S\\r\\nA˜ ÂÂÂÂÂ∞\\r\\n= op(1/\\r\\n√\\r\\nnA˜ )\\r\\n31\\nProposition 6.\\r\\n∣ R∆(β) − Rˆ ∆(β)∣ ≤ (∥β∥\\r\\n2\\r\\n1 + 1) φ(p, nA, n0\\r\\n)\\r\\n∣ R+(β) − Rˆ\\r\\n+(β)∣ ≤ (∥β∥\\r\\n2\\r\\n1 + 1) φ(p, nA, n0\\r\\n)\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0, where\\r\\nφ(p, nA, n0)\\r\\n.\\r\\n.= ∑\\r\\nA˜∈{A,0}\\r\\nV[Y\\r\\nA˜\\r\\n] max\\r\\nk\\r\\nV[X\\r\\nA˜\\r\\nk]\\r\\n⎛\\r\\n⎜⎜\\r\\n⎝\\r\\n√\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n+\\r\\n4q + 8 log(p)\\r\\nnA˜\\r\\n⎞\\r\\n⎟⎟\\r\\n⎠\\r\\nhence φ(p, nA, n0) = op(1/\\r\\n√\\r\\nmin(nA, n0))\\r\\nProof.\\r\\n∣ R∆(β) − Rˆ ∆(β)∣ ≤ ∣ RA(β) − Rˆ\\r\\nA(β)∣ + ∣ R0\\r\\n(β) − Rˆ\\r\\n0\\r\\n(β)∣ ≤ (∥β∥\\r\\n2\\r\\n1 + 1) φ(p, nA, n0\\r\\n)\\r\\nwhere in the last inequality we used proposition 5. The proof for ∣ R+(β)−Rˆ\\r\\n+(β)∣ is analogous.\\r\\nLemma 2 (Finite sample bound for worst population risk). If X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n, X\\r\\nA\\r\\nand Y\\r\\nA\\r\\nare\\r\\nmultivariate centred Gaussian variables\\r\\n∀β ∈ R\\r\\np\\r\\n∶ sup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β) ≤\\r\\n1\\r\\n2\\r\\nRˆ\\r\\n+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nRˆ ∆(β) + η(n) (6)\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0, where\\r\\nη(n)\\r\\n.\\r\\n.= (1 + τ )(∥β∥\\r\\n2\\r\\n1 + 1) φ+\\r\\n(p, n)\\r\\nφ+(p, n)\\r\\n.\\r\\n.= φ(p, nA) + φ(p, n0\\r\\n) and\\r\\nφ(p, nU)\\r\\n.\\r\\n.= V[Y\\r\\nU\\r\\n](max\\r\\n1≤k≤p\\r\\nV[X\\r\\nU\\r\\nk\\r\\n])\\r\\n⎛\\r\\n⎜⎜\\r\\n⎝\\r\\n√\\r\\n4q + 8 log(p)\\r\\nnU\\r\\n+\\r\\n4q + 8 log(p)\\r\\nnU\\r\\n⎞\\r\\n⎟⎟\\r\\n⎠\\r\\nHence η(n) = op(1/\\r\\n√\\r\\nmin(nA, n0))\\r\\nProof.\\r\\nsup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β)\\r\\n=\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nR∆(β) by lemma 1\\r\\n≤\\r\\n1\\r\\n2\\r\\nRˆ\\r\\n+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nRˆ ∆(β) + (1 + τ ) φ(β) by proposition 6\\r\\n32\\nA.4 Model selection\\r\\nCorollary 4 (Loss concentration).\\r\\n∀λ ∈ Λ ∣\\r\\n˜θS(λ) − ˆθS(λ)∣ ≤ ES φ˜(λ)\\r\\nwith probability exceeding 1 − 2e\\r\\n−q\\r\\nfor q > 0,\\r\\nwhere φ˜(λ)\\r\\n.\\r\\n.= (\\r\\nÂÂÂÂÂ\\r\\nβˆ\\r\\nS,0\\r\\nλ\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n1\\r\\n+ 1)φ(p, nA(S\\r\\nA\\r\\n, 1), n0(S\\r\\n0\\r\\n, 1))\\r\\nProof.\\r\\n˜θS(λ) = ES R∆(βˆ\\r\\nS,0\\r\\nλ\\r\\n) by definition\\r\\n≤ ES Rˆ ∆(βˆ\\r\\nS,0\\r\\nλ\\r\\n) + ES φ˜(λ) by proposition 6\\r\\n≤ ES ∣ Rˆ ∆(βˆ\\r\\nS,0\\r\\nλ\\r\\n)∣ + ES φ˜(λ)\\r\\n= ˆθS(λ) + ES φ˜(λ) by definition\\r\\nNoting that\\r\\n∣ R∆(β) − ∣ Rˆ ∆(β)∣∣ = ∣∣ R∆(β)∣ − ∣ Rˆ ∆(β)∣∣ ≤ ∣ R∆(β) − Rˆ ∆(β)∣\\r\\nby the reverse triangle inequality. An analogous proof can be constructed showing that\\r\\nˆθS(λ) ≤ ˜θS(λ) + ES φ˜(λ) which concludes the proof.\\r\\nLemma 3 (Asymptotic equivalence of sample and population selectors). If X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n,\\r\\nX\\r\\nA\\r\\nand Y\\r\\nA\\r\\nare multivariate centred Gaussian variables\\r\\n∣\\r\\n˜θS(λˆ\\r\\nS) − ˜θS(λ˜S)∣ p\\r\\n→ 0\\r\\nProof.\\r\\n˜θS(λ˜\\r\\nS) ≤ ˜θS(λˆS) since ˜θS(λ˜S) ≤ ˜θS(λ)∀λ ∈ Λ\\r\\n≤ ˆθS(λˆ\\r\\nS) + ES φ˜(λˆS) by corollary 4\\r\\n≤ ˆθS(λ˜\\r\\nS) + ES φ˜(λˆS) since ˆθS(λˆS) ≤ ˆθS(λ)∀λ ∈ Λ\\r\\n≤ ˜θS(λ˜\\r\\nS) + ES[φ˜(λˆS) + φ˜(λ˜S)] by corollary 4\\r\\nThus,\\r\\n0 ≤ ˜θS(λˆ\\r\\nS) − ˜θS(λ˜S) ≤ ES[φ˜(λˆS) + φ˜(λ˜S)] = op\\r\\n(1/\\r\\n√\\r\\nmin(nA, n0)) (20)\\r\\nwhich implies the stated lemma.\\r\\n33\\nCorollary 3 (Asymptotic equivalence of sample and S-optimal selectors). If X\\r\\n0\\r\\n,\\r\\nY\\r\\n0\\r\\n, X\\r\\nA\\r\\nand Y\\r\\nA\\r\\nare multivariate centred Gaussian variables\\r\\n∣ ES[\\r\\n˜θS,0\\r\\n(λˆ\\r\\nS) − ˜θS,0\\r\\n(λ˜\\r\\nS,0\\r\\n)]∣ p→ 0\\r\\nProof.\\r\\nES\\r\\n˜θS,0\\r\\n(λˆ\\r\\nS) = ˜θS(λˆS) by definition\\r\\n≤ ˜θS(λ˜\\r\\nS) + ES[φ˜(λˆS) + φ˜(λ˜S)] by eq. (20)\\r\\n≤ ˜θS(λ˜\\r\\nS,0\\r\\n) + ES[φ˜(λˆ\\r\\nS) + φ˜(λ˜S)] since ˜θS(λ˜S) ≤ ˜θS(λ)∀λ ∈ Λ\\r\\n= ES\\r\\n˜θS,0\\r\\n(λ˜\\r\\nS,0\\r\\n) + ES[φ˜(λˆ\\r\\nS) + φ˜(λ˜S)] by definition\\r\\nThus,\\r\\n0 ≤ ES[\\r\\n˜θS,0\\r\\n(λˆ\\r\\nS) − ˜θS,0\\r\\n(λ˜\\r\\nS,0\\r\\n)] ≤ ES[φ˜(λˆ\\r\\nS) + φ˜(λ˜S)] = op\\r\\n(1/\\r\\n√\\r\\nmin(nA, n0))\\r\\nB Population worst risk computation\\r\\nUsing lemma 1, we want to compute the worst risk at the population level. Henceforth, we\\r\\ngenerate (X\\r\\nA˜\\r\\n, Y\\r\\nA˜\\r\\n) = SEM(B, A˜ ) for A˜ ∈ {A, 0} such that E[ϵ] = 0, E[A] = 0, C[A] = Ip×p and\\r\\nC[ϵ] = I(p+1)×(p+1). For all β ∈ R\\r\\np\\r\\n, we have that\\r\\nsup\\r\\nA˜∈C1+τ\\r\\nRA˜ (β) =\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\n1 + 2τ\\r\\n2\\r\\nR∆(β) by lemma 1\\r\\ns.t. R+(β) = (β − βOLS)\\r\\nT G+(β − βOLS) (21)\\r\\nand R∆(β) = (β − βPA)\\r\\nT G∆(β − βPA) (22)\\r\\nwhere βOLS\\r\\n.\\r\\n.= G\\r\\n-1\\r\\n+ Z+. We proceed to compute both R+(β) and R∆(β). Consider the defini\\x02tions\\r\\nG∆ = C[X\\r\\nA\\r\\n] − C[X\\r\\n0\\r\\n] (23)\\r\\nG+ = C[X\\r\\nA\\r\\n] + C[X\\r\\n0\\r\\n] (24)\\r\\nand the following expansion for A˜ ∈ {A, 0}\\r\\nC[X\\r\\nA˜\\r\\n] = u(C[ϵ] + [\\r\\n0 0\\r\\nT\\r\\n0 C[A˜ ]\\r\\n])u\\r\\nT\\r\\nwhere u = [w M] from eq. (14)\\r\\n= ww\\r\\nT\\r\\n+ MMT+ M C[A˜ ]M\\r\\nT\\r\\nsince C[ϵ] = Ip×p\\r\\n= M (βCH β\\r\\nT\\r\\nCH +I + C[A˜ ]) M\\r\\nT\\r\\nsince M βCH = w by eq. (15)\\r\\n34\\nhence by replacing C[X\\r\\nA\\r\\n] and C[X\\r\\n0\\r\\n] in equations (23) and (24), we get\\r\\nG∆ = M C[A]M\\r\\nT\\r\\n= MMT\\r\\nG+ = M (2 βCH βCH\\r\\nT\\r\\n+3I) M\\r\\nT\\r\\n(25)\\r\\nthen plugging the last two equations into equations (22) and (21), we obtain\\r\\nR+(β) = 2\\r\\nÂÂÂÂÂ\\r\\nβ\\r\\nT\\r\\nCH M\\r\\nT\\r\\n(β − βOLS)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\n+ 3\\r\\nÂÂÂÂÂ\\r\\nM\\r\\nT\\r\\n(β − βOLS)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\n(26)\\r\\nR∆(β) =\\r\\nÂÂÂÂÂ\\r\\nM\\r\\nT\\r\\n(β − βPA)\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\n(27)\\r\\nWe compute M\\r\\nT\\r\\nβOLS to simplify the computation of R+(β)\\r\\nβOLS\\r\\n.\\r\\n.= G\\r\\n-1\\r\\n+ Z+ = G\\r\\n-1\\r\\n+ (C[X\\r\\nA\\r\\n, Y\\r\\nA\\r\\n] + C[X\\r\\n0\\r\\n, Y\\r\\n0\\r\\n])\\r\\nwhere for A˜ ∈ {A, 0}\\r\\nC[X\\r\\nA˜\\r\\n, Y\\r\\nA˜\\r\\n] = C[X\\r\\nA˜\\r\\n, βT\\r\\nPA X\\r\\nA˜\\r\\n+ϵY ]\\r\\n= C[X\\r\\nA˜\\r\\n] βPA + C[X\\r\\nA˜\\r\\n, ϵY ]\\r\\n= C[X\\r\\nA˜\\r\\n] βPA +u C[ϵ +\\r\\n⎡\\r\\n⎢⎢⎢⎢⎢⎢⎢⎢\\r\\n⎣\\r\\n0\\r\\nA˜\\r\\n⎤\\r\\n⎥⎥⎥⎥⎥⎥⎥⎥\\r\\n⎦\\r\\n, ϵY ]\\r\\n= C[X\\r\\nA˜\\r\\n] βPA +w V[ϵY ] since C[A˜ , ϵY ] = 0 and C[ϵX, ϵY ] = 0\\r\\n= C[X\\r\\nA˜\\r\\n] βPA +w\\r\\n= C[X\\r\\nA˜\\r\\n] βPA +M βCH by eq. (15)\\r\\nThus,\\r\\nβOLS\\r\\n.\\r\\n.= G\\r\\n-1\\r\\n+ Z+ = G\\r\\n-1\\r\\n+ (G+ βPA +2M βCH) = βPA +2 G-1+ M βCH\\r\\nand\\r\\nM\\r\\nT\\r\\nβOLS = M\\r\\nT\\r\\nβPA +2M\\r\\nT G-1\\r\\n+ M βCH\\r\\n= M\\r\\nT\\r\\nβPA +2 (2 βCH βCH\\r\\nT\\r\\n+3I)\\r\\n−1\\r\\nβCH by eq. (25)\\r\\n= M\\r\\nT\\r\\nβPA + (βCH βCH\\r\\nT\\r\\n+\\r\\n3\\r\\n2\\r\\nI)\\r\\n−1\\r\\nβCH (28)\\r\\nIn summary, an algorithm for computing supA˜∈C1+τ RA˜ (β) is\\r\\n35\\nAlgorithm 1 Computation of population worst risk for β\\r\\n1 M ← ((I − BX) − βCH βPA\\r\\nT\\r\\n)\\r\\n−1\\r\\n▷ By eq. (16)\\r\\n2 h ← M βPA\\r\\n3 m ← M\\r\\nT\\r\\nβOLS = h + (βCH βCH\\r\\nT\\r\\n+\\r\\n3\\r\\n2\\r\\nI)\\r\\n−1\\r\\nβCH ▷ By eq. (28)\\r\\n4 b ← M\\r\\nT\\r\\nβ\\r\\n5 δ ← M\\r\\nT\\r\\n(β − βOLS) = b − m\\r\\n6 R+(β) ← 2\\r\\nÂÂÂÂÂ\\r\\nβ\\r\\nT\\r\\nCH δ\\r\\nÂÂÂÂÂ\\r\\n2\\r\\n2\\r\\n+ 3 ∥δ∥\\r\\n2\\r\\n2 ▷ By eq. (26)\\r\\n7 R∆(β) ← ∥b − h∥\\r\\n2\\r\\n2 ▷ By eq. (27)\\r\\n8 supA˜∈C1+τ RA˜ (β) ←\\r\\n1\\r\\n2\\r\\nR+(β) +\\r\\n1+2τ\\r\\n2\\r\\nR∆(β)\\r\\n36'},\n",
       " {'name': '1911.02194v1.pdf',\n",
       "  'content': 'arXiv:1911.02194v1 [q-fin.MF] 6 Nov 2019\\r\\nA Rational Finance Explanation of the Stock\\r\\nPredictability Puzzle\\r\\nAbootaleb Shirvania, Svetlozar T. Rachevb, and Frank J. Fabozzic\\r\\naDepartment of Mathematics and Statistics, Texas Tech University, abootaleb.shirvani@ttu.edu\\r\\nbDepartment of Mathematics and Statistics, Texas Tech University, zari.rachev@ttu.edu\\r\\ncEDHEC Business School, frank.fabozzi@edhec.edu\\r\\nAbstract In this paper, we address one of the main puzzles in finance observed in the\\r\\nstock market by proponents of behavioral finance: the stock predictability puzzle. We offer a\\r\\nstatistical model within the context of rational finance which can be used without relying on\\r\\nbehavioral finance assumptions to model the predictability of stock returns. We incorporate\\r\\nthe predictability of stock returns into the well-known Black-Scholes option pricing formula.\\r\\nEmpirically, we analyze the option and spot trader’s market predictability of stock prices by\\r\\ndefining a forward-looking measure which we call “implied excess predictability”. The em\\x02pirical results indicate the effect of option trader’s predictability of stock returns on the price\\r\\nof stock options is an increasing function of moneyness, while this effect is decreasing for spot\\r\\ntraders. These empirical results indicate potential asymmetric predictability of stock prices\\r\\nby spot and option traders. We show in pricing options with the strike price significantly\\r\\nhigher or lower than the stock price, the predictability of the underlying stock’s return should\\r\\nbe incorporated into the option pricing formula. In pricing options that have moneyness close\\r\\nto one, stock return predictability is not incorporated into the option pricing model because\\r\\nstock return predictability is the same for both types of traders. In other words, spot traders\\r\\nand option traders are equally informed about the future value of the stock market in this\\r\\ncase. Comparing different volatility measures, we find that the difference between implied and\\r\\nrealized variances or variance risk premium can potentially be used as a stock return predictor.\\r\\nKeywords Predictability of stock returns; behavioral finance; rational dynamic stock\\r\\npricing theory; option pricing; Stratonovich integral.\\r\\n1\\n1 Introduction\\r\\nIn an efficient market, price discovery should be instantaneous and contemporaneous.1\\r\\nEmpirical evidence suggests that the excess aggregate stock market returns are predictable.\\r\\nUsing monthly, real, equal-weighted New York Stock Exchange returns from 1941–1986,\\r\\nFama and French (1988) found that the dividend–price ratio can explain 27% of the variation\\r\\nof cumulative stock returns over the subsequent four years. Campbell and Shiller (1988)\\r\\nspecify econometric models of dividend discounting that imply that price dividend ratios\\r\\npredict stock returns. These two studies were among the first to identify this as the “stock\\r\\npredictability puzzle.”\\r\\nThere are a good number of more recent empirical studies that have investigated the\\r\\npredictability of stock returns. Some believe stock return predictability is attribute to changes\\r\\nin business conditions, while others attribute it to market inefficiency.\\r\\nThe majority of work on the predictability of stock returns is based on statistical, macro,\\r\\nand fundamental factor analyses models.2 Recently, a good numbers of studies in behavioral\\r\\nfinance have examined behavioral factors that could lead to the predictability of stock returns.\\r\\nThe behavioral factors that proponents of behavioral finance have suggested that can lead to\\r\\nstock return predictability are (1) sentiment, (2) overconfidence, (3) optimism and wishful\\r\\nthinking, (4) conservatism, euphoria and gloom, (5) self-deception, (6) cursedness, (7) belief\\r\\nperseverance, and (8) anchoring.3\\r\\nMotivated by the empirical findings that stock returns are predictable, some researchers\\r\\nhave investigated the impact of stock return predictability on the prices of related assets.\\r\\nLo and Wang (1995), for example, discussed the effect of stock return predictability on the\\r\\nprice of stock options. They showed that even a small amount of predictability could have\\r\\na significant impact on option pricing. Liao and Chen (2006) demonstrated that the effect\\r\\nof autocorrelated returns on European option prices is significant. Huang et al. (2009) and\\r\\nPaschke and Prokopczusk (2010) offer even more recent examples of studies about the impact\\r\\nof stock returns predictability on the valuation of options. The upshot of these studies is that to\\r\\nobtain more realistic stock prices, it is essential to model and analyze stock return predictability\\r\\nand incorporate its impact into stock log-returns and option pricing models.\\r\\nModeling and analyzing the stock return predictability is crucial for stock and risk\\r\\n1See Kumar and Chaturvedul (2013).\\r\\n2See for example, Kandel and Stambaugh (1996), Neely and Weller (2000), Malkiel (2003),\\r\\nBarberis and Thaler (2003), Shiller (2003), Avramov (2003), Wachter and Warusawitharana (2009), Pesaran\\r\\n(2010), Zhou (2010), and Bekiros (2013). The models that have been used are (1) Conditional Capital Asset\\r\\nPricing Model, (2) vector autoregressive models, (3) Bayesian statistical factor analysis, (4) posterior moments of\\r\\nthe predictable regression coefficients, (5) posterior odds, (6) the information in stock prices, (7) business cycles\\r\\neffects, (8) stock predictability of future returns from initial dividend yields, (9) firm characteristics as stock return\\r\\npredictors, (10) anomalies, (11) predictive power of scaled-price ratios such as book-to-market and earnings-to\\x02price, forward spread, and short rate, (12) variance risk premia and variance spillovers, (13) momentum, market\\r\\nmemory and reversals, and (14) early announcements and others.\\r\\n3See Lewellen (2000), Barberis and Thaler (2003), Ferson (2006), Peleg (2000, Chapter 1), and\\r\\nDaniel and Hirshleifer (2015).\\r\\n2\\nmanagers.4 Lo and Wang (1995) introduced a model to price options when stock returns\\r\\nare predictable. Their model is based on a specially designed multivariate trending Orn\\x02stein–Uhlenbeck (O-U) process includes many parameters. The trending O-U processes with\\r\\nsmall dimensions such as univariate and bivariate processes are not realistic as noted by\\r\\nLo and Wang (1995). Moreover, in their model, predictability is induced by the drift parame\\x02ter, which is not a parameter in the classical Black-Scholes model.\\r\\nIn this paper, we propose a method to model the prediction of stock prices by adjusting\\r\\nthe stock predictability as a parameter with the Black and Scholes (1973) and Merton (1973)\\r\\nmodel framework by using the Stratonovich integral.5 In our model, predictability is viewed as\\r\\nthe dividend yield, which we refer to dividend yield due to predictability, and is incorporated\\r\\ninto the option pricing formula. We derive an option pricing model by incorporating the\\r\\npredictable stock returns within the classical Black-Scholes-Merton (BSM) framework.\\r\\nNext, we define implied excess predictability to compare an option trader’s predictability\\r\\nof stock returns with that of a trader in the cash market (i.e., spot trader). Using the observed\\r\\nprice of European call options based on the SPDR S&P 500 ETF (SPY), we plot the implied\\r\\nexcess predictability against “moneyness” and time to maturity. The pattern of the implied\\r\\nexcess predictability surface shows that at each maturity, an option trader’s predictability of\\r\\nthe SPY is an increasing function of moneyness. The turning point of the surface is where the\\r\\nmoneyness is close to 0.95. The effect of option trader’s predictability of stock returns on the\\r\\nprice of stock options increases when the moneyness increases from 0.95 to 1.20. Conversely„\\r\\nwhen the moneyness decreases from 0.95 to 0.7, the effect of spot trader’s predictability of\\r\\nstock returns on the price of stock options decreases. These empirical results indicate potential\\r\\nasymmetric predictability of stock prices by spot and option traders.\\r\\nWe demonstrate that in pricing an option with significant intrinsic value, stock return\\r\\npredictability should be incorporated into the BSM model. In pricing options that have\\r\\nmoneyness close to one stock predictability is not incorporated into the BSM model because\\r\\nstock predictability is the same for both types of traders. In other words, spot traders and option\\r\\ntraders are equally informed about stock market in this case. We show a popular stock market\\r\\nvolatility index — the CBOE volatility index (VIX 6) – is potentially more informative than the\\r\\nother volatility measures (historical, realized, and time series estimation method volatility) for\\r\\npredicting stock returns. The variance risk premium – the difference between implied variance\\r\\nand realized variance – can potentially predict stock market returns.\\r\\nThis paper is organized as follows. The next section describes our methodology for model\\x02ing the prediction of stock prices. Then we derive an option pricing formula by incorporating\\r\\nthe predictability of stock returns into the model. Section 3 describes the results of our model\\r\\nusing the S&P 500 index options. We then analyze and compare the prediction of stock market\\r\\nreturns by option and spot traders. Section 5 summarizes our findings.\\r\\n4See Shirvani et al. (2019).\\r\\n5See Kloeden et al. (2000, Chapter 2), Øksendal (2003, Chapter 5), and Syga (2015).\\r\\n6VIX is an index created by CBOE, representing 30-day implied volatility calculated by S&P500 options, see\\r\\nhttp://www.cboe.com/vix.\\r\\n3\\n2 The Predictability of Stock pricing\\r\\nA major issue raised by the proponents of behavioral finance is that prices are often pre\\x02dictable.7. More precisely, given a stochastic basis (Ω, F, F= (Ft\\r\\n, t ≥ 0),P ) a price process\\r\\nS (t) t ≥ 0, defined on (Ω,F,P) is not necessarily F-adapted, it is adapted to an augmented\\r\\nfiltration F\\r\\n(∗) ⊃\\r\\nÐ\\r\\nt≥o Ft\\r\\n, with F\\r\\n(∗)⊂F.\\r\\nAdmitting the fact that stock returns are predictable, we propose a method to model the\\r\\nprediction of stock returns by adjusting the predictability of stock returns. Our option pricing\\r\\nmodel is close to the idea put forth by Shiller (2003) of “smart money versus ordinary investors.”\\r\\nTo model the predictability of stock prices, we use the Stratonovich integral8:\\r\\n∫ T\\r\\n0\\r\\nθ (t) ◦\\r\\n(\\r\\n1\\r\\n2\\r\\n)dB (t)\\r\\n= lim0=t\\r\\n(0)<t(1)<···<t(k)=T,t(j)=j∆t, ∆t↓0\\r\\nÍk−1\\r\\nj=0\\r\\nθ\\r\\n\\x10\\r\\nt\\r\\n(j+1)+t(j)\\r\\n2\\r\\n\\x11 \\x10B(t\\r\\n(j+1)\\r\\n) − B(t\\r\\n(j)\\r\\n)\\r\\n\\x11\\r\\n.\\r\\n(1)\\r\\nIn (1), B (t), t ≥ 0, is a Brownian motion generating a stochastic basis(Ω,F, F= (Ft, t ≥ 0),P ),\\r\\nθ (t) t ≥ 0 is F-adapted left-continuous and locally bounded process. An important property\\r\\nof the Stratonovich integral is that it “looks into the future,” and therefore, price processes\\r\\nbased on the Stratonovich integral possess predictability properties. In sharp contrast, the Itô\\r\\nintegral:\\r\\n∫ T\\r\\n0\\r\\nθ(t)dB(t) = lim0=t\\r\\n(0)<t(1)<···<t(k)=T, t(j)=j∆t, ∆t↓0\\r\\nÕ\\r\\nk−1\\r\\nj=0\\r\\nθ\\r\\n\\x10\\r\\nt\\r\\n(j)\\r\\n\\x11 \\x10B(t\\r\\n(j+1)\\r\\n) − B(t\\r\\n(j)\\r\\n)\\r\\n\\x11\\r\\n(2)\\r\\n“does not look in the future,” and thus Itô prices are not predictable. Combining both integrals\\r\\n(1) and (2) within a Stratonovich α-integral with α ∈ [0, 1] we obtain:\\r\\n∫ T\\r\\n0\\r\\nθ (t) ◦\\r\\n(α)dB (t)\\r\\n= lim0=t\\r\\n(0)<t(1)<···<t(k)=T,t(j)=j∆t, ∆t↓0\\r\\nÍk−1\\r\\nj=0\\r\\nθ\\r\\n\\x10\\r\\nt\\r\\n(j)\\r\\n(1 − α) + αt\\r\\n(j+1)\\r\\n\\x11 \\x10B(t\\r\\n(j+1)\\r\\n) − B(t\\r\\n(j)\\r\\n)\\r\\n\\x11\\r\\n= 2α\\r\\n∫ T\\r\\n0\\r\\nθ (t) ◦\\r\\n(\\r\\n1\\r\\n2\\r\\n)dB (t)+ (1 − 2α)\\r\\n∫ T\\r\\n0\\r\\nθ (t) dB (t).\\r\\n(3)\\r\\nConsider a market with two assets: (i) a risky asset (stock) S with potentially predictive\\r\\nprice process S (t), t ≥ 0, following Itô stochastic differential equation (SDE):\\r\\ndS (t) = µ (t, S (t)) dt + σ (t, S (t)) dB (t), t ≥ 0, S (0) > 0, (4)\\r\\nwhere µ (t, S (t)) = µt S (t), and σ (t, S (t)) = σt S (t), For the regularity conditions implying\\r\\nexistence and uniqueness of the strong solution of (3), see Duffie (2001, Chapter 6). By the\\r\\nItô formula, stock price dynamics is given by\\r\\n7See, for example, Daniel and Hirshleifer (2015).\\r\\n8See Kloeden et al. (2000, Chapter 2), Øksendal (2003, Chapter 5), and Syga (2015).\\r\\n4\\nS (t) = S (0) exp \\x1a∫ t\\r\\n0\\r\\n\\x12\\r\\nµs −\\r\\n1\\r\\n2\\r\\nσ\\r\\n2\\r\\ns\\r\\n\\x13\\r\\nds +\\r\\n∫ t\\r\\n0\\r\\nσsdB (s)\\r\\n\\x1b\\r\\n, S (0) > 0, t ≥ 0.\\r\\n(ii) riskless asset (bond) B with price process β (t), t ≥ 0, defined by\\r\\ndβ (t) = rt β (t),rt = r (t, S (t)), β (0) > 0, (5)\\r\\nthat is, β (t) = β (t) exp \\x10∫ t\\r\\n0\\r\\nrsds\\x11t ≥ 0.\\r\\nConsider a European Contingent Claim (ECC) C with price process C (t) = C (t, S (t)),\\r\\nwhere C (t, x), t ≥ 0, x > 0, has continuous derivatives ∂C(t,x)\\r\\n∂t\\r\\nand ∂\\r\\n2C(t,x)\\r\\n∂x\\r\\n2\\r\\n. C’s terminal time\\r\\nis T > 0, and C ‘s terminal payoff is C (T) = C (T, S (T)) = g (S (T)), for some continuous\\r\\ng : (0, ∞) → R.\\r\\nAssume that a trader i\\r\\n(l)\\r\\ntakes a long position in C. Furthermore, when i\\r\\n(l)\\r\\ntrades stock\\r\\nS with possibly superior or inferior to (4), the following Stratonovich α SDE:\\r\\ndS (t) = µ (t, S (t)))dt + σ (t, S (t)) ◦\\r\\n(α)\\r\\ndB (t), t ≥ 0, S (0) > 0, α ∈ [0, 1] . (6)\\r\\nThus, the Stratonovich SDE\\r\\ndS (t) = µ (t, S (t)) dt + σ (t, S (t)) ◦\\r\\n(α)\\r\\ndB (t),\\r\\nis equivalent to the Itô SDE\\r\\ndS (t) =\\r\\n\\x10\\r\\nµ (t, S (t)) + ασ (t, S (t)) ∂σ(t,S(t))\\r\\n∂x\\r\\n\\x11\\r\\ndt + σ (t, S (t)) dB (t)\\r\\n= µ\\r\\n(α)\\r\\nt\\r\\nS (t) dt + σtS (t) dB (t), µ\\r\\n(α)\\r\\nt = µt + ασ2\\r\\nt\\r\\n, t ≥ 0, t ≥ 0.\\r\\n(7)\\r\\nAssume that a trader i\\r\\n(s)\\r\\ntakes a short position in C trading in the contract where i\\r\\n(l) had\\r\\ntaken the long position. i\\r\\n(l)\\r\\nand i\\r\\n(s) have entered the contract C as the only participants at the\\r\\nclosing bid-ask traded C-contract.9 i\\r\\n(s) observes only the dynamics of S traded by i(l)\\r\\nand\\r\\ngiven by (3). Furthermore, when i\\r\\n(s)\\r\\ntrades stock S, with dynamics following Stratonovich γ\\r\\nSDE:\\r\\ndS (t) = µ (t, S (t)) dt + σ (t, S (t)) ◦\\r\\n(γ)\\r\\ndB (t), t ≥ 0, S (0) > 0, (8)\\r\\nfor some γ ∈ [0, 1]; that is,\\r\\ndS (t) = µ\\r\\n(γ)\\r\\nt\\r\\nS (t) dt + σtS (t) dB (t), µ\\r\\n(α)\\r\\nt = µt + γσ2\\r\\nt\\r\\nt ≥ 0, t ≥ 0. (9)\\r\\nThe C-dynamics as traded by i\\r\\n(l)\\r\\nis determined by the Itô formula:\\r\\n9We assume that i\\r\\n(l)\\r\\nand i\\r\\n(s)\\r\\nare the two trading parties in a bid-ask trade of C providing the smallest bid-ask\\r\\nspread, which ultimately ends up with the trade transaction of C.\\r\\n5\\ndC (t, S (t))\\r\\n=\\r\\nn\\r\\n∂C(t,S(t))\\r\\n∂t\\r\\n+\\r\\n∂C(t,S(t))\\r\\n∂x\\r\\nµ\\r\\n(γ)\\r\\nt\\r\\nS (t) +\\r\\n1\\r\\n2\\r\\n∂\\r\\n2C(t,S(t))\\r\\n∂x\\r\\n2 σ\\r\\n2\\r\\nt\\r\\nS(t)\\r\\n2\\r\\no\\r\\ndt +\\r\\n∂C(t,S(t))\\r\\n∂x\\r\\nσtS (t) dB (t) .\\r\\n(10)\\r\\nTo hedge the risky position, i\\r\\n(s)\\r\\nforms a replicating self-financing strategy given by the\\r\\npair a (t), b (t), t ≥ 0, where C (t, S (t)) = a (t) S (t)+ b(t)β (t) with dC (t, S (t)) = a (t) dS (t)+\\r\\nb (t) dβ (t) . Thus,\\r\\ndC (t, S (t)) =\\r\\n\\x10\\r\\na (t) µ\\r\\n(γ)\\r\\nt + b (t)rt β (t)\\r\\n\\x11\\r\\nS (t) dt + a (t)σ (t, S (t)) dB (t) (11)\\r\\nFrom (10) and (11), i\\r\\n(s) obtains a (t) =\\r\\n∂C(t,S(t))\\r\\n∂x\\r\\n, and b (t) β (t) = C (t, S (t)) −\\r\\n∂C(t,S(t))\\r\\n∂x\\r\\nS (t) .\\r\\nEquating the terms with dt and setting S (t) = x, results in the following partial differential\\r\\nequation (PDE):\\r\\n0 =\\r\\n∂C (t, x)\\r\\n∂t\\r\\n+\\r\\n∂C (t, x)\\r\\n∂x\\r\\n\\x10\\r\\nrt − pσ\\r\\n2\\r\\nt\\r\\n\\x11\\r\\nx − rtC (t, x) +\\r\\n1\\r\\n2\\r\\n∂\\r\\n2C (t, x)\\r\\n∂x\\r\\n2\\r\\nσ\\r\\n2\\r\\nt\\r\\nx\\r\\n2\\r\\n, p = γ − α. (12)\\r\\nWe call p ∈ [−1, 1] the excess predictability of S traded by i\\r\\n(s) over the S-dynamic, when S\\r\\nis traded by i\\r\\n(l)\\r\\n. In the classical Black-Scholes model, dividends were not accounted for in\\r\\nthe model. If we assume that the stock S provides a continuous dividend yield of pσ\\r\\n2\\r\\nt\\r\\n(i.e.,\\r\\nthe dividend paid over interval (t, t + dt] equals pσ\\r\\n2\\r\\nt\\r\\nSt) we obtain the Black-Scholes partial\\r\\ndifferential equation given by (12). Borrowing this idea, stock with continuously compounded\\r\\ndividend yield pσ\\r\\n2\\r\\nt\\r\\n, we denote Dy (t) = pσ\\r\\n2\\r\\nt\\r\\nas the dividend yield due to predictability. As\\r\\nthe payment of dividends impacts the option price of the underlying stock, the stock return\\r\\npredictability impacts the price of options. Depending on the sign of p, Dy (t) could be positive\\r\\nor negative. When p = 0, we obtain the classical Black-Scholes equation.\\r\\nIn particular, C-price dynamics is given by10\\r\\nC (t) = E\\r\\nQ\\r\\nt\\r\\nn\\r\\ne\\r\\n−\\r\\n∫ T\\r\\nt\\r\\nru dug (S(T))}\\r\\no\\r\\n, t ∈ [0, T), (13)\\r\\nwhere Q is the equivalent martingale measure for the dividend-stock-price. That is, Q∼P, and\\r\\nthe discounted gain process G(Y)\\r\\n(t) = X\\r\\n(Y)\\r\\n(t) + D(Y)(t) is a Q-martingale,\\r\\nS (t) = S (0) exp \\x1a∫ t\\r\\n0\\r\\n\\x12\\r\\nµs −\\r\\n1\\r\\n2\\r\\nσ\\r\\n2\\r\\ns\\r\\n\\x13\\r\\nds +\\r\\n∫ t\\r\\n0\\r\\nσsdB (s)\\r\\n\\x1b\\r\\n, S (0) > 0, t ≥ 0.\\r\\nY (t) =\\r\\n1\\r\\nβ(t)\\r\\n, t ≥ 0, X\\r\\n(Y)\\r\\n(t) = X (t)Y (t), and dD(Y)(t) = Y (t) dD (t). The dynamics of S on Q\\r\\nis given by dS (t) =\\r\\n\\r\\nrt − Dy (t, x)\\r\\n\\x01\\r\\nS (t) dt + dB (t), where rtis the risk-free rate at time t.\\r\\nIn conclusion, with (13) we are able to incorporate the predictability of stock returns into\\r\\n10See Duffie (2001, Section 6).\\r\\n6\\noption pricing formula within the classical Black-Scholes-Merton framework.\\r\\nSuppose C is a European call option with maturity T and strike K, and g (S(T)) =\\r\\nmax (S (T) − K, 0). Then for time to maturity, τ = T − t, the value of a call option for a\\r\\ndividend-paying underlying stock in terms of the Black–Scholes parameters is 11\\r\\nC (t) = c (S (t), τ, K,rt, σt, p) = S (t) e\\r\\n−Dy (t)τΦ (d+ ) − Ke−rt τΦ (d− ), (14)\\r\\nwhere Dy (t) = pσ\\r\\n2\\r\\nt\\r\\n, Φ denotes the standard normal cumulative distribution function, and\\r\\nd± =\\r\\nln\\x12\\r\\nS(t)e\\r\\n−Dy(t)τ\\r\\nK e−rt τ\\r\\n\\x13\\r\\n±\\r\\n1\\r\\n2\\r\\nσ\\r\\n2\\r\\nt\\r\\nτ\\r\\nσt\\r\\n√\\r\\nτ\\r\\n. Given put–call parity, the price of a put option, P (t) is\\r\\nP (t) = C (t) + Dy (t) − St + Ke−rt τ.\\r\\n3 Implied dividend yield due to predictability\\r\\nIn this section, we compare the option and spot trader’s predictability of stock returns\\r\\nby defining the implied excess predictability. Implied excess predictability is a metric that\\r\\ncaptures the view of the option and spot trader of the likelihood moves in the stock price. It\\r\\ncan be used to predict the of stock price from two perspectives. An important characteristic\\r\\nof implied excess predictability is that it is forward looking. It compares the predictability of\\r\\nmarkets for the given underlying stock market index from two perspectives. Recall that implied\\r\\nexcess predictability is calibrated from the BSM option price formula.\\r\\nWe denote by p the excess predictability of S traded by i\\r\\n(s) over the S-dynamic, when S\\r\\nis traded by i\\r\\n(l)\\r\\n. To study i\\r\\n(s)\\r\\n’s stock return predictability (option trader) compared to i\\r\\n(l)\\r\\n(spot trader), we define implied excess predictability p = p\\r\\n\\x10\\r\\nS(t)\\r\\nK\\r\\n, τ\\x11as a function of moneyness\\r\\nS(t)\\r\\nK\\r\\nand time to maturity τ as the solution of\\r\\nc (S (t), τ, K,rt, σt, p) = C\\r\\n(market)\\r\\n(t, S (t), τ, K), (15)\\r\\nwhere C\\r\\n(market)\\r\\n(t, S (t), τ, K) is the call option prices of SPY12.\\r\\nWe assume that SPY-daily closing prices follow\\r\\nS (t) = S (0) exp \\x1a∫ t\\r\\n0\\r\\nνsds +\\r\\n∫ t\\r\\n0\\r\\nσsdB (s)\\r\\n\\x1b\\r\\n, S (0) > 0, t = k∆t, k ∈ N+ = {0, 1, . . . } , (16)\\r\\nwhere νs = µs −\\r\\n1\\r\\n2\\r\\nσ\\r\\n2\\r\\ns\\r\\n. Thus, the SPY-daily return series is given by\\r\\nR (t + ∆t) = ln \\x12\\r\\nS (t + ∆t)\\r\\nS (t)\\r\\n\\x13\\r\\n=\\r\\n∫ t+∆t\\r\\nt\\r\\nνsds +\\r\\n∫ t+∆t\\r\\nt\\r\\nσsdB (s), t = k∆t, k ∈ N+ . (17)\\r\\n11See, for example, Hull (2009), Chapter 13.\\r\\n12https://nance.yahoo.com/quote/SPY/options?p=SPY.\\r\\n7\\nThus, on Q, the SPY daily return is given by dS (t) =\\r\\n\\r\\nrt − Dy (t, x)\\r\\n\\x01\\r\\nS (t) dt +dB (t). The value\\r\\nof a call option for the time to maturity, τ = T − t, is given by (14). We calculate the implied\\r\\nexcess predictability by taking the option’s market price, entering it into the (15) formula, and\\r\\nback-solving for the value of p.\\r\\nHere, we compare the option and spot trader’s predictability of stock returns by using the\\r\\nimplied excess predictability. Rather than looking at individual stocks, our analysis will focus\\r\\non the aggregate stock market. In our case, the SPY is the proxy we use for the aggregate stock\\r\\nmarket. We compare the predictability of markets for the given underlying stock market index\\r\\nfrom two perspectives, by doing so it provides important insight about the view of option and\\r\\nspot traders regarding the future price of the stock market.\\r\\nWe use call option prices from 01/02/2015 to 01/10/2015 with different expiration dates\\r\\nand strike prices. The expiration date varies from 1/2/2015 to 6/20/2015, and the strike price\\r\\nvaries from 80 to 250 among different call option contracts. The midpoint of the bid and ask\\r\\nis used in the computation. As the underlying of the call option, the SPY index price was\\r\\n206.38 on 01/02/2015. We use the 10-year Treasury yield curve rate 13 on 01/02/2015 as the\\r\\nrisk-free rate rt, here rt = 0.0212.\\r\\nAs an estimates for σt, we use the following four metrics: (1) daily closing values of\\r\\nV IXt/\\r\\n√\\r\\n365; (2) historical volatility based on one-year historical data; (3) realized volatility\\r\\nover one-year historical data; and (4) estimated volatility over one-year by modeling time\\r\\nseries with classical methods ARIMA(1, 1)-GARCH(1, 1) with the Student’s t distribution as\\r\\nan innovation distribution. The minimum estimated value for σtis derived where the realized\\r\\nvolatility is applied and the maximum estimated value is derived where the daily closing values\\r\\nof VIX is used .\\r\\nSince implied excess predictability surfaces of all models are very similar, we plot the\\r\\nexcess predictability surface when σtis estimated from realized volatility. The implied excess\\r\\npredictability surface is graphed against both a standard measure of “moneyness” and time\\r\\nto maturity (in year) in Figure 1. Recall that a high value for p (close to one) means excess\\r\\npredictability of SPY daily return traded by i\\r\\n(s) over the predictability of SPY traded by i(l)\\r\\n.\\r\\nIn other words, p = 1 means that option traders potentially predict the future of the SPY returns\\r\\nbetter than the spot trader. The opposite is true when p = −1. Recall that the implied excess\\r\\npredictability surface is an increasing function of σt.\\r\\nFigure 1 indicates that at each maturity, implied excess predictability of option traders\\r\\nincrease as moneyness increases. Where the moneyness varies in (0, 0.7), the surface is flat at\\r\\npoint −1, indicating higher predictability of spot traders comparing to option traders. Thus,\\r\\nto price significant out-the-money options, the value of p in the model should be −1. Where\\r\\nthe moneyness varies in (1.05, 1.15), in-the-money options, the value of p starts increasing\\r\\nfrom 0.5, and flats out at point 1. This finding indicates that option traders can potentially\\r\\npredict market changes better than spot traders when the option is in-the-money. In this case,\\r\\nfor pricing in-the-money option, the value of p in the log-return model should be 1.\\r\\nThe turning point of the surface is where the moneyness is close to 0.95. When the\\r\\n13https://www.treasury.gov/.\\r\\n8\\n-0.8\\r\\n1.1\\r\\n-0.6\\r\\n-0.4\\r\\n1\\r\\n-0.2\\r\\n0.9\\r\\n0\\r\\n0.2\\r\\n0.8 0.25\\r\\n0.4\\r\\n0.7 0.2\\r\\n0.6\\r\\n0.6\\r\\n0.8\\r\\n0.15 0.5 0.1 0.4\\r\\n0.3 0.05\\r\\nFigure 1: Implied dividend yield against time to maturity and moneyness.\\r\\n0.05\\r\\n1.1\\r\\n0.1\\r\\n1\\r\\n0.15\\r\\n0.9\\r\\n0.2\\r\\n0.8 0.25\\r\\n0.25\\r\\n0.7\\r\\n0.3\\r\\n0.2\\r\\n0.35\\r\\n0.6 0.15 0.5 0.1 0.4\\r\\n0.3 0.05\\r\\nFigure 2: Relative difference of excess predictability VIX-Realized model against time to\\r\\nmaturity and Moneyness.\\r\\nmoneyness varies in (0.90, 1.05), p varies in (−0.5, 0.5). This is the range that spot and option\\r\\ntraders are equally informed about the market, and the predictability of the market is equal\\r\\nfor both traders. Thus, to price options with no significant intrinsic value, the classical BSM\\r\\nequation can be used.\\r\\nAs we mentioned, the other four surfaces are very similar. Here, instead of plotting the\\r\\n9\\nfour similar surfaces, we plot the relative difference of the excess predictability of each surface\\r\\nto the surface derived from realized variance, denoted by pi − p1, where i = 2, 3, 4. Here (1)\\r\\np2 refers to the excess predictability surface when σtis imputed from the VIX index, (2) p3\\r\\nis where σtimputed by historical volatility, and (3) p4 is where σtis estimated by time series\\r\\nmodels. Figures 2-4 show the relative difference of excess predictability for each surface. In\\r\\nall surfaces, where the moneyness varies in (0.90, 1.05), the relative difference is significant.\\r\\nAt each value for moneyness in (0.90, 1.05), the relative difference of excess predictability\\r\\nincreases as time to maturity increases.\\r\\nZhou (2010) defined variance risk premium at time t as the difference between the ex-ante\\r\\nrisk-neutral expectation and the objective or statistical expectation of the return variance over\\r\\nthe [t, t + 1] time interval,\\r\\nV ARt = E\\r\\nQ\\r\\nt\\r\\n(Var(rt+1)) − E\\r\\nP\\r\\nt\\r\\n(Var(rt+1)), (18)\\r\\nwhich is not directly observable in practice. In practice, the risk-neutral expectation of the return\\r\\nvariance, E\\r\\nQ\\r\\nt\\r\\n(Var(rt+1)), is typically replaced by the VIX2index and statistical expectation of\\r\\nthe return variance, E\\r\\nP\\r\\nt\\r\\n(Var(rt+1)), is estimated by realized variance.\\r\\nZhou (2010) showed that the difference between implied variance and realized variance\\r\\n((i.e., variance risk premium) can be used for the short-term predictability of equity returns,\\r\\nbond returns, forward premiums, and credit spreads. Comparing Figures 2-4, the most signif\\x02icant relative difference of excess predictability is observed in Figure 2. It indicates that the\\r\\nVIX index contains more information about the stock market compared to the other metrics.\\r\\nFigure 3, the historical-realized surface, has the minimum relative difference.\\r\\nRecall that the maximum and minimum values for σt are derived from the VIX and realized\\r\\nvolatility. As we observed, the most significant relative difference of excess predictability,\\r\\nFigure 2, is derived where the VIX and realized volatility are used in the model. Thus, by\\r\\ncomparing equation (18) for different volatility measures and the fact that excess predictability\\r\\nis an increasing function of σt, suggests that the variance risk premium measure potentially\\r\\ncontains more information compared to the other variance measures for predicting stock market\\r\\nreturns. The historical volatility measure is the poorest metric.\\r\\n4 Conclusion\\r\\nIn this paper, we studied the predictability of stock returns within the framework of rational\\r\\nfinance rather than relying on behavioral finance explanations. We proposed a method to\\r\\nmodel stock returns by incorporating the predictability of stock returns in the model and\\r\\nthen deriving an option pricing formula. To compare the predictability of stock returns by\\r\\noption traders and spot traders, we constructed a forward-looking measure that compares\\r\\nthe option and spot trader’s stock returns predictability which we called the “implied excess\\r\\npredictability measure.” The empirical results indicate that to price a significant in-the-money\\r\\nand out-the-money option, the option’s and spot trader’s predictability of stock returns should\\r\\n10\\n1.1\\r\\n0.05\\r\\n1\\r\\n0.1\\r\\n0.9\\r\\n0.8 0.25\\r\\n0.15\\r\\n0.7 0.2\\r\\n0.2\\r\\n0.6 0.15 0.5 0.1 0.4\\r\\n0.3 0.05\\r\\nFigure 3: Relative difference of excess predictability Historical-Realized model against time\\r\\nto maturity and Moneyness.\\r\\n0.05\\r\\n1.1\\r\\n0.1\\r\\n1\\r\\n0.15\\r\\n0.9\\r\\n0.2\\r\\n0.8 0.25\\r\\n0.25\\r\\n0.7 0.2\\r\\n0.3\\r\\n0.6 0.15\\r\\n0.35\\r\\n0.5 0.1 0.4\\r\\n0.3 0.05\\r\\nFigure 4: Relative difference of excess predictability time series-Realized model against time\\r\\nto maturity and Moneyness.\\r\\nbe incorporated into the BSM model. For options with a small intrinsic values, spot traders and\\r\\noption traders are equally informed, and the predictability of the market is equal for both traders.\\r\\nIn this case, the classical BSM model can be used for option pricing without incorporating\\r\\nstock return predictability. Finally, we showed that the difference between implied variance\\r\\n11\\nand realized variance, which we called variance risk premium, is an informative measure for\\r\\npredicting the market in contrast to other volatility measures.\\r\\nReferences\\r\\nAvramov, D. (2003). Stock return predictability and asset pricing models. Review of Financial\\r\\nStudies 17, 699–738.\\r\\nBarberis, N. and R. Thaler (2003). A survey of behavioral finance. In G. Harris and R. Stulz\\r\\n(Eds.), Handbook of the Economics of Finance, Chapter 18, pp. 1051–1121. North Holland,\\r\\nAmsterdam: Elsevier Science.\\r\\nBekiros, S. D. (2013). Irrational fads, short-term memory emulation, and asset predictability.\\r\\nReview of Financial Economics 22, 213–219.\\r\\nBlack, F. and M. Scholes (1973). The pricing of options and corporate liabilities. Journal of\\r\\nPolitical Economy 81, 637–654.\\r\\nCampbell, J. Y. and R. J. Shiller (1988). The dividend-price ratio and expectations of future\\r\\ndividends and discount factors. Review of Financial Studies 1(3), 195–228.\\r\\nDaniel, K. and D. Hirshleifer (2015). Overconfident investors, predictable returns, and exces\\x02sive trading. Journal of Economic Perspectives 29, 61–88.\\r\\nDuffie, D. (2001). Dynamic Asset Pricing Theory, 3rd Edition. Princeton University Press:\\r\\nPrinceton N.J.\\r\\nFama, F. E. and R. K. French (1988). Dividend yields and expected stock returns. Journal of\\r\\nFinancial Economics 22, 3–25.\\r\\nFerson, W. E. (2006). Conditional asset pricing. In A. C. Lee (Ed.), Encyclopedia of Finance,\\r\\nChapter 9, pp. 376–383.\\r\\nHuang, Y. C., C. W. Wu, and C. W. Wang (2009). Valuing american options under ARMA\\r\\nprocesses. International Research Journal of Finance and Economics 28, 152– 159.\\r\\nKandel, S. and R. F. Stambaugh (1996). On the predictability of stock returns: An asset\\x02allocation perspective. Journal of Finance 51, 385–424.\\r\\nKloeden, P. E., E. Platen, and H. Schurz (2000). Numerical Solution of SDE Through Computer\\r\\nExperiments. Heidelberg: Springer-Verlag.\\r\\nKumar, K. and C. Chaturvedul (2013). Price leadership between spot and futures markets.\\r\\nJournal of Applied Finance and Banking 3(1), 93–107.\\r\\n12\\nLewellen, J. W. (2000). On the Predictability of Stock Returns: Theory and Evidence. Ph.\\r\\nD. thesis, William E. Simon Graduate School of Business Administration, University of\\r\\nRochester.\\r\\nLiao, S. L. and C. C. Chen (2006). The valuation of European options when asset returns are\\r\\nautocorrelated. Journal of Futures Markets 26(1), 85–102.\\r\\nLo, A. W. and J. Wang (1995). Implementing option pricing models when asset returns are\\r\\npredictable. Journal of Finance 50(1), 87–129.\\r\\nMalkiel, B. G. (2003). The efficient market hypothesis and its critics. Journal of Economic\\r\\nPerspectives 17, 59–82.\\r\\nMerton, R. C. (1973). Theory of rational option pricing. Bell Journal of Economics and\\r\\nManagement Science, 6, 141–183.\\r\\nNeely, C. J. and P. Weller (2000). Predictability in international asset returns: A reexamination.\\r\\nJournal of Financial and Quantitative Analysis 35, 601–620.\\r\\nØksendal, B. K. (2003). Stochastic Differential Equations: An Introduction with Applications.\\r\\nHeidelberg: Springer-Verlag.\\r\\nPaschke, R. and M. Prokopczusk (2010). Commodity derivative valuation with autoregressive\\r\\nand moving average components in the price dynamics. Journal of Banking and Finance 34,\\r\\n2742–2752.\\r\\nPeleg, E. (2000). Three essays on asset pricing, portfolio choice and behavioral finance. Ph.\\r\\nD. thesis, Philosophy in Management, University of California.\\r\\nPesaran, M. H. (2010). Predictability of asset returns and the efficient market hypothesis. In\\r\\nA. Ullah and D. E. Giles (Eds.), Handbook of Empirical Economics and Finance, Chapter 11,\\r\\npp. 281–31. North Holland, Amsterdam: Elsevier Science.\\r\\nShiller, R. J. (2003). From efficient market theory to behavioral finance. Journal of Economic\\r\\nPerspectives 17, 83–104.\\r\\nShirvani, A., S. Rachev, and F. Fabozzi (2019). Multiple subordinated modeling of asset\\r\\nreturns. arXiv:1907.12600 [q-fin.MF] 7.\\r\\nSyga, J. (2015). Semimartingale measure in the investigation of Stratonovich-type stochastic\\r\\nintegrals and inclusions. Discussiones Mathematicae, Probability and Statistics 35, 7–27.\\r\\nWachter, J. A. and M. Warusawitharana (2009). Predictable returns and asset allocation:\\r\\nShould a skeptical investor time the market? Journal of Econometrics 148, 162–178.\\r\\nZhou, H. (2010). Variance risk premia, asset predictability puzzles, and macroeconomic\\r\\nuncertainty. Annual Review of Financial Economics 10(1), 481–497.\\r\\n13'},\n",
       " {'name': '2410.03805v2.pdf',\n",
       "  'content': 'LOCAL ATTENTION MECHANISM: BOOSTING THE\\r\\nTRANSFORMER ARCHITECTURE FOR LONG-SEQUENCE TIME\\r\\nSERIES FORECASTING\\r\\nA PREPRINT\\r\\nIgnacio Aguilera-Martos *, Andrés Herrera-Poyatos *, Julián Luengo *, Francisco Herrera * †\\r\\n* Andalusian Institute of Data Science and Computational Intelligence (DaSCI), University of Granada, Spain.\\r\\nEmails: nacheteam@ugr.es, andreshp@ugr.es, julianlm@decsai.ugr.es, herrera@decsai.ugr.es\\r\\n† ADIA Lab, AI Maryah Island, Abu Dhabi, United Arab Emirates\\r\\nOctober 15, 2024\\r\\nABSTRACT\\r\\nTransformers have become the leading choice in natural language processing over other deep learn\\x02ing architectures. This trend has also permeated the field of time series analysis, especially for\\r\\nlong-horizon forecasting, showcasing promising results both in performance and running time.\\r\\nIn this paper, we introduce Local Attention Mechanism (LAM), an efficient attention mechanism\\r\\ntailored for time series analysis. This mechanism exploits the continuity properties of time series\\r\\nto reduce the number of attention scores computed. We present an algorithm for implementing\\r\\nLAM in tensor algebra that runs in time and memory Θ(n log n), significantly improving upon the\\r\\nΘ(n\\r\\n2\\r\\n) time and memory complexity of traditional attention mechanisms. We also note the lack of\\r\\nproper datasets to evaluate long-horizon forecast models. Thus, we propose a novel set of datasets\\r\\nto improve the evaluation of models addressing long-horizon forecasting challenges.\\r\\nOur experimental analysis demonstrates that the vanilla transformer architecture magnified with\\r\\nLAM surpasses state-of-the-art models, including the vanilla attention mechanism. These results\\r\\nconfirm the effectiveness of our approach and highlight a range of future challenges in long-sequence\\r\\ntime series forecasting.\\r\\nKeywords Disease detection · Deep Learning · Small Data · Computer Vision\\r\\n1 Introduction\\r\\nWe are currently living in the information age, with vast amounts of data being continuously generated and requiring\\r\\nprocessing to maximise their utility. From this type of data, time series emerge as the natural choice for representation\\r\\nin several applications [1]. Time series, which consist of sequentially related data, are commonly encountered in\\r\\nsectors such as finance [2], industry [3], telecommunications [4], electricity [5, 6], and climatology [7, 8]. Time series\\r\\nforecasting involves predicting future time instances based on past information. An specific scenario of this problem\\r\\nis Long Sequence Time Series Forecasting (LSTF) [9], which implies a longer prediction horizon and represents the\\r\\nmost interesting and complex problem in time series forecasting. One of the main concerns of the LSTF problem\\r\\nis capturing long-term trends in the data. To adequately predict these long-term patterns, a substantial number of\\r\\ninstances is required to model complex behaviour effectively.\\r\\nTraditionally, the forecasting problem has been addressed using statistical methods like ARIMA [10]. However, these\\r\\nmodels are prone to significant long-term error accumulation [11], prompting the exploration of new alternatives\\r\\nwithin the field of Deep Learning. Convolutional models for time series [12, 13] leverage the concept correlation\\r\\nin consecutive time steps. Various models have been developed based on this architecture, typically incorporating\\r\\narXiv:2410.03805v2 [cs.LG] 13 Oct 2024\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nrecurrent layers [14] such as Long-Short Term Memory (LSTM) [15, 16, 17]. This type of network faces clear\\r\\nlimitations due to the complexity of recurrent operations [18] and the lack of capacity to capture long-term time\\r\\ndependencies [19]. Transformers, originally proposed for natural language processing [20], have been incorporated\\r\\ninto the field of time series and forecasting to address these needs. Nonetheless, current transformer models present\\r\\ntwo main limitations. First of all, the computational evaluation of the attention mechanism requires significant time and\\r\\nmemory due to its quadratic complexity on the length of the input. Secondly, the utilisation of attention mechanisms\\r\\nresults in a significant loss of positional information, which poses a critical challenge for time series forecasting [21].\\r\\nAs a first step to address the computational cost issue, an efficient attention mechanism based on probabilistic sampling\\r\\nof attentions has been proposed in [22]. However, to date, methodologies employing transformers for time-series\\r\\nforecasting have not sufficiently mitigated the loss of positional information [21]. Consequently, emerges a necessity\\r\\nfor an innovative attention mechanism that leverages the local and long-term temporal dependency properties of time\\r\\nseries, does not incur information loss and minimises computational and memory overheads.\\r\\nPreviously existing transformer models within the field of time series do not exploit this conception of locality and\\r\\npresent experimental studies with data sets of limited size, not only in terms of features but particularly in terms of\\r\\ninstances. If the goal is capturing long-term dependencies, large datasets are needed to adequately train the model\\r\\nweights. The primary defining feature of a time series is the dependence of past time steps on the current time\\r\\ninstant [23]. This dependence is more likely to be higher in closer time steps, specially in the case where the time\\r\\nseries comes from discretizising a continuous stochastic process.\\r\\nThis paper aims to achieve two primary objectives:\\r\\n• We introduce Local Attention Mechanism (LAM), a locality-based attention mechanism designed to enhance\\r\\nboth the efficiency and performance of time series transformers. This novel attention mechanism reduces\\r\\ncomputational and memory complexity to Θ(n log n) by excluding non-relevant computations in the atten\\x02tion matrix through the use of an additive mask. The key contribution here is a derivation of this attention\\r\\nmechanism that can be written in tensor algebra in such a way that only Θ(n log n) memory is needed.\\r\\n• We propose a more suitable benchmark for the LSTF problem, comprising four interdisciplinary and substan\\x02tially larger datasets than previous studies. This new experimental environments encompass datasets related\\r\\nto taxi usage, electricity consumption and industrial real-world problems.\\r\\nTo validate the proposed approach, the second objective outlined above will be pursued to generate a robust dataset for\\r\\nperformance analysis. For fairness and consistency, we will first utilize the datasets employed in the Informer papers\\r\\nand other state-of-the-art methods as an initial benchmark, before incorporating our own proposed dataset into the\\r\\nstudy.\\r\\nThe rest of this paper is structured as follows. In Section 2, we discuss the background on transformers and the\\r\\nLSTF problem. In Section 3, we detail our proposed mechanism, introducing all relevant details. In Section 4, we\\r\\ncompare our approach with the state-of-the-art methods using currently employed LSTF datasets. In Section 5, we\\r\\nextend the datasets used to better represent the complexities of the LSTF problem and establish a more comprehensive\\r\\nbenchmark. Finally, we present our conclusions and outline future work in Section 6.\\r\\n2 Transformers for Long Sequence Time-series Forecasting\\r\\nA time series is a sequence of random vectors Xt ∈ R\\r\\nd\\r\\nrecorded at regular time intervals, typically denoted as\\r\\n{Xt}\\r\\nT\\r\\nt=1. We use Xt to refer to the random vector and xt to represent the observation at time t [24]. Time series data\\r\\nis characterised by its temporal dependency, where each observation relies on previous values, potentially exhibiting\\r\\nlong-term patterns [25]. Such data often features distinct elements: trend (long-term data direction), seasonality (pre\\x02dictable periodic fluctuations) and cyclic patterns (less regular variations influenced by external factors). In this work\\r\\nwe focus on time series forecasting, which corresponds to the problem of estimating E [Xt+1| X1 = x1, . . . , Xt = xt].\\r\\nSpecifically, we address long sequence time-series forecasting (LSTF)[9], where the goal is predicting long sequences\\r\\nof future data points in datasets with extensive historical records, requiring of the application of advanced techniques\\r\\nto understand complex patterns and trends.\\r\\nNeural networks, particularly one-dimensional convolutional neural networks (1D CNNs) and Long Short-Term Mem\\x02ory (LSTM) networks, offer black-box solutions for time-series forecasting [26, 27]. 1D CNNs use convolutional\\r\\nlayers to identify local patterns and hierarchies within time series data, but are unable to capture longer time de\\x02pendencies [19]. Indeed, short-term prediction are greatly affected by locality, that is, the last few elements of the\\r\\nsequence greatly determine the trend of the model and, thus, the short-term prediction, unless seasonality and cyclic\\r\\npatterns play a role at that specific timestamp. LSTMs are able to take into account seasonality and cyclic patterns\\r\\nfor medium-term dependencies thanks to the use memory cells, overcoming the vanishing gradient problem via gating\\r\\n2\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nmechanisms. However, the recursive nature of LSTMs and the memory cells limitations makes them less efficient and\\r\\neffective for long-term predictions [22].\\r\\nTransformers have surpassed RNNs and LSTMs in several applications, such as NLP, thanks to their ability to model\\r\\nlong-range dependencies, learn from large datasets, and parallelise computations efficiently. Motivated by these prop\\x02erties, transformers have been deemed as excellent candidates to the LSTF problem [28, 22]. Indeed, unlike traditional\\r\\nmethods that rely on fixed time windows, lagged observations and/or recursive neural networks, transformers can pro\\x02cess long sequences efficiently, making them suitable for both short-term and long-term forecasting tasks [29].\\r\\nTransformers are complex encoder-decoder architectures that encode the sequence of samples and use the extracted\\r\\ninformation to predict the several future elements of the sequence via a decoder. The key element to understand these\\r\\narchitectures is the attention mechanism, see [20]. In a nutshell, the main conclusion of [20] is that performance in\\r\\nNLP applications can be significantly improved by eliminating the recurrence structure of deep learning models and\\r\\ninstead focusing exclusively on exploiting the attention mechanism to extract relevant information from sequences of\\r\\ndata. In this work we refer to this encoding-decoding model as the vanilla transformer.\\r\\nWhen it comes to the application of transformers to time-series problems, progress has been significantly slower\\r\\nthan in NLP. This is due to the fact that the attention mechanism presents two main limitations: loss of positional\\r\\ninformation, and computational and memory cost [28]. The former problem is solved in the application of the vanilla\\r\\ntransformer to NLP problems via a positional encoding of the input, and residual connections, which we will be\\r\\nexplained in Section 2.3 and Section 2.4 respectively. However, the validity of this approach in the case of time series\\r\\nremains controversial, see, for instance [21], suggesting that further research is needed. We will come back to this\\r\\nproblem when introducing our attention mechanism, LAM, in Section 3, which greatly reduces the impact of loss of\\r\\npositional information. On the other hand, improving the computational and memory cost of the attention mechanism\\r\\nhas actually been addressed in the time-series literature. The evaluation of the original attention mechanism has time\\r\\nand memory complexity Θ(n\\r\\n2\\r\\n), where n is the width of this layer. As a consequence of the memory bottleneck, not\\r\\nevery data scientist has access to the computing resources needed to train and evaluate a transformer model with the\\r\\nfull attention mechanism on large sequences of data, preventing transformers from becoming a machine learning state\\r\\nof the art in time series forecasting in practice. The most remarkable solution to accelerate the attention mechanism\\r\\nfor time series forecasting is that of [22], where the authors propose a probabilistic attention mechanism that runs in\\r\\ntime and memory Θ(n log n). Other proposed solutions to this problem involve applying other sorts of mechanisms\\r\\nto the sequence of data instead of the attention mechanism, see for instance [30].\\r\\nIn the rest of this section, we describe the elements of the vanilla transformer, which will serve as the base model for our\\r\\nexperimentation, and the probabilistic attention mechanism of [22]. This section is organised as follows. In Section 2.1\\r\\nwe briefly recapitulate the canonical attention mechanism introduced in [20]. In Section 2.2 we summarise the concept\\r\\nof probabilistic attention and point out some of its weaknesses. In Section 2.3 we describe the positional encoding\\r\\nused in our experiments. Finally, in Section 2.4 we summarise the encoder-architecture of the vanilla transformer.\\r\\n2.1 Attention Mechanism\\r\\nThe attention mechanism is a layer of a neural network whose inputs consist of three sequences with n elements: the\\r\\nlists of queries and keys, each entry being a vector of dimension dq, and the list of values, each entry having dimension\\r\\ndv. For example, when applied to time series as a first layer, these three sequences could be set to the input of model,\\r\\nthat is, n consecutive data points of the time series, X = (x1, x2, . . . , xn). In this particular case, dq = dv = dmodel,\\r\\nso each of the three inputs of the attention layer is actually an n × dmodel matrix. In the general case, the queries, keys\\r\\nand values are matrices Q, K and V with dimensions n × dq, n × dq and n × dv, respectively.\\r\\nThe attention matrix of Q and K is defined as\\r\\nFullAttentionScores(Q, K) = softmax \\r\\nQKT\\r\\np\\r\\ndq\\r\\n!\\r\\n, (1)\\r\\nwhich has dimension n × n. Note that the i-th row of QKTconsists of the scalar products of the i-th query with all\\r\\nthe keys. Provided that the scalar product of two vectors a and b satisfies ⟨a, b⟩ = ∥a∥2∥b∥2 cos θ, where θ is the angle\\r\\nbetween the vectors a and b. Thus, the i-th row of QKTranges from −∥Qi∥2∥Kj∥2 to ∥Qi∥2∥Kj∥2. The softmax\\r\\nfunction here guarantees that attention matrix is stochastic, that is, each row consists of non-negative real numbers\\r\\nthat add up to one. The entries of the attention matrix are called attention scores. The canonical attention mechanism\\r\\n(refereed in this work as full attention mechanism) is then defined as\\r\\nFullAttn(Q, K, V) = softmax \\r\\nQKT\\r\\np\\r\\ndq\\r\\n!\\r\\nV, (2)\\r\\n3\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nso the i-th output is a weighted average of the values. The scale factor 1/\\r\\np\\r\\ndq does not change the output of softmax;\\r\\nits purpose is to help scaling the gradients of softmax during training in order to avoid gradients too close to 0, which\\r\\nmay decrease the rate at which a transformer is able to learn from data.\\r\\nThe most common application of attention in transformers corresponds to the case Q = K. In this setting, the\\r\\nfull attention mechanism resembles a smooth non-parametric regression [31, 32] of V in terms of Q = K, [33],\\r\\neffectively providing a smoothing of V . We refer to self-attention as the case when Q = K = V . Intuitively, self\\x02attention weighs in the similarity of different elements in a sequence and smooths the input data. Empirically this\\r\\nprocess has been shown to be highly effective in capturing both local and global patterns in data [34]. However, this\\r\\nintuition has not been corroborated from a theoretical point of view, where mathematical results have only managed\\r\\nto unveil the Lipschitz constant of the attention mechanism [35].\\r\\nTransformers use an improvement of the attention mechanism, known as multi-head attention. Given an attention\\r\\nmechanism Attn(·, ·, ·), which may be FullAttn or one of the other mechanisms that we will introduce later in this\\r\\nwork. The idea here is applying linear projections (on the dimension of the characteristics) to learn h representations\\r\\nof the inputs Q, K and V . The weights of these linear projections will be optimised in the learning phase, aiming to\\r\\nlearn representations that boost the performance of the attention mechanism. Then the attention mechanism is applied\\r\\nto each of the h triples formed by the projections of Q, K and V . Here h is called the number of heads. Formally, for\\r\\npositive integers da and h, the multi-head attention layer is defined as\\r\\nMultiHead(Q, K, V ) = Concat(head1, . . . , headh)WO,\\r\\nwhere headi = Attn(QWQ\\r\\ni\\r\\n, KW K\\r\\ni\\r\\n, V WV\\r\\ni\\r\\n),\\r\\nand the matrices W\\r\\nQ\\r\\ni\\r\\n, W K\\r\\ni\\r\\n, WV\\r\\ni\\r\\nand WO\\r\\ni\\r\\nare parameter matrices of the model with W\\r\\nQ\\r\\ni ∈ R\\r\\nda×dq\\r\\n, W K\\r\\ni ∈ R\\r\\nda×dq\\r\\n,\\r\\nWV\\r\\ni ∈ R\\r\\nda×dv and WO\\r\\ni ∈ R\\r\\nhda×dv . Here the Concat operator concatenates tensors head1, head2, . . . , headh on\\r\\nthe last dimension.\\r\\nOne usually picks h and da such that hda = dv. In the time series literature h is typically set to the number of variables\\r\\nin the model, so da = 1. We highlight that the number of parameters of this layer is da(2dq + (h + 1)dv), which\\r\\ndoes not depend on n. Thus, the multi-head attention mechanism allows us to add extra layers to the model without\\r\\nsignificantly increasing the number of parameters.\\r\\nA critique to the full attention mechanism is the potential loss of positional information. First of all, note that the\\r\\noutput of FullAttn(Q, K, V) is a matrix whose i-th row is P\\r\\nj=1 ai,jVj . This average Pj=1 ai,jVj results in a partial\\r\\nloss of position as it may be the case that ai,j is large for a Vj that is far away from Viin the input sequence. That is,\\r\\nwe lose the sense of temporal locality of the values as we average values with entries far away in the input sequence.\\r\\nWe formalise this loss of local information is in Proposition 1. First, let us introduce some notation. For a permutation\\r\\nπ : {0, 1, . . . , n − 1} → {0, 1, . . . , n − 1}, the permutation matrix Pπ is the matrix with dimensions n × n defined as\\r\\nPπ[i, j] = 1 for j = π(i) and Pπ[i, j] = 0 otherwise.\\r\\nProposition 1. In the case of the full attention mechanism, for any positive integers h and da and any permutation\\r\\nmatrix Pπ, we have\\r\\nMultiHead(PπQ, PπK, PπV ) = Pπ MultiHead(Q, K, V ).\\r\\nProof. We first prove the proposition for h = 1, that is, we show that\\r\\nFullAttn(PπQ, PπK, PπV ) = Pπ FullAttn(Q, K, V )\\r\\nLet ai,j and aˆi,j be the attention scores for the original inputs and the permuted inputs, respec\\x02tively. Note that (PπQ)(PπK)\\r\\nT = PπQKT PT\\r\\nπ\\r\\n. That is, aˆi,j = aπ(i),π(j). Thus, the i-th row\\r\\nof FullAttn(PπQ, PπK, PπV ) is the vector Pn\\r\\nj=1 aπ(i),π(j)Vπ(j) =\\r\\nPn\\r\\nj=1 aπ(i),jVj as we wanted. Fi\\x02nally, we note that Concat(Pπhead1, . . . , Pπheadh)WO = Pπ Concat(head1, . . . , headh)WO, as the tensors\\r\\nhead1, head2, . . . , headh are concatenated on the last dimension and Pπ permutes the second to last dimension.\\r\\nAs we will see in Section 2.4, usually only the last layer of the vanilla transformer is a fully-linear layer; the rest\\r\\nof layers are attention layers with projections acting on the dimension of the characteristics. Therefore, positional\\r\\ninformation is only exploited in these last fully-linear layers. However, during the repeated application of attention\\r\\nlayers positional information is reduced due to the averages carried out in the full attention mechanism. This issue\\r\\nis addressed by the implementation of the positional encoding, which aims to minimise this reduction of positional\\r\\ninformation when applying attention layers. However, it seems that, in the case of time series, positional encoding\\r\\nis not enough. Indeed, in experiments, on average a vanilla transformer trained for a particular time series produces\\r\\nsimilar predictions on an original input and a permuted version of this input [21].\\r\\n4\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\n2.2 Probabilistic Attention\\r\\nAmong state-of-the-art models, Informer is distinguished as one of the most frequently cited and top-performing [22].\\r\\nThe authors propose a probabilistic attention mechanism that runs in time and memory Θ(n log n). The key idea\\r\\nhere is selecting u queries, where u = 5⌈log n⌉, and setting the rest of the queries to zeroes. To select these queries,\\r\\nthe authors introduce a sparsity measurement for the queries, which approximates the Kullback-Leibler divergence of\\r\\nthe attention scores of a query and the uniform distribution. Finding the queries whose attention scores have largest\\r\\nsparsity measurement would require to compute the whole attention matrix to begin with. Instead, the authors propose\\r\\na heuristic method: for each query Qi, they sample Θ(log n) scalar products of the form ⟨Qi, Kj ⟩ and approximate\\r\\nthe sparsity measurement of the query Qi using these samples. Let us denote by ProbSelect(Q, K) the method that\\r\\nselects the top u queries of Q with this sampling heuristic and masks the rest of the queries to zeroes. Then, the\\r\\nprobabilistic attention mechanism is\\r\\nProbAttn(Q, K, V) = softmax \\r\\nProbSelect(Q, K)KT\\r\\np\\r\\ndq\\r\\n!\\r\\nV.\\r\\nThe product ProbSelect(Q, K)KT does not have to be fully computed. Indeed, for each query Qithat is not selected,\\r\\nthe corresponding attention scores are uniform, that is, aij = 1/n, as Qi = 0. Thus, we only have to carry out n scalar\\r\\nproducts for each one of the u = Θ(log n) rows selected to calculate ProbAttn(Q, K, V), leading to Θ(n log n)\\r\\ncomplexity.\\r\\nEven though the ProbAttn method does significantly reduce the time and memory complexity of the attention mech\\x02anism and seems to be able to capture the most significant attention scores in practice according to the experiments\\r\\ncarried out in [22], it does lead to a significant loss of information. Indeed, the output of ProbAttn(Q, K, V) has\\r\\nn−u entries that are equal to the average of the values. We believe this to be a major drawback of this attention mech\\x02anism, and this is corroborated in our experimentation. The authors of [22] try to fix this issue by applying residual\\r\\nconnections in their model, as well as a complex architecture that exploits convolutions in conjunction with ProbAttn.\\r\\nHowever, our experiments show that this is not enough, as the vanilla transformer is able to significantly outperform\\r\\nInformer.\\r\\n2.3 Positional encoding\\r\\nPositional encoding in transformers is essential for providing sequence information to the model, which inherently\\r\\nlacks an understanding of order due to its self-attention mechanism (see Proposition 1). Positional encoding involves\\r\\nadding a positional tensor to the input sequence, which is usually derived from sine and cosine functions, to indicate\\r\\neach entry relative or absolute position. In our experiments, we add to the input matrix the positional encoding\\r\\nemployed in the vanilla model,\\r\\nPE(i, j) = sin(i/10000j/dmodel ) + cos(i/10000j/dmodel ).\\r\\n2.4 The Encoder-Decoder architecture\\r\\nThe encoder-decoder architecture in transformers forms the backbone of many advanced models in natural language\\r\\nprocessing and beyond. In this architecture, the encoder processes the input sequence, transforming it into a rich,\\r\\ncontextual representation. Here we briefly describe the architecture proposed in [20], which we use in our model\\r\\nin conjunction with LAM. Each encoder layer consists of a multi-head self-attention mechanism, which allows the\\r\\nmodel to weigh the importance of different parts of the input relative to each other, and two linear projections acting\\r\\non the dimension of the variables (i.e. the last dimension of the input) with activation functions. Formally, these linear\\r\\nprojections act as the matrix WO in the definition of MultiHead, they are represented as dmodel × dmodel matrices,\\r\\nbut they have LeakyReLu activation functions on the outputs. The purpose of projection layers is to learn a domain\\r\\nrepresentation that boosts the performance of the attention mechanism.\\r\\nThe decoder layers are similar to the encoder layers. The main difference is that they apply two multi-head self\\x02attention layers. In the first multi-head self-attention, the queries, keys and values are set to the input of the layer,\\r\\nwhereas in the second one the keys and queries are set to the output of the encoder and the values to the input of the\\r\\nlayer. This is illustrated in Figure 1. This second attention layer that helps the decoder to focus on relevant parts of the\\r\\ninput sequence, enhancing its ability to produce contextually relevant outputs. We highlight the presence of residual\\r\\nconnections in the encoder and decoder layers. These residual connections consist in adding the input of the attention\\r\\nmechanism its the output, reducing the loss of positional information.\\r\\nOnce the encoder and decoder layers are defined, the architecture of the vanilla transformer is relatively simple:\\r\\nthe encoder consists of N consecutive applications of the encoder layer and the decoder consists of N consecutive\\r\\n5\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\napplications of the decoder layer. The value of N can be picked in each application, and will be provided it in each one\\r\\nof our experiments. We note that increasing N does not significantly change the number of parameters of the model,\\r\\nas the feed forward layers applied in the encoder and decoder layers are linear projections, as described above, which\\r\\nhave d\\r\\n2\\r\\nmodel parameters. That is, the number of parameters in the encoder and the decoder layers is Θ(d\\r\\n2\\r\\nmodel).\\r\\nFinally, the output of the last decoder layer goes through a linear layer. To reduce the number of parameters, we have\\r\\nimplemented this layer as a projection on the time dimension, that is, if the input of the transformer has n elements of\\r\\nthe time series, this linear layer corresponds to a matrix n × m that m computes linear combinations of the n points\\r\\nproduced in the last decoding layer, where m is the number of points that we want to forecast. This is the output of\\r\\nour model, as we do not need to perform a softmax layer to compute probabilities.\\r\\nFigure 1: Transformer architecture used for LAM.\\r\\n3 Local Attention Mechanism (LAM)\\r\\nIn this section we introduce our proposal, the LAM mechanism, and an efficient algorithm, based on expressing LAM\\r\\nin tensor algebra, that leads to Θ(n log n) memory and time complexity. This algorithm has been implemented in\\r\\nstock Pytorch and TensorFlow without the need for any low level CUDA code.\\r\\nThe high-level fundamental behind LAM is exploiting locality in order to re-define the attention mechanism. Pre\\x02vious studies on time series have empirically shown that the rows of the attention scores matrix behave as long tail\\r\\n6\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\ndistributions [28, 22]. This behaviour is present at any layer independently of the depth. For example, the coefficients\\r\\nan−1,0, an−1,1, . . . , an−1,n−1 of the last row of the score matrix are mostly increasing, with most of the probability\\r\\nmass function being allocated to the last few coefficients, which contrasts to the behaviour of the attention mechanism\\r\\non language related tasks [36]. Even though both NLP and time series forecasting deal with sequences of data, the\\r\\nmain difference relies on the nature of this data. In NLP words are embedded into a vector space so that words related\\r\\nin meaning have similar dot product. Moreover, words with related meaning may come up far apart in a paragraph\\r\\nwhereas consecutive words may have small dot product. On the other hand, most variables in time series problems\\r\\nare continuous and, thus, data tends to be extremely similar around a particular timestamp, hence leading to the larger\\r\\nattention scores in that region. This motivates the following working hypothesis for this research.\\r\\nHypothesis 1. Let Xt be a (mostly) continuous time series. On inputs xt−n+1, . . . , xt, for any j ∈ {0, 1, . . . , n − 1},\\r\\nthe attention scores of the row corresponding to xt−n+1+i, denoted ai,0, ai,1, . . . , ai,n−1, are mostly concentrated\\r\\naround aj,j .\\r\\nPrevious attempts have exploited this sparsity properties of self-attention in time series modelling to design selec\\x02tive masking strategies that, for each row of the attention scores matrix, set most entries to zero. For example, the\\r\\nLogSparse Transformer [28] (denoted LogTrans in our experimentation) uses, for each key with timestamp t0, atten\\x02tion scores with those queries whose timestamp t is in a local neighbourhood of t0 and those queries whose timestamp\\r\\nt satisfies t0 − t = 2jfor some positive integer j. The LongFormer [37] extends the above work to a more com\\x02plicated sparse configuration in the realm of NLP. These masking strategies partially resolve the loss of positional\\r\\ninformation problem of the attention mechanism as the input is only partially combined at each attention layer and,\\r\\nthus, Proposition 1 does not hold for such mechanisms. However, it is not clear how to obtain an improvement in\\r\\ntime and memory complexity, as these attention mechanisms have not been written in tensor algebra in an efficient\\r\\nway. Instead, sparse matrices are used to perform computations and represent the attention matrix, leading to minor\\r\\nmemory and computational efficiency improvements.\\r\\nIn this work, we propose the Local Attention Mechanism (LAM), which only computes dot products on nearby inputs\\r\\nof the attention layer. As we will see, our proposed attention can be implemented efficiently in tensor algebra, leading\\r\\nto almost linear time and memory complexity in practice.\\r\\nDefinition 1 (Local Attention). Let L be a neighbourhood size (in our experiments L = 4⌈log n⌉) and let M ∈\\r\\n{0, −∞}n×n be the square matrix with Mi,j = 0 when i − L + 1 ≤ j ≤ i and Mi,j = −∞ otherwise. Then LAM is\\r\\ndefined as\\r\\nLAM(Q, K, V ) = softmax \\r\\nQKT + M\\r\\np\\r\\ndq\\r\\n!\\r\\nV. (3)\\r\\nNote that adding the matrix M and applying softmax leads to several zero entries in the matrix\\r\\nsoftmax (QKT + M)/\\r\\np\\r\\ndq\\r\\n\\x01\\r\\n. In fact, the scores corresponding to the scalar/dot product of the query Qi with the\\r\\nkeys Kj with i − L + 1 ≤ j ≤ i are the only ones that may not be zero. That is, the j-th output of LAM(Q, K, V ) is\\r\\na weighted average of Vj−L+1, Vj−L+2, . . . , Vj . This is visualised in Figure 2a.\\r\\nAt first glance, the brute force implementation of LAM by computing QKT gives quadratic time and memory\\r\\ncomplexity. Here we explain how to achieve Θ(n log n) complexity in time and memory with a tensor algebra re\\x02formulation of LAM. For ease of exposition, assume that L divides n (the general case will be addressed at the end of\\r\\nthis section). Our algorithm to compute LAM is provided in Algorithm 1, although the reader may find the following\\r\\nmathematical description useful.\\r\\nLet s = n/L. The key idea is splitting Q into s blocks of dimensions L×dq and determining the smallest submatrices\\r\\nof KTthat we have to multiply these blocks by so that the end results contain the dot products\\r\\n⟨Qi, Kj ⟩ for all r ∈ {0, 1, . . . , s − 1}, i ∈ {rL, r(L + 1) − 1}\\r\\nand j with i − L + 1 ≤ j ≤ i.\\r\\n(4)\\r\\nNote that these dot products are exactly the ones not masked by M in the definition of LAM. In our algorithm, we\\r\\ncompute 2L − 1 dot products for each row of Q instead of the L dot products described in Eq. (4). The extra number\\r\\nof dot products computed will allow us to write LAM in tensor algebra, without affecting the time and memory\\r\\ncomplexity. We formally introduce this “splitting” idea in Definition 2.\\r\\nDefinition 2 (The tensor TQ). Let s = n/L. We define TQ as the tensor with dimension s × L × dq such that\\r\\nTQ[r, i1, t] = Q[rL + i1, t] for all r ∈ {0, 1, . . . , s − 1}, i1 ∈ {0, 1, . . . , L − 1} and t ∈ {0, 1, . . . , dq − 1}.\\r\\n7\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nLet r ∈ {0, 1, . . . , s − 1} be the index of the block of Q under consideration. We claim that it suffices to compute the\\r\\ndot products\\r\\n⟨QrL+i1, Kj ⟩ = ⟨TQ[r, i1, ·], Kj ⟩ for all r ∈ {0, 1, . . . , s − 1},\\r\\ni1 ∈ {0, 1, . . . , L − 1} and\\r\\nj with (r − 1)L + 1 ≤ j ≤ (r + 1)L − 1.\\r\\nIndeed, with the change of variables i = rL + i1, using i1 ∈ {0, 1, . . . , L − 1} we find that rL ≤ i ≤ (r + 1)L − 1\\r\\nand, thus,\\r\\n[i − L + 1, i] ⊆ [(r − 1)L + 1,(r + 1)L − 1]. (5)\\r\\nThis motivates the following definition.\\r\\nDefinition 3 (The tensor TK). Let s = n/L. We define TK as the tensor with dimensions s×(2L−1)×dq such that, for\\r\\neach for all r ∈ {0, . . . , s−1}, j1 ∈ {0, 1, . . . , 2L−2} and t ∈ {0, 1, . . . , dq−1}, TK[r, j1, t] = K[(r−1)L+1+j1, t]\\r\\nwhen (r − 1)L + 1 + j1 ≥ 0 and TK[0, j1, t] = 0 otherwise.\\r\\nWe note that (r − 1)L + 1 + j1 < 0 only when j1 < (1 − r)L − 1, which only occurs in the case r = 0 and\\r\\nj1 ≤ L − 2. Indeed, for r = 0 in Eq. (4), we only have to compute the dot product ⟨Qi1, Kj ⟩ for 0 ≤ j ≤ i1. The\\r\\nextra zeroes added in Tk for r = 0 allow us to maintain the dimension of TK[r, ·, ·] across all values of r. Lemma 2\\r\\nallows us to go back and forth between the indexes (i, j) and (i1, j1) for the changes of variables i = rL + i1 and\\r\\nj = (r − 1)L + 1 + j1.\\r\\nLemma 2. Let r ∈ {0, 1, . . . , s − 1}. For each i1 ∈ {0, 1, . . . , L − 1} and j1 ∈ {0, 1, . . . , 2L − 2}, let i = rL + i1\\r\\nand j = (r − 1)L + 1 + j1. We have i − L + 1 ≤ j ≤ i if and only if i1 ≤ j1 ≤ i1 + L − 1.\\r\\nProof. We note that j − i + L − 1 = j1 − i1 and i − j = i1 + L − 1 − j1 by definition of i and j. Thus, we have\\r\\nj1 − i1 ≥ 0 if and only if j − i + L − 1 ≥ 0. Equivalently, i1 ≤ j1 if and only if i − L + 1 ≤ j, which deals with the\\r\\nlower bounds of the statement. Analogously, i − j ≥ 0 if and only if i1 + L − 1 − j1 ≥ 0 or, equivalently, j ≤ i if and\\r\\nonly if j1 ≤ i1 + L − 1, which finishes the proof.\\r\\nNow we can formally prove that our definition of TQ and TK capture all the dot products needed in the definition of\\r\\nLAM (definition 1), which is also visualised in Figure 2.\\r\\nLemma 3. For each r ∈ {0, 1, . . . , s − 1}, the matrix defined as TA[r, ·, ·] = TQ[r, ·, ·]TK[r, ·, ·]\\r\\nT has dimensions\\r\\nL × (2L − 1). For each i ∈ {0, 1, . . . , n − 1} and j with i − L + 1 ≤ j ≤ i, setting i1 = i (mod L), r = (i − i1)/L\\r\\nand j1 = j − (r − 1)L − 1, we have r ∈ {0, 1, . . . , s − 1}, i1 ∈ {0, 1, . . . , L − 1} and j1 ∈ {0, 1, . . . , 2L − 2}.\\r\\nMoreover, the equality ⟨Qi, Kj ⟩ = TA[r, i1, j1] holds.\\r\\nProof. Recall that TQ has dimension s×L×dq and TK has dimension s×(2L−1)×dq, so TK[r, ·, ·]\\r\\nT has dimensions\\r\\ndq × (2L − 1) and TQ[r, ·, ·]TK[r, ·, ·]\\r\\nT has dimensions L × (2L − 1) as we wanted. For the second claim, first, we\\r\\ncheck that indeed i1 ∈ {0, 1, . . . , L − 1} (which follows by the definition of mod L) and r ∈ {0, 1, . . . , s − 1}\\r\\n(which follows from s = n/L > (i − i1)/L = r). To prove that j1 ∈ {0, 1, . . . , 2L − 2}, we use i − L + 1 ≤ j ≤ i,\\r\\ni = i1 + rL and j1 = j − (r − 1)L − 1, obtaining\\r\\n0 ≤ j − i + L − 1 = j − i1 − rL + L − 1 = j1 − i1 ≤ j1\\r\\nand\\r\\nj1 = j − (r − 1)L − 1 ≤ i − (r − 1)L − 1 = i1 + rL − (r − 1)L − 1\\r\\n= i1 + L − 1 ≤ L − 1 + L − 1 = 2L − 2.\\r\\nFinally, note that by definition of TQ and TK, we have Qi = TQ[r, i1, ·] and Kj = TK[r, j1, ·] as (r − 1)L + 1 + j1 =\\r\\nj ≥ 0. Note that TA[r, i1, j1] = ⟨TQ[r, i1, ·], TK[r, j1, ·]⟩ by definition of matrix multiplication. Thus, we conclude\\r\\nthat ⟨Qi, Kj ⟩ = TA[r, i1, j1].\\r\\n8\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\n(a) Visualisation of the product QKT. In green, we highlight\\r\\nthe dot products needed in LAM, as the rest are set to −∞\\r\\nwhen adding M in Eq. (3).\\r\\n(b) In light brown we have the block TQ[0, ·, ·] of Q, the non\\x02zero elements of the block TK[0, ·, ·]\\r\\nT\\r\\nof KT, and the dot prod\\x02ucts corresponding to TQ[0, ·, ·]TK[0, ·, ·]\\r\\nT\\r\\n.\\r\\n(c) In brown we have the block TQ[1, ·, ·] of Q, the block\\r\\nTK[1, ·, ·]\\r\\nT\\r\\nof KT, and the dot products corresponding to\\r\\nTQ[1, ·, ·]TK[1, ·, ·]\\r\\nT\\r\\n.\\r\\n(d) In dark brown we have the block TQ[2, ·, ·] of Q, the\\r\\nblock TK[2, ·, ·]\\r\\nT\\r\\nof KT, and the dot products corresponding\\r\\nto TQ[2, ·, ·]TK[2, ·, ·]\\r\\nT\\r\\n.\\r\\nFigure 2: Example of our algorithm to compute the dot products of QKT needed in LAM for n = 6, dq = 3 and\\r\\nL = 2. We note that s = n/L = 3. The dot products needed in LAM are highlighted in green in (a), the dot products\\r\\ncomputed in our algorithm are highlighted in a shade of brown in QKTin Subfigures (b), (c), (d). We note that every\\r\\ndot product highlighted in green in (a) is in brown in either (b), (c) or (d), as Lemma 3 claims.\\r\\nAt this point we describe how we express softmax (QKT + M)/\\r\\np\\r\\ndq\\r\\n\\x01\\r\\nin terms of TA[r, ·, ·] = TQ[r, ·, ·]TK[r, ·, ·]\\r\\nT\\r\\n.\\r\\nFirst, we mask the undesired dot products using a tensor of dimension s × L × (2L − 2) that emulates the matrix M\\r\\nof Definition 1.\\r\\nDefinition 4 (The tensor TM). Let s = n/L. We define TM as the tensor with dimension s × L × (2L − 2) such that\\r\\nTM[r, i1, j1] = 0 when i1 ≤ j1 ≤ i1 + L − 1 and TM[r, i1, j1] = −∞ otherwise.\\r\\nIn view of Lemma 2, adding TM to TA acts as the same masking as adding M to QKT, we are just operating on the\\r\\nvariables (i1, j1) instead of (i, j). Figure 3 visualises the computation softmax(TA + TM).\\r\\nFigure 3: Visualisation of computation of the attention scores for LAM. For the matrix QKT we show in brown the\\r\\ndot products computed in Figure 2 (that is, those dot products computed for TA), where light brown corresponds to\\r\\nFigure 2b, medium brown corresponds to Figure 2c and dark brown corresponds to Figure 2d. The attention scores\\r\\nof LAM are computed after adding TM (the sub-blocks of M given in Definition 4), to TA and applying the Softmax\\r\\nfunction to each row. In green, we highlight the attention scores computed by LAM.\\r\\n9\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nWe can now prove that TS = softmax((TQT\\r\\nT\\r\\nK + TM)/\\r\\np\\r\\ndq) contains the attention scores of\\r\\nsoftmax (QKT + M)/\\r\\np\\r\\ndq\\r\\n\\x01\\r\\n, other than the extra zeroes that we have not computed.\\r\\nLemma 4. The tensor defined as TS = softmax((TQT\\r\\nT\\r\\nK + TM)/\\r\\np\\r\\ndq), where softmax is applied on the last\\r\\ndimension, has dimensions s × L × (2L − 1). Let S be the matrix softmax (QKT + M)/\\r\\np\\r\\ndq\\r\\n\\x01\\r\\n. For each\\r\\ni ∈ {0, 1, . . . , n−1} and j with i−L+ 1 ≤ j ≤ i, setting i1 = i (mod L), r = (i−i1)/L and j1 = j −(r−1)L−1,\\r\\nwe have S[i, j] = TS[r, i1, j1]. Moreover, all the other entries of S and TA are zero.\\r\\nProof. The dimensions of TS follow from the dimensions of TA = TQT\\r\\nT\\r\\nK, see Lemma 3. Let i, j ∈ {0, 1, . . . , n −1}.\\r\\nWe consider the change of variables i1 = i (mod L), r = (i − i1)/L and j1 = j − (r − 1)L − 1. For the rest of the\\r\\nproof we distinguish two cases:\\r\\n• The case when i−L+ 1 ≤ j ≤ i. In view of Lemma 3, we have r ∈ {0, 1, . . . , s−1}, i1 ∈ {0, 1, . . . , L−1}\\r\\nand j1 ∈ {0, 1, . . . , 2L − 2}, and, moreover, ⟨Qi, Kj ⟩ = TA[r, i1, j1], where TA = TQT\\r\\nT\\r\\nK.\\r\\n• The case when the i − L + 1 ≤ j ≤ i does not hold. In such case, M[i, j] = −∞ by definition, so\\r\\n(QKT + M)[i, j] = −∞. Moreover, either j1 ̸∈ {0, 2, . . . , 2L − 2} and TM[r, i1, j1] is not defined, or\\r\\nj1 ∈ {0, 2, . . . , 2L − 2}. In the latter case, we are in the setting of Lemma 2, so, as i − L + 1 ≤ j ≤ i\\r\\ndoes not hold, the inequality i1 ≤ j1 ≤ i1 + L − 1 does not hold. From the definition of TM, we conclude\\r\\nthat (TA + TM)[r, i1, j1] = −∞. That is, we have shown that either (TA + TM)[r, i1, j1] is not defined and\\r\\n(QKT + M)[i, j] = −∞, or (TA + TM)[r, i1, j1] = −∞ and (QKT + M)[i, j] = −∞.\\r\\nApplying the operator softmax to the i-th row of (QKT + M), leads to all entries equal to zero except for those\\r\\nentries indexed by (i, j) with i − L + 1 ≤ j ≤ i (a total of L entries). Analogously, applying the operator softmax to\\r\\n(TA+TM)[r, i1, ·], leads to all entries equal to zero except for those entries indexed by (i1, j1) with i1 ≤ j1 ≤ i1+L−1\\r\\n(a total of L entries). Moreover, L non-zero entries are the same in both cases. Summarising, we have shown that:\\r\\n• when i − L + 1 ≤ j ≤ i, we have S[i, j] = TS[r, i1, j1].\\r\\n• when i − L + 1 ≤ j ≤ i does not hold, we have S[i, j] = 0 and either TS[r, i1, j1] is not defined (case\\r\\nj1 ̸∈ {0, 1, . . . , 2L − 2}) or TS[r, i1, j1] = 0.\\r\\nThis dichotomy concludes the proof.\\r\\nUp to now, we have managed to determine the attention scores of LAM while avoiding to compute all the entries of\\r\\nthe product QKT. It remains to multiply the attention scores matrix by the values tensor V . This operation can be\\r\\nsimilarly performed by extracting blocks of V , which motivates Definition 5, and can be visualised in Figure 4.\\r\\nDefinition 5 (The tensor TV ). Let s = n/L. We define TV as the tensor with dimensions s × (2L − 1) × dmodel\\r\\nsuch that, for each for all r ∈ {0, . . . , s − 1}, j1 ∈ {0, 1, . . . , 2L − 2} and t ∈ {0, 1, . . . , dmodel − 1}, TV [r, j1, t] =\\r\\nV [(r − 1)L + 1 + j1, t] when (r − 1)L + 1 + j1 ≥ 0 and TV [0, j1, t] = 0 otherwise.\\r\\nLemma 5. The tensor defined as TLAM = softmax((TQT\\r\\nT\\r\\nK + TM)/\\r\\np\\r\\ndq)TV , where softmax is applied on the last\\r\\ndimension, has dimensions s×L×dmodel. For each i ∈ {0, 1, . . . , n−1}, setting i1 = i (mod L) and r = (i−i1)/L,\\r\\nwe have LAM(Q, K, V )[i, ·] = TLAM[r, i1, ·].\\r\\nProof. Recall that TS = softmax((TQT\\r\\nT\\r\\nK + TM)/\\r\\np\\r\\ndq) has dimensions s × L × (2L − 1), see Lemma 4, and TV has\\r\\ndimensions s × (2L − 1) × dmodel. Thus, TLAM has dimensions s × L × dmodel. Now, le ti ∈ {0, 1, . . . , n − 1} and\\r\\nset i1 = i (mod L) and r = (i − i1)/L. We note that i1 ∈ {0, 1, . . . , L − 1} and r ∈ {0, 1, . . . , s − 1}. By Lemma\\r\\n4, for all j1 ∈ {0, 1, . . . , 2L − 2}, we have TS[r, i1, j1] = S[i, j] for j = r(L − 1) + 1 + j1, and S[i, j] = 0 for other\\r\\nvalues of j. Therefore, we find that\\r\\nTLAM[r, i, ·] = TS[r, i1, ·]TV [r, ·, ·] = S[i, ·]V,\\r\\nwhich equals LAM(Q, K, V )[i, ·], as we wanted to prove.\\r\\n10\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\n(a) Visualisation of the product A · V . In green we have the\\r\\nattention scores computed in TS, which correspond to the non\\x02zero attention scores of S (Lemma 4).\\r\\n(b) In light brown we have the block TV [0, ·, ·] and the block\\r\\nLAM(Q, K, V )[0 : L, ·]. We note that LAM(Q, K, V )[0 :\\r\\nL, ·] equals the product A[0 : L, 0 : L]TV [0, ·, ·].\\r\\n(c) In brown we have the block TV [1, ·, ·] and the block\\r\\nLAM(Q, K, V )[L : 2L, ·]. We note that LAM(Q, K, V )[L :\\r\\n2L, ·] equals the product A[L : 2L, L − 1 : 2L]TV [1, ·, ·].\\r\\n(d) In dark brown we have the blocks TV [2, ·, ·] and\\r\\nLAM(Q, K, V )[2L : 3L, ·]. We note LAM(Q, K, V )[2L :\\r\\n3L, ·] equals the product A[2L : 3L, 2L−1 : 3L]TV [1, ·, ·].\\r\\nFigure 4: Example of our algorithm to compute the dot products of SV needed in LAM for n = 6, dmodel = 3 and\\r\\nL = 2, where S = softmax (QKT + M)/\\r\\np\\r\\ndq\\r\\n\\x01\\r\\n. We note that s = n/L = 3. The values tensor is divided s in\\r\\nblocks, with a different shadow of brown, in Subfigures (b), (c), (d).\\r\\nOur algorithm to compute LAM(Q, K, V ) can now be inferred from Lemma 5 and is explicitly given in Algorithm 1.\\r\\nAlgorithm 1 Efficient implementation of LAM with tensor algebra\\r\\nRequire: The query and key matrices Q and K, and the value matrix V .\\r\\nEnsure: Q and K have dimensions n × dq and V has dimensions n × dv. Let s = n/L. Let TM be the tensor with\\r\\ndimensions s × L × (2L − 1) such that TM[r, i1, j1] = 0 when i1 ≤ j1 ≤ i1 + L − 1 and TM[r, i1, j1] = −∞\\r\\notherwise. Here the tensor TM is a local mask, that will be applied to each one of the s ”splits\" or blocks of QKT\\r\\nthat we compute, emulating the work of the mask M in Eq. (3). This tensor is pre-computed.\\r\\n1: Let TQ be the tensor with dimension s × L × dq such that TQ[r, i1, t] = Q[rL + i1, t].\\r\\n2: Let TK be the tensor with dimension s × (2L − 1) × dq such that TK[r, j1, t] = 0 when (r − 1)L + 1 + j1 < 0\\r\\nand TK[r, j1, t] = K[(r − 1)L + 1 + j1, t] otherwise.\\r\\n3: Let TV be the tensor with dimension s×(2L−1)×dmodel such that TV [r, j1, t] = −∞ when (r−1)L+1+j1 < 0\\r\\nand TV [r, j1, t] = V [(r − 1)L + 1 + j1, t] otherwise.\\r\\n4: TA = TQT\\r\\nT\\r\\nK. Note that TA has dimensions s × L × (2L − 1) and that the matrix multiplication is on the last two\\r\\ndimensions of the tensor.\\r\\n5: TS = SoftMax(TA + TM).\\r\\n6: We compute the tensor TR = TS · TV , which has dimensions s × L × dmodel.\\r\\n7: return Concatenate TR[0, ·, ·], . . . , TR[s − 1, ·, ·] on the dimension corresponding to L, obtaining a tensor with\\r\\ndimensions n × dmodel. This is the output of the algorithm.\\r\\nThe computation of TQ, TK and TV can be performed in linear time on the sizes of Q, K and V . In order to do so, we\\r\\npre-compute a tensor with the indexes that we want to extract from Q, K and V for each one of the s blocks of TQ, TK\\r\\nand TV . This allows us to implement Algorithm 1 in any environment for deep learning based on tensor algebra, see\\r\\nthe repository1for our Pytorch implementation. The case when L does not divide n is analogous, with the difference\\r\\n1\\r\\nhttps://github.com/ari-dasci/S-LAM\\r\\n11\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nthat the block s − 1 of Q, K and V is treated separately in the code to account for the extra number of entries. That is,\\r\\nwe use Algorithm 1 to compute the first (s − 1)L rows of LAM(Q, K, V ), and compute the last n − (s − 1)L rows of\\r\\nLAM(Q, K, V ) as a separate tensor computation. As this extra rows are most L − 1, the complexity of the algorithm\\r\\ndoes not change. We refer to our code for details.\\r\\nWe can now prove the main theoretical result of this work on combining the lemmas proved in this section and counting\\r\\nhow many steps are performed in Algorithm 1.\\r\\nTheorem 6. Algorithm 1 computes LAM(Q, K, V ) in time and memory Θ(nL), where time complexity is measured\\r\\nin the number of dot products of vectors performed.\\r\\nProof. The fact that Algorithm 1 does indeed compute the attention scores that we are interested in and that the output\\r\\nis that of LAM(Q, K, V ) follows from Lemma 5. We have performed sL · (2L − 1) = (2L − 1)n = Θ(n log n) dot\\r\\nproducts. This computation takes time and memory Θ(sL2) = Θ(Ln).\\r\\nCorollary 7. For L = ⌈4 log n⌉, Algorithm 1 computes LAM(Q, K, V ) in time and memory Θ(n log n).\\r\\nProof. This follows from instantiating L in Theorem 6.\\r\\n4 Empirical comparison to the state of the art\\r\\nIn this section, we present the experimental setup and results yielded by the vanilla transformer fuelled by our attention\\r\\nmechanism, LAM, and compare these to the state of the art in time-series forecasting via transformers. In this section\\r\\nwe follow the evaluation framework proposed in the Informer [22] paper. We will begin with a brief explanation of the\\r\\ndatasets utilised and the algorithms involved in Section 4.1, moreover, we will provide details regarding the hardware\\r\\nused in our experiments and the open-access repository where the code is published. In Section 4.2, we analyse\\r\\nthe results obtained including a Bayes test analysis. In Section 4.3 we perform a comparison between the attention\\r\\nmechanisms LAM and ProbAttn utilising the vanilla transformer architecture. Finally, in Section 4.4 we argue that\\r\\nthe datasets that are used in the literature for benchmarking of models for the LSTF problem are not sufficient, which\\r\\nmotivates Section 5.\\r\\n4.1 Experimental Details\\r\\nDatasets: For the sake of fairness we first follow the experimental methodology of Informer [22], we use four subsets\\r\\nof ETT: ETTh1 and ETTh2 (with hourly sampling), and ETTm1 and ETTm2 (with a 15-minute granularity). The ECL\\r\\ndataset records electricity consumption data for 321 clients, converted into hourly consumption, with ’MT 320’ as the\\r\\ntarget variable. The Weather dataset consists of climatological data from approximately 1,600 locations in the United\\r\\nStates, collected hourly over four years, featuring ’wet bulb’ as the target variable along with 11 other climate features.\\r\\nEach dataset was divided into training, validation, and test sets. Details of the datasets can be observed in Table 1.\\r\\nDataset Instances Features Sampling rate\\r\\nETTh1 8,640 7 1 hour\\r\\nETTh2 8,640 7 1 hour\\r\\nETTm1 34,560 7 15 minutes\\r\\nETTm2 34,560 7 15 minutes\\r\\nWeather 24,544 12 1 hour\\r\\nECL 18,412 321 1 hour\\r\\nTable 1: Datasets description. Number of instances, features and sampling rates are displayed in the table.\\r\\nMultivariate prediction, error metrics and validation: The primary objective of this comparison is to determine\\r\\nwhich model produces the least error in multivariate predictions on the datasets described above. Multivariate pre\\x02diction involves using a fixed-size input sequence to predict a fixed-size output sequence across all variables in the\\r\\ndataset. The problem of multi-variable forecasting is significantly more challenging than single-variable forecasting.\\r\\nIncorporating more than one input variable allows for the extraction of more valuable information by capturing de\\x02pendencies between variables. However, forecasting multiple variables simultaneously requires more sophisticated\\r\\nmodelling, as errors can accumulate across each component. Furthermore, including more variables increases the\\r\\nmodel’s complexity, posing a challenge to assigning meaningful values to the weights. Consequently, the results ob\\x02tained from multi-variable forecasting carry more significance and provide a more comprehensive evaluation of model\\r\\n12\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nperformance. To ensure the validity and representativeness of the results, we have incorporated Bayesian tests [38],\\r\\nwhich serve to facilitate and formalise the evaluation process.\\r\\nData has been standardised to have a mean of zero and a standard deviation of one. Errors were assessed using\\r\\nreconstruction error, with Mean Absolute Error (MAE) and Mean Squared Error (MSE) serving as the evaluation\\r\\nmetrics.\\r\\nGiven the long training times needed in the LSTF problem, we present the results of one training execution, as it is\\r\\nstandard in the literature. Due to the nature of time series, the methodology employed consists in dividing each dataset\\r\\ninto training, validation, and test sets to monitor training and assess performance effectively. Specifically, the division\\r\\nis 12/4/4 months for ETT, 15/3/4 months for ECL, and 28/10/10 months for WTH as in [22].\\r\\nAlgorithms: We have maintained the same comparative framework as presented in the Informer paper, utilising the\\r\\nfive algorithms selected by the original authors, being a representative sample of algorithms involving statistical meth\\x02ods and recurrent networks. This selection comprises ARIMA [39], Prophet [40], LSTMa [41], LST-Net [24], and\\r\\nDeepAR [42]. To better represent the state-of-the-art in time series forecasting, we have also included Reformer [43]\\r\\nand LogSparse self-attention [28], alongside the Informer model [22]. All these models have undergone hyperparam\\x02eter optimisation to determine the most suitable configuration for each dataset.\\r\\nHardware and Code: The experiments were conducted on an NVIDIA DGX-A100 workstation, which includes\\r\\n1TB of RAM, two AMD Rome 7742 processors, and eight NVIDIA A100 graphics cards, each equipped with 40GB\\r\\nof memory. For implementation and experimentation, we developed a Python-based codebase, utilising the PyTorch\\r\\nframework for the development and training of neural networks. To ensure transparency and reproducibility, we have\\r\\nmade the source code available through a dedicated GitHub repository within the ARI-DaSCI organisation2.\\r\\n4.2 Local Attention Mechanism against the state-of-the-art\\r\\nFirst, we will analyse a comparison between Informer and the number of parameters of the vanilla transformer incor\\x02porating LAM as attention mechanism, in order to test the improvement that arises from incorporating our proposal on\\r\\nvanilla architecture. It is important to highlight that the architecture accompanying our attention mechanism is exactly\\r\\nthe vanilla architecture, based on Vaswani’s [20] original paper, without any modifications to the encoding-decoding\\r\\nframework or the positional encoding. The only difference between both models is the inclusion of LAM instead of the\\r\\nfull attention mechanism. In the experimentation presented in this section the number of encoding and decoding layers\\r\\nin the vanilla transformer is 3, as opposed to 4 encoding layers and 2 decoding layers in the Informer architecture [22].\\r\\nModel Seq_len Parameters Ratio Num. encoding/decoding layers\\r\\nInformer 48 12,455,745 2.1 4/2\\r\\nLocal Attention 5,900,000 6\\r\\nInformer 168 12,455,745 2.1 3/3\\r\\nLocal Attention 5,900,000 6\\r\\nInformer 336 12,455,745 24/2\\r\\nLocal Attention 6,000,000 6\\r\\nInformer 720 12,455,745 1.9 3/3\\r\\nLocal Attention 6,400,000 6\\r\\nInformer 960 12,455,745 1.8 4/2\\r\\nLocal Attention 6,800,000 6\\r\\nTable 2: Number of parameters of Informer and our Local Attention transformer model over the ECL dataset. The\\r\\nratio is computed as Local Attention model parameters divided by Informer parameters. Seq_len parameter stands\\r\\nfor the size of the input and prediction window. The number of encoding/decoding layers presents two numbers, a/b,\\r\\nwhere a is the number of encoding layers in the model and b is the number of decoding layers.\\r\\nTable 2 presents the size of both models on a fixed dataset while varying the size of the input window. Our model\\r\\nexhibits sensitivity to this variation, whereas Informer does not. The reason for this is that Informer has a fixed\\r\\narchitecture that accepts an input window of size at most 512 time-steps. For smaller input window sizes, the input is\\r\\n2Link to our GitHub repository: https://github.com/ari-dasci/S-LAM\\r\\n13\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nextended with zeroes until reaching the mentioned size. Note that all the parameters in the Informer model are still used\\r\\nin the model independently of the size of the input window. In the optimal scenario, we achieve a parameter reduction\\r\\nfactor of 2.1, and in the worst scenario, a factor of 1.9. Therefore, the model incorporating LAM is significantly more\\r\\ncompact in terms of the number of parameters compared to Informer, and consequently, in terms of memory usage.\\r\\nThis compactness could allow us to concatenate more layers of our attention mechanism, thereby more accurately\\r\\ncapturing long-term temporal dependencies. However, to highlight the effect of our attention mechanism in prediction\\r\\naccuracy, we have chosen to have a similar number of encoding and decoding layers as Informer.\\r\\nMethods LAM Informer LogTrans Reformer LSTMa LSTnet\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1\\r\\n24 0.471 0.448 0.577 0.549 0.686 0.604 0.991 0.754 0.650 0.624 1.293 0.901\\r\\n48 0.560 0.545 0.685 0.625 0.766 0.757 1.313 0.906 0.702 0.675 1.456 0.960\\r\\n168 1.011 0.807 0.931 0.752 1.002 0.846 1.824 1.138 1.212 0.867 1.997 1.214\\r\\n336 0.923 0.756 1.128 0.873 1.362 0.952 2.117 1.280 1.424 0.994 2.655 1.369\\r\\n720 0.993 0.732 1.215 0.896 1.397 1.291 2.415 1.520 1.960 1.322 2.143 1.380\\r\\nETTh2\\r\\n24 0.588 0.543 0.720 0.665 0.828 0.750 1.531 1.613 1.143 0.813 2.742 1.457\\r\\n48 0.780 0.692 1.457 1.001 1.806 1.034 1.871 1.735 1.671 1.221 3.567 1.687\\r\\n168 3.209 1.479 3.489 1.515 4.070 1.681 4.660 1.846 4.117 1.674 3.242 2.513\\r\\n336 6.289 2.192 2.723 1.340 3.875 1.763 4.028 1.688 3.434 1.549 2.544 2.591\\r\\n720 2.835 1.204 3.467 1.473 3.913 1.552 5.381 2.015 3.963 1.788 4.625 3.709\\r\\nETTm1\\r\\n24 0.264 0.301 0.323 0.369 0.419 0.412 0.724 0.607 0.621 0.629 1.968 1.170\\r\\n48 0.714 0.561 0.494 0.503 0.507 0.583 1.098 0.777 1.392 0.939 1.999 1.215\\r\\n96 0.487 0.511 0.678 0.614 0.768 0.792 1.433 0.945 1.339 0.913 2.762 1.542\\r\\n288 0.522 0.522 1.056 0.786 1.462 1.320 1.820 1.094 1.740 1.124 1.257 2.076\\r\\n672 0.774 0.658 1.192 0.926 1.669 1.461 2.187 1.232 2.736 1.555 1.917 2.941\\r\\nWeather\\r\\n24 0.307 0.349 0.335 0.381 0.435 0.477 0.655 0.583 0.546 0.570 0.615 0.545\\r\\n48 0.409 0.450 0.395 0.459 0.426 0.495 0.729 0.666 0.829 0.677 0.660 0.589\\r\\n168 0.527 0.537 0.608 0.567 0.727 0.671 1.318 0.855 1.038 0.835 0.748 0.647\\r\\n336 0.582 0.576 0.702 0.620 0.754 0.670 1.930 1.167 1.657 1.059 0.782 0.683\\r\\n720 0.762 0.670 0.831 0.731 0.885 0.773 2.726 1.575 1.536 1.109 0.851 0.757\\r\\nECL\\r\\n48 0.294 0.379 0.344 0.393 0.355 0.418 1.404 0.999 0.486 0.572 0.369 0.445\\r\\n168 0.282 0.375 0.368 0.424 0.368 0.432 1.515 1.069 0.574 0.602 0.394 0.476\\r\\n336 0.287 0.375 0.381 0.431 0.373 0.439 1.601 1.104 0.886 0.795 0.419 0.477\\r\\n720 0.368 0.391 0.406 0.443 0.409 0.454 2.009 1.170 1.676 1.095 0.556 0.565\\r\\n960 0.417 0.497 0.460 0.548 0.477 0.589 2.141 1.387 1.591 1.128 0.605 0.599\\r\\nCount 43 7 0 0 0 0\\r\\nTable 3: Multivariate long sequence time-series forecasting results on the datasets described in Section 4.1.\\r\\nAfter comparing the sizes of the models, we proceed to examine the results of multivariate predictions. A comprehen\\x02sive comparison of all algorithms for the ETTh1, ETTh2, ETTm1, Weather, and ECL datasets is presented in Table 3.\\r\\nThe most notable aspect of the table is the superior performance of LAM. Out of 50 combinations of results and\\r\\nmetrics, LAM ranks highest in 43, followed by Informer, which leads in 7. When compared to Reformer and Log\\x02Trans, our approach significantly outperforms their results. This superiority is even more pronounced in comparison to\\r\\nLSTMa and LST-Net, particularly with longer prediction horizons, where the performance gap is the largest. This can\\r\\nbe attributed to the capability of our architecture to capture long-term dependencies. While LSTM-based algorithms\\r\\nperform well with short prediction horizons, LAM demonstrates substantial improvement and superior performance\\r\\nwith longer prediction horizons, supporting the hypothesis that transformers are better equipped for LSTF problems.\\r\\nFocusing on a direct comparison between LAM and Informer allows for a more detailed case-by-case analysis. Overall,\\r\\nLAM outperforms Informer, particularly for large forecast horizons in the ETT datasets. For example, in the case of\\r\\nETTh2 with a forecast window of 720, LAM achieves an MSE of 2.8 compared to Informer’s 3.4. Although LAM also\\r\\ndemonstrates superior performance in the ECL and Weather datasets, the margin of superiority is not as pronounced\\r\\nas in the ETT datasets.\\r\\nTo better support this comparison, we conducted two Bayesian tests: one focusing on the MSE and the other on the\\r\\nMAE, using a 0.05 ROPE. As illustrated in Figure 5a and Figure 5b, there is statistically significant superiority of\\r\\nLAM with respect to the rest of algorithms considered. These significant differences underscore the robustness and\\r\\n14\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\np(Informer) = 0.000\\r\\np(rope) = 0.000\\r\\np(LAM) = 1.000\\r\\n(a) MSE Bayesian test\\r\\np(Informer) = 0.000\\r\\np(rope) = 0.000\\r\\np(LAM) = 1.000\\r\\n(b) MAE Bayesian test\\r\\nFigure 5: Bayesian tests for Informer vs LAM\\r\\neffectiveness of our model compared to existing LSTF algorithms. Consequently, this provides strong validation for\\r\\nour experimentation methodology and supports the reliability of the conclusions drawn from our study.\\r\\n4.3 Comparing LAM and ProbAttn within the same architecture framework\\r\\nFollowing the above comparison, it is pertinent to consider whether the two attention mechanisms, LAM and ProbAttn\\r\\n(Informer’s attention mechanism) are competing on equal terms. Informer is a significantly more complex model than\\r\\nthe vanilla transformer integrating LAM. The Informer model utilises its own positional encoding based on various\\r\\ntime frequencies (seconds, minutes, hours, weeks, etc.) and incorporates layers such as one-dimensional convolu\\x02tions to facilitate feature extraction from the dataset. Therefore, before proceeding with further experimentation, it\\r\\nis essential to compare LAM and ProbAttn by aligning the frameworks under which both attention mechanisms are\\r\\nimplemented. To this end, we will compare the results obtained by ProbAttn and LAM within the vanilla transformer\\r\\narchitecture, to isolate the effects of Informer’s attention mechanism with respect to the characteristics of its architec\\x02ture. This involves using only fully connected layers and the sine-based positional encoding from the original proposal\\r\\nby Vaswani et al [20], instead of the complex architecture of Informer.\\r\\nThe results presented in Table 4 provide a clear illustration of the performance differences. Notably, LAM outperforms\\r\\nProbAttn in 37 instances, while ProbAttn prevails in 23. However, it is crucial to also examine the cumulative error\\r\\nacross all datasets and scenarios to establish a comprehensive metric. Specifically, the cumulative MSE for LAM is\\r\\n28.979, compared to 34.841 for ProbAttn. This indicates that, overall, LAM exhibits a lower cumulative error than\\r\\nProbAttn. Similarly, for MAE, LAM achieves an accumulated value of 19.976, whereas ProbAttn records 21.666.\\r\\nAdditionally, it is noteworthy that, on average, LAM reduces the error of ProbAttn by approximately 25% for longer\\r\\nprediction horizons. This finding corroborates the improvement and stability of our model’s results under the LSTF\\r\\nproblem.\\r\\nTo corroborate the data presented in the table, we include the corresponding Bayesian tests, as illustrated in Figure 6.\\r\\nThese tests reveal that the differences are significant in favour of our proposal, with a probability of 97.2% for MSE\\r\\nand a probability of 93.9% for MAE of LAM being better than ProbAttn. These results support the effectiveness of\\r\\nour proposal within the same comparison framework.\\r\\nIn view of the results obtained in this experimentation, we argue that the complex architecture used in Informer is\\r\\nnot properly justified. Therefore, the question of determining a transformer architecture that successfully exploits\\r\\nthe structure of long-term time series forecasting problems remains open. We note that in the paper that introduced\\r\\nInformer [22] there is no comparison to the vanilla transformer and, thus, no empirical justification of the changes\\r\\nintroduced at the architecture level – the study focuses on comparing ProbAttn with FullAttention within their own ar\\x02chitecture and comparing Informer to other models specifically designed for time series forecasting. For completeness,\\r\\nwe also include comparisons between LAM and FullAttention in Section 5.\\r\\n15\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nAttention LAM ProbAttn\\r\\nMetric MSE MAE MSE MAE\\r\\nETTh1\\r\\n24 0.471 0.448 0.498 0.507\\r\\n48 0.560 0.545 0.501 0.515\\r\\n168 1.011 0.807 1.071 0.804\\r\\n336 0.923 0.756 1.002 0.796\\r\\n720 0.993 0.732 1.470 0.963\\r\\nETTh2\\r\\n24 0.588 0.543 0.742 0.683\\r\\n48 0.780 0.692 0.881 0.715\\r\\n168 3.209 1.479 2.652 1.380\\r\\n336 6.289 2.192 1.487 0.950\\r\\n720 2.835 1.204 3.234 1.314\\r\\nETTm1\\r\\n24 0.264 0.301 0.543 0.495\\r\\n48 0.714 0.561 0.698 0.560\\r\\n96 0.487 0.511 0.547 0.522\\r\\n288 0.522 0.522 0.502 0.505\\r\\n672 0.774 0.658 0.737 0.661\\r\\nETTm2\\r\\n24 0.453 0.507 0.482 0.502\\r\\n48 0.874 0.661 0.521 0.535\\r\\n96 0.520 0.526 0.788 0.641\\r\\n288 1.137 0.799 1.639 1.045\\r\\n672 1.340 0.933 10.35 2.609\\r\\nWTH\\r\\n24 0.307 0.349 0.375 0.406\\r\\n48 0.409 0.450 0.406 0.449\\r\\n168 0.527 0.537 0.524 0.533\\r\\n336 0.582 0.576 0.563 0.563\\r\\n720 0.762 0.670 0.854 0.832\\r\\nECL\\r\\n48 0.294 0.379 0.279 0.372\\r\\n168 0.282 0.375 0.288 0.382\\r\\n336 0.287 0.375 0.333 0.417\\r\\n720 0.368 0.391 0.406 0.443\\r\\n960 0.417 0.497 0.468 0.567\\r\\nCount 37 23\\r\\nAccumulated 28.979 19.976 34.841 21.666\\r\\nTable 4: MSE and MAE comparison between LAM and ProbAttn mechanism. Best results in bold.\\r\\n4.4 On the suitability of benchmark models for the LSTF problem\\r\\nAs a conclusion to this experimentation, we have identified several areas for improvement in the literature. Firstly,\\r\\nall datasets utilised have a relatively small number of samples; none exceeds 100,000 instances (Table 1). Trans\\x02former models for long-sequence time series forecasting, being highly complex architectures in terms of the number\\r\\nof weights, require a substantial amount of data to effectively train all parameters. The current datasets are insufficient\\r\\nin terms of sample size for this task, which produces results that are not sufficiently conclusive. Additionally, the\\r\\n16\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\np(ProbAttention) = 0.028\\r\\np(rope) = 0.000\\r\\np(LAM) = 0.972\\r\\n(a) MSE Bayesian test\\r\\np(ProbAttention) = 0.061\\r\\np(rope) = 0.000\\r\\np(LAM) = 0.939\\r\\n(b) MAE Bayesian test\\r\\nFigure 6: Bayesian tests ProbAttn vs LAM\\r\\ndatasets do not always encompass a large set of features. An increase in the number of variables or features inherently\\r\\nincreases the complexity of multivariate prediction for LSTF, leading to more interesting problems.\\r\\nAnalysing the ETT datasets in greater detail, we observe that there are four variants of the same dataset. Specifically,\\r\\nthe nature of the data for ETTh1 and ETTm1 is the same as for ETTh2 and ETTm2; they represent the same problem\\r\\nbut are derived from different sources. Additionally, each dataset has been sampled at 15-minute intervals (ETTm)\\r\\nand one-hour intervals (ETTh). In the context of long-term time series forecasting, it is crucial to provide as much\\r\\ngranularity as possible to extend the prediction horizon effectively and meaningfully as otherwise the model will\\r\\nessentially be trained to predict averages. Thus, including two variants of the same problem with different sampling\\r\\nintervals does not appear to be justified.\\r\\nRecall that the WTH dataset is deals with the prediction of weather-related values. Weather forecasting is known to\\r\\nbe an exceedingly complex problem that requires extensive information to predict future instances with any degree of\\r\\naccuracy. Despite this complexity, weather forecasting is typically not aimed at long prediction horizons due to the\\r\\ninherent increase in error over time. Therefore, we consider this dataset to be unsuitable as a benchmark dataset for\\r\\nlong-term time series forecasting. Additionally, the WTH dataset only provides hourly resolution, limiting the ability\\r\\nto extend the prediction horizon per instance without significantly increasing the risk of a high margin of error.\\r\\nLastly, let us analyse the ECL dataset, which represents the electricity consumption of 321 stations within the electric\\x02ity grid over approximately two years with an hourly sampling frequency. While the scope of this dataset is relatively\\r\\nsimilar to that of the ETT dataset, certain distinctions can be observed. Notably, the dataset has a very low resolution\\r\\nof data collection. With only two years of data and an hourly sampling frequency to model a total of 321 variables\\r\\nrepresenting all stations, the feature-to-instance ratio is excessively high. This imbalance makes it challenging to\\r\\neffectively address the problem using this dataset.\\r\\nFor these reasons, we believe that the currently used datasets, while useful for measuring model performance, are\\r\\ninsufficient to establish a comprehensive benchmark for time series analysis in general, or for the Long Sequence\\r\\nTime-series Forecasting (LSTF) problem in particular. Consequently, we recognise the necessity of obtaining new\\r\\ndatasets that more accurately represent the LSTF problem to better evaluate model performance. This need will be\\r\\naddressed in the following section of this paper.\\r\\n5 Towards a suitable collection of datasets for LSTF benchmarking\\r\\nTo evaluate the performance of LAM in LSTF problems, we have highlighted a new set of experiments using datasets\\r\\nfrom various domains with real-world characteristics, allowing the exploration of inherent characteristics of trans\\x02formers for LSTF. This section details the experimental framework, including the datasets and algorithms involved.\\r\\nFinally, we present and discuss the results obtained.\\r\\n17\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\n5.1 Experimental Details\\r\\nDatasets: For this experiment, we selected four datasets: Individual Household Electric Power (IHEP) [44], New York\\r\\nTaxi (NYTaxi) [45], Residential Power Battery (RPB) [46], and Time-series Industrial Anomaly (TiNA) [47, 48].\\r\\nThe IHEP dataset contains over 200,000 samples with 9 features, representing household electricity consumption\\r\\nmeasurements over a 4-year period. The NYTaxi dataset includes over 2 million instances with 6 features, such as\\r\\ndistance travelled, number of passengers, amount paid, and trip duration. These metrics are aggregated by summing all\\r\\ntrips within the same hour, allowing the dataset to support predictions for subsequent hours, days, or weeks. The RPB\\r\\ndataset comprises more than 500,000 samples with 6 features, measuring the actual consumption of a population, solar\\r\\npower generation, and battery levels. Finally, the TiNA dataset includes over 38 million instances with 108 features,\\r\\nsampled every two seconds. This dataset is a real-world dataset provided by ArcelorMittal, reflecting data from an ore\\r\\nmining machine.\\r\\nThese datasets contain a large number of instances, allowing us to extend the prediction horizons substantially com\\x02pared to the Informer-derived experimentation. Specifically, for the RPB and IHEP datasets, the window sizes are 180,\\r\\n360, 720, 1440, and 4320, corresponding to 3 hours, 6 hours, 12 hours, 24 hours, and 3 days, respectively. For the\\r\\nNYTaxi dataset, the sequence sizes used were 120, 240, 720, 1440, and 2160, representing 2 days, 4 days, 12 days, 24\\r\\ndays, and 36 days, respectively. Finally, for the TiNA dataset, sequence sizes of 360, 720, 1440, 2880, and 5760 were\\r\\nused, corresponding to 3 hours, 6 hours, 12 hours, 24 hours, and 2 days, respectively. As shown, the predictions are\\r\\nlong-term, aiming for the longest possible prediction horizon that is meaningful for each problem. The sequence size\\r\\nrefers to both the input size of the network and the size of the prediction to be made.\\r\\nDataset Instances Features Sampling\\r\\nIHEP 2,075,259 7 1 minute\\r\\nNYTaxi 2,091,896 6 1 hour\\r\\nRPB 614,880 4 1 minute\\r\\nTiNA 38,000,000 103 1 second\\r\\nTable 5: Datasets description. Number of instances, features and sampling rates are displayed in the table.\\r\\nMultivariate prediction and error metrics: Within this framework, the same target task and metrics outlined in the\\r\\nprevious section are employed, alongside Bayesian tests. For validation, a data partitioning scheme of 70% training\\r\\ndata and 30% testing data has been utilised.\\r\\nAlgorithms: In the experimentation provided in Section 4, Informer and LAM are the closest models in terms of\\r\\nperformance. The other algorithms, including transformers such as LogTrans and Reformer, showed significantly\\r\\npoorer results. Consequently, we have decided to exclude these algorithms from this comparison to obtain concise\\r\\nand more interpretable results. Therefore, we have retained Informer and our proposed model, and added the vanilla\\r\\ntransformer with its attention mechanism (FullAttention) to serve as a baseline. For our proposed model and the vanilla\\r\\nattention mechanism, the optimal depth was determined through hyperparameter tuning. The transformer architecture\\r\\nemployed is a fully connected, symmetric structure with an equal number of encoding and decoding layers. The depth\\r\\nwas varied between 2, 3, 5, 7, and 10 layers, contingent upon memory capacity, to identify the best configuration.\\r\\nThis parameter will be named as number of encoding-decoding layers for the rest of the section. By using the same\\r\\narchitecture to baseline the vanilla attention mechanism, we ensure that the observed results can be solely attributed to\\r\\nour proposal and the network depth. The same standardisation and metrics introduced in the previous section are used\\r\\nfor the present one. The train/validation/test split utilised in this study allocated 70% of the data for training, with 15%\\r\\nof this subset further designated for validation and the remaining 85% for training. The remaining 30% of the data\\r\\nwas used as the test set.\\r\\nHardware and Code: The same hardware used in the experimentation of Section 4 has been kept in order to obtain\\r\\nconsistent results. The code is also hosted in the repository3 provided in previous sections.\\r\\n5.2 Results and Analysis\\r\\nIn this section, we will analyse the results obtained from the datasets previously described in the framework explanation\\r\\nsection. First, we refer to the results presented in Table 6. A notable observation from this table is the presence of\\r\\nempty cells, which indicate execution problems due to memory limitations for certain parameter combinations. As\\r\\nevident, our proposed model is the only one capable of running in all scenarios. This is attributable to the low number\\r\\n3\\r\\nhttps://github.com/ari-dasci/S-LAM\\r\\n18\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nof parameters in our model (shown in Section 4), resulting from its simplicity in the encoder-decoder architecture\\r\\nas opposed to Informer’s architecture, and the Θ(n log n) memory requirement for LAM as opposed to the Θ(n\\r\\n2\\r\\n)\\r\\nmemory ussage of FullAttention. Out of the total 20 execution scenarios, Informer successfully executes in 17, while\\r\\nthe vanilla transformer executes in 12. These findings highlight the superior performance of our model when dealing\\r\\nwith large prediction horizons and highlights the importance of efficient attention mechanisms, both from a time and\\r\\nmemory point of view.\\r\\nIn terms of performance, LAM shows clear superiority. The table presents error results using MSE and MAE metrics.\\r\\nOut of the 40 cells that summarise both metrics, LAM achieves the lowest error in 32 instances and ties in 2. Informer\\r\\nprevails in 4 instances, while the Vanilla transformer ties in 2 and wins in 2. When analysed by dataset, NYTaxi\\r\\nappears to be the easiest to address, as all three algorithms exhibit lower error rates for this dataset. Conversely, for\\r\\nthe RPB dataset, Informer and our proposal yield similar results, indicating close performance. In the IHEP dataset,\\r\\nand particularly in the TiNA dataset, there is a significant performance gap favouring our proposal. Comparing with\\r\\nthe Vanilla transformer, it is evident that while it runs in fewer instances, it generally performs better than Informer,\\r\\nexcept perhaps for the RPB dataset, where their results are comparable.\\r\\nAttention LAM Informer Vanilla\\r\\nMetric MSE MAE Layers MSE MAE MSE MAE Layers\\r\\nIHEP\\r\\n180 0.956 0.665 5 1.272 0.818 1.021 0.702 5\\r\\n360 0.942 0.670 7 1.295 0.828 0.979 0.706 5\\r\\n720 0.973 0.678 5 1.244 0.817 0.973 0.678 3\\r\\n1440 0.898 0.644 10 - - - - -\\r\\n4320 0.977 0.698 3 - - - - -\\r\\nNYTaxi\\r\\n120 0.076 0.209 5 0.366 0.480 0.112 0.248 3\\r\\n240 0.082 0.221 5 0.376 0.515 0.068 0.203 2\\r\\n720 0.082 0.212 5 0.359 0.496 0.092 0.223 3\\r\\n1440 0.112 0.239 3 0.291 0.435 - - -\\r\\n2160 0.103 0.223 10 0.4 0.505 - - -\\r\\nRPB\\r\\n180 2.999 1.321 3 3.402 1.431 3.123 1.335 2\\r\\n360 2.985 1.304 7 2.90 1.344 3.015 1.327 3\\r\\n720 2.996 1.319 3 3.383 1.419 3.054 1.334 2\\r\\n1440 3.081 1.346 3 2.926 1.35 3.114 1.362 2\\r\\n4320 3.027 1.325 2 2.822 1.314 - - -\\r\\nTiNA\\r\\n360 1.843 0.858 3 2.044 0.954 1.854 0.863 3\\r\\n720 1.849 0.863 7 2.148 0.976 - - -\\r\\n1440 1.857 0.867 10 2.033 0.940 - - -\\r\\n2880 1.827 0.850 10 2.020 0.944 - - -\\r\\n5760 1.838 0.853 7 - - - - -\\r\\nCount 34 4 4\\r\\nMean Accumulated 1.475 0.768 1.722 0.915 1.582 0.816\\r\\nTable 6: Multivariate forecasting over the Individual Household Electric Power dataset (IHEP), New York Taxi dataset\\r\\n(NYTaxi), Residential Power Battery dataset (RPB) and Time-series Industrial Anomaly dataset (TiNA). MSE and\\r\\nMAE are presented in the table, the lower the better, best results in bold. Empty cells are due to the impossibility of\\r\\nexecuting the algorithm for these parameters due to memory problems given the size of the model. Layers represent\\r\\nthe best configuration of encoding-decoding layers used to obtain the presented results. Count stands for best result\\r\\ncounts. Mean Accumulated is computed as the added error divided by the number of valid rows in the table (non\\x02empty). The Vanilla algorithm is the original transformer proposed in [20].\\r\\n19\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nAlthough the results presented in the aforementioned table demonstrate the validity and quality of our proposal, the\\r\\ninfluence of the number of layers within this comparison warrants further examination. The results may be highly\\r\\nsensitive to this configuration, potentially presenting a distorted image of the outcomes. As observed in Table 6, there\\r\\nis a variety of optimal configurations for each dataset and sequence size. Therefore, we present the results with a\\r\\nfixed architecture for both Vanilla and LAMin Table 7. These results were obtained using the largest architecture that\\r\\ncan run completely on all datasets for both models: 5 encoding-decoding layers for Vanilla and 7 encoding-decoding\\r\\nlayers for LAM. This approach provides a consistent performance comparison with a fixed architecture, allowing us\\r\\nto contrast these findings with the results in the aforementioned table.\\r\\nAs observed in the Table 7, LAM consistently emerges as the superior algorithm in this comparison. Specifically,\\r\\nit outperforms the other algorithms 31 times, whereas Informer obtains better results 5 times, and Vanilla wins 4\\r\\ntimes. This demonstrates the clear superiority of our proposal over both the baseline and Informer models. Regarding\\r\\nthe mean of the MSE and MAE across rows with valid values, Vanilla appears to have a slight edge. However,\\r\\nit is important to note that these results are significantly influenced by Vanilla’s inability to run under numerous\\r\\ncircumstances due to its computational demands. Consequently, these findings strongly establish the validity of our\\r\\nproposal, even when utilising a fixed architecture.\\r\\nAttention LAM Informer Vanilla\\r\\nMetric MSE MAE MSE MAE MSE MAE\\r\\nIHEP\\r\\n180 0.994 0.692 1.272 0.818 1.036 0.706\\r\\n360 0.942 0.670 1.295 0.828 0.979 0.706\\r\\n720 0.984 0.695 1.244 0.817 1.070 0.730\\r\\n1440 1.030 0.739 - - - -\\r\\n4320 1.013 0.702 - - - -\\r\\nNYTaxi\\r\\n120 0.147 0.285 0.366 0.480 0.162 0.286\\r\\n240 0.119 0.263 0.376 0.515 0.105 0.248\\r\\n720 0.112 0.244 0.359 0.496 0.119 0.248\\r\\n1440 0.151 0.270 0.291 0.435 - -\\r\\n2160 0.182 0.312 0.4 0.505 - -\\r\\nRPB\\r\\n180 2.977 1.312 3.402 1.431 3.196 1.387\\r\\n360 2.985 1.304 2.90 1.344 3.024 1.325\\r\\n720 3.168 1.378 3.383 1.419 3.153 1.372\\r\\n1440 3.137 1.368 2.926 1.35 - -\\r\\n4320 3.093 1.342 2.822 1.314 - -\\r\\nTiNA\\r\\n360 1.862 0.870 2.044 0.954 - -\\r\\n720 1.849 0.863 2.148 0.976 - -\\r\\n1440 1.863 0.867 2.033 0.940 - -\\r\\n2880 1.851 0.860 2.020 0.944 - -\\r\\n5760 1.838 0.853 - - - -\\r\\nCount 31 5 4\\r\\nMean Accumulated 1.514 0.794 1.722 0.915 1.427 0.778\\r\\nTable 7: Multivariate forecasting over the Individual Household Electric Power dataset (IHEP), New York Taxi dataset\\r\\n(NYTaxi), Residential Power Battery dataset (RPB) and Time-series Industrial Anomaly dataset (TiNA). MSE and\\r\\nMAE are presented in the table, the lower the better, best results in bold. Empty cells are due to the impossibility of\\r\\nexecuting the algorithm for these parameters due to memory problems given the size of the model. Fixed architectures\\r\\nfor LAM and Vanilla have been employed, being 7 encoding-decoding layers for LAM and 5 for Vanilla. Count stands\\r\\nfor best result counts. Mean Accumulated is computed as the added error divided by the number of valid rows in the\\r\\ntable (non-empty). The Vanilla algorithm is the original transformer proposed in [20].\\r\\n20\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nTo obtain an overall assessment, we utilised the mean cumulative error in both MAE and MSE as summary metrics.\\r\\nThis average was calculated by considering only the rows with valid values for each algorithm, excluding those without\\r\\nresults. This approach may inadvertently favour the Vanilla transformer and Informer, as they do not execute in\\r\\nscenarios with higher error rates (i.e., long prediction horizons), and therefore, lack results in those contexts. Despite\\r\\nthis potential bias, our model still achieves the lowest average cumulative error in both MSE and MAE. The Vanilla\\r\\ntransformer follows, with Informer exhibiting the highest average cumulative error.\\r\\np(Vanilla) = 0.000\\r\\np(rope) = 0.000\\r\\np(LAM) = 1.000\\r\\n(a) MSE Bayesian test\\r\\np(Vanilla) = 0.000\\r\\np(rope) = 0.000\\r\\np(LAM) = 1.000\\r\\n(b) MAE Bayesian test\\r\\nFigure 7: Bayesian tests for Vanilla vs LAM\\r\\np(Informer) = 0.000\\r\\np(rope) = 0.000\\r\\np(LAM) = 1.000\\r\\n(a) MSE Bayesian test\\r\\np(Informer) = 0.000\\r\\np(rope) = 0.000\\r\\np(LAM) = 1.000\\r\\n(b) MAE Bayesian test\\r\\nFigure 8: Bayesian tests for Informer vs LAM\\r\\nTo substantiate the argument that our algorithm outperforms both Informer and the Vanilla transformer, we conducted\\r\\ntwo Bayesian tests. The results, presented in Figure 7 and Figure 8, clearly indicate statistically significant differences\\r\\nbetween the algorithms, favouring our proposal. These findings affirm the validity of our model.\\r\\nIn this experimentation, as mentioned at the beginning of this section, we varied the number of encoding-decoding\\r\\nlayers to determine the optimal configuration. To study the effect of these architectural variations, the results are\\r\\npresented in bar charts, which can be viewed in Figure 9 and Figure 10, with a common legend in Figure 9c. It is\\r\\n21\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nimportant to note that, depending on the dataset size, some combinations of model layers may encounter memory\\r\\nissues and fail to execute, as observed in the TiNA dataset for sizes 1440, 2880, and 5760.\\r\\n180 360 720 1440 4320\\r\\nSequence Length\\r\\n0.00\\r\\n0.05\\r\\n0.10\\r\\n0.15\\r\\n0.20\\r\\n0.25\\r\\nMean Squared Error\\r\\n(a) IHE dataset\\r\\n120 240 720 1440 2160\\r\\nSequence Length\\r\\n0.00\\r\\n0.05\\r\\n0.10\\r\\n0.15\\r\\n0.20\\r\\n0.25\\r\\nMean Squared Error\\r\\n(b) NYTaxi dataset\\r\\n(c) Legend for the barplots.\\r\\nFigure 9: Barplots for the MSE of LAM to showcase the performance depending on the number of encoding-decoding\\r\\nlayers used. The datasets shown are IHE and NYTaxi. MSE is presented in the barplot in the y-axis, sequence size\\r\\n(input and output) is presented in the x-axis. The colour of the bar is defined by the number of layers of the model.\\r\\nThe results exhibit some variation. Let us call by N-encoder-decoder the LAM model with N encoding layers and N\\r\\ndecoding layers. For the IHEP dataset, the 2-encoder-decoder model is not optimal in any scenario. Specifically, the\\r\\n5-encoder-decoder model performs best for size 180, the 7-encoder-decoder model for size 360, the 5-encoder-decoder\\r\\nmodel for size 720, the 10-encoder-decoder model for size 1440, and the 3-encoder-decoder model for size 4320. This\\r\\nindicates that there is no consistent trend across all sequence sizes for this dataset. However, the 10-encoder-decoder\\r\\nconfiguration yields favourable results for the longer sequence sizes of 1440 and 4320, suggesting that a higher number\\r\\nof layers may be beneficial for longer prediction horizons.\\r\\nFor the NYTaxi dataset, the most effective configurations generally consist of 2 and 3 encoding and decoding layers,\\r\\nwith the 10-encoder-decoder architecture performing best for the longest prediction horizon. These results are ex\\x02pected, as the dataset does not represent a particularly complex problem, allowing it to be effectively addressed with a\\r\\nsmaller architectural configuration in most cases.\\r\\nFor the RPB dataset, the optimal architecture is the 7-encoder-decoder model for sizes 180 and 360, the 3-encoder\\x02decoder model for sizes 720 and 1440, and a tie between the 2-encoder-decoder and 4-encoder-decoder for size 4320.\\r\\nThis dataset does not appear to represent a particularly complex problem over extended prediction horizons, as evi\\x02denced by the lack of improvement in results with an increased number of attention layers.\\r\\nFinally, we present the results obtained from the TiNA dataset, which is the most complex due to its large number of\\r\\ninstances and variables. Consequently, it is the dataset that stands to benefit the most from architectures with more\\r\\nparameters. For this dataset, the 10-encoder-decoder architecture is unable to execute for sizes 1440 and above. For\\r\\nsize 360, the optimal architecture is the 3-encoder-decoder configuration, while the 7-encoder-decoder model performs\\r\\nbest for all other cases. The TiNA dataset clearly demonstrates the performance advantages of higher architectural\\r\\ndepth, particularly for the sequence size of 5760.\\r\\n6 Conclusions and future work\\r\\nThis paper proposes a novel attention mechanism for transformers, specifically designed for time series prediction.\\r\\nThis mechanism leverages the locality of the data and maintains a theoretical efficiency of Θ(log(n)). Our pro\\x0222\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\n180 360 720 1440 4320\\r\\nSequence Length\\r\\n0.00\\r\\n0.05\\r\\n0.10\\r\\n0.15\\r\\n0.20\\r\\n0.25\\r\\n0.30\\r\\n0.35\\r\\n0.40\\r\\nMean Squared Error\\r\\n(a) RPB dataset\\r\\n360 720 1440 2880 5760\\r\\nSequence Length\\r\\n0.00\\r\\n0.01\\r\\n0.02\\r\\n0.03\\r\\n0.04\\r\\n0.05\\r\\n0.06\\r\\n0.07\\r\\nMean Squared Error\\r\\n(b) TiNA dataset\\r\\n(c) Legend for the barplots.\\r\\nFigure 10: Barplots for the MSE of LAM to showcase the performance depending on the number of encoding-decoding\\r\\nlayers used. The datasets shown are RPB and TiNa. MSE is presented in the barplot in the y-axis, sequence size (input\\r\\nand output) is presented in the x-axis. The colour of the bar is defined by the number of layers of the model.\\r\\nposed mechanism has demonstrated superior performance compared to state-of-the-art attention-based mechanisms\\r\\nand models, including Informer, LogFormer, and Reformer, as well as LSTM models.\\r\\nAdditionally, we have highlighted the necessity for new and improved datasets for the evaluation of the Long Sequence\\r\\nTime-series Forecasting (LSTF) problem, in contrast to existing benchmarks. To address this need, we have introduced\\r\\nnew datasets that facilitate a more accurate assessment of model quality in these scenarios.\\r\\nOn larger and more challenging datasets, our proposed mechanism LAM has consistently outperformed not only\\r\\nInformer but also the baseline represented by the vanilla model across nearly all scenarios. Furthermore, due to the\\r\\nsimplicity of the model accompanying our mechanism, our proposal is uniquely capable of executing effectively in all\\r\\nthe studied scenarios. Additionally, we observed that the results exhibit low sensitivity to the depth of the architecture\\r\\nemployed. By maintaining a fixed architecture size, we confirmed that the hypothesis remains valid.\\r\\nConducted experiments have also yielded a strong model performance and improved metrics when employing a larger\\r\\nnumber of attention layers for complex problems. This suggests that, for problems that are challenging to model or that\\r\\ninvolve long prediction horizons, the results benefit significantly from utilising a greater number of attention layers.\\r\\nFinally, we have identified several areas for improvement and further research to enhance the performance of future\\r\\nmodels and advance the field. Firstly, as an existing challenge, it is essential to establish a comprehensive benchmark\\r\\nthat includes datasets, methodologies, and metrics to evaluate new proposals consistently. Maintaining an up-to-date\\r\\ntable of state-of-the-art model results is also crucial. Additionally, our study exclusively proposes an attention mecha\\x02nism without a dedicated architecture; thus, future work should focus on developing and integrating such architectures.\\r\\nLastly, considering the promising results obtained with a basic model employing a novel attention mechanism, it is\\r\\nimperative to conduct rigorous ablation studies. These studies will help discern which components positively impact\\r\\nthe results and which do not, thereby guiding the development of more effective models.\\r\\nAcknowledgements\\r\\nThis research results from the Strategic Project IAFER-Cib (C074/23), as a result of the collaboration agreement\\r\\nsigned between the National Institute of Cybersecurity (INCIBE) and the University of Granada. This initiative is\\r\\ncarried out within the framework of the Recovery, Transformation and Resilience Plan funds, financed by the European\\r\\nUnion (Next Generation). Ignacio Aguilera-Martos was supported by the Ministry of Science of Spain under the FPI\\r\\nprogramme PRE2021-100169.\\r\\n23\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\nReferences\\r\\n[1] Jessica Lin, Eamonn Keogh, Stefano Lonardi, and Bill Chiu. A symbolic representation of time series, with\\r\\nimplications for streaming algorithms. In Proceedings of the 8th ACM SIGMOD workshop on Research issues\\r\\nin data mining and knowledge discovery, pages 2–11, 2003.\\r\\n[2] Omer Berat Sezer, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. Financial time series forecasting with\\r\\ndeep learning: A systematic literature review: 2005–2019. Applied Soft Computing, 90:106181, 2020.\\r\\n[3] Tianyu Wang, Henry Leung, Jun Zhao, and Wei Wang. Multiseries featural lstm for partial periodic time-series\\r\\nprediction: A case study for steel industry. IEEE Transactions on Instrumentation and Measurement, 69(9):1–10,\\r\\n2020.\\r\\n[4] Mousa Alizadeh, Mohammad TH Beheshti, Amin Ramezani, and Hadis Saadatinezhad. Network traffic fore\\x02casting based on fixed telecommunication data using deep learning. In 2020 6th Iranian Conference on Signal\\r\\nProcessing and Intelligent Systems (ICSPIS), pages 1–7. IEEE, 2020.\\r\\n[5] Carlos E Velasquez, Matheus Zocatelli, Fidellis BGL Estanislau, and Victor F Castro. Analysis of time series\\r\\nmodels for brazilian electricity demand forecasting. Energy, 247:1–16, 2022.\\r\\n[6] Ismail Shah, Hasnain Iftikhar, and Sajid Ali. Modeling and forecasting electricity demand and prices: A com\\x02parison of alternative approaches. Journal of Mathematics, 2022(1):1–14, 2022.\\r\\n[7] Zahra Karevan and Johan AK Suykens. Transductive lstm for time-series prediction: An application to weather\\r\\nforecasting. Neural Networks, 125:1–9, 2020.\\r\\n[8] Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani, Francesco Palmieri,\\r\\nand Yonghuai Liu. Temporal convolutional neural (tcn) network for an effective weather forecasting using time\\x02series data from the local weather station. Soft Computing, 24:16453–16482, 2020.\\r\\n[9] Zonglei Chen, Minbo Ma, Tianrui Li, Hongjun Wang, and Chongshou Li. Long sequence time-series forecasting\\r\\nwith deep learning: A survey. Information Fusion, 97:1–36, 2023.\\r\\n[10] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis: forecasting\\r\\nand control. John Wiley & Sons, 2015.\\r\\n[11] Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R Gormley, and Jason Eisner. Limitations of autoregressive\\r\\nmodels and their alternatives. arXiv preprint arXiv:2010.11939, 2020.\\r\\n[12] Irena Koprinska, Dengsong Wu, and Zheng Wang. Convolutional neural networks for energy time series fore\\x02casting. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1–8, 2018.\\r\\n[13] Renzhuo Wan, Shuping Mei, Jun Wang, Min Liu, and Fan Yang. Multivariate temporal convolutional network:\\r\\nA deep neural networks approach for multivariate time series forecasting. Electronics, 8(8), 2019.\\r\\n[14] Hansika Hewamalage, Christoph Bergmeir, and Kasun Bandara. Recurrent neural networks for time series\\r\\nforecasting: Current status and future directions. International Journal of Forecasting, 37(1):388–427, 2021.\\r\\n[15] Felipe P. Marinho, Paulo A. C. Rocha, Ajalmar R. R. Neto, and Francisco D. V. Bezerra. Short-Term Solar\\r\\nIrradiance Forecasting Using CNN-1D, LSTM, and CNN-LSTM Deep Neural Networks: A Case Study With the\\r\\nFolsom (USA) Dataset. Journal of Solar Energy Engineering, 145(4):1–11, 11 2022.\\r\\n[16] Hatice Vildan Dudukcu, Murat Taskiran, Zehra Gulru Cam Taskiran, and Tulay Yildirim. Temporal convolutional\\r\\nnetworks with rnn approach for chaotic time series prediction. Applied Soft Computing, 133:1–14, 2023.\\r\\n[17] Ignacio Aguilera-Martos, Ángel M. García-Vico, Julián Luengo, Sergio Damas, Francisco J. Melero, José Javier\\r\\nValle-Alonso, and Francisco Herrera. Tsfedl: A python library for time series spatio-temporal feature extraction\\r\\nand prediction using deep learning. Neurocomputing, 517:223–228, 2023.\\r\\n[18] Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural networks for se\\x02quence learning. arXiv preprint arXiv:1506.00019, 2015.\\r\\n[19] Safwan Mahmood Al-Selwi, Mohd Fadzil Hassan, Said Jadid Abdulkadir, and Amgad Muneer. Lstm ineffi\\x02ciency in long-term dependencies regression problems. Journal of Advanced Research in Applied Sciences and\\r\\nEngineering Technology, 30(3):16–31, 2023.\\r\\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\\r\\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\r\\n[21] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In\\r\\nProceedings of the AAAI conference on artificial intelligence, number 9, pages 1–8, 2023.\\r\\n24\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\n[22] Haoyi Zhou, Jianxin Li, Shanghang Zhang, Shuai Zhang, Mengyi Yan, and Hui Xiong. Expanding the prediction\\r\\ncapacity in long sequence time-series forecasting. Artificial Intelligence, 318:Paper No. 103886, 29, 2023.\\r\\n[23] Peter Bloomfield. Fourier analysis of time series: an introduction. John Wiley & Sons, 2004.\\r\\n[24] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal pat\\x02terns with deep neural networks. In The 41st international ACM SIGIR conference on research & development\\r\\nin information retrieval, pages 95–104, 2018.\\r\\n[25] Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.\\r\\n[26] Muhammad Sajjad, Zulfiqar Ahmad Khan, Amin Ullah, Tanveer Hussain, Waseem Ullah, Mi Young Lee, and\\r\\nSung Wook Baik. A novel CNN-GRU-based hybrid approach for short-term residential load forecasting. IEEE\\r\\nAccess, 8:1–10, 2020.\\r\\n[27] Tae-Young Kim and Sung-Bae Cho. Predicting residential energy consumption using CNN-LSTM neural net\\x02works. Energy, 182:72–81, 2019.\\r\\n[28] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing\\r\\nthe locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural\\r\\ninformation processing systems, 32, 2019.\\r\\n[29] Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretable\\r\\nmulti-horizon time series forecasting. International Journal of Forecasting, 37(4):1748–1764, 2021.\\r\\n[30] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced\\r\\ndecomposed transformer for long-term series forecasting. In International Conference on Machine Learning,\\r\\npages 1–19. PMLR, 2022.\\r\\n[31] Geoffrey S Watson. Smooth regression analysis. Sankhya: The Indian Journal of Statistics, Series A ¯ , pages\\r\\n359–372, 1964.\\r\\n[32] Gordon J Johnston. Smooth nonparametric regression analysis. Technical report, North Carolina State University.\\r\\nDept. of Statistics, 1979.\\r\\n[33] Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho. Fourierformer: Transformer\\r\\nmeets generalized fourier integral theorem. Advances in Neural Information Processing Systems, 35:1–15, 2022.\\r\\n[34] Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed Osama Ahmed, Yoshua Bengio, and Greg Mori.\\r\\nAttention as an rnn. arXiv preprint arXiv:2405.13956, 2024.\\r\\n[35] James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of attention. arXiv\\r\\npreprint arXiv:2007.02876, 2020.\\r\\n[36] Andrea Galassi, Marco Lippi, and Paolo Torroni. Attention in natural language processing. IEEE Transactions\\r\\non Neural Networks and Learning Systems, 32(10):4291–4308, 2020.\\r\\n[37] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint\\r\\narXiv:2004.05150, 2020.\\r\\n[38] Alessio Benavoli, Giorgio Corani, Francesca Mangili, Marco Zaffalon, and Fabrizio Ruggeri. A bayesian\\r\\nwilcoxon signed-rank test based on the dirichlet process. In International Conference on Machine Learning,\\r\\npages 1026–1034. PMLR, 2014.\\r\\n[39] Adebiyi A Ariyo, Adewumi O Adewumi, and Charles K Ayo. Stock price prediction using the arima model. In\\r\\n2014 UKSim-AMSS 16th international conference on computer modelling and simulation, pages 106–112. IEEE,\\r\\n2014.\\r\\n[40] Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37–45, 2018.\\r\\n[41] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align\\r\\nand translate. arXiv preprint arXiv:1409.0473, 2014.\\r\\n[42] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with\\r\\nautoregressive recurrent networks. International Journal of Forecasting, 36(3):1–11, 2020.\\r\\n[43] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint\\r\\narXiv:2001.04451, 2020.\\r\\n[44] Georges Hebrail and Alice Berard. Individual Household Electric Power Consumption. UCI Machine Learning\\r\\nRepository, 2012.\\r\\n[45] New York Taxi and Limousine Commission. Tcl trip record data, July 2024.\\r\\n25\\nLocal Attention Mechanism for Long-Sequence Time Series Forecasting A PREPRINT\\r\\n[46] Christoph Bergmeir, Quang Bui, Frits de Nijs, and Peter Stuckey. Residential power and battery data, August\\r\\n2023.\\r\\n[47] Time-series industrial anomaly dataset, 2022.\\r\\n[48] Ignacio Aguilera-Martos, Marta García-Barzana, Diego García-Gil, Jacinto Carrasco, David López, Julián Lu\\x02engo, and Francisco Herrera. Multi-step histogram based outlier scores for unsupervised anomaly detection:\\r\\nArcelormittal engineering dataset case of study. Neurocomputing, 544:1–32, 2023.\\r\\n26'},\n",
       " {'name': '1406.1078v3.pdf',\n",
       "  'content': \"Learning Phrase Representations using RNN Encoder–Decoder\\r\\nfor Statistical Machine Translation\\r\\nKyunghyun Cho\\r\\nBart van Merrienboer Caglar Gulcehre ¨\\r\\nUniversite de Montr ´ eal ´\\r\\nfirstname.lastname@umontreal.ca\\r\\nDzmitry Bahdanau\\r\\nJacobs University, Germany\\r\\nd.bahdanau@jacobs-university.de\\r\\nFethi Bougares Holger Schwenk\\r\\nUniversite du Maine, France ´\\r\\nfirstname.lastname@lium.univ-lemans.fr\\r\\nYoshua Bengio\\r\\nUniversite de Montr ´ eal, CIFAR Senior Fellow ´\\r\\nfind.me@on.the.web\\r\\nAbstract\\r\\nIn this paper, we propose a novel neu\\x02ral network model called RNN Encoder–\\r\\nDecoder that consists of two recurrent\\r\\nneural networks (RNN). One RNN en\\x02codes a sequence of symbols into a fixed\\x02length vector representation, and the other\\r\\ndecodes the representation into another se\\x02quence of symbols. The encoder and de\\x02coder of the proposed model are jointly\\r\\ntrained to maximize the conditional prob\\x02ability of a target sequence given a source\\r\\nsequence. The performance of a statisti\\x02cal machine translation system is empiri\\x02cally found to improve by using the con\\x02ditional probabilities of phrase pairs com\\x02puted by the RNN Encoder–Decoder as an\\r\\nadditional feature in the existing log-linear\\r\\nmodel. Qualitatively, we show that the\\r\\nproposed model learns a semantically and\\r\\nsyntactically meaningful representation of\\r\\nlinguistic phrases.\\r\\n1 Introduction\\r\\nDeep neural networks have shown great success in\\r\\nvarious applications such as objection recognition\\r\\n(see, e.g., (Krizhevsky et al., 2012)) and speech\\r\\nrecognition (see, e.g., (Dahl et al., 2012)). Fur\\x02thermore, many recent works showed that neu\\x02ral networks can be successfully used in a num\\x02ber of tasks in natural language processing (NLP).\\r\\nThese include, but are not limited to, language\\r\\nmodeling (Bengio et al., 2003), paraphrase detec\\x02tion (Socher et al., 2011) and word embedding ex\\x02traction (Mikolov et al., 2013). In the field of sta\\x02tistical machine translation (SMT), deep neural\\r\\nnetworks have begun to show promising results.\\r\\n(Schwenk, 2012) summarizes a successful usage\\r\\nof feedforward neural networks in the framework\\r\\nof phrase-based SMT system.\\r\\nAlong this line of research on using neural net\\x02works for SMT, this paper focuses on a novel neu\\x02ral network architecture that can be used as a part\\r\\nof the conventional phrase-based SMT system.\\r\\nThe proposed neural network architecture, which\\r\\nwe will refer to as an RNN Encoder–Decoder, con\\x02sists of two recurrent neural networks (RNN) that\\r\\nact as an encoder and a decoder pair. The en\\x02coder maps a variable-length source sequence to a\\r\\nfixed-length vector, and the decoder maps the vec\\x02tor representation back to a variable-length target\\r\\nsequence. The two networks are trained jointly to\\r\\nmaximize the conditional probability of the target\\r\\nsequence given a source sequence. Additionally,\\r\\nwe propose to use a rather sophisticated hidden\\r\\nunit in order to improve both the memory capacity\\r\\nand the ease of training.\\r\\nThe proposed RNN Encoder–Decoder with a\\r\\nnovel hidden unit is empirically evaluated on the\\r\\ntask of translating from English to French. We\\r\\ntrain the model to learn the translation probabil\\x02ity of an English phrase to a corresponding French\\r\\nphrase. The model is then used as a part of a stan\\x02dard phrase-based SMT system by scoring each\\r\\nphrase pair in the phrase table. The empirical eval\\x02uation reveals that this approach of scoring phrase\\r\\npairs with an RNN Encoder–Decoder improves\\r\\nthe translation performance.\\r\\nWe qualitatively analyze the trained RNN\\r\\nEncoder–Decoder by comparing its phrase scores\\r\\nwith those given by the existing translation model.\\r\\nThe qualitative analysis shows that the RNN\\r\\nEncoder–Decoder is better at capturing the lin\\x02guistic regularities in the phrase table, indirectly\\r\\nexplaining the quantitative improvements in the\\r\\noverall translation performance. The further anal\\x02ysis of the model reveals that the RNN Encoder–\\r\\nDecoder learns a continuous space representation\\r\\nof a phrase that preserves both the semantic and\\r\\nsyntactic structure of the phrase.\\r\\narXiv:1406.1078v3 [cs.CL] 3 Sep 2014\\n2 RNN Encoder–Decoder\\r\\n2.1 Preliminary: Recurrent Neural Networks\\r\\nA recurrent neural network (RNN) is a neural net\\x02work that consists of a hidden state h and an\\r\\noptional output y which operates on a variable\\x02length sequence x = (x1, . . . , xT ). At each time\\r\\nstep t, the hidden state hhti of the RNN is updated\\r\\nby\\r\\nhhti = f\\r\\n\\r\\nhht−1i, xt\\r\\n\\x01\\r\\n, (1)\\r\\nwhere f is a non-linear activation func\\x02tion. f may be as simple as an element\\x02wise logistic sigmoid function and as com\\x02plex as a long short-term memory (LSTM)\\r\\nunit (Hochreiter and Schmidhuber, 1997).\\r\\nAn RNN can learn a probability distribution\\r\\nover a sequence by being trained to predict the\\r\\nnext symbol in a sequence. In that case, the output\\r\\nat each timestep t is the conditional distribution\\r\\np(xt| xt−1, . . . , x1). For example, a multinomial\\r\\ndistribution (1-of-K coding) can be output using a\\r\\nsoftmax activation function\\r\\np(xt,j = 1 | xt−1, . . . , x1) =\\r\\nexp wjhhti\\r\\n\\x01\\r\\nPK\\r\\nj\\r\\n0=1 exp \\r\\nwj\\r\\n0hhti\\r\\n\\x01,\\r\\n(2)\\r\\nfor all possible symbols j = 1, . . . , K, where wj\\r\\nare the rows of a weight matrix W. By combining\\r\\nthese probabilities, we can compute the probabil\\x02ity of the sequence x using\\r\\np(x) = Y\\r\\nT\\r\\nt=1\\r\\np(xt| xt−1, . . . , x1). (3)\\r\\nFrom this learned distribution, it is straightfor\\x02ward to sample a new sequence by iteratively sam\\x02pling a symbol at each time step.\\r\\n2.2 RNN Encoder–Decoder\\r\\nIn this paper, we propose a novel neural network\\r\\narchitecture that learns to encode a variable-length\\r\\nsequence into a fixed-length vector representation\\r\\nand to decode a given fixed-length vector rep\\x02resentation back into a variable-length sequence.\\r\\nFrom a probabilistic perspective, this new model\\r\\nis a general method to learn the conditional dis\\x02tribution over a variable-length sequence condi\\x02tioned on yet another variable-length sequence,\\r\\ne.g. p(y1, . . . , yT0 | x1, . . . , xT ), where one\\r\\nx1 x2 xT\\r\\nyT' y2 y1\\r\\nc\\r\\nDecoder\\r\\nEncoder\\r\\nFigure 1: An illustration of the proposed RNN\\r\\nEncoder–Decoder.\\r\\nshould note that the input and output sequence\\r\\nlengths T and T\\r\\n0 may differ.\\r\\nThe encoder is an RNN that reads each symbol\\r\\nof an input sequence x sequentially. As it reads\\r\\neach symbol, the hidden state of the RNN changes\\r\\naccording to Eq. (1). After reading the end of\\r\\nthe sequence (marked by an end-of-sequence sym\\x02bol), the hidden state of the RNN is a summary c\\r\\nof the whole input sequence.\\r\\nThe decoder of the proposed model is another\\r\\nRNN which is trained to generate the output se\\x02quence by predicting the next symbol yt given the\\r\\nhidden state hhti. However, unlike the RNN de\\x02scribed in Sec. 2.1, both yt and hhti are also con\\x02ditioned on yt−1 and on the summary c of the input\\r\\nsequence. Hence, the hidden state of the decoder\\r\\nat time t is computed by,\\r\\nhhti = f\\r\\n\\r\\nhht−1i, yt−1, c\\r\\n\\x01\\r\\n,\\r\\nand similarly, the conditional distribution of the\\r\\nnext symbol is\\r\\nP(yt|yt−1, yt−2, . . . , y1, c) = g\\r\\n\\r\\nhhti, yt−1, c\\r\\n\\x01\\r\\n.\\r\\nfor given activation functions f and g (the latter\\r\\nmust produce valid probabilities, e.g. with a soft\\x02max).\\r\\nSee Fig. 1 for a graphical depiction of the pro\\x02posed model architecture.\\r\\nThe two components of the proposed RNN\\r\\nEncoder–Decoder are jointly trained to maximize\\r\\nthe conditional log-likelihood\\r\\nmax\\r\\nθ\\r\\n1\\r\\nN\\r\\nX\\r\\nN\\r\\nn=1\\r\\nlog pθ(yn | xn), (4)\\nwhere θ is the set of the model parameters and\\r\\neach (xn, yn) is an (input sequence, output se\\x02quence) pair from the training set. In our case,\\r\\nas the output of the decoder, starting from the in\\x02put, is differentiable, we can use a gradient-based\\r\\nalgorithm to estimate the model parameters.\\r\\nOnce the RNN Encoder–Decoder is trained, the\\r\\nmodel can be used in two ways. One way is to use\\r\\nthe model to generate a target sequence given an\\r\\ninput sequence. On the other hand, the model can\\r\\nbe used to score a given pair of input and output\\r\\nsequences, where the score is simply a probability\\r\\npθ(y | x) from Eqs. (3) and (4).\\r\\n2.3 Hidden Unit that Adaptively Remembers\\r\\nand Forgets\\r\\nIn addition to a novel model architecture, we also\\r\\npropose a new type of hidden unit (f in Eq. (1))\\r\\nthat has been motivated by the LSTM unit but is\\r\\nmuch simpler to compute and implement.1 Fig. 2\\r\\nshows the graphical depiction of the proposed hid\\x02den unit.\\r\\nLet us describe how the activation of the j-th\\r\\nhidden unit is computed. First, the reset gate rj is\\r\\ncomputed by\\r\\nrj = σ\\r\\n\\x10\\r\\n[Wrx]\\r\\nj +\\r\\n\\x02\\r\\nUrhht−1i\\r\\n\\x03\\r\\nj\\r\\n\\x11\\r\\n, (5)\\r\\nwhere σ is the logistic sigmoid function, and [.]\\r\\nj\\r\\ndenotes the j-th element of a vector. x and ht−1\\r\\nare the input and the previous hidden state, respec\\x02tively. Wr and Ur are weight matrices which are\\r\\nlearned.\\r\\nSimilarly, the update gate zj is computed by\\r\\nzj = σ\\r\\n\\x10\\r\\n[Wzx]\\r\\nj +\\r\\n\\x02\\r\\nUzhht−1i\\r\\n\\x03\\r\\nj\\r\\n\\x11\\r\\n. (6)\\r\\nThe actual activation of the proposed unit hj is\\r\\nthen computed by\\r\\nh\\r\\nhti\\r\\nj = zjh\\r\\nht−1i\\r\\nj + (1 − zj )h˜\\r\\nhti\\r\\nj\\r\\n, (7)\\r\\nwhere\\r\\nh˜\\r\\nhti\\r\\nj = φ\\r\\n\\x10\\r\\n[Wx]\\r\\nj +\\r\\n\\x02\\r\\nU\\r\\n\\r\\nr \\x0c hht−1i\\r\\n\\x01\\x03\\r\\nj\\r\\n\\x11\\r\\n. (8)\\r\\nIn this formulation, when the reset gate is close\\r\\nto 0, the hidden state is forced to ignore the pre\\x02vious hidden state and reset with the current input\\r\\n1 The LSTM unit, which has shown impressive results in\\r\\nseveral applications such as speech recognition, has a mem\\x02ory cell and four gating units that adaptively control the in\\x02formation flow inside the unit, compared to only two gating\\r\\nunits in the proposed hidden unit. For details on LSTM net\\x02works, see, e.g., (Graves, 2012).\\r\\nz\\r\\nr h h\\r\\n~ x\\r\\nFigure 2: An illustration of the proposed hidden\\r\\nactivation function. The update gate z selects\\r\\nwhether the hidden state is to be updated with\\r\\na new hidden state h˜. The reset gate r decides\\r\\nwhether the previous hidden state is ignored. See\\r\\nEqs. (5)–(8) for the detailed equations of r, z, h\\r\\nand h˜.\\r\\nonly. This effectively allows the hidden state to\\r\\ndrop any information that is found to be irrelevant\\r\\nlater in the future, thus, allowing a more compact\\r\\nrepresentation.\\r\\nOn the other hand, the update gate controls how\\r\\nmuch information from the previous hidden state\\r\\nwill carry over to the current hidden state. This\\r\\nacts similarly to the memory cell in the LSTM\\r\\nnetwork and helps the RNN to remember long\\x02term information. Furthermore, this may be con\\x02sidered an adaptive variant of a leaky-integration\\r\\nunit (Bengio et al., 2013).\\r\\nAs each hidden unit has separate reset and up\\x02date gates, each hidden unit will learn to capture\\r\\ndependencies over different time scales. Those\\r\\nunits that learn to capture short-term dependencies\\r\\nwill tend to have reset gates that are frequently ac\\x02tive, but those that capture longer-term dependen\\x02cies will have update gates that are mostly active.\\r\\nIn our preliminary experiments, we found that\\r\\nit is crucial to use this new unit with gating units.\\r\\nWe were not able to get meaningful result with an\\r\\noft-used tanh unit without any gating.\\r\\n3 Statistical Machine Translation\\r\\nIn a commonly used statistical machine translation\\r\\nsystem (SMT), the goal of the system (decoder,\\r\\nspecifically) is to find a translation f given a source\\r\\nsentence e, which maximizes\\r\\np(f | e) ∝ p(e | f)p(f),\\r\\nwhere the first term at the right hand side is called\\r\\ntranslation model and the latter language model\\r\\n(see, e.g., (Koehn, 2005)). In practice, however,\\r\\nmost SMT systems model log p(f | e) as a log\\x02linear model with additional features and corre-\\nsponding weights:\\r\\nlog p(f | e) = X\\r\\nN\\r\\nn=1\\r\\nwnfn(f, e) + log Z(e), (9)\\r\\nwhere fn and wn are the n-th feature and weight,\\r\\nrespectively. Z(e) is a normalization constant that\\r\\ndoes not depend on the weights. The weights are\\r\\noften optimized to maximize the BLEU score on a\\r\\ndevelopment set.\\r\\nIn the phrase-based SMT framework\\r\\nintroduced in (Koehn et al., 2003) and\\r\\n(Marcu and Wong, 2002), the translation model\\r\\nlog p(e | f) is factorized into the translation\\r\\nprobabilities of matching phrases in the source\\r\\nand target sentences.2 These probabilities are\\r\\nonce again considered additional features in the\\r\\nlog-linear model (see Eq. (9)) and are weighted\\r\\naccordingly to maximize the BLEU score.\\r\\nSince the neural net language model was pro\\x02posed in (Bengio et al., 2003), neural networks\\r\\nhave been used widely in SMT systems. In\\r\\nmany cases, neural networks have been used to\\r\\nrescore translation hypotheses (n-best lists) (see,\\r\\ne.g., (Schwenk et al., 2006)). Recently, however,\\r\\nthere has been interest in training neural networks\\r\\nto score the translated sentence (or phrase pairs)\\r\\nusing a representation of the source sentence as\\r\\nan additional input. See, e.g., (Schwenk, 2012),\\r\\n(Son et al., 2012) and (Zou et al., 2013).\\r\\n3.1 Scoring Phrase Pairs with RNN\\r\\nEncoder–Decoder\\r\\nHere we propose to train the RNN Encoder–\\r\\nDecoder (see Sec. 2.2) on a table of phrase pairs\\r\\nand use its scores as additional features in the log\\x02linear model in Eq. (9) when tuning the SMT de\\x02coder.\\r\\nWhen we train the RNN Encoder–Decoder, we\\r\\nignore the (normalized) frequencies of each phrase\\r\\npair in the original corpora. This measure was\\r\\ntaken in order (1) to reduce the computational ex\\x02pense of randomly selecting phrase pairs from a\\r\\nlarge phrase table according to the normalized fre\\x02quencies and (2) to ensure that the RNN Encoder–\\r\\nDecoder does not simply learn to rank the phrase\\r\\npairs according to their numbers of occurrences.\\r\\nOne underlying reason for this choice was that the\\r\\nexisting translation probability in the phrase ta\\x02ble already reflects the frequencies of the phrase\\r\\n2 Without loss of generality, from here on, we refer to\\r\\np(e | f) for each phrase pair as a translation model as well\\r\\npairs in the original corpus. With a fixed capacity\\r\\nof the RNN Encoder–Decoder, we try to ensure\\r\\nthat most of the capacity of the model is focused\\r\\ntoward learning linguistic regularities, i.e., distin\\x02guishing between plausible and implausible trans\\x02lations, or learning the “manifold” (region of prob\\x02ability concentration) of plausible translations.\\r\\nOnce the RNN Encoder–Decoder is trained, we\\r\\nadd a new score for each phrase pair to the exist\\x02ing phrase table. This allows the new scores to en\\x02ter into the existing tuning algorithm with minimal\\r\\nadditional overhead in computation.\\r\\nAs Schwenk pointed out in (Schwenk, 2012),\\r\\nit is possible to completely replace the existing\\r\\nphrase table with the proposed RNN Encoder–\\r\\nDecoder. In that case, for a given source phrase,\\r\\nthe RNN Encoder–Decoder will need to generate\\r\\na list of (good) target phrases. This requires, how\\x02ever, an expensive sampling procedure to be per\\x02formed repeatedly. In this paper, thus, we only\\r\\nconsider rescoring the phrase pairs in the phrase\\r\\ntable.\\r\\n3.2 Related Approaches: Neural Networks in\\r\\nMachine Translation\\r\\nBefore presenting the empirical results, we discuss\\r\\na number of recent works that have proposed to\\r\\nuse neural networks in the context of SMT.\\r\\nSchwenk in (Schwenk, 2012) proposed a simi\\x02lar approach of scoring phrase pairs. Instead of the\\r\\nRNN-based neural network, he used a feedforward\\r\\nneural network that has fixed-size inputs (7 words\\r\\nin his case, with zero-padding for shorter phrases)\\r\\nand fixed-size outputs (7 words in the target lan\\x02guage). When it is used specifically for scoring\\r\\nphrases for the SMT system, the maximum phrase\\r\\nlength is often chosen to be small. However, as the\\r\\nlength of phrases increases or as we apply neural\\r\\nnetworks to other variable-length sequence data,\\r\\nit is important that the neural network can han\\x02dle variable-length input and output. The pro\\x02posed RNN Encoder–Decoder is well-suited for\\r\\nthese applications.\\r\\nSimilar to (Schwenk, 2012), Devlin et al.\\r\\n(Devlin et al., 2014) proposed to use a feedfor\\x02ward neural network to model a translation model,\\r\\nhowever, by predicting one word in a target phrase\\r\\nat a time. They reported an impressive improve\\x02ment, but their approach still requires the maxi\\x02mum length of the input phrase (or context words)\\r\\nto be fixed a priori.\\nAlthough it is not exactly a neural network they\\r\\ntrain, the authors of (Zou et al., 2013) proposed\\r\\nto learn a bilingual embedding of words/phrases.\\r\\nThey use the learned embedding to compute the\\r\\ndistance between a pair of phrases which is used\\r\\nas an additional score of the phrase pair in an SMT\\r\\nsystem.\\r\\nIn (Chandar et al., 2014), a feedforward neural\\r\\nnetwork was trained to learn a mapping from a\\r\\nbag-of-words representation of an input phrase to\\r\\nan output phrase. This is closely related to both the\\r\\nproposed RNN Encoder–Decoder and the model\\r\\nproposed in (Schwenk, 2012), except that their in\\x02put representation of a phrase is a bag-of-words.\\r\\nA similar approach of using bag-of-words repre\\x02sentations was proposed in (Gao et al., 2013) as\\r\\nwell. Earlier, a similar encoder–decoder model us\\x02ing two recursive neural networks was proposed\\r\\nin (Socher et al., 2011), but their model was re\\x02stricted to a monolingual setting, i.e. the model\\r\\nreconstructs an input sentence. More recently, an\\x02other encoder–decoder model using an RNN was\\r\\nproposed in (Auli et al., 2013), where the de\\x02coder is conditioned on a representation of either\\r\\na source sentence or a source context.\\r\\nOne important difference between the pro\\x02posed RNN Encoder–Decoder and the approaches\\r\\nin (Zou et al., 2013) and (Chandar et al., 2014) is\\r\\nthat the order of the words in source and tar\\x02get phrases is taken into account. The RNN\\r\\nEncoder–Decoder naturally distinguishes between\\r\\nsequences that have the same words but in a differ\\x02ent order, whereas the aforementioned approaches\\r\\neffectively ignore order information.\\r\\nThe closest approach related to the proposed\\r\\nRNN Encoder–Decoder is the Recurrent Contin\\x02uous Translation Model (Model 2) proposed in\\r\\n(Kalchbrenner and Blunsom, 2013). In their pa\\x02per, they proposed a similar model that consists\\r\\nof an encoder and decoder. The difference with\\r\\nour model is that they used a convolutional n-gram\\r\\nmodel (CGM) for the encoder and the hybrid of\\r\\nan inverse CGM and a recurrent neural network\\r\\nfor the decoder. They, however, evaluated their\\r\\nmodel on rescoring the n-best list proposed by the\\r\\nconventional SMT system and computing the per\\x02plexity of the gold standard translations.\\r\\n4 Experiments\\r\\nWe evaluate our approach on the English/French\\r\\ntranslation task of the WMT’14 workshop.\\r\\n4.1 Data and Baseline System\\r\\nLarge amounts of resources are available to build\\r\\nan English/French SMT system in the framework\\r\\nof the WMT’14 translation task. The bilingual\\r\\ncorpora include Europarl (61M words), news com\\x02mentary (5.5M), UN (421M), and two crawled\\r\\ncorpora of 90M and 780M words respectively.\\r\\nThe last two corpora are quite noisy. To train\\r\\nthe French language model, about 712M words of\\r\\ncrawled newspaper material is available in addi\\x02tion to the target side of the bitexts. All the word\\r\\ncounts refer to French words after tokenization.\\r\\nIt is commonly acknowledged that training sta\\x02tistical models on the concatenation of all this\\r\\ndata does not necessarily lead to optimal per\\x02formance, and results in extremely large mod\\x02els which are difficult to handle. Instead, one\\r\\nshould focus on the most relevant subset of the\\r\\ndata for a given task. We have done so by\\r\\napplying the data selection method proposed in\\r\\n(Moore and Lewis, 2010), and its extension to bi\\x02texts (Axelrod et al., 2011). By these means we\\r\\nselected a subset of 418M words out of more\\r\\nthan 2G words for language modeling and a\\r\\nsubset of 348M out of 850M words for train\\x02ing the RNN Encoder–Decoder. We used the\\r\\ntest set newstest2012 and 2013 for data\\r\\nselection and weight tuning with MERT, and\\r\\nnewstest2014 as our test set. Each set has\\r\\nmore than 70 thousand words and a single refer\\x02ence translation.\\r\\nFor training the neural networks, including the\\r\\nproposed RNN Encoder–Decoder, we limited the\\r\\nsource and target vocabulary to the most frequent\\r\\n15,000 words for both English and French. This\\r\\ncovers approximately 93% of the dataset. All the\\r\\nout-of-vocabulary words were mapped to a special\\r\\ntoken ([UNK]).\\r\\nThe baseline phrase-based SMT system was\\r\\nbuilt using Moses with default settings. This sys\\x02tem achieves a BLEU score of 30.64 and 33.3 on\\r\\nthe development and test sets, respectively (see Ta\\x02ble 1).\\r\\n4.1.1 RNN Encoder–Decoder\\r\\nThe RNN Encoder–Decoder used in the experi\\x02ment had 1000 hidden units with the proposed\\r\\ngates at the encoder and at the decoder. The in\\x02put matrix between each input symbol xhti and the\\r\\nhidden unit is approximated with two lower-rank\\r\\nmatrices, and the output matrix is approximated\\nModels BLEU\\r\\ndev test\\r\\nBaseline 30.64 33.30\\r\\nRNN 31.20 33.87\\r\\nCSLM + RNN 31.48 34.64\\r\\nCSLM + RNN + WP 31.50 34.54\\r\\nTable 1: BLEU scores computed on the develop\\x02ment and test sets using different combinations of\\r\\napproaches. WP denotes a word penalty, where\\r\\nwe penalizes the number of unknown words to\\r\\nneural networks.\\r\\nsimilarly. We used rank-100 matrices, equivalent\\r\\nto learning an embedding of dimension 100 for\\r\\neach word. The activation function used for h˜ in\\r\\nEq. (8) is a hyperbolic tangent function. The com\\x02putation from the hidden state in the decoder to\\r\\nthe output is implemented as a deep neural net\\x02work (Pascanu et al., 2014) with a single interme\\x02diate layer having 500 maxout units each pooling\\r\\n2 inputs (Goodfellow et al., 2013).\\r\\nAll the weight parameters in the RNN Encoder–\\r\\nDecoder were initialized by sampling from an\\r\\nisotropic zero-mean (white) Gaussian distribution\\r\\nwith its standard deviation fixed to 0.01, except\\r\\nfor the recurrent weight parameters. For the re\\x02current weight matrices, we first sampled from a\\r\\nwhite Gaussian distribution and used its left singu\\x02lar vectors matrix, following (Saxe et al., 2014).\\r\\nWe used Adadelta and stochastic gradient\\r\\ndescent to train the RNN Encoder–Decoder\\r\\nwith hyperparameters \\x0f = 10−6and ρ =\\r\\n0.95 (Zeiler, 2012). At each update, we used 64\\r\\nrandomly selected phrase pairs from a phrase ta\\x02ble (which was created from 348M words). The\\r\\nmodel was trained for approximately three days.\\r\\nDetails of the architecture used in the experi\\x02ments are explained in more depth in the supple\\x02mentary material.\\r\\n4.1.2 Neural Language Model\\r\\nIn order to assess the effectiveness of scoring\\r\\nphrase pairs with the proposed RNN Encoder–\\r\\nDecoder, we also tried a more traditional approach\\r\\nof using a neural network for learning a target\\r\\nlanguage model (CSLM) (Schwenk, 2007). Espe\\x02cially, the comparison between the SMT system\\r\\nusing CSLM and that using the proposed approach\\r\\nof phrase scoring by RNN Encoder–Decoder will\\r\\nclarify whether the contributions from multiple\\r\\nneural networks in different parts of the SMT sys\\x02tem add up or are redundant.\\r\\nWe trained the CSLM model on 7-grams\\r\\nfrom the target corpus. Each input word\\r\\nwas projected into the embedding space R\\r\\n512\\r\\n,\\r\\nand they were concatenated to form a 3072-\\r\\ndimensional vector. The concatenated vector was\\r\\nfed through two rectified layers (of size 1536 and\\r\\n1024) (Glorot et al., 2011). The output layer was\\r\\na simple softmax layer (see Eq. (2)). All the\\r\\nweight parameters were initialized uniformly be\\x02tween −0.01 and 0.01, and the model was trained\\r\\nuntil the validation perplexity did not improve for\\r\\n10 epochs. After training, the language model\\r\\nachieved a perplexity of 45.80. The validation set\\r\\nwas a random selection of 0.1% of the corpus. The\\r\\nmodel was used to score partial translations dur\\x02ing the decoding process, which generally leads to\\r\\nhigher gains in BLEU score than n-best list rescor\\x02ing (Vaswani et al., 2013).\\r\\nTo address the computational complexity of\\r\\nusing a CSLM in the decoder a buffer was\\r\\nused to aggregate n-grams during the stack\\x02search performed by the decoder. Only when\\r\\nthe buffer is full, or a stack is about to\\r\\nbe pruned, the n-grams are scored by the\\r\\nCSLM. This allows us to perform fast matrix\\x02matrix multiplication on GPU using Theano\\r\\n(Bergstra et al., 2010; Bastien et al., 2012).\\r\\n−60 −50 −40 −30 −20 −10 0\\r\\n−14\\r\\n−12\\r\\n−10\\r\\n−8\\r\\n−6\\r\\n−4\\r\\n−2\\r\\n0\\r\\nRNN Scores (log)\\r\\nTM Scores (log)\\r\\nFigure 3: The visualization of phrase pairs accord\\x02ing to their scores (log-probabilities) by the RNN\\r\\nEncoder–Decoder and the translation model.\\r\\n4.2 Quantitative Analysis\\r\\nWe tried the following combinations:\\r\\n1. Baseline configuration\\r\\n2. Baseline + RNN\\r\\n3. Baseline + CSLM + RNN\\r\\n4. Baseline + CSLM + RNN + Word penalty\\nSource Translation Model RNN Encoder–Decoder\\r\\nat the end of the [a la fin de la] [´r la fin des annees] [ ´ etre sup- ˆ\\r\\nprimes´ a la fin de la] `\\r\\n[a la fin du] [ ` a la fin des] [ ` a la fin de la] `\\r\\nfor the first time [r \\rc pour la premirere fois] [ ¨ et´ e donn ´ es pour ´\\r\\nla premiere fois] [ ` et´ e comm ´ emor ´ ee pour la ´\\r\\npremiere fois] `\\r\\n[pour la premiere fois] [pour la premi ` ere fois ,] `\\r\\n[pour la premiere fois que] `\\r\\nin the United States\\r\\nand\\r\\n[? aux ?tats-Unis et] [et´ e ouvertes aux ´ Etats- ´\\r\\nUnis et] [et´ e constat ´ ees aux ´ Etats-Unis et] ´\\r\\n[aux Etats-Unis et] [des Etats-Unis et] [des\\r\\nEtats-Unis et] ´\\r\\n, as well as [?s , qu’] [?s , ainsi que] [?re aussi bien que] [, ainsi qu’] [, ainsi que] [, ainsi que les]\\r\\none of the most [?t ?l’ un des plus] [?l’ un des plus] [etre retenue ˆ\\r\\ncomme un de ses plus]\\r\\n[l’ un des] [le] [un des]\\r\\n(a) Long, frequent source phrases\\r\\nSource Translation Model RNN Encoder–Decoder\\r\\n, Minister of Commu\\x02nications and Trans\\x02port[Secretaire aux communications et aux trans- ´\\r\\nports :] [Secretaire aux communications et aux ´\\r\\ntransports]\\r\\n[Secretaire aux communications et aux trans- ´\\r\\nports] [Secretaire aux communications et aux ´\\r\\ntransports :]\\r\\ndid not comply with\\r\\nthe\\r\\n[vestimentaire , ne correspondaient pas a des] `\\r\\n[susmentionnee n’ ´ etait pas conforme aux] ´\\r\\n[present ´ ees n’ ´ etaient pas conformes ´ a la] `\\r\\n[n’ ont pas respecte les] [n’ ´ etait pas conforme ´\\r\\naux] [n’ ont pas respecte la] ´\\r\\nparts of the world . [\\rc gions du monde .] [regions du monde con- ´\\r\\nsider´ ees .] [r ´ egion du monde consid ´ er´ ee .] ´\\r\\n[parties du monde .] [les parties du monde .]\\r\\n[des parties du monde .]\\r\\nthe past few days . [le petit texte .] [cours des tout derniers jours .]\\r\\n[les tout derniers jours .]\\r\\n[ces derniers jours .] [les derniers jours .] [cours\\r\\ndes derniers jours .]\\r\\non Friday and Satur\\x02day[vendredi et samedi a la] [vendredi et samedi ` a]`\\r\\n[se deroulera vendredi et samedi ,] ´\\r\\n[le vendredi et le samedi] [le vendredi et samedi]\\r\\n[vendredi et samedi]\\r\\n(b) Long, rare source phrases\\r\\nTable 2: The top scoring target phrases for a small set of source phrases according to the translation\\r\\nmodel (direct translation probability) and by the RNN Encoder–Decoder. Source phrases were randomly\\r\\nselected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a Cyrillic\\r\\nletter ghe.\\r\\nThe results are presented in Table 1. As ex\\x02pected, adding features computed by neural net\\x02works consistently improves the performance over\\r\\nthe baseline performance.\\r\\nThe best performance was achieved when we\\r\\nused both CSLM and the phrase scores from the\\r\\nRNN Encoder–Decoder. This suggests that the\\r\\ncontributions of the CSLM and the RNN Encoder–\\r\\nDecoder are not too correlated and that one can\\r\\nexpect better results by improving each method in\\x02dependently. Furthermore, we tried penalizing the\\r\\nnumber of words that are unknown to the neural\\r\\nnetworks (i.e. words which are not in the short\\x02list). We do so by simply adding the number of\\r\\nunknown words as an additional feature the log\\x02linear model in Eq. (9).3 However, in this case we\\r\\n3 To understand the effect of the penalty, consider the set\\r\\nof all words in the 15,000 large shortlist, SL. All words x\\r\\ni ∈/\\r\\nSL are replaced by a special token [UNK] before being scored\\r\\nby the neural networks. Hence, the conditional probability of\\r\\nany x\\r\\ni\\r\\nt ∈/ SL is actually given by the model as\\r\\np (xt = [UNK] | x<t) = p (xt ∈/ SL | x<t)\\r\\n=\\r\\nX\\r\\nx\\r\\nj\\r\\nt ∈/SL\\r\\np\\r\\n\\x10\\r\\nx\\r\\nj\\r\\nt\\r\\n| x<t\\x11≥ p\\r\\n\\x10\\r\\nx\\r\\ni\\r\\nt\\r\\n| x<t\\x11,\\r\\nwhere x<t is a shorthand notation for xt−1, . . . , x1.\\r\\nwere not able to achieve better performance on the\\r\\ntest set, but only on the development set.\\r\\n4.3 Qualitative Analysis\\r\\nIn order to understand where the performance im\\x02provement comes from, we analyze the phrase pair\\r\\nscores computed by the RNN Encoder–Decoder\\r\\nagainst the corresponding p(f | e) from the trans\\x02lation model. Since the existing translation model\\r\\nrelies solely on the statistics of the phrase pairs in\\r\\nthe corpus, we expect its scores to be better esti\\x02mated for the frequent phrases but badly estimated\\r\\nfor rare phrases. Also, as we mentioned earlier\\r\\nin Sec. 3.1, we further expect the RNN Encoder–\\r\\nDecoder which was trained without any frequency\\r\\ninformation to score the phrase pairs based rather\\r\\non the linguistic regularities than on the statistics\\r\\nof their occurrences in the corpus.\\r\\nWe focus on those pairs whose source phrase is\\r\\nlong (more than 3 words per source phrase) and\\r\\nAs a result, the probability of words not in the shortlist is\\r\\nalways overestimated. It is possible to address this issue by\\r\\nbacking off to an existing model that contain non-shortlisted\\r\\nwords (see (Schwenk, 2007)) In this paper, however, we opt\\r\\nfor introducing a word penalty instead, which counteracts the\\r\\nword probability overestimation.\\nSource Samples from RNN Encoder–Decoder\\r\\nat the end of the [a la fin de la] ( ` ×11)\\r\\nfor the first time [pour la premiere fois] ( ` ×24) [pour la premiere fois que] ( ` ×2)\\r\\nin the United States and [aux Etats-Unis et] ( ´ ×6) [dans les Etats-Unis et] ( ´ ×4)\\r\\n, as well as [, ainsi que] [,] [ainsi que] [, ainsi qu’] [et UNK]\\r\\none of the most [l’ un des plus] (×9) [l’ un des] (×5) [l’ une des plus] (×2)\\r\\n(a) Long, frequent source phrases\\r\\nSource Samples from RNN Encoder–Decoder\\r\\n, Minister of Communica\\x02tions and Transport[ , ministre des communications et le transport] (×13)\\r\\ndid not comply with the [n’ tait pas conforme aux] [n’ a pas respect l’] (×2) [n’ a pas respect la] (×3)\\r\\nparts of the world . [arts du monde .] (×11) [des arts du monde .] (×7)\\r\\nthe past few days . [quelques jours .] (×5) [les derniers jours .] (×5) [ces derniers jours .] (×2)\\r\\non Friday and Saturday [vendredi et samedi] (×5) [le vendredi et samedi] (×7) [le vendredi et le samedi] (×4)\\r\\n(b) Long, rare source phrases\\r\\nTable 3: Samples generated from the RNN Encoder–Decoder for each source phrase used in Table 2. We\\r\\nshow the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder–Decoder scores.\\r\\nFigure 4: 2–D embedding of the learned word representation. The left one shows the full embedding\\r\\nspace, while the right one shows a zoomed-in view of one region (color–coded). For more plots, see the\\r\\nsupplementary material.\\r\\nfrequent. For each such source phrase, we look\\r\\nat the target phrases that have been scored high\\r\\neither by the translation probability p(f | e) or\\r\\nby the RNN Encoder–Decoder. Similarly, we per\\x02form the same procedure with those pairs whose\\r\\nsource phrase is long but rare in the corpus.\\r\\nTable 2 lists the top-3 target phrases per source\\r\\nphrase favored either by the translation model\\r\\nor by the RNN Encoder–Decoder. The source\\r\\nphrases were randomly chosen among long ones\\r\\nhaving more than 4 or 5 words.\\r\\nIn most cases, the choices of the target phrases\\r\\nby the RNN Encoder–Decoder are closer to ac\\x02tual or literal translations. We can observe that the\\r\\nRNN Encoder–Decoder prefers shorter phrases in\\r\\ngeneral.\\r\\nInterestingly, many phrase pairs were scored\\r\\nsimilarly by both the translation model and the\\r\\nRNN Encoder–Decoder, but there were as many\\r\\nother phrase pairs that were scored radically dif\\x02ferent (see Fig. 3). This could arise from the\\r\\nproposed approach of training the RNN Encoder–\\r\\nDecoder on a set of unique phrase pairs, discour\\x02aging the RNN Encoder–Decoder from learning\\r\\nsimply the frequencies of the phrase pairs from the\\r\\ncorpus, as explained earlier.\\r\\nFurthermore, in Table 3, we show for each of\\r\\nthe source phrases in Table 2, the generated sam\\x02ples from the RNN Encoder–Decoder. For each\\r\\nsource phrase, we generated 50 samples and show\\r\\nthe top-five phrases accordingly to their scores.\\r\\nWe can see that the RNN Encoder–Decoder is\\r\\nable to propose well-formed target phrases with\\x02out looking at the actual phrase table. Importantly,\\r\\nthe generated phrases do not overlap completely\\r\\nwith the target phrases from the phrase table. This\\r\\nencourages us to further investigate the possibility\\r\\nof replacing the whole or a part of the phrase table\\nFigure 5: 2–D embedding of the learned phrase representation. The top left one shows the full represen\\x02tation space (5000 randomly selected points), while the other three figures show the zoomed-in view of\\r\\nspecific regions (color–coded).\\r\\nwith the proposed RNN Encoder–Decoder in the\\r\\nfuture.\\r\\n4.4 Word and Phrase Representations\\r\\nSince the proposed RNN Encoder–Decoder is not\\r\\nspecifically designed only for the task of machine\\r\\ntranslation, here we briefly look at the properties\\r\\nof the trained model.\\r\\nIt has been known for some time that\\r\\ncontinuous space language models using\\r\\nneural networks are able to learn seman\\x02tically meaningful embeddings (See, e.g.,\\r\\n(Bengio et al., 2003; Mikolov et al., 2013)). Since\\r\\nthe proposed RNN Encoder–Decoder also projects\\r\\nto and maps back from a sequence of words into\\r\\na continuous space vector, we expect to see a\\r\\nsimilar property with the proposed model as well.\\r\\nThe left plot in Fig. 4 shows the 2–D embedding\\r\\nof the words using the word embedding matrix\\r\\nlearned by the RNN Encoder–Decoder. The pro\\x02jection was done by the recently proposed Barnes\\x02Hut-SNE (van der Maaten, 2013). We can clearly\\r\\nsee that semantically similar words are clustered\\r\\nwith each other (see the zoomed-in plots in Fig. 4).\\r\\nThe proposed RNN Encoder–Decoder naturally\\r\\ngenerates a continuous-space representation of a\\r\\nphrase. The representation (c in Fig. 1) in this\\r\\ncase is a 1000-dimensional vector. Similarly to the\\r\\nword representations, we visualize the representa\\x02tions of the phrases that consists of four or more\\r\\nwords using the Barnes-Hut-SNE in Fig. 5.\\r\\nFrom the visualization, it is clear that the RNN\\r\\nEncoder–Decoder captures both semantic and syn\\x02tactic structures of the phrases. For instance, in\\r\\nthe bottom-left plot, most of the phrases are about\\r\\nthe duration of time, while those phrases that are\\r\\nsyntactically similar are clustered together. The\\r\\nbottom-right plot shows the cluster of phrases that\\r\\nare semantically similar (countries or regions). On\\r\\nthe other hand, the top-right plot shows the phrases\\r\\nthat are syntactically similar.\\r\\n5 Conclusion\\r\\nIn this paper, we proposed a new neural network\\r\\narchitecture, called an RNN Encoder–Decoder\\r\\nthat is able to learn the mapping from a sequence\\nof an arbitrary length to another sequence, possi\\x02bly from a different set, of an arbitrary length. The\\r\\nproposed RNN Encoder–Decoder is able to either\\r\\nscore a pair of sequences (in terms of a conditional\\r\\nprobability) or generate a target sequence given a\\r\\nsource sequence. Along with the new architecture,\\r\\nwe proposed a novel hidden unit that includes a re\\x02set gate and an update gate that adaptively control\\r\\nhow much each hidden unit remembers or forgets\\r\\nwhile reading/generating a sequence.\\r\\nWe evaluated the proposed model with the task\\r\\nof statistical machine translation, where we used\\r\\nthe RNN Encoder–Decoder to score each phrase\\r\\npair in the phrase table. Qualitatively, we were\\r\\nable to show that the new model is able to cap\\x02ture linguistic regularities in the phrase pairs well\\r\\nand also that the RNN Encoder–Decoder is able to\\r\\npropose well-formed target phrases.\\r\\nThe scores by the RNN Encoder–Decoder were\\r\\nfound to improve the overall translation perfor\\x02mance in terms of BLEU scores. Also, we\\r\\nfound that the contribution by the RNN Encoder–\\r\\nDecoder is rather orthogonal to the existing ap\\x02proach of using neural networks in the SMT sys\\x02tem, so that we can improve further the perfor\\x02mance by using, for instance, the RNN Encoder–\\r\\nDecoder and the neural net language model to\\x02gether.\\r\\nOur qualitative analysis of the trained model\\r\\nshows that it indeed captures the linguistic regu\\x02larities in multiple levels i.e. at the word level as\\r\\nwell as phrase level. This suggests that there may\\r\\nbe more natural language related applications that\\r\\nmay benefit from the proposed RNN Encoder–\\r\\nDecoder.\\r\\nThe proposed architecture has large potential\\r\\nfor further improvement and analysis. One ap\\x02proach that was not investigated here is to re\\x02place the whole, or a part of the phrase table by\\r\\nletting the RNN Encoder–Decoder propose target\\r\\nphrases. Also, noting that the proposed model is\\r\\nnot limited to being used with written language,\\r\\nit will be an important future research to apply the\\r\\nproposed architecture to other applications such as\\r\\nspeech transcription.\\r\\nAcknowledgments\\r\\nKC, BM, CG, DB and YB would like to thank\\r\\nNSERC, Calcul Quebec, Compute Canada, the ´\\r\\nCanada Research Chairs and CIFAR. FB and HS\\r\\nwere partially funded by the European Commis\\x02sion under the project MateCat, and by DARPA\\r\\nunder the BOLT project.\\r\\nReferences\\r\\n[Auli et al.2013] Michael Auli, Michel Galley, Chris\\r\\nQuirk, and Geoffrey Zweig. 2013. Joint language\\r\\nand translation modeling with recurrent neural net\\x02works. In Proceedings of the ACL Conference on\\r\\nEmpirical Methods in Natural Language Processing\\r\\n(EMNLP), pages 1044–1054.\\r\\n[Axelrod et al.2011] Amittai Axelrod, Xiaodong He,\\r\\nand Jianfeng Gao. 2011. Domain adaptation via\\r\\npseudo in-domain data selection. In Proceedings of\\r\\nthe ACL Conference on Empirical Methods in Natu\\x02ral Language Processing (EMNLP), pages 355–362.\\r\\n[Bastien et al.2012] Fred´ eric Bastien, Pascal Lamblin, ´\\r\\nRazvan Pascanu, James Bergstra, Ian J. Goodfellow,\\r\\nArnaud Bergeron, Nicolas Bouchard, and Yoshua\\r\\nBengio. 2012. Theano: new features and speed im\\x02provements. Deep Learning and Unsupervised Fea\\x02ture Learning NIPS 2012 Workshop.\\r\\n[Bengio et al.2003] Yoshua Bengio, Rejean Ducharme, ´\\r\\nPascal Vincent, and Christian Janvin. 2003. A neu\\x02ral probabilistic language model. J. Mach. Learn.\\r\\nRes., 3:1137–1155, March.\\r\\n[Bengio et al.2013] Y. Bengio, N. Boulanger\\x02Lewandowski, and R. Pascanu. 2013. Advances\\r\\nin optimizing recurrent networks. In Proceedings\\r\\nof the 38th International Conference on Acoustics,\\r\\nSpeech, and Signal Processing (ICASSP 2013),\\r\\nMay.\\r\\n[Bergstra et al.2010] James Bergstra, Olivier Breuleux,\\r\\nFred´ eric Bastien, Pascal Lamblin, Razvan Pascanu, ´\\r\\nGuillaume Desjardins, Joseph Turian, David Warde\\x02Farley, and Yoshua Bengio. 2010. Theano: a CPU\\r\\nand GPU math expression compiler. In Proceedings\\r\\nof the Python for Scientific Computing Conference\\r\\n(SciPy), June. Oral Presentation.\\r\\n[Chandar et al.2014] Sarath Chandar, Stanislas Lauly,\\r\\nHugo Larochelle, Mitesh Khapra, Balaraman Ravin\\x02dran, Vikas Raykar, and Amrita Saha. 2014. An au\\x02toencoder approach to learning bilingual word repre\\x02sentations. arXiv:1402.1454 [cs.CL], Febru\\x02ary.\\r\\n[Dahl et al.2012] George E. Dahl, Dong Yu, Li Deng,\\r\\nand Alex Acero. 2012. Context-dependent pre\\x02trained deep neural networks for large vocabulary\\r\\nspeech recognition. IEEE Transactions on Audio,\\r\\nSpeech, and Language Processing, 20(1):33–42.\\r\\n[Devlin et al.2014] Jacob Devlin, Rabih Zbib,\\r\\nZhongqiang Huang, Thomas Lamar, Richard\\r\\nSchwartz, , and John Makhoul. 2014. Fast and\\r\\nrobust neural network joint models for statistical\\r\\nmachine translation. In Proceedings of the ACL\\r\\n2014 Conference, ACL ’14, pages 1370–1380.\\n[Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau\\r\\nYih, and Li Deng. 2013. Learning semantic repre\\x02sentations for the phrase translation model. Techni\\x02cal report, Microsoft Research.\\r\\n[Glorot et al.2011] X. Glorot, A. Bordes, and Y. Ben\\x02gio. 2011. Deep sparse rectifier neural networks. In\\r\\nAISTATS’2011.\\r\\n[Goodfellow et al.2013] Ian J. Goodfellow, David\\r\\nWarde-Farley, Mehdi Mirza, Aaron Courville, and\\r\\nYoshua Bengio. 2013. Maxout networks. In\\r\\nICML’2013.\\r\\n[Graves2012] Alex Graves. 2012. Supervised Se\\x02quence Labelling with Recurrent Neural Networks.\\r\\nStudies in Computational Intelligence. Springer.\\r\\n[Hochreiter and Schmidhuber1997] S. Hochreiter and\\r\\nJ. Schmidhuber. 1997. Long short-term memory.\\r\\nNeural Computation, 9(8):1735–1780.\\r\\n[Kalchbrenner and Blunsom2013] Nal Kalchbrenner\\r\\nand Phil Blunsom. 2013. Two recurrent continuous\\r\\ntranslation models. In Proceedings of the ACL Con\\x02ference on Empirical Methods in Natural Language\\r\\nProcessing (EMNLP), pages 1700–1709.\\r\\n[Koehn et al.2003] Philipp Koehn, Franz Josef Och,\\r\\nand Daniel Marcu. 2003. Statistical phrase-based\\r\\ntranslation. In Proceedings of the 2003 Conference\\r\\nof the North American Chapter of the Association\\r\\nfor Computational Linguistics on Human Language\\r\\nTechnology - Volume 1, NAACL ’03, pages 48–54.\\r\\n[Koehn2005] P. Koehn. 2005. Europarl: A parallel cor\\x02pus for statistical machine translation. In Machine\\r\\nTranslation Summit X, pages 79–86, Phuket, Thai\\x02land.\\r\\n[Krizhevsky et al.2012] Alex Krizhevsky, Ilya\\r\\nSutskever, and Geoffrey Hinton. 2012. Ima\\x02geNet classification with deep convolutional neural\\r\\nnetworks. In Advances in Neural Information\\r\\nProcessing Systems 25 (NIPS’2012).\\r\\n[Marcu and Wong2002] Daniel Marcu and William\\r\\nWong. 2002. A phrase-based, joint probability\\r\\nmodel for statistical machine translation. In Pro\\x02ceedings of the ACL-02 Conference on Empirical\\r\\nMethods in Natural Language Processing - Volume\\r\\n10, EMNLP ’02, pages 133–139.\\r\\n[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,\\r\\nKai Chen, Greg Corrado, and Jeff Dean. 2013. Dis\\x02tributed representations of words and phrases and\\r\\ntheir compositionality. In Advances in Neural Infor\\x02mation Processing Systems 26, pages 3111–3119.\\r\\n[Moore and Lewis2010] Robert C. Moore and William\\r\\nLewis. 2010. Intelligent selection of language\\r\\nmodel training data. In Proceedings of the ACL\\r\\n2010 Conference Short Papers, ACLShort ’10,\\r\\npages 220–224, Stroudsburg, PA, USA.\\r\\n[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho,\\r\\nand Y. Bengio. 2014. How to construct deep recur\\x02rent neural networks. In Proceedings of the Second\\r\\nInternational Conference on Learning Representa\\x02tions (ICLR 2014), April.\\r\\n[Saxe et al.2014] Andrew M. Saxe, James L. McClel\\x02land, and Surya Ganguli. 2014. Exact solutions\\r\\nto the nonlinear dynamics of learning in deep lin\\x02ear neural networks. In Proceedings of the Second\\r\\nInternational Conference on Learning Representa\\x02tions (ICLR 2014), April.\\r\\n[Schwenk et al.2006] Holger Schwenk, Marta R. Costa\\x02Jussa, and Jos ` e A. R. Fonollosa. 2006. Continuous ´\\r\\nspace language models for the iwslt 2006 task. In\\r\\nIWSLT, pages 166–173.\\r\\n[Schwenk2007] Holger Schwenk. 2007. Continuous\\r\\nspace language models. Comput. Speech Lang.,\\r\\n21(3):492–518, July.\\r\\n[Schwenk2012] Holger Schwenk. 2012. Continuous\\r\\nspace translation models for phrase-based statisti\\x02cal machine translation. In Martin Kay and Chris\\x02tian Boitet, editors, Proceedings of the 24th Inter\\x02national Conference on Computational Linguistics\\r\\n(COLIN), pages 1071–1080.\\r\\n[Socher et al.2011] Richard Socher, Eric H. Huang, Jef\\x02frey Pennington, Andrew Y. Ng, and Christopher D.\\r\\nManning. 2011. Dynamic pooling and unfolding\\r\\nrecursive autoencoders for paraphrase detection. In\\r\\nAdvances in Neural Information Processing Systems\\r\\n24.\\r\\n[Son et al.2012] Le Hai Son, Alexandre Allauzen, and\\r\\nFranc¸ois Yvon. 2012. Continuous space transla\\x02tion models with neural networks. In Proceedings of\\r\\nthe 2012 Conference of the North American Chap\\x02ter of the Association for Computational Linguistics:\\r\\nHuman Language Technologies, NAACL HLT ’12,\\r\\npages 39–48, Stroudsburg, PA, USA.\\r\\n[van der Maaten2013] Laurens van der Maaten. 2013.\\r\\nBarnes-hut-sne. In Proceedings of the First Inter\\x02national Conference on Learning Representations\\r\\n(ICLR 2013), May.\\r\\n[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,\\r\\nVictoria Fossum, and David Chiang. 2013. De\\x02coding with large-scale neural language models im\\x02proves translation. Proceedings of the Conference\\r\\non Empirical Methods in Natural Language Pro\\x02cessing, pages 1387–1392.\\r\\n[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:\\r\\nan adaptive learning rate method. Technical report,\\r\\narXiv 1212.5701.\\r\\n[Zou et al.2013] Will Y. Zou, Richard Socher,\\r\\nDaniel M. Cer, and Christopher D. Manning.\\r\\n2013. Bilingual word embeddings for phrase-based\\r\\nmachine translation. In Proceedings of the ACL\\r\\nConference on Empirical Methods in Natural\\r\\nLanguage Processing (EMNLP), pages 1393–1398.\\nA RNN Encoder–Decoder\\r\\nIn this document, we describe in detail the architecture of the RNN Encoder–Decoder used in the exper\\x02iments.\\r\\nLet us denote an source phrase by X = (x1, x2, . . . , xN ) and a target phrase by Y =\\r\\n(y1, y2, . . . , yM). Each phrase is a sequence of K-dimensional one-hot vectors, such that only one\\r\\nelement of the vector is 1 and all the others are 0. The index of the active (1) element indicates the word\\r\\nrepresented by the vector.\\r\\nA.1 Encoder\\r\\nEach word of the source phrase is embedded in a 500-dimensional vector space: e(xi) ∈ R\\r\\n500\\r\\n. e(x) is\\r\\nused in Sec. 4.4 to visualize the words.\\r\\nThe hidden state of an encoder consists of 1000 hidden units, and each one of them at time t is\\r\\ncomputed by\\r\\nh\\r\\nhti\\r\\nj = zjh\\r\\nht−1i\\r\\nj + (1 − zj )h˜\\r\\nhti\\r\\nj\\r\\n,\\r\\nwhere\\r\\nh˜\\r\\nhti\\r\\nj = tanh \\x10\\r\\n[We(xt)]j +\\r\\n\\x02\\r\\nU\\r\\n\\r\\nr \\x0c hht−1i\\r\\n\\x01\\x03\\r\\nj\\r\\n\\x11\\r\\n,\\r\\nzj =σ\\r\\n\\x10\\r\\n[Wze(xt)]j +\\r\\n\\x02\\r\\nUzhht−1i\\r\\n\\x03\\r\\nj\\r\\n\\x11\\r\\n,\\r\\nrj =σ\\r\\n\\x10\\r\\n[Wre(xt)]j +\\r\\n\\x02\\r\\nUrhht−1i\\r\\n\\x03\\r\\nj\\r\\n\\x11\\r\\n.\\r\\nσ and \\x0c are a logistic sigmoid function and an element-wise multiplication, respectively. To make the\\r\\nequations uncluttered, we omit biases. The initial hidden state h\\r\\nh0i\\r\\nj\\r\\nis fixed to 0.\\r\\nOnce the hidden state at the N step (the end of the source phrase) is computed, the representation of\\r\\nthe source phrase c is\\r\\nc = tanh \\x10VhhNi\\r\\n\\x11\\r\\n.\\r\\nA.1.1 Decoder\\r\\nThe decoder starts by initializing the hidden state with\\r\\nh\\r\\n0h0i = tanh \\r\\nV0c\\r\\n\\x01\\r\\n,\\r\\nwhere we will use ·\\r\\n0\\r\\nto distinguish parameters of the decoder from those of the encoder.\\r\\nThe hidden state at time t of the decoder is computed by\\r\\nh\\r\\n0hti\\r\\nj = z\\r\\n0\\r\\njh\\r\\n0ht−1i\\r\\nj + (1 − z\\r\\n0\\r\\nj )h˜0\\r\\nhti\\r\\nj\\r\\n,\\r\\nwhere\\r\\nh˜0\\r\\nhti\\r\\nj = tanh \\x10\\x02W0\\r\\ne(yt−1)\\r\\n\\x03\\r\\nj\\r\\n+ r\\r\\n0\\r\\nj\\r\\n\\x02\\r\\nU0h\\r\\n0\\r\\nht−1i + Cc\\x03\\r\\n\\x11\\r\\n,\\r\\nz\\r\\n0\\r\\nj =σ\\r\\n\\x10\\x02W0\\r\\nze(yt−1)\\r\\n\\x03\\r\\nj\\r\\n+\\r\\n\\x02\\r\\nU0\\r\\nzh\\r\\n0\\r\\nht−1i\\r\\n\\x03\\r\\nj\\r\\n+ [Czc]\\r\\nj\\r\\n\\x11\\r\\n,\\r\\nr\\r\\n0\\r\\nj =σ\\r\\n\\x10\\x02W0\\r\\nre(yt−1)\\r\\n\\x03\\r\\nj\\r\\n+\\r\\n\\x02\\r\\nU0\\r\\nrh\\r\\n0\\r\\nht−1i\\r\\n\\x03\\r\\nj\\r\\n+ [Crc]\\r\\nj\\r\\n\\x11\\r\\n,\\r\\nand e(y0) is an all-zero vector. Similarly to the case of the encoder, e(y) is an embedding of a target\\r\\nword.\\r\\nUnlike the encoder which simply encodes the source phrase, the decoder is learned to generate a target\\r\\nphrase. At each time t, the decoder computes the probability of generating j-th word by\\r\\np(yt,j = 1 | yt−1, . . . , y1, X) =\\r\\nexp gjshti\\r\\n\\x01\\r\\nPK\\r\\nj\\r\\n0=1 exp \\r\\ngj\\r\\n0shti\\r\\n\\x01,\\nwhere the i-element of shti\\r\\nis\\r\\ns\\r\\nhti\\r\\ni = max n\\r\\ns\\r\\n0hti\\r\\n2i−1\\r\\n, s0hti\\r\\n2i\\r\\no\\r\\nand\\r\\ns\\r\\n0hti = Ohh0hti + Oyyt−1 + Occ.\\r\\nIn short, the s\\r\\nhti\\r\\ni\\r\\nis a so-called maxout unit.\\r\\nFor the computational efficiency, instead of a single-matrix output weight G, we use a product of two\\r\\nmatrices such that\\r\\nG = GlGr,\\r\\nwhere Gl ∈ R\\r\\nK×500 and Gr ∈ R500×1000\\r\\n.\\r\\nB Word and Phrase Representations\\r\\nHere, we show enlarged plots of the word and phrase representations in Figs. 4–5.\\nFigure 6: 2–D embedding of the learned word representation. The top left one shows the full embedding space, while the other three figures show the zoomed-in\\r\\nview of specific regions (color–coded).\\nFigure 7: 2–D embedding of the learned phrase representation. The top left one shows the full representation space (1000 randomly selected points), while the\\r\\nother three figures show the zoomed-in view of specific regions (color–coded).\"},\n",
       " {'name': '2211.14730v2.pdf',\n",
       "  'content': 'Published as a conference paper at ICLR 2023\\r\\nA TIME SERIES IS WORTH 64 WORDS:\\r\\nLONG-TERM FORECASTING WITH TRANSFORMERS\\r\\nYuqi Nie1∗, Nam H. Nguyen2 ∗, Phanwadee Sinthong2, Jayant Kalagnanam2\\r\\n1Princeton University 2\\r\\nIBM Research\\r\\nynie@princeton.edu, nnguyen@us.ibm.com, Gift.Sinthong@ibm.com,\\r\\njayant@us.ibm.com\\r\\nABSTRACT\\r\\nWe propose an efficient design of Transformer-based models for multivariate\\r\\ntime series forecasting and self-supervised representation learning. It is based on\\r\\ntwo key components: (i) segmentation of time series into subseries-level patches\\r\\nwhich are served as input tokens to Transformer; (ii) channel-independence where\\r\\neach channel contains a single univariate time series that shares the same embed\\x02ding and Transformer weights across all the series. Patching design naturally\\r\\nhas three-fold benefit: local semantic information is retained in the embedding;\\r\\ncomputation and memory usage of the attention maps are quadratically reduced\\r\\ngiven the same look-back window; and the model can attend longer history. Our\\r\\nchannel-independent patch time series Transformer (PatchTST) can improve the\\r\\nlong-term forecasting accuracy significantly when compared with that of SOTA\\r\\nTransformer-based models. We also apply our model to self-supervised pre\\x02training tasks and attain excellent fine-tuning performance, which outperforms\\r\\nsupervised training on large datasets. Transferring of masked pre-trained repre\\x02sentation on one dataset to others also produces SOTA forecasting accuracy.\\r\\n1 INTRODUCTION\\r\\nForecasting is one of the most important tasks in time series analysis. With the rapid growth of deep\\r\\nlearning models, the number of research works has increased significantly on this topic (Bryan &\\r\\nStefan, 2021; Torres et al., 2021; Lara-Ben´ıtez et al., 2021). Deep models have shown excellent\\r\\nperformance not only on forecasting tasks, but also on representation learning where abstract rep\\x02resentation can be extracted and transferred to various downstream tasks such as classification and\\r\\nanomaly detection to attain state-of-the-art performance.\\r\\nAmong deep learning models, Transformer has achieved great success on various application fields\\r\\nsuch as natural language processing (NLP) (Kalyan et al., 2021), computer vision (CV) (Khan et al.,\\r\\n2021), speech (Karita et al., 2019), and more recently time series (Wen et al., 2022), benefiting from\\r\\nits attention mechanism which can automatically learn the connections between elements in a se\\x02quence, thus becomes ideal for sequential modeling tasks. Informer (Zhou et al., 2021), Autoformer\\r\\n(Wu et al., 2021), and FEDformer (Zhou et al., 2022) are among the best variants of the Transformer\\r\\nmodel successfully applying to time series data. Unfortunately, regardless of the complicated design\\r\\nof Transformer-based models, it is shown in the recent paper (Zeng et al., 2022) that a very simple\\r\\nlinear model can outperform all of the previous models on a variety of common benchmarks and it\\r\\nchallenges the usefulness of Transformer for time series forecasting. In this paper, we attempt to an\\x02swer this question by proposing a channel-independence patch time series Transformer (PatchTST)\\r\\nmodel that contains two key designs:\\r\\n• Patching. Time series forecasting aims to understand the correlation between data in each dif\\x02ferent time steps. However, a single time step does not have semantic meaning like a word in a\\r\\nsentence, thus extracting local semantic information is essential in analyzing their connections.\\r\\nMost of the previous works only use point-wise input tokens, or just a handcrafted information\\r\\n∗Equal contribution.\\r\\n1\\r\\narXiv:2211.14730v2 [cs.LG] 5 Mar 2023\\nPublished as a conference paper at ICLR 2023\\r\\nModels L N patch method MSE\\r\\nChannel-independent\\r\\nPatchTST\\r\\n96 96 0.518\\r\\n380 96 down-sampled 0.447\\r\\n336 336 0.397\\r\\n336 42 X 0.367\\r\\n336 42 X self-supervised 0.349\\r\\nChannel-mixing FEDFormer 336 336 0.597\\r\\nDLinear 336 336 0.410\\r\\nRunning time (s) with L = 336\\r\\nDataset w. patch w.o. patch Gain\\r\\nTraffic 464 10040 x 22\\r\\nElectricity 300 5730 x 19\\r\\nWeather 156 680 x 4\\r\\nTable 1: A case study of multivariate time series forecasting on Traffic dataset. The prediction hori\\x02zon is 96. Results with different look-back window L and number of input tokens N are reported.\\r\\nThe best result is in bold and the second best is underlined. Down-sampled means sampling every\\r\\n4 step and adding the last value. All the results are from supervised training except the best result\\r\\nwhich uses self-supervised learning.\\r\\nfrom series. In contrast, we enhance the locality and capture comprehensive semantic informa\\x02tion that is not available in point-level by aggregating time steps into subseries-level patches.\\r\\n• Channel-independence. A multivariate time series is a multi-channel signal, and each Trans\\x02former input token can be represented by data from either a single channel or multiple channels.\\r\\nDepending on the design of input tokens, different variants of the Transformer architecture have\\r\\nbeen proposed. Channel-mixing refers to the latter case where the input token takes the vec\\x02tor of all time series features and projects it to the embedding space to mix information. On\\r\\nthe other hand, channel-independence means that each input token only contains information\\r\\nfrom a single channel. This was proven to work well with CNN (Zheng et al., 2014) and linear\\r\\nmodels (Zeng et al., 2022), but hasn’t been applied to Transformer-based models yet.\\r\\nWe offer a snapshot of our key results in Table 1 by doing a case study on Traffic dataset, which\\r\\nconsists of 862 time series. Our model has several advantages:\\r\\n1. Reduction on time and space complexity: The original Transformer has O(N2) complexity\\r\\non both time and space, where N is the number of input tokens. Without pre-processing, N\\r\\nwill have the same value as input sequence length L, which becomes a primary bottleneck\\r\\nof computation time and memory in practice. By applying patching, we can reduce N by a\\r\\nfactor of the stride: N ≈ L/S, thus reducing the complexity quadratically. Table 1 illustrates\\r\\nthe usefulness of patching. By setting patch length P = 16 and stride S = 8 with L = 336,\\r\\nthe training time is significantly reduced as much as 22 time on large datasets.\\r\\n2. Capability of learning from longer look-back window: Table 1 shows that by increasing look\\x02back window L from 96 to 336, MSE can be reduced from 0.518 to 0.397. However, simply\\r\\nextending L comes at the cost of larger memory and computational usage. Since time series\\r\\noften carries heavily temporal redundant information, some previous work tried to ignore parts\\r\\nof data points by using downsampling or a carefully designed sparse connection of attention\\r\\n(Li et al., 2019) and the model still yields sufficient information to forecast well. We study the\\r\\ncase when L = 380 and the time series is sampled every 4 steps with the last point added to\\r\\nsequence, resulting in the number of input tokens being N = 96. The model achieves better\\r\\nMSE score (0.447) than using the data sequence containing the most recent 96 time steps\\r\\n(0.518), indicating that longer look-back window conveys more important information even\\r\\nwith the same number of input tokens. This leads us to think of a question: is there a way\\r\\nto avoid throwing values while maintaining a long look-back window? Patching is a good\\r\\nanswer to it. It can group local time steps that may contain similar values while at the same\\r\\ntime enables the model to reduce the input token length for computational benefit. As evident\\r\\nin Table 1, MSE score is further reduced from 0.397 to 0.367 with patching when L = 336.\\r\\n3. Capability of representation learning: With the emergence of powerful self-supervised learn\\x02ing techniques, sophisticated models with multiple non-linear layers of abstraction are re\\x02quired to capture abstract representation of the data. Simple models like linear ones (Zeng\\r\\net al., 2022) may not be preferred for that task due to its limited expressibility. With our\\r\\nPatchTST model, we not only confirm that Transformer is actually effective for time series\\r\\nforecasting, but also demonstrate the representation capability that can further enhance the\\r\\nforecasting performance. Our PatchTST has achieved the best MSE (0.349) in Table 1.\\r\\n2\\nPublished as a conference paper at ICLR 2023\\r\\nWe introduce our approach in more detail and conduct extensive experiments in the following sec\\x02tions to conclusively prove our claims. We not only demonstrate the model effectiveness with super\\x02vised forecasting results and ablation studies, but also achieves SOTA self-supervised representation\\r\\nlearning and transfer learning performance.\\r\\n2 RELATED WORK\\r\\nPatch in Transformer-based Models. Transformer (Vaswani et al., 2017) has demonstrated a sig\\x02nificant potential on different data modalities. Among all applications, patching is an essential\\r\\npart when local semantic information is important. In NLP, BERT (Devlin et al., 2018) considers\\r\\nsubword-based tokenization (Schuster & Nakajima, 2012) instead of performing character-based\\r\\ntokenization. In CV, Vision Transformer (ViT) (Dosovitskiy et al., 2021) is a milestone work that\\r\\nsplits an image into 16×16 patches before feeding into the Transformer model. The following influ\\x02ential works such as BEiT (Bao et al., 2022) and masked autoencoders (He et al., 2021) are all using\\r\\npatches as input. Similarly, in speech researchers are using convolutions to extract information in\\r\\nsub-sequence levels from raw audio input (Baevski et al., 2020; Hsu et al., 2021).\\r\\nTransformer-based Long-term Time Series Forecasting. There is a large body of work that tries\\r\\nto apply Transformer models to forecast long-term time series in recent years. We here summarize\\r\\nsome of them. LogTrans (Li et al., 2019) uses convolutional self-attention layers with LogSparse\\r\\ndesign to capture local information and reduce the space complexity. Informer (Zhou et al., 2021)\\r\\nproposes a ProbSparse self-attention with distilling techniques to extract the most important keys\\r\\nefficiently. Autoformer (Wu et al., 2021) borrows the ideas of decomposition and auto-correlation\\r\\nfrom traditional time series analysis methods. FEDformer (Zhou et al., 2022) uses Fourier enhanced\\r\\nstructure to get a linear complexity. Pyraformer (Liu et al., 2022) applies pyramidal attention module\\r\\nwith inter-scale and intra-scale connections which also get a linear complexity.\\r\\nMost of these models focus on designing novel mechanisms to reduce the complexity of original\\r\\nattention mechanism, thus achieving better performance on forecasting, especially when the pre\\x02diction length is long. However, most of the models use point-wise attention, which ignores the\\r\\nimportance of patches. LogTrans (Li et al., 2019) avoids a point-wise dot product between the key\\r\\nand query, but its value is still based on a single time step. Autoformer (Wu et al., 2021) uses auto\\x02correlation to get patch level connections, but it is a handcrafted design which doesn’t include all\\r\\nthe semantic information within a patch. Triformer (Cirstea et al., 2022) proposes patch attention,\\r\\nbut the purpose is to reduce complexity by using a pseudo timestamp as the query within a patch,\\r\\nthus it neither treats a patch as a input unit, nor reveals the semantic importance behind it.\\r\\nTime Series Representation Learning. Besides supervised learning, self-supervised learning is\\r\\nalso an important research topic since it has shown the potential to learn useful representations for\\r\\ndownstream tasks. There are many non-Transformer-based models proposed in recent years to learn\\r\\nrepresentations in time series (Franceschi et al., 2019; Tonekaboni et al., 2021; Yang & Hong, 2022;\\r\\nYue et al., 2022). Meanwhile, Transformer is known to be an ideal candidate towards foundation\\r\\nmodels (Bommasani et al., 2021) and learning universal representations. However, although people\\r\\nhave made attempts on Transformer-based models like time series Transformer (TST) (Zerveas et al.,\\r\\n2021) and TS-TCC (Eldele et al., 2021), the potential is still not fully realized yet.\\r\\n3 PROPOSED METHOD\\r\\n3.1 MODEL STRUCTURE\\r\\nWe consider the following problem: given a collection of multivariate time series samples with look\\x02back window L : (x1, ..., xL) where each xt at time step t is a vector of dimension M, we would\\r\\nlike to forecast T future values (xL+1, ..., xL+T ). Our PatchTST is illustrated in Figure 1 where the\\r\\nmodel makes use of the vanilla Transformer encoder as its core architecture.\\r\\nForward Process. We denote a i-th univariate series of length L starting at time index 1 as\\r\\nx\\r\\n(i)\\r\\n1:L = (x\\r\\n(i)\\r\\n1\\r\\n, ..., x\\r\\n(i)\\r\\nL\\r\\n) where i = 1, ..., M. The input (x1, ..., xL) is split to M univariate series\\r\\nx\\r\\n(i) ∈ R1×L, where each of them is fed independently into the Transformer backbone according to\\r\\n3\\nPublished as a conference paper at ICLR 2023\\r\\nInput Univariate Series\\r\\nInstance Norm + Patching\\r\\n ∈ ℝ!×#\\r\\nChannel\\x02independence\\r\\nTransformer Backbone\\r\\nConcatenate\\r\\n(%) ∈ ℝ\\'×#,  = 1, … ,  +(%) ∈ ℝ\\'×(,  = 1, … , \\r\\n+ ∈ ℝ!×(\\r\\n(\") ∈ ℝ$×&\\r\\nProjection + Position Embedding\\r\\nTransformer Encoder\\r\\nFlatten + Linear Head\\r\\n$ Output Univariate Series (\") ∈ ℝ$×\\'\\r\\nInput Univariate Series\\r\\nInstance Norm + Patching\\r\\nProjection + Position Embedding\\r\\nTransformer Encoder\\r\\nLinear Layer\\r\\n(a) PatchTST Model Overview\\r\\n(b) Transformer Backbone (Supervised) (c) Transformer Backbone (Self-supervised)\\r\\nReconstructed \\r\\nMasked Patches\\r\\nMulti-Head \\r\\nAttention\\r\\nAdd & Norm\\r\\nFeed \\r\\nForward\\r\\nAdd & Norm\\r\\nn×\\r\\n(\\r\\n(\") ∈ ℝ)×*\\r\\n(\") ∈ ℝ)×*\\r\\n+\\r\\n(\") ∈ ℝ,×*\\r\\nFigure 1: PatchTST architecture. (a) Multivariate time series data is divided into different chan\\x02nels. They share the same Transformer backbone, but the forward processes are independent. (b)\\r\\nEach channel univariate series is passed through instance normalization operator and segmented into\\r\\npatches. These patches are used as Transformer input tokens. (c) Masked self-supervised represen\\x02tation learning with PatchTST where patches are randomly selected and set to zero. The model will\\r\\nreconstruct the masked patches.\\r\\nour channel-independence setting. Then the Transformer backbone will provide prediction results\\r\\nxˆ\\r\\n(i) = (ˆx\\r\\n(i)\\r\\nL+1, ..., xˆ\\r\\n(i)\\r\\nL+T\\r\\n) ∈ R\\r\\n1×T\\r\\naccordingly .\\r\\nPatching. Each input univariate time series x\\r\\n(i)\\r\\nis first divided into patches which can be either\\r\\noverlapped or non-overlapped. Denote the patch length as P and the stride - the non overlapping\\r\\nregion between two consecutive patches as S, then the patching process will generate the a sequence\\r\\nof patches x\\r\\n(i)\\r\\np ∈ R\\r\\nP ×N where N is the number of patches, N = b\\r\\n(L−P )\\r\\nS\\r\\nc + 2. Here, we pad S\\r\\nrepeated numbers of the last value x\\r\\n(i)\\r\\nL ∈ R to the end of the original sequence before patching.\\r\\nWith the use of patches, the number of input tokens can reduce from L to approximately L/S.\\r\\nThis implies the memory usage and computational complexity of the attention map are quadrati\\x02cally decreased by a factor of S. Thus constrained on the training time and GPU memory, patch\\r\\ndesign can allow the model to see the longer historical sequence, which can significantly improve\\r\\nthe forecasting performance, as demonstrated in Table 1.\\r\\nTransformer Encoder. We use a vanilla Transformer encoder that maps the observed signals to the\\r\\nlatent representations. The patches are mapped to the Transformer latent space of dimension D via a\\r\\ntrainable linear projection Wp ∈ R\\r\\nD×P , and a learnable additive position encoding Wpos ∈ RD×N\\r\\nis applied to monitor the temporal order of patches: x\\r\\n(i)\\r\\nd = Wpx\\r\\n(i)\\r\\np + Wpos, where x\\r\\n(i)\\r\\nd ∈ R\\r\\nD×N\\r\\ndenote the input that will be fed into Transformer encoder in Figure 1. Then each head h = 1, ..., H\\r\\nin multi-head attention will transform them into query matrices Q\\r\\n(i)\\r\\nh = (x\\r\\n(i)\\r\\nd\\r\\n)\\r\\nTWQ\\r\\nh\\r\\n, key matrices\\r\\nK\\r\\n(i)\\r\\nh = (x\\r\\n(i)\\r\\nd\\r\\n)\\r\\nTWK\\r\\nh\\r\\nand value matrices V\\r\\n(i)\\r\\nh = (x\\r\\n(i)\\r\\nd\\r\\n)\\r\\nTWV\\r\\nh\\r\\n, where WQ\\r\\nh\\r\\n,WK\\r\\nh ∈ R\\r\\nD×dk and\\r\\n4\\nPublished as a conference paper at ICLR 2023\\r\\nWV\\r\\nh ∈ R\\r\\nD×D. After that a scaled production is used for getting attention output O\\r\\n(i)\\r\\nh ∈ R\\r\\nD×N :\\r\\n(O\\r\\n(i)\\r\\nh\\r\\n)\\r\\nT = Attention(Q\\r\\n(i)\\r\\nh\\r\\n, K(i)\\r\\nh\\r\\n, V (i)\\r\\nh\\r\\n) = Softmax(\\r\\nQ\\r\\n(i)\\r\\nh K\\r\\n(i)\\r\\nh\\r\\nT\\r\\n√\\r\\ndk\\r\\n)V\\r\\n(i)\\r\\nh\\r\\nThe multi-head attention block also includes BatchNorm 1layers and a feed forward network with\\r\\nresidual connections as shown in Figure 1. Afterwards it generates the representation denoted as\\r\\nz\\r\\n(i) ∈ RD×N . Finally a flatten layer with linear head is used to obtain the prediction result xˆ(i) =\\r\\n(ˆx\\r\\n(i)\\r\\nL+1, ..., xˆ\\r\\n(i)\\r\\nL+T\\r\\n) ∈ R\\r\\n1×T\\r\\n.\\r\\nLoss Function. We choose to use the MSE loss to measure the discrepancy between the prediction\\r\\nand the ground truth. The loss in each channel is gathered and averaged over M time series to get\\r\\nthe overall objective loss: L = Ex\\r\\n1\\r\\nM\\r\\nPM\\r\\ni=1 kxˆ\\r\\n(i)\\r\\nL+1:L+T − x\\r\\n(i)\\r\\nL+1:L+T\\r\\nk\\r\\n2\\r\\n2\\r\\n.\\r\\nInstance Normalization. This technique has recently been proposed to help mitigating the distri\\x02bution shift effect between the training and testing data (Ulyanov et al., 2016; Kim et al., 2022).\\r\\nIt simply normalizes each time series instance x\\r\\n(i) with zero mean and unit standard deviation. In\\r\\nessence, we normalize each x\\r\\n(i) before patching and the mean and deviation are added back to the\\r\\noutput prediction.\\r\\n3.2 REPRESENTATION LEARNING\\r\\nSelf-supervised representation learning has become a popular approach to extract high level abstract\\r\\nrepresentation from unlabelled data. In this section, we apply PatchTST to obtain useful represen\\x02tation of the multivariate time series. We will show that the learnt representation can be effectively\\r\\ntransferred to forecasting tasks. Among popular methods to learn representation via self-supervise\\r\\npre-training, masked autoencoder has been applied successfully to NLP (Devlin et al., 2018) and\\r\\nCV (He et al., 2021) domains. This technique is conceptually simple: a portion of input sequence is\\r\\nintentionally removed at random and the model is trained to recover the missing contents.\\r\\nMasked encoder has been recently employed in time series and delivered notable performance on\\r\\nclassification and regression tasks (Zerveas et al., 2021). The authors proposed to apply the multi\\x02variate time series to Transformer, where each input token is a vector xi consisting of time series\\r\\nvalues at time step i-th. Masking is placed randomly within each time series and across different\\r\\nseries. However, there are two potential issues with this setting: First, masking is applied at the level\\r\\nof single time steps. The masked values at the current time step can be easily inferred by interpo\\x02lating with the immediate proceeding or succeeding time values without high level understanding\\r\\nof the entire sequence, which deviates from our goal of learning important abstract representation\\r\\nof the whole signal. Zerveas et al. (2021) proposed complex randomization strategies to resolve the\\r\\nproblem in which groups of time series with different sizes are randomly masked.\\r\\nSecond, the design of the output layer for forecasting task can be troublesome. Given the repre\\x02sentation vectors zt ∈ R\\r\\nD corresponding to all L time steps, mapping these vectors to the output\\r\\ncontaining M variables each with prediction horizon T via a linear map requires a parameter matrix\\r\\nW of dimension (L · D) × (M · T). This matrix can be particularly oversized if either one or all\\r\\nof these four values are large. This may cause overfitting when the number of downstream training\\r\\nsamples is scarce.\\r\\nOur proposed PatchTST can naturally overcome the aforementioned issues. As shown in Figure 1,\\r\\nwe use the same Transformer encoder as the supervised settings. The prediction head is removed and\\r\\na D × P linear layer is attached. As opposed to supervised model where patches can be overlapped,\\r\\nwe divide each input sequence into regular non-overlapping patches. It is for convenience to ensure\\r\\nobserved patches do not contain information of the masked ones. We then select a subset of the\\r\\npatch indices uniformly at random and mask the patches according to these selected indices with\\r\\nzero values. The model is trained with MSE loss to reconstruct the masked patches.\\r\\nWe emphasize that each time series will have its own latent representation that are cross-learned via\\r\\na shared weight mechanism. This design can allow the pre-training data to contain different number\\r\\nof time series than the downstream data, which may not be feasible by other approaches.\\r\\n1Zerveas et al. (2021) has shown that BatchNorm outperforms LayerNorm in time series Transformer.\\r\\n5\\nPublished as a conference paper at ICLR 2023\\r\\n4 EXPERIMENTS\\r\\n4.1 LONG-TERM TIME SERIES FORECASTING\\r\\nDatasets. We evaluate the performance of our proposed PatchTST on 8 popular datasets, including\\r\\nWeather, Traffic, Electricity, ILI and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). These\\r\\ndatasets have been extensively utilized for benchmarking and publicly available on (Wu et al., 2021).\\r\\nThe statistics of those datasets are summarized in Table 2. We would like to highlight several large\\r\\ndatasets: Weather, Traffic, and Electricity. They have many more number of time series, thus the\\r\\nresults would be more stable and less susceptible to overfitting than other smaller datasets.\\r\\nDatasets Weather Traffic Electricity ILI ETTh1 ETTh2 ETTm1 ETTm2\\r\\nFeatures 21 862 321 7 7 7 7 7\\r\\nTimesteps 52696 17544 26304 966 17420 17420 69680 69680\\r\\nTable 2: Statistics of popular datasets for benchmark.\\r\\nBaselines and Experimental Settings. We choose the SOTA Transformer-based models, includ\\x02ing FEDformer (Zhou et al., 2022), Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021),\\r\\nPyraformer (Liu et al., 2022), LogTrans (Li et al., 2019), and a recent non-Transformer-based model\\r\\nDLinear (Zeng et al., 2022) as our baselines. All of the models are following the same experimental\\r\\nsetup with prediction length T ∈ {24, 36, 48, 60} for ILI dataset and T ∈ {96, 192, 336, 720} for\\r\\nother datasets as in the original papers. We collect baseline results from Zeng et al. (2022) with the\\r\\ndefault look-back window L = 96 for Transformer-based models, and L = 336 for DLinear. But in\\r\\norder to avoid under-estimating the baselines, we also run FEDformer, Autoformer and Informer for\\r\\nsix different look-back window L ∈ {24, 48, 96, 192, 336, 720}, and always choose the best results\\r\\nto create strong baselines. More details about the baselines could be found in Appendix A.1.2. We\\r\\ncalculate the MSE and MAE of multivariate time series forecasting as metrics.\\r\\nModel Variants. We propose two versions of PatchTST in Table 3. PatchTST/64 implies the\\r\\nnumber of input patches is 64, which uses the look-back window L = 512. PatchTST/42 means the\\r\\nnumber of input patches is 42, which has the default look-back window L = 336. Both of them\\r\\nuse patch length P = 16 and stride S = 8. Thus, we could use PatchTST/42 as a fair comparison\\r\\nto DLinear and other Transformer-based models, and PatchTST/64 to explore even better results on\\r\\nlarger datasets. More experimental details are provided in Appendix A.1.\\r\\nResults. Table 3 shows the multivariate long-term forecasting results. Overall, our model outper\\x02form all baseline methods. Quantitatively, compared with the best results that Transformer-based\\r\\nmodels can offer, PatchTST/64 achieves an overall 21.0% reduction on MSE and 16.7% reduction\\r\\non MAE, while PatchTST/42 attains an overall 20.2% reduction on MSE and 16.4% reduction on\\r\\nMAE. Compared with the DLinear model, PatchTST can still outperform it in general, especially\\r\\non large datasets (Weather, Traffic, Electricity) and ILI dataset. We also experiment with univariate\\r\\ndatasets where the results are provided in Appendix A.3.\\r\\n4.2 REPRESENTATION LEARNING\\r\\nIn this section, we conduct experiments with masked self-supervised learning where we set the\\r\\npatches to be non-overlapped. Otherwise stated, across all representation learning experiments the\\r\\ninput sequence length is chosen to be 512 and patch size is set to 12, which results in 42 patches.\\r\\nWe consider high masking ratio where 40% of the patches are masked with zero values. We first\\r\\napply self-supervised pre-training on the datasets mentioned in Section 4.1 for 100 epochs. Once\\r\\nthe pre-trained model on each dataset is available, we perform supervised training to evaluate the\\r\\nlearned representation with two options: (a) linear probing and (b) end-to-end fine-tuning. With (a),\\r\\nwe only train the model head for 20 epochs while freezing the rest of the network; With (b), we\\r\\napply linear probing for 10 epochs to update the model head and then end-to-end fine-tuning the\\r\\nentire network for 20 epochs. It was proven that a two-step strategy with linear probing followed\\r\\nby fine-tuning can outperform only doing fine-tuning directly (Kumar et al., 2022). We select a few\\r\\nrepresentative results on below, and a full benchmark can be found in Appendix A.5.\\r\\n6\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST/64 PatchTST/42 DLinear FEDformer Autoformer Informer Pyraformer LogTrans\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.149 0.198 0.152 0.199 0.176 0.237 0.238 0.314 0.249 0.329 0.354 0.405 0.896 0.556 0.458 0.490\\r\\n192 0.194 0.241 0.197 0.243 0.220 0.282 0.275 0.329 0.325 0.370 0.419 0.434 0.622 0.624 0.658 0.589\\r\\n336 0.245 0.282 0.249 0.283 0.265 0.319 0.339 0.377 0.351 0.391 0.583 0.543 0.739 0.753 0.797 0.652\\r\\n720 0.314 0.334 0.320 0.335 0.323 0.362 0.389 0.409 0.415 0.426 0.916 0.705 1.004 0.934 0.869 0.675\\r\\nTraffic\\r\\n96 0.360 0.249 0.367 0.251 0.410 0.282 0.576 0.359 0.597 0.371 0.733 0.410 2.085 0.468 0.684 0.384\\r\\n192 0.379 0.256 0.385 0.259 0.423 0.287 0.610 0.380 0.607 0.382 0.777 0.435 0.867 0.467 0.685 0.390\\r\\n336 0.392 0.264 0.398 0.265 0.436 0.296 0.608 0.375 0.623 0.387 0.776 0.434 0.869 0.469 0.734 0.408\\r\\n720 0.432 0.286 0.434 0.287 0.466 0.315 0.621 0.375 0.639 0.395 0.827 0.466 0.881 0.473 0.717 0.396\\r\\nElectricity\\r\\n96 0.129 0.222 0.130 0.222 0.140 0.237 0.186 0.302 0.196 0.313 0.304 0.393 0.386 0.449 0.258 0.357\\r\\n192 0.147 0.240 0.148 0.240 0.153 0.249 0.197 0.311 0.211 0.324 0.327 0.417 0.386 0.443 0.266 0.368\\r\\n336 0.163 0.259 0.167 0.261 0.169 0.267 0.213 0.328 0.214 0.327 0.333 0.422 0.378 0.443 0.280 0.380\\r\\n720 0.197 0.290 0.202 0.291 0.203 0.301 0.233 0.344 0.236 0.342 0.351 0.427 0.376 0.445 0.283 0.376\\r\\nILI\\r\\n24 1.319 0.754 1.522 0.814 2.215 1.081 2.624 1.095 2.906 1.182 4.657 1.449 1.420 2.012 4.480 1.444\\r\\n36 1.579 0.870 1.430 0.834 1.963 0.963 2.516 1.021 2.585 1.038 4.650 1.463 7.394 2.031 4.799 1.467\\r\\n48 1.553 0.815 1.673 0.854 2.130 1.024 2.505 1.041 3.024 1.145 5.004 1.542 7.551 2.057 4.800 1.468\\r\\n60 1.470 0.788 1.529 0.862 2.368 1.096 2.742 1.122 2.761 1.114 5.071 1.543 7.662 2.100 5.278 1.560\\r\\nETTh1\\r\\n96 0.370 0.400 0.375 0.399 0.375 0.399 0.376 0.415 0.435 0.446 0.941 0.769 0.664 0.612 0.878 0.740\\r\\n192 0.413 0.429 0.414 0.421 0.405 0.416 0.423 0.446 0.456 0.457 1.007 0.786 0.790 0.681 1.037 0.824\\r\\n336 0.422 0.440 0.431 0.436 0.439 0.443 0.444 0.462 0.486 0.487 1.038 0.784 0.891 0.738 1.238 0.932\\r\\n720 0.447 0.468 0.449 0.466 0.472 0.490 0.469 0.492 0.515 0.517 1.144 0.857 0.963 0.782 1.135 0.852\\r\\nETTh2\\r\\n96 0.274 0.337 0.274 0.336 0.289 0.353 0.332 0.374 0.332 0.368 1.549 0.952 0.645 0.597 2.116 1.197\\r\\n192 0.341 0.382 0.339 0.379 0.383 0.418 0.407 0.446 0.426 0.434 3.792 1.542 0.788 0.683 4.315 1.635\\r\\n336 0.329 0.384 0.331 0.380 0.448 0.465 0.400 0.447 0.477 0.479 4.215 1.642 0.907 0.747 1.124 1.604\\r\\n720 0.379 0.422 0.379 0.422 0.605 0.551 0.412 0.469 0.453 0.490 3.656 1.619 0.963 0.783 3.188 1.540\\r\\nETTm1\\r\\n96 0.293 0.346 0.290 0.342 0.299 0.343 0.326 0.390 0.510 0.492 0.626 0.560 0.543 0.510 0.600 0.546\\r\\n192 0.333 0.370 0.332 0.369 0.335 0.365 0.365 0.415 0.514 0.495 0.725 0.619 0.557 0.537 0.837 0.700\\r\\n336 0.369 0.392 0.366 0.392 0.369 0.386 0.392 0.425 0.510 0.492 1.005 0.741 0.754 0.655 1.124 0.832\\r\\n720 0.416 0.420 0.420 0.424 0.425 0.421 0.446 0.458 0.527 0.493 1.133 0.845 0.908 0.724 1.153 0.820\\r\\nETTm2\\r\\n96 0.166 0.256 0.165 0.255 0.167 0.260 0.180 0.271 0.205 0.293 0.355 0.462 0.435 0.507 0.768 0.642\\r\\n192 0.223 0.296 0.220 0.292 0.224 0.303 0.252 0.318 0.278 0.336 0.595 0.586 0.730 0.673 0.989 0.757\\r\\n336 0.274 0.329 0.278 0.329 0.281 0.342 0.324 0.364 0.343 0.379 1.270 0.871 1.201 0.845 1.334 0.872\\r\\n720 0.362 0.385 0.367 0.385 0.397 0.421 0.410 0.420 0.414 0.419 3.001 1.267 3.625 1.451 3.048 1.328\\r\\nTable 3: Multivariate long-term forecasting results with supervised PatchTST. We use prediction\\r\\nlengths T ∈ {24, 36, 48, 60} for ILI dataset and T ∈ {96, 192, 336, 720} for the others. The best\\r\\nresults are in bold and the second best are underlined.\\r\\nModels PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Prob. Sup.\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.144 0.193 0.158 0.209 0.152 0.199 0.176 0.237 0.238 0.314 0.249 0.329 0.354 0.405\\r\\n192 0.190 0.236 0.203 0.249 0.197 0.243 0.220 0.282 0.275 0.329 0.325 0.370 0.419 0.434\\r\\n336 0.244 0.280 0.251 0.285 0.249 0.283 0.265 0.319 0.339 0.377 0.351 0.391 0.583 0.543\\r\\n720 0.320 0.335 0.321 0.336 0.320 0.335 0.323 0.362 0.389 0.409 0.415 0.426 0.916 0.705\\r\\nTraffic\\r\\n96 0.352 0.244 0.399 0.294 0.367 0.251 0.410 0.282 0.576 0.359 0.597 0.371 0.733 0.410\\r\\n192 0.371 0.253 0.412 0.298 0.385 0.259 0.423 0.287 0.610 0.380 0.607 0.382 0.777 0.435\\r\\n336 0.381 0.257 0.425 0.306 0.398 0.265 0.436 0.296 0.608 0.375 0.623 0.387 0.776 0.434\\r\\n720 0.425 0.282 0.460 0.323 0.434 0.287 0.466 0.315 0.621 0.375 0.639 0.395 0.827 0.466\\r\\nElectricity\\r\\n96 0.126 0.221 0.138 0.237 0.130 0.222 0.140 0.237 0.186 0.302 0.196 0.313 0.304 0.393\\r\\n192 0.145 0.238 0.156 0.252 0.148 0.240 0.153 0.249 0.197 0.311 0.211 0.324 0.327 0.417\\r\\n336 0.164 0.256 0.170 0.265 0.167 0.261 0.169 0.267 0.213 0.328 0.214 0.327 0.333 0.422\\r\\n720 0.193 0.291 0.208 0.297 0.202 0.291 0.203 0.301 0.233 0.344 0.236 0.342 0.351 0.427\\r\\nTable 4: Multivariate long-term forecasting results with self-supervised PatchTST. We use prediction\\r\\nlengths T ∈ {96, 192, 336, 720}. The best results are in bold and the second best are underlined.\\r\\nComparison with Supervised Methods. Table 4 compares the performance of PatchTST (with\\r\\nfine-tuning, linear probing, and supervising from scratch) with other supervised method. As shown\\r\\nin the table, on large datasets our pre-training procedure contributes a clear improvement compared\\r\\nto supervised training from scratch. By just fine-tuning the model head (linear probing), the fore\\x02casting performance is already comparable with training the entire network from scratch and better\\r\\nthan DLinear. The best results are observed with end-to-end fine-tuning. Self-supervised PatchTST\\r\\nsignificantly outperforms other Transformer-based models on all the datasets.\\r\\nTransfer Learning. We test the capability of transfering the pre-trained model to downstream\\r\\ntasks. In particular, we pre-train the model on Electricity dataset and fine-tune on other datasets. We\\r\\nobserve from Table 5 that overall the fine-tuning MSE is lightly worse than pre-training and fine\\x02tuning on the same dataset, which is reasonable. The fine-tuning performance is also worse than\\r\\nsupervised training in some cases. However, the forecasting performance is still better than other\\r\\n7\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Prob. Sup.\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.145 0.195 0.163 0.216 0.152 0.199 0.176 0.237 0.238 0.314 0.249 0.329 0.354 0.405\\r\\n192 0.193 0.243 0.205 0.252 0.197 0.243 0.220 0.282 0.275 0.329 0.325 0.370 0.419 0.434\\r\\n336 0.244 0.280 0.253 0.289 0.249 0.283 0.265 0.319 0.339 0.377 0.351 0.391 0.583 0.543\\r\\n720 0.321 0.337 0.320 0.336 0.320 0.335 0.323 0.362 0.389 0.409 0.415 0.426 0.916 0.705\\r\\nTraffic\\r\\n96 0.388 0.273 0.400 0.288 0.367 0.251 0.410 0.282 0.576 0.359 0.597 0.371 0.733 0.410\\r\\n192 0.400 0.277 0.412 0.293 0.385 0.259 0.423 0.287 0.610 0.380 0.607 0.382 0.777 0.435\\r\\n336 0.408 0.280 0.425 0.307 0.398 0.265 0.436 0.296 0.608 0.375 0.623 0.387 0.776 0.434\\r\\n720 0.447 0.310 0.457 0.317 0.434 0.287 0.466 0.315 0.621 0.375 0.639 0.395 0.827 0.466\\r\\nTable 5: Transfer learning task: PatchTST is pre-trained on Electricity dataset and the model is\\r\\ntransferred to other datasets. The best results are in bold and the second best are underlined.\\r\\nModels IMP. PatchTST BTSF TS2Vec TNC TS-TCC Transferred Self-supervised\\r\\nMetrics MSE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1\\r\\n24 42.3% 0.312 0.362 0.322 0.369 0.541 0.519 0.599 0.534 0.632 0.596 0.653 0.610\\r\\n48 44.7% 0.339 0.378 0.354 0.385 0.613 0.524 0.629 0.555 0.705 0.688 0.720 0.693\\r\\n168 34.5% 0.424 0.437 0.419 0.424 0.640 0.532 0.755 0.636 1.097 0.993 1.129 1.044\\r\\n336 48.5% 0.472 0.472 0.445 0.446 0.864 0.689 0.907 0.717 1.454 0.919 1.492 1.076\\r\\n720 48.8% 0.508 0.507 0.487 0.478 0.993 0.712 1.048 0.790 1.604 1.118 1.603 1.206\\r\\nTable 6: Representation learning methods comparison. Column name transferred implies pre\\x02training PatchTST on Traffic dataset and transferring the representation to ETTh1, while self\\x02supervised implies both pre-training and linear probing on ETTh1. The best and second best results\\r\\nare in bold and underlined. IMP. denotes the improvement on best results of PatchTST compared to\\r\\nthat of baselines, which is in the range of 34.5% to 48.8% on various prediction lengths.\\r\\nmodels. Note that as opposed to supervised PatchTST where the entire model is trained for each\\r\\nprediction horizon, here we only retrain the linear head or the entire model for much fewer epochs,\\r\\nwhich results in significant computational time reduction.\\r\\nComparison with Other Self-supervised Methods. We compare our self-supervised model with\\r\\nBTSF (Yang & Hong, 2022), TS2Vec (Yue et al., 2022), TNC (Tonekaboni et al., 2021), and TS\\x02TCC (Eldele et al., 2021) which are among the state-of-the-art contrastive learning representation\\r\\nmethods for time series 2. We test the forecasting performance on ETTh1 dataset, where we only\\r\\napply linear probing after the learned representation is obtained (only fine-tune the last linear layer)\\r\\nto make the comparison fair. Results from Table 6 strongly indicates the superior performance of\\r\\nPatchTST, both from pre-training on its own ETTh1 data (self-supervised columns) or pre-training\\r\\non Traffic (transferred columns).\\r\\n4.3 ABLATION STUDY\\r\\nPatching and Channel-independence. We study the effects of patching and channel-independence\\r\\nin Table 7. We include FEDformer as the SOTA benchmark for Transformer-based model. By com\\x02paring results with and without the design of patching / channel-independence accordingly, one can\\r\\nobserve that both of them are important factors in improving the forecasting performance. The moti\\x02vation of patching is natural; furthermore this technique improves the running time and memory con\\x02sumption as shown in Table 1 due to shorter Transformer sequence input. Channel-independence, on\\r\\nthe other hand, may not be as intuitive as patching is in terms of the technical advantages. Therefore,\\r\\nwe provide an in-depth analysis on the key factors that make channel-independence more preferable\\r\\nin Appendix A.7. More ablation study results are available in Appendix A.4.\\r\\nVarying Look-back Window. In principle, a longer look-back window increases the receptive field,\\r\\nwhich will potentially improves the forecasting performance. However, as argued in (Zeng et al.,\\r\\n2022), this phenomenon hasn’t been observed in most of the Transformer-based models. We also\\r\\ndemonstrate in Figure 2 that in most cases, these Transformer-based baselines have not benefited\\r\\nfrom longer look-back window L, which indicates their ineffectiveness in capturing temporal in\\x02formation. In contrast, our PatchTST consistently reduces the MSE scores as the receptive field\\r\\nincreases, which confirms our model’s capability to learn from longer look-back window.\\r\\n2We cite results of TS2Vec from (Yue et al., 2022) and {BTSF,TNC,TS-TCC} from (Yang & Hong, 2022).\\r\\n8\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST FEDformer P+CI CI P Original\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.152 0.199 0.164 0.213 0.168 0.223 0.177 0.236 0.238 0.314\\r\\n192 0.197 0.243 0.205 0.250 0.213 0.262 0.221 0.270 0.275 0.329\\r\\n336 0.249 0.283 0.255 0.289 0.266 0.300 0.271 0.306 0.339 0.377\\r\\n720 0.320 0.335 0.327 0.343 0.351 0.359 0.340 0.353 0.389 0.409\\r\\nTraffic\\r\\n96 0.367 0.251 0.397 0.271 0.595 0.376 - - 0.576 0.359\\r\\n192 0.385 0.259 0.411 0.276 0.612 0.387 - - 0.610 0.380\\r\\n336 0.398 0.265 0.423 0.282 0.651 0.391 - - 0.608 0.375\\r\\n720 0.434 0.287 0.457 0.309 - - - - 0.621 0.375\\r\\nElectricity\\r\\n96 0.130 0.222 0.136 0.231 0.196 0.307 0.205 0.318 0.186 0.302\\r\\n192 0.148 0.240 0.164 0.263 0.215 0.323 - - 0.197 0.311\\r\\n336 0.167 0.261 0.168 0.262 0.228 0.338 - - 0.213 0.328\\r\\n720 0.202 0.291 0.219 0.312 0.244 0.345 - - 0.233 0.344\\r\\nTable 7: Ablation study of patching and channel-independence in PatchTST. 4 cases are included:\\r\\n(a) both patching and channel-independence are included in model (P+CI); (b) only channel\\x02independence (CI); (c) only patching (P); (d) neither of them is included (Original TST model).\\r\\nPatchTST means supervised PatchTST/42. ’-’ in table means the model runs out of GPU memory\\r\\n(NVIDIA A40 48GB) even with batch size 1. The best results are in bold.\\r\\nFigure 2: Forecasting performance (MSE) with varying look-back windows on 3 large\\r\\ndatasets: Electricity, Traffic, and Weather. The look-back windows are selected to be L =\\r\\n24, 48, 96, 192, 336, 720, and the prediction horizons are T = 96, 720. We use supervised\\r\\nPatchTST/42 and other open-source Transformer-based baselines for this experiment.\\r\\n5 CONCLUSION AND FUTURE WORK\\r\\nThis paper proposes an effective design of Transformer-based models for time series forecasting\\r\\ntasks by introducing two key components: patching and channel-independent structure. Compared\\r\\nto the previous works, it could capture local semantic information and benefit from longer look-back\\r\\nwindows. We not only show that our model outperforms other baselines in supervised learning, but\\r\\nalso prove its promising capability in self-supervised representation learning and transfer learning.\\r\\nOur model exhibits the potential to be the based model for future work of Transformer-based fore\\x02casting and be a building block for time series foundation models. Patching is simple but proven to\\r\\nbe an effective operator that can be transferred easily to other models. Channel-independence, on\\r\\nthe other hand, can be further exploited to incorporate the correlation between different channels. It\\r\\nwould be an important future step to model the cross-channel dependencies properly.\\r\\n9\\nPublished as a conference paper at ICLR 2023\\r\\nREFERENCES\\r\\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame\\x02work for self-supervised learning of speech representations. Advances in Neural Information\\r\\nProcessing Systems, 33:12449–12460, 2020.\\r\\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\\r\\nand recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\\r\\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image trans\\x02formers. In International Conference on Learning Representations, 2022. URL https:\\r\\n//openreview.net/forum?id=p-BhZSz59o4.\\r\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\\r\\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu\\x02nities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\\r\\nG. E. P. Box and Gwilym M. Jenkins. Time series analysis, forecasting and control. 1970.\\r\\nLim Bryan and Zohren Stefan. Time-series forecasting with deep learning: a survey. Phil. Trans. R.\\r\\nSoc. A, 2021. doi: 10.1098/rsta.2020.0209.\\r\\nDefu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bix\\x02iong Xu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time\\x02series forecasting. Advances in neural information processing systems, 33:17766–17778, 2020.\\r\\nWeiqi Chen, Wenwei Wang, Bingqing Peng, Qingsong Wen, Tian Zhou, and Liang Sun. Learning to\\r\\nrotate: Quaternion transformer for complicated periodical time series forecasting. In Proceedings\\r\\nof the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 146–156,\\r\\n2022.\\r\\nYuzhou Chen, Ignacio Segovia, and Yulia R Gel. Z-gcnets: time zigzags at graph convolutional\\r\\nnetworks for time series forecasting. In International Conference on Machine Learning, pp.\\r\\n1684–1694. PMLR, 2021.\\r\\nRazvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, and Shirui Pan.\\r\\nTriformer: Triangular, variable-specific attentions for long sequence multivariate time series\\r\\nforecasting. In Proceedings of the Thirty-First International Joint Conference on Artificial In\\x02telligence, IJCAI-22, pp. 1994–2001, 7 2022. doi: 10.24963/ijcai.2022/277. URL https:\\r\\n//doi.org/10.24963/ijcai.2022/277.\\r\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\\r\\nbidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL\\r\\nhttp://arxiv.org/abs/1810.04805.\\r\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\r\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko\\x02reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni\\x02tion at scale. In International Conference on Learning Representations, 2021. URL https:\\r\\n//openreview.net/forum?id=YicbFdNTTy.\\r\\nEmadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and\\r\\nCuntai Guan. Time-series representation learning via temporal and contextual contrasting. In\\r\\nProceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21,\\r\\npp. 2352–2359, 2021.\\r\\nEugene F Fama. Efficient capital markets: A review of theory and empirical work. The journal of\\r\\nFinance, 25(2):383–417, 1970.\\r\\nJean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation\\r\\nlearning for multivariate time series. In Advances in Neural Information Processing Systems,\\r\\nvolume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/\\r\\n53c6de78244e9f528eb3e1cda69699bb-Paper.pdf.\\r\\n10\\nPublished as a conference paper at ICLR 2023\\r\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked au- ´\\r\\ntoencoders are scalable vision learners. CoRR, abs/2111.06377, 2021. URL https://arxiv.\\r\\norg/abs/2111.06377.\\r\\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\\r\\narXiv:1606.08415, 2016.\\r\\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 1997.\\r\\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\\r\\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\\r\\nprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\\r\\n29:3451–3460, 2021.\\r\\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha. Ammus: A sur\\x02vey of transformer-based pretrained models in natural language processing. arXiv preprint\\r\\narXiv:2108.05542, 2021.\\r\\nShigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang,\\r\\nMasao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al. A compar\\x02ative study on transformer vs rnn in speech applications. In IEEE Automatic Speech Recognition\\r\\nand Understanding Workshop (ASRU), pp. 449–456. IEEE, 2019.\\r\\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and\\r\\nMubarak Shah. Transformers in vision: A survey. ACM Computing Surveys (CSUR), 2021.\\r\\nTaesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Re\\x02versible instance normalization for accurate time-series forecasting against distribution shift. In\\r\\nInternational Conference on Learning Representations, 2022. URL https://openreview.\\r\\nnet/forum?id=cGDAkQo1C0p.\\r\\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine\\x02tuning can distort pretrained features and underperform out-of-distribution. In International Con\\x02ference on Learning Representations, 2022. URL https://openreview.net/forum?\\r\\nid=UYneFzXSJWh.\\r\\nPedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa, and Jose C Riquelme. An experimental review on ´\\r\\ndeep learning architectures for time series forecasting. International Journal of Neural Systems,\\r\\n31(03):2130001, 2021.\\r\\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and\\r\\nXifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer\\r\\non time series forecasting. In Advances in Neural Information Processing Systems, vol\\x02ume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/\\r\\n6775a0635c302542da2c32aa19d86be0-Paper.pdf.\\r\\nShizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.\\r\\nPyraformer: Low-complexity pyramidal attention for long-range time series modeling and fore\\x02casting. In International Conference on Learning Representations, 2022.\\r\\nBarbara Rossi. Exchange rate predictability. Journal of Economic Literature, 51(4):1063–1119, De\\x02cember 2013. doi: 10.1257/jel.51.4.1063. URL https://www.aeaweb.org/articles?\\r\\nid=10.1257/jel.51.4.1063.\\r\\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic fore\\x02casting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–\\r\\n1191, 2020.\\r\\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In IEEE International\\r\\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5149–5152, 2012. doi:\\r\\n10.1109/ICASSP.2012.6289079.\\r\\n11\\nPublished as a conference paper at ICLR 2023\\r\\nSana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for\\r\\ntime series with temporal neighborhood coding. In International Conference on Learning Repre\\x02sentations, 2021. URL https://openreview.net/forum?id=8qDwejCuCN.\\r\\nJose F Torres, Dalil Hadjout, Abderrazak Sebaa, Francisco Mart ´ ´ınez-Alvarez, and Alicia Troncoso. ´\\r\\nDeep learning for time series forecasting: a survey. Big Data, 9(1):3–21, 2021.\\r\\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in\\x02gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.\\r\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\r\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor\\x02mation Processing Systems, volume 30, 2017. URL https://proceedings.neurips.\\r\\ncc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\r\\nQingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun.\\r\\nTransformers in time series: A survey. arXiv preprint arXiv:2202.07125, 2022.\\r\\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transform\\x02ers with Auto-Correlation for long-term series forecasting. In Advances in Neural Information\\r\\nProcessing Systems, 2021.\\r\\nLing Yang and Shenda Hong. Unsupervised time-series representation learning with iterative bilin\\x02ear temporal-spectral fusion. In ICML, 2022.\\r\\nZhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and\\r\\nBixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI\\r\\nConference on Artificial Intelligence, volume 36, pp. 8980–8987, 2022.\\r\\nAiling Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\\r\\nforecasting? arXiv preprint arXiv:2205.13504, 2022.\\r\\nGeorge Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eick\\x02hoff. A transformer-based framework for multivariate time series representation learning. In\\r\\nProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp.\\r\\n2114–2124, 2021.\\r\\nYi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J Leon Zhao. Time series classification using multi\\x02channels deep convolutional neural networks. In International conference on web-age information\\r\\nmanagement, pp. 298–310. Springer, 2014.\\r\\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\r\\nInformer: Beyond efficient transformer for long sequence time-series forecasting. In The Thirty\\x02Fifth AAAI Conference on Artificial Intelligence, volume 35, pp. 11106–11115, 2021.\\r\\nTian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency\\r\\nenhanced decomposed transformer for long-term series forecasting. In Proc. 39th International\\r\\nConference on Machine Learning, 2022.\\r\\n12\\nPublished as a conference paper at ICLR 2023\\r\\nA APPENDIX\\r\\nA.1 EXPERIMENTAL DETAILS\\r\\nA.1.1 DATASETS\\r\\nWe use 8 popular multivariate datasets provided in (Wu et al., 2021) for forecasting and representa\\x02tion learning. Weather3 dataset collects 21 meteorological indicators in Germany, such as humidity\\r\\nand air temperature. Traffic4 dataset records the road occupancy rates from different sensors on San\\r\\nFrancisco freeways. Electricity5is a dataset that describes 321 customers’ hourly electricity con\\x02sumption. ILI6 dataset collects the number of patients and influenza-like illness ratio in a weekly\\r\\nfrequency. ETT7(Electricity Transformer Temperature) datasets are collected from two different\\r\\nelectric transformers labeled with 1 and 2, and each of them contains 2 different resolutions (15\\r\\nminutes and 1 hour) denoted with m and h. Thus, in total we have 4 ETT datasets: ETTm1, ETTm2,\\r\\nETTh1, and ETTh2.\\r\\nThere is an additional Exchange-rate8 dataset mentioned in the original paper, which is the daily\\r\\nexchange-rate of eight different countries. However, financial datasets generally have different prop\\x02erties compared to time series datasets in other fields, for example the predictability. It is well known\\r\\nthat if a market is efficient, the best prediction for xt will be just xt−1 (Fama, 1970). Rossi (2013)\\r\\nargues that the toughest benchmark for exchange-rate forecasting is a random walk without drift.\\r\\nAlso, Zeng et al. (2022) shows that by simply repeating the last value in the look-back window, the\\r\\nMSE loss on exchange-rate dataset can outperform or be comparable to the best results. Therefore,\\r\\nwe are prudent in containing it into our benchmark.\\r\\nA.1.2 DETAILS OF BASELINE SETTINGS\\r\\nThe default look-back windows for different baseline models could be different. For Transformer\\x02based models, the default look-back window is L = 96; and for DLinear, the default look-back win\\x02dow is L = 336. The reason of this difference is that Transformer-based baselines are easy to overfit\\r\\nwhen look-back window is long while DLinear tend to underfit. However, this can possibly lead to\\r\\nan under-estimation of the baselines. To address this issue, we re-run FEDformer, Autoformer and\\r\\nInformer by ourselves for six different look-back window L ∈ {24, 48, 96, 192, 336, 720}. And for\\r\\neach forecasting task (aka each different prediction length on each dataset), we choose the best one\\r\\nfrom those six results. Thus it could be a strong baseline.\\r\\nThe ILI dataset is much smaller than the other datasets, so a different set of parameters is applied\\r\\n(the default look-back windows of Transformer-based models and DLinear are L = 36 and L =\\r\\n104 respectively; we run FEDformer, Autoformer and Informer for six different look-back window\\r\\nL ∈ {24, 36, 48, 60, 104, 144} and choose the best results).\\r\\nA.1.3 BASELINES FROM TRADITIONAL MODELS\\r\\nTime series has been an ancient field of study, with many traditional models developed, for exam\\x02ple the famous ARIMA model (Box & Jenkins, 1970). With the bloom of deep learning commu\\x02nity, many new models were proposed for sequence modeling and time series forecasting before\\r\\nTransformer appears, such as LSTM (Hochreiter & Schmidhuber, 1997), TCN (Bai et al., 2018) and\\r\\nDeepAR (Salinas et al., 2020). However, they are demonstrated to be not as effective as Transformer\\x02based models in long-term forecasting tasks (Zhou et al., 2021; Wu et al., 2021), thus we don’t\\r\\ninclude them in our baselines.\\r\\n3\\r\\nhttps://www.bgc-jena.mpg.de/wetter/\\r\\n4\\r\\nhttps://pems.dot.ca.gov/\\r\\n5\\r\\nhttps://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\r\\n6\\r\\nhttps://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\\r\\n7\\r\\nhttps://github.com/zhouhaoyi/ETDataset\\r\\n8\\r\\nhttps://github.com/laiguokun/multivariate-time-series-data\\r\\n13\\nPublished as a conference paper at ICLR 2023\\r\\nA.1.4 MODEL PARAMETERS\\r\\nBy default, PatchTST contains 3 encoder layers with head number H = 16 and dimension of latent\\r\\nspace D = 128. The feed forward network in Transformer encoder block consists of 2 linear\\r\\nlayers with GELU (Hendrycks & Gimpel, 2016) activation function: one projecting the hidden\\r\\nrepresentation D = 128 to a new dimension F = 256, and another layer that project it back to\\r\\nD = 128. For very small datasets (ILI, ETTh1, ETTh2), a reduced size of parameters is used\\r\\n(H = 4, D = 16 and F = 128) to mitigate the possible overfitting. Dropout with probability 0.2 is\\r\\napplied in the encoders for all experiments. The code will be publicly available.\\r\\nA.1.5 IMPLEMENTATION DETAILS\\r\\nAlthough PatchTST processes channels in parallel which has to make multiple copies of the Trans\\x02former’s weights, the computation can be implemented efficiently and does not require any special\\r\\noperator. The batch of samples of x ∈ RM×L with size B × M × L is passed through the patching\\r\\noperator to generate a 4D tensor of size B ×M × P × N which represents a batch of x\\r\\n(i)\\r\\np ∈ R\\r\\nP ×N\\r\\nin M series. By reshaping the tensor to form a 3D one of size (B · M) × P × N, this batch can be\\r\\nconsumed by any standard Transformer implementation.\\r\\nWe further argue that our proposed PatchTST contains additional benefits: The components in Trans\\x02former backbone module shown in Figure 1 can differ across different input series, for instance the\\r\\nembedding layers and head layers. Note that if the embedding layers are designed differently for\\r\\neach group of time series, the reshaping step will be applied after embedding. Besides, the number\\r\\nof variables in the multivariate time series during the training may not need to match the number of\\r\\nseries for testing. This is especially beneficial for self-supervised pre-training where the pre-training\\r\\ndata can have different number of variables from the fine-tuning data.\\r\\nA.2 VISUALIZATION\\r\\nWe visualize the long-term forecasting results of supervised PatchTST/42 and other baselines in\\r\\nFigure 3. Here, we predict 192 steps ahead on Weather and Eletricity datasets and 60 steps ahead on\\r\\nILI dataset. PatchTST provides the best forecasting both in terms of scale and bias.\\r\\nFigure 3: Visualization of 192-step forecasting on {Weather, Traffic} datasets with the look-back\\r\\nwindow L = 336 and 60-step forecasting on ILI dataset with L = 104. PatchTST (in red) can\\r\\ncapture the trend and closest to the ground truth (in blue).\\r\\nA.3 UNIVARIATE FORECASTING\\r\\nTable 8 summaries the results of univariate forecasting on ETT datasets. There is a target feature\\r\\n”oil temperature” within those datasets, which is the univariate series that we are trying to forecast.\\r\\nWe cite the baseline results from (Zeng et al., 2022).\\r\\nA.4 MORE RESULTS ON ABLATION STUDY\\r\\nA.4.1 VARYING PATCH LENGTH\\r\\nThis experiment studies the effect of patch lengths to the forecasting performance. We fix the look\\x02back window to be 336 and vary the patch lengths P = [4, 8, 16, 24, 32, 40]. The stride length is set\\r\\n14\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST/64 PatchTST/42 DLinear FEDformer Autoformer Informer LogTrans\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1\\r\\n96 0.059 0.189 0.055 0.179 0.056 0.180 0.079 0.215 0.071 0.206 0.193 0.377 0.283 0.468\\r\\n192 0.074 0.215 0.071 0.205 0.071 0.204 0.104 0.245 0.114 0.262 0.217 0.395 0.234 0.409\\r\\n336 0.076 0.220 0.081 0.225 0.098 0.244 0.119 0.270 0.107 0.258 0.202 0.381 0.386 0.546\\r\\n720 0.087 0.236 0.087 0.232 0.189 0.359 0.142 0.299 0.126 0.283 0.183 0.355 0.475 0.629\\r\\nETTh2\\r\\n96 0.131 0.284 0.129 0.282 0.131 0.279 0.128 0.271 0.153 0.306 0.213 0.373 0.217 0.379\\r\\n192 0.171 0.329 0.168 0.328 0.176 0.329 0.185 0.330 0.204 0.351 0.227 0.387 0.281 0.429\\r\\n336 0.171 0.336 0.185 0.351 0.209 0.367 0.231 0.378 0.246 0.389 0.242 0.401 0.293 0.437\\r\\n720 0.223 0.380 0.224 0.383 0.276 0.426 0.278 0.420 0.268 0.409 0.291 0.439 0.218 0.387\\r\\nETTm1\\r\\n96 0.026 0.123 0.026 0.121 0.028 0.123 0.033 0.140 0.056 0.183 0.109 0.277 0.049 0.171\\r\\n192 0.040 0.151 0.039 0.150 0.045 0.156 0.058 0.186 0.081 0.216 0.151 0.310 0.157 0.317\\r\\n336 0.053 0.174 0.053 0.173 0.061 0.182 0.084 0.231 0.076 0.218 0.427 0.591 0.289 0.459\\r\\n720 0.073 0.206 0.074 0.207 0.080 0.210 0.102 0.250 0.110 0.267 0.438 0.586 0.430 0.579\\r\\nETTm2\\r\\n96 0.065 0.187 0.065 0.186 0.063 0.183 0.067 0.198 0.065 0.189 0.088 0.225 0.075 0.208\\r\\n192 0.093 0.231 0.094 0.231 0.092 0.227 0.102 0.245 0.118 0.256 0.132 0.283 0.129 0.275\\r\\n336 0.121 0.266 0.120 0.265 0.119 0.261 0.130 0.279 0.154 0.305 0.180 0.336 0.154 0.302\\r\\n720 0.172 0.322 0.171 0.322 0.175 0.320 0.178 0.325 0.182 0.335 0.300 0.435 0.160 0.321\\r\\nTable 8: Univariate long-term forecasting results with supervised PatchTST. ETT datasets are used\\r\\nwith prediction lengths T ∈ {96, 192, 336, 720}. The best results are in bold.\\r\\nthe same as patch length, meaning no overlapping between patches. The model is trained to predict\\r\\n96 steps. One observation from Figure 4 is that MSE scores don’t vary significantly with different\\r\\nchoices of P, which indicate the robustness of our model against the patch length hyperparameter.\\r\\nOverall, PatchTST benefits from increased patch length, not only in forecasting performance but also\\r\\nin the computational reduction. The ideal patch length may depend on the dataset, but P between\\r\\n{8, 16} seems to be general good numbers.\\r\\nFigure 4: MSE scores with varying patch lengths P = [2, 4, 8, 12, 16, 24, 32, 40] where the look\\x02back window is 336 and the prediction length is 96.\\r\\nA.4.2 VARYING LOOK-BACK WINDOW\\r\\nHere we provide a full benchmark of quantitative results in Table 9 for varying look-back window in\\r\\nsupervised PatchTST/42 regarding Figure 2 in the main text. Generally speaking, our model gains\\r\\nperformance improvement with increasing look-back window, which show the effectiveness of our\\r\\nmodel in learning information from longer receptive field.\\r\\nA.4.3 PATCHING AND CHANNEL-INDEPENDENCE\\r\\nImplementation Details. For ablation study on patching and channel independence in section 4.3,\\r\\nwe run different variants of PatchTST:\\r\\n• Both patching and channel-independence are included in model (P+CI): this is the full\\r\\nPatchTST model that we have proposed in paper.\\r\\n• Only channel-independence (CI): we simply set both patch length P and stride S to be 1 to\\r\\navoid patching and only keep channel-independence.\\r\\n• Only patching (P): referring to the implementation in A.1.5, instead of reshaping the 4D tensor\\r\\nfrom B×M ×P ×N to (B·M)×P ×N, we reshape it to B×(M ·P)×N for channel-mixing\\r\\nwith patching.\\r\\n15\\nPublished as a conference paper at ICLR 2023\\r\\nL 24(24) 48(36) 96(48) 192(60) 336(104) 720(144)\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.222 0.246 0.212 0.243 0.178 0.219 0.160 0.204 0.152 0.199 0.147 0.198\\r\\n192 0.265 0.279 0.254 0.277 0.224 0.259 0.204 0.245 0.197 0.243 0.190 0.240\\r\\n336 0.325 0.322 0.310 0.316 0.278 0.298 0.257 0.285 0.249 0.283 0.242 0.282\\r\\n720 0.404 0.374 0.385 0.365 0.350 0.346 0.329 0.338 0.320 0.335 0.304 0.328\\r\\nTraffic\\r\\n96 0.766 0.419 0.671 0.381 0.477 0.305 0.401 0.267 0.367 0.251 0.365 0.251\\r\\n192 0.725 0.398 0.616 0.356 0.471 0.299 0.406 0.268 0.385 0.259 0.382 0.258\\r\\n336 0.752 0.410 0.635 0.364 0.485 0.305 0.421 0.277 0.398 0.265 0.398 0.267\\r\\n720 0.786 0.427 0.673 0.383 0.518 0.325 0.452 0.297 0.434 0.287 0.436 0.289\\r\\nElectricity\\r\\n96 0.268 0.316 0.225 0.293 0.174 0.259 0.138 0.230 0.130 0.222 0.130 0.224\\r\\n192 0.259 0.316 0.217 0.291 0.178 0.265 0.149 0.243 0.148 0.240 0.147 0.241\\r\\n336 0.283 0.335 0.238 0.309 0.196 0.282 0.169 0.262 0.167 0.261 0.163 0.259\\r\\n720 0.321 0.365 0.278 0.342 0.237 0.316 0.211 0.299 0.202 0.291 0.197 0.290\\r\\nILI\\r\\n24 3.062 1.118 1.610 0.803 1.281 0.704 1.300 0.700 1.522 0.814 1.470 0.793\\r\\n36 2.732 1.071 1.262 0.731 1.251 0.752 1.367 0.776 1.430 0.834 1.518 0.856\\r\\n48 3.059 1.117 1.991 0.845 1.901 0.879 1.690 0.812 1.673 0.854 1.834 0.921\\r\\n60 2.610 1.057 1.702 0.829 1.611 0.844 1.526 0.795 1.529 0.862 1.656 0.885\\r\\nETTh1\\r\\n96 0.464 0.445 0.410 0.417 0.393 0.408 0.382 0.401 0.375 0.399 0.376 0.408\\r\\n192 0.521 0.474 0.469 0.448 0.445 0.434 0.428 0.425 0.414 0.421 0.413 0.431\\r\\n336 0.570 0.498 0.516 0.469 0.484 0.451 0.451 0.436 0.431 0.436 0.445 0.454\\r\\n720 0.575 0.522 0.509 0.487 0.480 0.471 0.452 0.459 0.449 0.466 0.458 0.477\\r\\nETTh2\\r\\n96 0.333 0.362 0.307 0.348 0.294 0.343 0.285 0.340 0.274 0.336 0.279 0.341\\r\\n192 0.422 0.409 0.397 0.399 0.377 0.393 0.356 0.386 0.339 0.379 0.349 0.386\\r\\n336 0.442 0.432 0.412 0.420 0.381 0.409 0.350 0.395 0.331 0.380 0.375 0.409\\r\\n720 0.462 0.453 0.434 0.441 0.412 0.433 0.395 0.427 0.379 0.422 0.394 0.434\\r\\nETTm1\\r\\n96 0.624 0.481 0.424 0.403 0.321 0.360 0.291 0.340 0.290 0.342 0.295 0.348\\r\\n192 0.671 0.507 0.468 0.429 0.362 0.384 0.328 0.365 0.332 0.369 0.334 0.373\\r\\n336 0.714 0.533 0.501 0.453 0.392 0.402 0.365 0.389 0.366 0.392 0.361 0.393\\r\\n720 0.744 0.554 0.553 0.484 0.450 0.435 0.422 0.423 0.420 0.424 0.416 0.419\\r\\nETTm2\\r\\n96 0.212 0.290 0.189 0.272 0.178 0.260 0.169 0.254 0.165 0.255 0.162 0.254\\r\\n192 0.282 0.334 0.260 0.317 0.249 0.307 0.230 0.294 0.220 0.292 0.216 0.293\\r\\n336 0.354 0.376 0.328 0.359 0.313 0.346 0.280 0.329 0.278 0.329 0.269 0.329\\r\\n720 0.458 0.433 0.429 0.415 0.400 0.398 0.378 0.386 0.367 0.385 0.350 0.380\\r\\nTable 9: Multivariate long-term forecasting results with varying look-back window L in supervised\\r\\nPatchTST/42.\\r\\n• Neither patching nor channel-independence is included (Original), which is just the original\\r\\nTST model Zerveas et al. (2021).\\r\\nNote that the default number of maximum epochs for Electricity and Traffic datasets are reduced\\r\\nfrom 100 to 20 for ablation experiments due to the huge space and time complexity of the original\\r\\ntime series Transformer and channel independent model (no patching).\\r\\nFull Benchmark of Ablation Study. The full benchmark is shown in Table 10, which is a com\\x02pleted version of Table 7 in the main text. We can observe that patching together with channel\\x02independence achieves the best results from the table, especially on larger datasets (Weather, Traf\\x02fic, and Electricity) where the models are less susceptible to overfitting and thus the results would\\r\\nbe more convincing. As one can see the improvement is robust on both patching and channel\\x02independence. It is interesting to see that the improvement on ILI dataset is significant as well.\\r\\nA.4.4 INSTANCE NORMALIZATION\\r\\nNormalization is a technology used in many time series model to improve the forecasting perfor\\x02mance (Kim et al., 2022; Chen et al., 2022; Zeng et al., 2022). In this experiment, we perform\\r\\nanalysis on the effect of the instance normalization in our model. We train two models PatchTST/64\\r\\nand PatchTST/42 with and without using instance normalization and observe the evaluated scores.\\r\\nAs indicated in Table 11, instance normalization improves the forecasting performance slightly on\\r\\ntwo models. However, even without instance normalization operator, PatchTST still outperforms\\r\\nother Transformer methods on most of the datasets. This is to confirm that the improvement mainly\\r\\ncomes from patching and channel independence designs.\\r\\nA.5 MORE RESULTS ON SELF-SUPERVISED REPRESENTATION LEARNING\\r\\nA.5.1 FULL BENCHMARK OF MULTIVARIATE FORECASTING\\r\\nIn this section we provide a full benchmark of multivariate forecasting results with self-supervised\\r\\nPatchTST in Table 12, which is an extended version of Table 4.\\r\\n16\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST FEDformer P+CI CI P Original\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.152 0.199 0.164 0.213 0.168 0.223 0.177 0.236 0.238 0.314\\r\\n192 0.197 0.243 0.205 0.250 0.213 0.262 0.221 0.270 0.275 0.329\\r\\n336 0.249 0.283 0.255 0.289 0.266 0.300 0.271 0.306 0.339 0.377\\r\\n720 0.320 0.335 0.327 0.343 0.351 0.359 0.340 0.353 0.389 0.409\\r\\nTraffic\\r\\n96 0.367 0.251 0.397 0.271 0.595 0.376 - - 0.576 0.359\\r\\n192 0.385 0.259 0.411 0.276 0.612 0.387 - - 0.610 0.380\\r\\n336 0.398 0.265 0.423 0.282 0.651 0.391 - - 0.608 0.375\\r\\n720 0.434 0.287 0.457 0.309 - - - - 0.621 0.375\\r\\nElectricity\\r\\n96 0.130 0.222 0.136 0.231 0.196 0.307 0.205 0.318 0.186 0.302\\r\\n192 0.148 0.240 0.164 0.263 0.215 0.323 - - 0.197 0.311\\r\\n336 0.167 0.261 0.168 0.262 0.228 0.338 - - 0.213 0.328\\r\\n720 0.202 0.291 0.219 0.312 0.244 0.345 - - 0.233 0.344\\r\\nILI\\r\\n24 1.522 0.814 2.111 1.048 2.157 0.964 2.737 1.081 2.624 1.095\\r\\n36 1.430 0.834 2.000 1.002 2.564 1.058 2.126 0.935 2.516 1.021\\r\\n48 1.673 0.854 2.167 1.029 2.348 1.022 2.178 0.971 2.505 1.041\\r\\n60 1.529 0.862 2.075 1.021 2.486 1.065 2.354 1.026 2.742 1.122\\r\\nETTh1\\r\\n96 0.375 0.399 0.365 0.395 0.416 0.438 0.455 0.459 0.376 0.415\\r\\n192 0.414 0.421 0.403 0.415 0.459 0.464 0.503 0.486 0.423 0.446\\r\\n336 0.431 0.436 0.430 0.433 0.484 0.480 0.514 0.503 0.444 0.462\\r\\n720 0.449 0.466 0.449 0.454 0.500 0.494 0.531 0.520 0.469 0.492\\r\\nETTh2\\r\\n96 0.274 0.336 0.277 0.337 0.334 0.388 0.348 0.394 0.332 0.374\\r\\n192 0.339 0.379 0.343 0.384 0.381 0.418 0.395 0.424 0.407 0.446\\r\\n336 0.331 0.380 0.333 0.383 0.361 0.414 0.369 0.419 0.400 0.447\\r\\n720 0.379 0.422 0.379 0.420 0.423 0.448 0.433 0.458 0.412 0.469\\r\\nETTm1\\r\\n96 0.290 0.342 0.300 0.354 0.326 0.368 0.324 0.370 0.326 0.390\\r\\n192 0.332 0.369 0.333 0.374 0.391 0.405 0.373 0.398 0.365 0.415\\r\\n336 0.366 0.392 0.369 0.397 0.427 0.425 0.415 0.421 0.392 0.425\\r\\n720 0.420 0.424 0.413 0.423 0.481 0.457 0.480 0.459 0.446 0.458\\r\\nETTm2\\r\\n96 0.165 0.255 0.166 0.259 0.195 0.274 0.208 0.289 0.180 0.271\\r\\n192 0.220 0.292 0.223 0.295 0.259 0.314 0.265 0.328 0.252 0.318\\r\\n336 0.278 0.329 0.279 0.330 0.297 0.345 0.323 0.365 0.324 0.364\\r\\n720 0.367 0.385 0.370 0.387 0.400 0.404 0.469 0.444 0.410 0.420\\r\\nTable 10: Ablation study of patching (P) and channel-independence (CI) in PatchTST/42. A full\\r\\nbenchmark regarding Table 7. The best results are in bold. ’-’ in table means the model runs out of\\r\\nGPU memory (NVIDIA A40 48GB) even with batch size 1.\\r\\n17\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST/64 (+in) PatchTST/64 (-in) PatchTST/42 (+in) PatchTST/42 (-in) FEDformer Autoformer Informer\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.149 0.198 0.161 0.219 0.152 0.199 0.156 0.210 0.238 0.314 0.249 0.329 0.354 0.405\\r\\n192 0.194 0.241 0.201 0.254 0.197 0.243 0.199 0.250 0.275 0.329 0.325 0.370 0.419 0.434\\r\\n336 0.245 0.282 0.253 0.298 0.249 0.283 0.248 0.294 0.339 0.377 0.351 0.391 0.583 0.543\\r\\n720 0.314 0.334 0.323 0.357 0.320 0.335 0.313 0.342 0.389 0.409 0.415 0.426 0.916 0.705\\r\\nTraffic\\r\\n96 0.360 0.249 0.413 0.295 0.367 0.251 0.425 0.299 0.576 0.359 0.597 0.371 0.733 0.410\\r\\n192 0.379 0.256 0.425 0.302 0.385 0.259 0.439 0.302 0.610 0.380 0.607 0.382 0.777 0.435\\r\\n336 0.392 0.264 0.435 0.307 0.398 0.265 0.456 0.316 0.608 0.375 0.623 0.387 0.776 0.434\\r\\n720 0.432 0.286 0.473 0.321 0.434 0.287 0.488 0.333 0.621 0.375 0.639 0.395 0.827 0.466\\r\\nElectricity\\r\\n96 0.129 0.222 0.133 0.230 0.130 0.222 0.131 0.226 0.186 0.302 0.196 0.313 0.304 0.393\\r\\n192 0.147 0.240 0.148 0.244 0.148 0.240 0.150 0.244 0.197 0.311 0.211 0.324 0.327 0.417\\r\\n336 0.163 0.259 0.164 0.262 0.167 0.261 0.168 0.267 0.213 0.328 0.214 0.327 0.333 0.422\\r\\n720 0.197 0.290 0.196 0.291 0.202 0.291 0.201 0.298 0.233 0.344 0.236 0.342 0.351 0.427\\r\\nILI\\r\\n24 1.319 0.754 3.563 1.317 1.522 0.814 3.489 1.345 2.624 1.095 2.906 1.182 4.657 1.449\\r\\n36 1.579 0.870 3.426 1.205 1.430 0.834 4.629 1.550 2.516 1.021 2.585 1.038 4.650 1.463\\r\\n48 1.553 0.815 4.309 1.449 1.673 0.854 3.746 1.383 2.505 1.041 3.024 1.145 5.004 1.542\\r\\n60 1.470 0.788 4.065 1.402 1.529 0.862 5.174 1.622 2.742 1.122 2.761 1.114 5.071 1.543\\r\\nETTh1\\r\\n96 0.370 0.400 0.385 0.410 0.375 0.399 0.388 0.412 0.376 0.415 0.435 0.446 0.941 0.769\\r\\n192 0.413 0.429 0.417 0.432 0.414 0.421 0.430 0.438 0.423 0.446 0.456 0.457 1.007 0.786\\r\\n336 0.422 0.440 0.439 0.449 0.431 0.436 0.454 0.458 0.444 0.462 0.486 0.487 1.038 0.784\\r\\n720 0.447 0.468 0.478 0.494 0.449 0.466 0.494 0.497 0.469 0.492 0.515 0.517 1.144 0.857\\r\\nETTh2\\r\\n96 0.274 0.337 0.299 0.359 0.274 0.336 0.313 0.374 0.332 0.374 0.332 0.368 1.549 0.952\\r\\n192 0.341 0.382 0.354 0.404 0.339 0.379 0.402 0.432 0.407 0.446 0.426 0.434 3.792 1.542\\r\\n336 0.329 0.384 0.374 0.420 0.331 0.380 0.448 0.465 0.400 0.447 0.477 0.479 4.215 1.642\\r\\n720 0.379 0.422 0.479 0.492 0.379 0.422 0.688 0.588 0.412 0.469 0.453 0.490 3.656 1.619\\r\\nETTm1\\r\\n96 0.293 0.346 0.308 0.358 0.290 0.342 0.308 0.358 0.326 0.390 0.510 0.492 0.626 0.560\\r\\n192 0.333 0.370 0.335 0.375 0.332 0.369 0.356 0.390 0.365 0.415 0.514 0.495 0.725 0.619\\r\\n336 0.369 0.392 0.362 0.392 0.366 0.392 0.389 0.411 0.392 0.425 0.510 0.492 1.005 0.741\\r\\n720 0.416 0.420 0.432 0.429 0.420 0.424 0.430 0.439 0.446 0.458 0.527 0.493 1.133 0.845\\r\\nETTm2\\r\\n96 0.166 0.256 0.172 0.258 0.165 0.255 0.167 0.257 0.180 0.271 0.205 0.293 0.355 0.462\\r\\n192 0.223 0.296 0.245 0.306 0.220 0.292 0.226 0.303 0.252 0.318 0.278 0.336 0.595 0.586\\r\\n336 0.274 0.329 0.306 0.346 0.278 0.329 0.301 0.348 0.324 0.364 0.343 0.379 1.270 0.871\\r\\n720 0.362 0.385 0.391 0.404 0.367 0.385 0.392 0.407 0.410 0.420 0.414 0.419 3.001 1.267\\r\\nTable 11: Multivariate long-term forecasting results of supervised PatchTST with instance normal\\x02ization (+in) or without instance normalization (-in). The best results are in bold and the second best\\r\\nare underlined. Although the models perform slightly better with instance normalization, compared\\r\\nto other Transformer models, the proposed approach achieve significantly better forecasting on most\\r\\nof the datasets even without instance normalization.\\r\\nA.5.2 FULL BENCHMARK OF TRANSFER LEARNING\\r\\nIn this section we provide Table 13 which contains the results of pre-training on Electricity dataset\\r\\nthen transferred to other 6 datasets. Except Traffic data, the number of time series employed in the\\r\\npre-training is much larger than the number of series during fine-tuning. It is a full version with\\r\\nrespect to Table 5 in the main text and more cogently proves the capability to do transfer learning\\r\\nusing our PatchTST model.\\r\\nA.6 ROBUSTNESS ANALYSIS\\r\\nA.6.1 RESULTS WITH DIFFERENT RANDOM SEEDS\\r\\nThe results reported in the main text and appendix above are run with the fixed random seed 2021.\\r\\nTo examine the robustness of our results, we train the supervised PatchTST model with 5 different\\r\\nrandom seeds: 2019, 2020, 2021, 2022, 2023 and calculate the MSE and MAE scores with each\\r\\nselected seed. The mean and standard derivation of the results are reported in Table 14. It is clear\\r\\nthat the variances are considerably small which indicates the robustness against choice of random\\r\\nseeds of our model.\\r\\nWe also validate the self-supervised PatchTST model on different runs. We pre-train the model once\\r\\nand fine-tune the model 5 times with different random batch selections. The mean and standard\\r\\nderivation across different runs are also provided in Table 14. We also observe that the variance is\\r\\ninsignificant especially on large datasets while higher variance can be seen on smaller datasets.\\r\\n18\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Prob. Sup.\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.144 0.193 0.158 0.209 0.152 0.199 0.176 0.237 0.238 0.314 0.249 0.329 0.354 0.405\\r\\n192 0.190 0.236 0.203 0.249 0.197 0.243 0.220 0.282 0.275 0.329 0.325 0.370 0.419 0.434\\r\\n336 0.244 0.280 0.251 0.285 0.249 0.283 0.265 0.319 0.339 0.377 0.351 0.391 0.583 0.543\\r\\n720 0.320 0.335 0.321 0.336 0.320 0.335 0.323 0.362 0.389 0.409 0.415 0.426 0.916 0.705\\r\\nTraffic\\r\\n96 0.352 0.244 0.399 0.294 0.367 0.251 0.410 0.282 0.576 0.359 0.597 0.371 0.733 0.410\\r\\n192 0.371 0.253 0.412 0.298 0.385 0.259 0.423 0.287 0.610 0.380 0.607 0.382 0.777 0.435\\r\\n336 0.381 0.257 0.425 0.306 0.398 0.265 0.436 0.296 0.608 0.375 0.623 0.387 0.776 0.434\\r\\n720 0.425 0.282 0.460 0.323 0.434 0.287 0.466 0.315 0.621 0.375 0.639 0.395 0.827 0.466\\r\\nElectricity\\r\\n96 0.126 0.221 0.138 0.237 0.130 0.222 0.140 0.237 0.186 0.302 0.196 0.313 0.304 0.393\\r\\n192 0.145 0.238 0.156 0.252 0.148 0.240 0.153 0.249 0.197 0.311 0.211 0.324 0.327 0.417\\r\\n336 0.164 0.256 0.170 0.265 0.167 0.261 0.169 0.267 0.213 0.328 0.214 0.327 0.333 0.422\\r\\n720 0.193 0.291 0.208 0.297 0.202 0.291 0.203 0.301 0.233 0.344 0.236 0.342 0.351 0.427\\r\\nETTh1\\r\\n96 0.366 0.397 0.371 0.400 0.375 0.399 0.375 0.399 0.376 0.415 0.435 0.446 0.941 0.769\\r\\n192 0.431 0.443 0.411 0.428 0.414 0.421 0.405 0.416 0.423 0.446 0.456 0.457 1.007 0.786\\r\\n336 0.450 0.456 0.445 0.446 0.431 0.436 0.439 0.443 0.444 0.462 0.486 0.487 1.038 0.784\\r\\n720 0.472 0.484 0.487 0.478 0.449 0.466 0.472 0.490 0.469 0.492 0.515 0.517 1.144 0.857\\r\\nETTh2\\r\\n96 0.284 0.343 0.285 0.344 0.274 0.336 0.289 0.353 0.332 0.374 0.332 0.368 1.549 0.952\\r\\n192 0.355 0.387 0.356 0.387 0.339 0.379 0.383 0.418 0.407 0.446 0.426 0.434 3.792 1.542\\r\\n336 0.379 0.411 0.377 0.410 0.331 0.380 0.448 0.465 0.400 0.447 0.477 0.479 4.215 1.642\\r\\n720 0.400 0.435 0.395 0.434 0.379 0.422 0.605 0.551 0.412 0.469 0.453 0.490 3.656 1.619\\r\\nETTm1\\r\\n96 0.289 0.344 0.292 0.348 0.290 0.342 0.299 0.343 0.326 0.390 0.510 0.492 0.626 0.560\\r\\n192 0.323 0.368 0.329 0.369 0.332 0.369 0.335 0.365 0.365 0.415 0.514 0.495 0.725 0.619\\r\\n336 0.353 0.387 0.364 0.391 0.366 0.392 0.369 0.386 0.392 0.425 0.510 0.492 1.005 0.741\\r\\n720 0.398 0.416 0.415 0.419 0.420 0.424 0.425 0.421 0.446 0.458 0.527 0.493 1.133 0.845\\r\\nETTm2\\r\\n96 0.166 0.256 0.167 0.257 0.165 0.255 0.167 0.260 0.180 0.271 0.205 0.293 0.355 0.462\\r\\n192 0.221 0.295 0.229 0.300 0.220 0.292 0.224 0.303 0.252 0.318 0.278 0.336 0.595 0.586\\r\\n336 0.278 0.333 0.289 0.343 0.278 0.329 0.281 0.342 0.324 0.364 0.343 0.379 1.270 0.871\\r\\n720 0.365 0.388 0.363 0.386 0.367 0.385 0.397 0.421 0.410 0.420 0.414 0.419 3.001 1.267\\r\\nTable 12: Multivariate long-term forecasting results with self-supervised PatchTST. An full bench\\x02mark regarding Table 4. The best results are in bold.\\r\\nModels PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Prob. Sup.\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.145 0.195 0.163 0.216 0.152 0.199 0.176 0.237 0.238 0.314 0.249 0.329 0.354 0.405\\r\\n192 0.193 0.243 0.205 0.252 0.197 0.243 0.220 0.282 0.275 0.329 0.325 0.370 0.419 0.434\\r\\n336 0.244 0.280 0.253 0.289 0.249 0.283 0.265 0.319 0.339 0.377 0.351 0.391 0.583 0.543\\r\\n720 0.321 0.337 0.320 0.336 0.320 0.335 0.323 0.362 0.389 0.409 0.415 0.426 0.916 0.705\\r\\nTraffic\\r\\n96 0.388 0.273 0.400 0.288 0.367 0.251 0.410 0.282 0.576 0.359 0.597 0.371 0.733 0.410\\r\\n192 0.400 0.277 0.412 0.293 0.385 0.259 0.423 0.287 0.610 0.380 0.607 0.382 0.777 0.435\\r\\n336 0.408 0.280 0.425 0.307 0.398 0.265 0.436 0.296 0.608 0.375 0.623 0.387 0.776 0.434\\r\\n720 0.447 0.310 0.457 0.317 0.434 0.287 0.466 0.315 0.621 0.375 0.639 0.395 0.827 0.466\\r\\nETTh1\\r\\n96 0.368 0.398 0.372 0.402 0.375 0.399 0.375 0.399 0.376 0.415 0.435 0.446 0.941 0.769\\r\\n192 0.425 0.439 0.411 0.428 0.414 0.421 0.405 0.416 0.423 0.446 0.456 0.457 1.007 0.786\\r\\n336 0.470 0.471 0.442 0.454 0.431 0.436 0.439 0.443 0.444 0.462 0.486 0.487 1.038 0.784\\r\\n720 0.472 0.484 0.497 0.501 0.449 0.466 0.472 0.490 0.469 0.492 0.515 0.517 1.144 0.857\\r\\nETTh2\\r\\n96 0.285 0.345 0.280 0.341 0.274 0.334 0.289 0.353 0.332 0.374 0.332 0.368 1.549 0.952\\r\\n192 0.350 0.388 0.350 0.387 0.339 0.379 0.383 0.418 0.407 0.446 0.426 0.434 3.792 1.542\\r\\n336 0.378 0.410 0.373 0.410 0.331 0.380 0.448 0.465 0.400 0.447 0.477 0.479 4.215 1.642\\r\\n720 0.401 0.438 0.398 0.436 0.379 0.422 0.605 0.551 0.412 0.469 0.453 0.490 3.656 1.619\\r\\nETTm1\\r\\n96 0.288 0.345 0.291 0.346 0.290 0.342 0.299 0.343 0.326 0.390 0.510 0.492 0.626 0.560\\r\\n192 0.330 0.372 0.335 0.373 0.332 0.369 0.335 0.365 0.365 0.415 0.514 0.495 0.725 0.619\\r\\n336 0.359 0.392 0.365 0.391 0.366 0.392 0.369 0.386 0.392 0.425 0.510 0.492 1.005 0.741\\r\\n720 0.406 0.421 0.423 0.424 0.420 0.424 0.425 0.421 0.446 0.458 0.527 0.493 1.133 0.845\\r\\nETTm2\\r\\n96 0.164 0.256 0.166 0.257 0.165 0.255 0.167 0.260 0.180 0.271 0.205 0.293 0.355 0.462\\r\\n192 0.223 0.296 0.221 0.295 0.220 0.292 0.224 0.303 0.252 0.318 0.278 0.336 0.595 0.586\\r\\n336 0.277 0.332 0.277 0.332 0.278 0.329 0.281 0.342 0.324 0.364 0.343 0.379 1.270 0.871\\r\\n720 0.365 0.387 0.368 0.389 0.367 0.385 0.397 0.421 0.410 0.420 0.414 0.419 3.001 1.267\\r\\nTable 13: Transfer learning task: PatchTST is pre-trained on Electricity dataset and the model is\\r\\ntransferred to other datasets. A full benchmark regarding Table 13. The best results are in bold.\\r\\nA.6.2 RESULTS WITH DIFFERENT MODEL PARAMETERS\\r\\nTo see whether PatchTST is sensitive to the choice of Transformer settings, we perform another ex\\x02periments with varying model parameters. We vary the number of Transformer layers L = {3, 4, 5}\\r\\nand select the model dimension D = {128, 256} while the inner-layer of the feed forward network\\r\\nis F = 2D. In total, there are 6 different sets of model hyper-parameters to examine. Figure 5\\r\\nshows the MSE scores of these combinations on different datasets. Except ILI dataset reveals high\\r\\nvariance with different hyper-parameter settings, other datasets are robust to the choice of model\\r\\nhyper-parameters.\\r\\n19\\nPublished as a conference paper at ICLR 2023\\r\\nL PatchTST/42 supervised PatchTST/42 self-supervised\\r\\nMetric MSE MAE MSE MAE Weather\\r\\n96 0.1525±0.0024 0.2002±0.0023 0.1450±0.0008 0.1937±0.0010\\r\\n192 0.1975±0.0015 0.2434±0.0010 0.1893±0.0003 0.2364±0.0006\\r\\n336 0.2494±0.0012 0.2841±0.0014 0.2413±0.0003 0.2774±0.0005\\r\\n720 0.3194±0.0002 0.3352±0.0003 0.3156±0.0020 0.3316±0.0016\\r\\nTraffic\\r\\n96 0.3669±0.0006 0.2504±0.0007 0.3528±0.0022 0.2443±0.0016\\r\\n192 0.3858±0.0004 0.2586±0.0004 0.3729±0.0013 0.2531±0.0009\\r\\n336 0.3994±0.0010 0.2672±0.0016 0.3846±0.0020 0.2588±0.0011\\r\\n720 0.4383±0.0097 0.2913±0.0104 0.4241±0.0007 0.2816±0.0010\\r\\nElectricity\\r\\n96 0.1304±0.0006 0.2234±0.0006 0.1256±0.0002 0.2210±0.0003\\r\\n192 0.1482±0.0002 0.2403±0.0002 0.1451±0.0002 0.2397±0.0010\\r\\n336 0.1659±0.0006 0.2596±0.0006 0.1624±0.0010 0.2576±0.0009\\r\\n720 0.2019±0.0006 0.2917±0.0006 0.1990±0.0002 0.2916±0.0002\\r\\nETTh1\\r\\n96 0.3752±0.0008 0.3999±0.0004 0.3700±0.0035 0.4001±0.0023\\r\\n192 0.4127±0.0012 0.4207±0.0006 0.4146±0.0012 0.4287±0.0013\\r\\n336 0.4278±0.0033 0.4334±0.0028 0.4285±0.0018 0.4402±0.0017\\r\\n720 0.4462±0.0035 0.4637±0.0027 0.4670±0.0052 0.4768±0.0033\\r\\nETTh2\\r\\n96 0.2749±0.0005 0.3363±0.0006 0.2869±0.0039 0.3439±0.0016\\r\\n192 0.3385±0.0010 0.3789±0.0014 0.3523±0.0048 0.3855±0.0027\\r\\n336 0.3288±0.0010 0.3823±0.0027 0.3779±0.0057 0.4112±0.0030\\r\\n720 0.3784±0.0010 0.4212±0.0009 0.3993±0.0054 0.4385±0.0038\\r\\nETTm1\\r\\n96 0.2893±0.0009 0.3415±0.0007 0.2876±0.0012 0.3427±0.0011\\r\\n192 0.3316±0.0008 0.3695±0.0007 0.3296±0.0026 0.3688±0.0016\\r\\n336 0.3661±0.0022 0.3914±0.0012 0.3583±0.0015 0.3879±0.0016\\r\\n720 0.4200±0.0056 0.4243±0.0033 0.4094±0.0044 0.4193±0.0013\\r\\nETTm2\\r\\n96 0.1647±0.0011 0.2538±0.0010 0.1637±0.0020 0.2537±0.0024\\r\\n192 0.2223±0.0018 0.2936±0.0014 0.2175±0.0011 0.2908±0.0013\\r\\n336 0.2775±0.0020 0.3297±0.0010 0.2706±0.0016 0.3260±0.0016\\r\\n720 0.3648±0.0024 0.3833±0.0010 0.3539±0.0023 0.3799±0.0024\\r\\nTable 14: Multivariate long-term forecasting results with different random seeds in supervised and\\r\\nself-supervised PatchTST/42. The best results are in bold.\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n0.354\\r\\n0.362\\r\\n0.37\\r\\n0.378\\r\\n0.386\\r\\nMSE\\r\\netth1\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n0.27\\r\\n0.28\\r\\n0.29\\r\\n0.3\\r\\netth2\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n0.135\\r\\n0.14\\r\\n0.145\\r\\n0.15\\r\\nweather\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n2.0\\r\\n2.1\\r\\n2.2\\r\\n2.3\\r\\nillness\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n0.27\\r\\n0.28\\r\\n0.29\\r\\nMSE\\r\\nettm1\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n0.15\\r\\n0.155\\r\\n0.16\\r\\n0.165\\r\\n0.17\\r\\nettm2\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n0.37\\r\\n0.38\\r\\ntraffic\\r\\n1 2 3 4 5 6\\r\\nParameter Combination\\r\\n0.115\\r\\n0.12\\r\\n0.125\\r\\n0.13\\r\\nelectricity\\r\\nFigure 5: MSE scores with varying model parameters. Each bar indicates the MSE score of a\\r\\nparameter combination. The combinations (L, D) = (3, 128), (3, 256), (4, 128), (4, 256), (5, 128),\\r\\n(5, 256) are orderly labeled from 1 to 6 in the figure. The model is run with supervised PatchTST/42\\r\\nto forecast 96 steps. For Traffic and Electricity datasets, we reduce the maximum number of epochs\\r\\nto 50 to save computational time.\\r\\nA.7 CHANNEL-INDEPENDENCE ANALYSIS\\r\\nIntuitively, channel-mixing models should outperform the channel-independent ones since they have\\r\\nmore flexibility to explore the cross-channel information, while with channel-independence the cor\\x02relation is indirectly learnt via weight sharing. However, this is contrast to what we have observed.\\r\\nIn Section A.7.1 we will provide an in-depth analysis on why channel-independence has better fore\\x02casting performance than channel mixing, and in Section A.7.2 we show that channel-independence\\r\\nis a general technique that can be used not only for PatchTST but also for the other models.\\r\\n20\\nPublished as a conference paper at ICLR 2023\\r\\nA.7.1 CHANNEL-INDEPENDENCE VS CHANNEL-MIXING\\r\\nWe find 3 key factors that makes channel-independent models more preferable:\\r\\n• Adaptability: Since each time series is passed through the Transformer separately, it generates\\r\\nits own attention maps. That means different series can learn different attention patterns for\\r\\ntheir prediction, as shown in Figure 6. In contrast, with the channel mixing approach, all the\\r\\nseries share the same attention patterns, which may be harmful if the underlying multivariate\\r\\ntime series carries series of different behaviors. Figure 6 reveals an interesting observation that\\r\\nthe prediction of unrelated time series relies on different attention patterns while similar series\\r\\ncan produce similar maps (e.g. series 11, 25, and 81 contain similar patterns while they are\\r\\ndifferent from others). We suspect this adaptability is one of the main reasons why PatchTST\\r\\nperforms much better forecasting than Informer and other channel-mixing models.\\r\\n• Channel-mixing models may need more training data to match the performance of the channel\\x02independent ones. The flexibility of learning cross-channel correlations could be a double\\x02edged sword, because it may need much more data to learn the information from different\\r\\nchannels and different time steps jointly and appropriately, while channel-independent models\\r\\nonly focus on learning information along the time axis. We examine this assumption by exper\\x02iments where we train the models with varying training data size and and the result is shown\\r\\non left panel of Figure 7. It is clear that channel-independent models converges faster against\\r\\nthe size of training data. As what we have observed in the figure and Table 2, the size of those\\r\\nwidely used time series datasets may not be large enough for channel-mixing models to obtain\\r\\nsimilar performances in supervised learning.\\r\\n• Channel-independent models are less likely to overfit data during training. We record the MSE\\r\\nloss on test data and plot on the right panel of Figure 7. Channel-mixing models show over\\x02fitting after a few initial epochs, while channel-independent models continue optimizing the\\r\\nloss with more training epochs. The best trained models are determined by validation data,\\r\\nwhich are approximately the lowest points in the test loss curves. It is clear that the forecasting\\r\\nperformance of channel-independent models are better.\\r\\nFurthermore, we would like to comment on a few additional technical advantages of channel\\x02independence: (1) Possibility of learning spatial correlations across series: Although we haven’t\\r\\nfocused on this research in our paper, the channel-independence design can be naturally extended\\r\\nto learn cross-channel relationships by using methods like graph neural networks (Cao et al., 2020;\\r\\nChen et al., 2021). (2) Multi-task learning where different loss types can be imposed on different\\r\\ntime series where the same underlying Transformer model is shared. (3) More robust to noise: If\\r\\nnoise is dominant in one or several series, this noise will be projected to other series in the embed\\x02ding space if we mix channels. Channel independence can mitigate this problem by only retaining\\r\\nthe noise in these noisy channels. We can further alleviate the noise by introducing smaller weights\\r\\nto the objective losses that associate with noisy channels.\\r\\nA.7.2 PERFORMANCE OF CHANNEL-INDEPENDENCE ON OTHER MODELS\\r\\nTo show that channel-independence is a general technique that can be applied to the other models,\\r\\nwe apply it to Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), and FEDformer (Zhou\\r\\net al., 2022). The results are shown in Table 15. The channel-independent technique can improve the\\r\\nforecasting performance of those models generally. Although they are still not able to outperform\\r\\nPatchTST which is based on vanilla attention mechanism, we believe that more performance boost\\r\\nand computational reduction can be obtained with more advanced attention designs incorporating\\r\\nthe channel-independence architecture.\\r\\n21\\nPublished as a conference paper at ICLR 2023\\r\\nModels PatchTST/42 Informer Informer-CI Autoformer Autoformer-CI FEDformer FEDformer-CI\\r\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather\\r\\n96 0.152 0.199 0.300 0.384 0.174 0.232 0.266 0.336 0.227 0.289 0.217 0.296 0.214 0.278\\r\\n192 0.197 0.243 0.598 0.544 0.214 0.270 0.307 0.367 0.269 0.318 0.276 0.336 0.258 0.322\\r\\n336 0.249 0.283 0.578 0.523 0.266 0.310 0.359 0.395 0.315 0.344 0.339 0.380 0.302 0.336\\r\\n720 0.320 0.335 1.059 0.741 0.327 0.356 0.419 0.428 0.384 0.389 0.403 0.428 0.374 0.369\\r\\nTraffic\\r\\n96 0.367 0.251 0.719 0.391 0.705 0.402 0.613 0.388 - - 0.587 0.366 - -\\r\\n192 0.385 0.259 0.696 0.379 0.720 0.407 0.616 0.382 - - 0.604 0.373 - -\\r\\n336 0.398 0.265 0.777 0.420 0.750 0.421 0.622 0.337 - - 0.621 0.383 - -\\r\\n720 0.434 0.287 0.864 0.472 - - 0.660 0.408 - - 0.626 0.382 - -\\r\\nElectricity\\r\\n96 0.130 0.222 0.274 0.368 0.203 0.299 0.201 0.317 - - 0.193 0.308 - -\\r\\n192 0.148 0.240 0.296 0.386 0.221 0.316 0.222 0.334 - - 0.201 0.315 - -\\r\\n336 0.167 0.261 0.300 0.394 0.241 0.337 0.231 0.338 - - 0.214 0.329 - -\\r\\n720 0.202 0.291 0.373 0.439 0.314 0.391 0.254 0.361 - - 0.246 0.355 - -\\r\\nILI\\r\\n24 1.522 0.814 5.764 1.677 5.514 1.629 3.483 1.287 4.210 1.500 3.228 1.260 3.280 1.264\\r\\n36 1.430 0.834 4.755 1.467 5.515 1.628 3.103 1.148 2.809 1.162 2.679 1.080 2.862 1.126\\r\\n48 1.673 0.854 4.763 1.469 5.263 1.574 2.669 1.085 3.218 1.267 2.622 1.078 2.834 1.150\\r\\n60 1.529 0.862 5.264 1.564 5.330 1.602 2.770 1.125 3.627 1.396 2.857 1.157 3.115 1.240\\r\\nETTh1\\r\\n96 0.375 0.399 0.865 0.713 0.590 0.517 0.449 0.459 0.414 0.421 0.376 0.419 0.387 0.407\\r\\n192 0.414 0.421 1.008 0.792 0.677 0.566 0.500 0.482 0.453 0.448 0.420 0.448 0.439 0.438\\r\\n336 0.431 0.436 1.107 0.809 0.710 0.600 0.521 0.496 0.496 0.468 0.459 0.465 0.479 0.455\\r\\n720 0.449 0.466 1.181 0.865 0.777 0.660 0.514 0.512 0.662 0.568 0.506 0.507 0.485 0.478\\r\\nETTh2\\r\\n96 0.274 0.336 3.755 1.525 0.390 0.410 0.358 0.397 0.337 0.373 0.346 0.388 0.297 0.348\\r\\n192 0.339 0.379 5.602 1.931 0.456 0.463 0.456 0.452 0.409 0.419 0.429 0.439 0.382 0.399\\r\\n336 0.331 0.380 4.721 1.835 0.523 0.503 0.482 0.486 0.432 0.443 0.496 0.487 0.410 0.428\\r\\n720 0.379 0.422 3.647 1.625 0.843 0.661 0.515 0.511 0.443 0.463 0.463 0.474 0.422 0.444\\r\\nETTm1\\r\\n96 0.290 0.342 0.672 0.571 0.383 0.414 0.505 0.475 0.455 0.441 0.379 0.419 0.408 0.413\\r\\n192 0.332 0.369 0.795 0.669 0.420 0.434 0.553 0.496 0.598 0.512 0.426 0.441 0.445 0.432\\r\\n336 0.366 0.392 1.212 0.871 0.465 0.467 0.621 0.537 0.566 0.504 0.445 0.459 0.476 0.452\\r\\n720 0.420 0.424 1.166 0.823 0.529 0.502 0.671 0.561 0.680 0.557 0.543 0.490 0.533 0.481\\r\\nETTm2\\r\\n96 0.165 0.255 0.365 0.453 0.208 0.298 0.255 0.339 0.218 0.308 0.203 0.287 0.198 0.284\\r\\n192 0.220 0.292 0.533 0.563 0.274 0.345 0.281 0.340 0.281 0.339 0.269 0.328 0.259 0.320\\r\\n336 0.278 0.329 1.363 0.887 0.351 0.394 0.339 0.372 0.336 0.370 0.325 0.366 0.315 0.353\\r\\n720 0.367 0.385 3.379 1.338 0.482 0.474 0.433 0.432 0.428 0.418 0.421 0.415 0.412 0.406\\r\\nTable 15: Channel-independence for other models. CI denote channel-independence. Baselines\\r\\nwithout CI are cited from Zeng et al. (2022). The better results between CI and non-CI versions are\\r\\nin bold. PatchTST/42 is placed on the left for easy reference to other CI-based models. ’-’ denotes\\r\\nrunning out of GPU memory even with batch size 1, or exceeding the maximum running time (12\\r\\nhours).\\r\\n22\\nPublished as a conference paper at ICLR 2023\\r\\nFigure 6: Attention maps and the forecasting of a few time series from Electricity dataset run with\\r\\nsupervised PatchTST/64. Attention map is calculated by averaging the attention matrices over all the\\r\\nheads and across all the layers. For each time series, we show the attention map and the prediction\\r\\nin orange. The blue curves are the actual data. The curves before the back lines are the actual input\\r\\ndata. Channel-independence design allow each series to learn its own attention map for forecasting\\r\\nin which the pattern can be more similar for more correlated series and different otherwise.\\r\\n23\\nPublished as a conference paper at ICLR 2023\\r\\nFigure 7: Channel-independence vs channel-mixing on Weather dataset. The base model is\\r\\nPatchTST/42, and the prediction length is 96. We plot the mean values and error bars with 5 differ\\x02ent random seeds: {2019, 2020, 2021, 2022, 2023}. Left Panel: Test loss vs train size. Here, train\\r\\nsize denotes the fraction of the training data that is used to learn the model from scratch. Channel\\x02independence contributes to a quicker convergence as more training data is available. Right Panel:\\r\\nTest loss vs epochs. Here, we use full train data and plot the first 20 epochs. Channel-mixing model\\r\\nquickly overfits the data.\\r\\n24'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
