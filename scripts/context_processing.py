from langchain.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from config import Config

# ğŸ“Œ ì²­í¬ë³„ ì»¨í…ìŠ¤íŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸
CONTEXT_PROMPT = ChatPromptTemplate.from_template(
    """
    You're an expert in document analysis. Provide a concise context (2-3 sentences) for this chunk.

    <document>
    {document}
    </document>

    <chunk>
    {chunk}
    </chunk>
    
    Provide a concise context (2-3 sentences) for this chunk, considering the following guidelines:
    1. Identify the main topic or concept discussed in the chunk.
    2. Mention any relevant information or comparisons from the broader document context.
    3. If applicable, note how this information relates to the overall theme or purpose of the document.
    4. Include any key figures, dates, or percentages that provide important context.
    5. Do not use phrases like "This chunk discusses..." or "The chunk is about...". Instead, directly state that information.
    
    Please give a short succint context to situate this chunk within the overall document for the purpose of summarization.

    Context:
    """.strip()
)
# RecursiveCharacterTextSplitter: ì¬ê·€ì  ë¬¸ì ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°. ë¬¸ì„œë¥¼ ì¼ì •í•œ í¬ê¸°(chunk_size)ë¡œ ë‚˜ëˆ„ë˜, ì²­í¬ ê°„ chunk_overlap ë§Œí¼ ì¤‘ì²©ì„ ìœ ì§€.
# RecursiveCharacterTextSplitterëŠ” ë¬¸ì¥ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì²­í‚¹.

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=Config.Preprocessing.CHUNK_SIZE,
    chunk_overlap=Config.Preprocessing.CHUNK_OVERLAP
)

def create_llm() -> ChatOllama:
    """Ollama LLM ìƒì„±"""
    return ChatOllama(
        model=Config.Preprocessing.LLM, 
        temperature=Config.Model.TEMPERATURE, 
        seed=Config.SEED, 
        keep_alive=-1
    )

def _generate_context(llm: ChatOllama, document: str, chunk: str) -> str:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì²­í¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    messages = CONTEXT_PROMPT.format_messages(document=document, chunk=chunk)
    response = llm.invoke(messages)
    return response.content

# # _generate_context()ë¥¼ í™œìš©í•´ ê° ì²­í¬ì— ë¬¸ë§¥ ì •ë³´ë¥¼ ì¶”ê°€.
def _create_chunks(document: Document) -> list:
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  í›„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"""
    chunks = text_splitter.split_documents([document])

    if not Config.Preprocessing.CONTEXTUALIZE_CHUNKS:
        return chunks

    llm = create_llm()
    contextual_chunks = []

    for chunk in chunks:
        context = _generate_context(llm, document.page_content, chunk.page_content)
        chunk_with_context = f"{context}\n\n{chunk.page_content}"
        contextual_chunks.append(Document(page_content=chunk_with_context, metadata=chunk.metadata))

    return contextual_chunks
